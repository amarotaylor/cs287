{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Dirichlet\n",
    "from torch import LongTensor\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "from torchtext.data.iterator import BPTTIterator, Iterator, BucketIterator\n",
    "from torchtext.data import Batch, Dataset, Field\n",
    "from torch.utils.data import DataLoader\n",
    "from namedtensor import ntorch\n",
    "from namedtensor.text import NamedField\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch):\n",
    "    data = batch.text.transpose(0,1)\n",
    "    X,y = data[:,:-1],data[:,1:]\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(TEXT.vocab) 10001\n"
     ]
    }
   ],
   "source": [
    "TEXT = Field()\n",
    "train, val, test = torchtext.datasets.LanguageModelingDataset.splits(\n",
    "    path=\".\", \n",
    "    train=\"train.txt\", validation=\"valid.txt\", test=\"valid.txt\", text_field=TEXT)\n",
    "\n",
    "# Data distributed with the assignment\n",
    "TEXT.build_vocab(train)\n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlen = 80\n",
    "train_iter, val_iter, test_iter = torchtext.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=24, device='cuda', bptt_len=seqlen, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_iter)\n",
    "batch = next(it) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Size of text batch [max bptt length, batch size] torch.Size([80, 24])\n",
      "Second in batch tensor([   0, 5456,    3, 4982,   97,  956,    7, 6420,    0, 5447,    0,  750,\n",
      "          28,  116,   12,    0, 2278,    0, 4821,   10,   81,    6,   13,    4,\n",
      "           7,   63,   37,   13,    4,   22,   21,   13,    4,    7,   63,   37,\n",
      "          13,    4,   22,   79,  205,  288,   16,   15,   43,   26, 1575,    6,\n",
      "           0, 4982,   10, 1068,  209,    3,    2,   62,  288,   25,   33, 2488,\n",
      "           3, 4982,   10,  142, 4124,    0,   16,    2,  496,  784,    5, 1320,\n",
      "        1273,    6, 4982,    3,    8,  410,   19,    0], device='cuda:0')\n",
      "Converted back to string:  <unk> meridian <eos> ratners group plc a fast-growing <unk> london-based <unk> raised its price for <unk> specialty <unk> weisfield 's inc. to $ N a share or $ N million from $ N a share or $ N million after another concern said it would be prepared to <unk> ratners 's initial offer <eos> the other concern was n't identified <eos> ratners 's chairman gerald <unk> said the deal remains of substantial benefit to ratners <eos> in london at <unk>\n",
      "Converted back to string:  yesterday ratners 's shares were up N pence N cents at N pence $ N <eos> the sweetened offer has acceptances from more than N N of weisfield 's shareholders and it is scheduled for completion by dec. N <eos> the acquisition of <unk> weisfield 's raises ratners 's u.s. presence to N stores <eos> about N N of ratners 's profit already is derived from the u.s. <eos> carnival cruise lines inc. said potential problems with the construction of\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of text batch [max bptt length, batch size]\", batch.text.size(), file=sys.stderr)\n",
    "print(\"Second in batch\", batch.text[:, 2], file=sys.stderr)\n",
    "print(\"Converted back to string: \", \" \".join([TEXT.vocab.itos[i] for i in batch.text[:, 2].data]), file=sys.stderr)\n",
    "batch = next(it)\n",
    "print(\"Converted back to string: \", \" \".join([TEXT.vocab.itos[i] for i in batch.text[:, 2].data]), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the vocabulary with word embeddings\n",
    "url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
    "TEXT.vocab.load_vectors(vectors=Vectors('wiki.simple.vec', url=url)) \n",
    "word2vec = TEXT.vocab.vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = word2vec.shape[1]\n",
    "embed_size = word2vec.shape[1]\n",
    "n_words = word2vec.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSM = nn.LogSoftmax(dim=2)\n",
    "from torch.nn.utils import weight_norm\n",
    "### adapted from https://github.com/locuslab/TCN/ ###\n",
    "class Chomp1d(torch.nn.Module):\n",
    "    '''Ensure causal convolutions by removing right most items'''\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "    \n",
    "    \n",
    "class TC_block(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_out, kernel, stride, dilation, padding, dropout=0.2):\n",
    "        super(TC_block, self).__init__()\n",
    "        self.conv1 = weight_norm(torch.nn.Conv1d(n_in, n_out, kernel,stride=stride,\n",
    "                                             padding=padding, dilation=dilation))\n",
    "      \n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_out, n_out, kernel,stride=stride,\n",
    "                                           padding=padding, dilation=dilation))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "    \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "    \n",
    "        self.block = torch.nn.Sequential(self.conv1,self.chomp1,self.relu1,self.dropout1,\n",
    "                                          self.conv2,self.chomp2,self.relu2,self.dropout2)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "        if n_in != n_out:\n",
    "            self.conv_re = nn.Conv1d(n_in,n_out,kernel_size=1,stride=1,padding=0)\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        # skip connection\n",
    "        if x.shape[1]!=out.shape[1]:\n",
    "            x = self.conv_re(x)\n",
    "        return self.relu(out + x)\n",
    "      \n",
    "      \n",
    "      \n",
    "class TCN(torch.nn.Module):\n",
    "    def __init__(self, n_layers, n_filters, kernel=2, dropout=0.2, embedding_size = 1000, n_words = 10001,\n",
    "                tied=True,embedding = None):  \n",
    "        super(TCN, self).__init__()\n",
    "        blocks = []\n",
    "        self.embedding_size = embedding_size\n",
    "        self.n_words = n_words\n",
    "        if embedding is None:\n",
    "            self.embedding = nn.Embedding(self.n_words,self.embedding_size)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(self.n_words,self.embedding_size)\n",
    "            self.embedding.data = embedding\n",
    "        self.n_filters = [self.embedding_size] + n_filters\n",
    "        \n",
    "                \n",
    "        for i in range(1,n_layers):\n",
    "            dilation = 2 ** i\n",
    "            n_in = self.n_filters[i-1]\n",
    "            n_out = self.n_filters[i]\n",
    "            blocks.append(TC_block(n_in, n_out, kernel, stride=1, dilation=dilation, padding=(kernel-1) * dilation, dropout=dropout))\n",
    "            \n",
    "        self.network = nn.Sequential(*blocks)\n",
    "        self.receptive_field = 1 + 2*(kernel-1)*(2 ** n_layers-1) + 1\n",
    "        self.output_layer = nn.Linear(n_filters[-1], n_words)\n",
    "        self.relu = nn.ReLU()\n",
    "        if tied:\n",
    "            self.output_layer.weight = self.embedding.weight\n",
    "        self.drop = nn.Dropout(0.25)\n",
    "        self.init_weights()\n",
    "    def init_weights(self):\n",
    "        self.embedding.weight.data.normal_(0, 0.01)\n",
    "        self.output_layer.bias.data.fill_(0)\n",
    "        self.output_layer.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.drop(self.embedding(x))\n",
    "        hook = self.network(embed.transpose(1,2)).transpose(1,2)\n",
    "        return self.output_layer(hook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network with 11780201 parameters\n",
      "Receptive field of network is 64\n"
     ]
    }
   ],
   "source": [
    "model = TCN(5, [600,600,600,600], kernel=2, dropout=0.5, embedding_size = 600, n_words = n_words,tied=True)\n",
    "model.cuda()\n",
    "model_parameters = filter(lambda x: x.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print('Network with {} parameters'.format(params))\n",
    "print('Receptive field of network is {}'.format(model.receptive_field))\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode=\"min\",patience=5,min_lr=1e-6,factor=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(e=0):\n",
    "    model.train()\n",
    "    batch_idx = 0\n",
    "    for batch in train_iter:\n",
    "        X,y = get_batch(batch)\n",
    "        prob = model(X)\n",
    "        # skip some chars for loss\n",
    "        skip = int(X.shape[1]/2)\n",
    "        target = y[:, skip:].contiguous()\n",
    "        output = prob[:, skip:,:].contiguous().transpose(1,2)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.35)\n",
    "        optimizer.step()\n",
    "        batch_idx +=1\n",
    "        if np.mod(batch_idx,100) == 0:\n",
    "            batch_size = X.shape[0]\n",
    "            ppl = np.exp(loss.cpu().detach() / (batch_size * (seqlen-skip))) # update\n",
    "            acc = torch.sum(torch.argmax(prob.cpu().detach(),dim=2) == y.cpu().detach()).float() / torch.FloatTensor([batch_size*(seqlen-skip)])\n",
    "            print('Epoch: %d, Batch: %d, loss: %.4f , Train PPL: %.4f, Train Acc: %.4f' % (e, batch_idx, loss.cpu().detach(), ppl, acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(e=0):\n",
    "    model.eval()\n",
    "    batch_idx = 0\n",
    "    ppl = 0\n",
    "    acc = 0\n",
    "    total_loss = 0\n",
    "    for batch in val_iter:\n",
    "        X,y = get_batch(batch)\n",
    "        prob = model(X)\n",
    "        # skip some chars for loss\n",
    "        skip = int(X.shape[1]/2)\n",
    "        target = y[:, skip:].contiguous()\n",
    "        output = prob[:, skip:,:].contiguous().transpose(1,2)\n",
    "        total_loss += criterion(output, target).cpu().detach()\n",
    "        batch_idx +=1\n",
    "        batch_size = X.shape[0]\n",
    "        ppl += np.exp(total_loss.cpu().detach() / (batch_size * (seqlen-skip))) # update\n",
    "        acc += torch.sum(torch.argmax(prob.cpu().detach(),dim=2) == y.cpu().detach()).float() / torch.FloatTensor([batch_size*(seqlen-skip)])\n",
    "    print('Validation --- Epoch: %d, total loss: %.4f , PPL: %.4f, Acc: %.4f' % (e, total_loss.cpu().detach(), ppl/batch_idx, acc/batch_idx))\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 100, loss: 6.7071 , Train PPL: 1.0068, Train Acc: 0.1331\n"
     ]
    }
   ],
   "source": [
    "best_vloss = 1e8\n",
    "for e in range(100):\n",
    "    train_loop(e)\n",
    "    validation_loss = validation_loop(e)\n",
    "    scheduler.step(validation_loss)\n",
    "    print('lr = {}'.format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "    if validation_loss < best_vloss:\n",
    "                with open(\"model_5_layers.pt\", 'wb') as f:\n",
    "                    print('wrote model')\n",
    "                    torch.save(model, f)\n",
    "                best_vloss = validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_saved = torch.load('model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions_for_kaggle(model):\n",
    "    sentences = []\n",
    "    for i, l in enumerate(open(\"/home/amaro/cs287/hw2/input.txt\"), 1):\n",
    "        sentences.append(re.split(' ', l))\n",
    "    tokenized = []\n",
    "    for s in sentences:\n",
    "        t_s = []\n",
    "        for w in s:\n",
    "            if w != '___\\n':\n",
    "                t_s.append(TEXT.vocab.stoi[w])\n",
    "        tokenized.append(t_s)\n",
    "    \n",
    "    predictions = []\n",
    "    for i in tokenized:\n",
    "        X = torch.tensor(i,dtype=torch.long,device='cuda')\n",
    "        pred = torch.squeeze(model(torch.unsqueeze(X,0)))[-1,:]\n",
    "        tokens = torch.argsort(pred,descending=True)[:20]\n",
    "        l_  = [TEXT.vocab.itos[j] for j in tokens]\n",
    "        predictions.append(' '.join(l_))\n",
    "    \n",
    "    out = pd.DataFrame(index=range(1,len(predictions)+1))\n",
    "    out.index.names = ['id']\n",
    "    out['word'] = predictions\n",
    "    out.to_csv('/home/amaro/cs287/hw2/predictions_2_TCN.txt',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_predictions_for_kaggle(model_saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
