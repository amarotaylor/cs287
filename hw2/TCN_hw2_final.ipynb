{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Dirichlet\n",
    "from torch import LongTensor\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "from torchtext.data.iterator import BPTTIterator, Iterator, BucketIterator\n",
    "from torchtext.data import Batch, Dataset, Field\n",
    "from torch.utils.data import DataLoader\n",
    "from namedtensor import ntorch\n",
    "from namedtensor.text import NamedField\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch):\n",
    "    data = batch[0]\n",
    "    X,y = data[:,:-1],data[:,1:]\n",
    "    return X,y\n",
    "\n",
    "def make_predictions_for_kaggle_from_ensemble(models):\n",
    "    sentences = []\n",
    "    for i, l in enumerate(open(\"/home/amaro/cs287/hw2/input.txt\"), 1):\n",
    "        sentences.append(re.split(' ', l))\n",
    "    tokenized = []\n",
    "    for s in sentences:\n",
    "        t_s = []\n",
    "        for w in s:\n",
    "            if w != '___\\n':\n",
    "                t_s.append(TEXT.vocab.stoi[w])\n",
    "        tokenized.append(t_s)\n",
    "    \n",
    "    predictions = []\n",
    "    for i in tokenized:\n",
    "        X = torch.tensor(i,dtype=torch.long,device='cuda')\n",
    "        prob = [m(torch.unsqueeze(X,0)) for m in models]\n",
    "        pred = torch.squeeze(torch.mean(torch.stack(prob,dim=3),dim=3))[-1,:]\n",
    "        tokens = torch.argsort(pred,descending=True)[:20]\n",
    "        l_  = [TEXT.vocab.itos[j] for j in tokens]\n",
    "        predictions.append(' '.join(l_))\n",
    "    \n",
    "    out = pd.DataFrame(index=range(1,len(predictions)+1))\n",
    "    out.index.names = ['id']\n",
    "    out['word'] = predictions\n",
    "    out.to_csv('/home/amaro/cs287/hw2/predictions_ensemble_TCN.txt',sep=',')\n",
    "\n",
    "def make_predictions_for_kaggle(model):\n",
    "    sentences = []\n",
    "    for i, l in enumerate(open(\"/home/amaro/cs287/hw2/input.txt\"), 1):\n",
    "        sentences.append(re.split(' ', l))\n",
    "    tokenized = []\n",
    "    for s in sentences:\n",
    "        t_s = []\n",
    "        for w in s:\n",
    "            if w != '___\\n':\n",
    "                t_s.append(TEXT.vocab.stoi[w])\n",
    "        tokenized.append(t_s)\n",
    "    \n",
    "    predictions = []\n",
    "    for i in tokenized:\n",
    "        X = torch.tensor(i,dtype=torch.long,device='cuda')\n",
    "        pred = torch.squeeze(model(torch.unsqueeze(X,0)))[-1,:]\n",
    "        tokens = torch.argsort(pred,descending=True)[:20]\n",
    "        l_  = [TEXT.vocab.itos[j] for j in tokens]\n",
    "        predictions.append(' '.join(l_))\n",
    "    \n",
    "    out = pd.DataFrame(index=range(1,len(predictions)+1))\n",
    "    out.index.names = ['id']\n",
    "    out['word'] = predictions\n",
    "    out.to_csv('/home/amaro/cs287/hw2/predictions_2_TCN.txt',sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(TEXT.vocab) 10001\n"
     ]
    }
   ],
   "source": [
    "TEXT = Field()\n",
    "train, val, test = torchtext.datasets.LanguageModelingDataset.splits(\n",
    "    path=\".\", \n",
    "    train=\"train.txt\", validation=\"valid.txt\", test=\"valid.txt\", text_field=TEXT)\n",
    "\n",
    "# Data distributed with the assignment\n",
    "TEXT.build_vocab(train)\n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlen = 80\n",
    "train_iter, val_iter, test_iter = torchtext.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=24, device='cuda', bptt_len=seqlen, repeat=False)\n",
    "\n",
    "def make_dataloader(train,shuffle=True):\n",
    "    train_loader = BPTTIterator(train,16,80,device='cuda',repeat=False)\n",
    "    text = []\n",
    "    for i in train_loader:\n",
    "        text.append(i.text)\n",
    "    training_data = torch.cat(text[:-1],dim=1)\n",
    "    train_data = torch.utils.data.TensorDataset(training_data.transpose(1,0))\n",
    "    t_loader=DataLoader(train_data,batch_size=16,shuffle=shuffle)\n",
    "    return t_loader\n",
    "train_iter = make_dataloader(train,shuffle=True)\n",
    "val_iter = make_dataloader(val,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_iter)\n",
    "batch = next(it) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the vocabulary with word embeddings\n",
    "url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
    "TEXT.vocab.load_vectors(vectors=Vectors('wiki.simple.vec', url=url)) \n",
    "word2vec = TEXT.vocab.vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = word2vec.shape[1]\n",
    "embed_size = word2vec.shape[1]\n",
    "n_words = word2vec.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSM = nn.LogSoftmax(dim=2)\n",
    "from torch.nn.utils import weight_norm\n",
    "### adapted from https://github.com/locuslab/TCN/ ###\n",
    "class Chomp1d(torch.nn.Module):\n",
    "    '''Ensure causal convolutions by removing right most items'''\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "    \n",
    "    \n",
    "class TC_block(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_out, kernel, stride, dilation, padding, dropout=0.2):\n",
    "        super(TC_block, self).__init__()\n",
    "        self.conv1 = weight_norm(torch.nn.Conv1d(n_in, n_out, kernel,stride=stride,\n",
    "                                             padding=padding, dilation=dilation))\n",
    "      \n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_out, n_out, kernel,stride=stride,\n",
    "                                           padding=padding, dilation=dilation))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "    \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "    \n",
    "        self.block = torch.nn.Sequential(self.conv1,self.chomp1,self.relu1,self.dropout1,\n",
    "                                          self.conv2,self.chomp2,self.relu2,self.dropout2)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "        if n_in != n_out:\n",
    "            self.conv_re = nn.Conv1d(n_in,n_out,kernel_size=1,stride=1,padding=0)\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        # skip connection\n",
    "        if x.shape[1]!=out.shape[1]:\n",
    "            x = self.conv_re(x)\n",
    "        return self.relu(out + x)\n",
    "      \n",
    "      \n",
    "      \n",
    "class TCN(torch.nn.Module):\n",
    "    def __init__(self, n_layers, n_filters, kernel=2, dropout=0.2, embedding_size = 1000, n_words = 10001,\n",
    "                tied=True,embedding = None):  \n",
    "        super(TCN, self).__init__()\n",
    "        blocks = []\n",
    "        self.embedding_size = embedding_size\n",
    "        self.n_words = n_words\n",
    "        if embedding is None:\n",
    "            self.embedding = nn.Embedding(self.n_words,self.embedding_size)\n",
    "            self.init_embedding = True\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(self.n_words,self.embedding_size)\n",
    "            self.embedding.data = embedding\n",
    "            self.init_embedding = False\n",
    "        self.n_filters = [self.embedding_size] + n_filters\n",
    "        \n",
    "                \n",
    "        for i in range(1,n_layers):\n",
    "            dilation = 2 ** i\n",
    "            n_in = self.n_filters[i-1]\n",
    "            n_out = self.n_filters[i]\n",
    "            blocks.append(TC_block(n_in, n_out, kernel, stride=1, dilation=dilation, padding=(kernel-1) * dilation, dropout=dropout))\n",
    "            \n",
    "        self.network = nn.Sequential(*blocks)\n",
    "        self.receptive_field = 1 + 2*(kernel-1)*(2 ** n_layers-1) + 1\n",
    "        self.output_layer = nn.Linear(n_filters[-1], n_words)\n",
    "        self.relu = nn.ReLU()\n",
    "        if tied:\n",
    "            self.output_layer.weight = self.embedding.weight\n",
    "        self.drop = nn.Dropout(0.25)\n",
    "        self.init_weights()\n",
    "    def init_weights(self):\n",
    "        if self.init_embedding:\n",
    "            self.embedding.weight.data.normal_(0, 0.01)\n",
    "        self.output_layer.bias.data.fill_(0)\n",
    "        self.output_layer.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.drop(self.embedding(x))\n",
    "        hook = self.network(embed.transpose(1,2)).transpose(1,2)\n",
    "        return self.output_layer(hook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(e=0):\n",
    "    model.train()\n",
    "    batch_idx = 0\n",
    "    for batch in train_iter:\n",
    "        X,y = get_batch(batch)\n",
    "        prob = model(X)\n",
    "        # skip some chars for loss\n",
    "        skip = int(X.shape[1]/2)\n",
    "        target = y[:, skip:].contiguous()\n",
    "        output = prob[:, skip:,:].contiguous().transpose(1,2)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.35)\n",
    "        optimizer.step()\n",
    "        batch_idx +=1\n",
    "        if np.mod(batch_idx,100) == 0:\n",
    "            batch_size = X.shape[0]\n",
    "            ppl = np.exp(loss.cpu().detach() / (batch_size * (seqlen-skip))) # update\n",
    "            acc = torch.sum(torch.argmax(prob.cpu().detach(),dim=2) == y.cpu().detach()).float() / torch.FloatTensor([batch_size*(seqlen-skip)])\n",
    "            print('Epoch: %d, Batch: %d, loss: %.4f , Train PPL: %.4f, Train Acc: %.4f' % (e, batch_idx, loss.cpu().detach(), ppl, acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(e=0):\n",
    "    model.eval()\n",
    "    batch_idx = 0\n",
    "    ppl = []\n",
    "    acc = 0\n",
    "    total_loss = 0\n",
    "    for batch in val_iter:\n",
    "        X,y = get_batch(batch)\n",
    "        prob = model(X)\n",
    "        # skip some chars for loss\n",
    "        skip = int(X.shape[1]/2)\n",
    "        target = y[:, skip:].contiguous()\n",
    "        output = prob[:, skip:,:].contiguous().transpose(1,2)\n",
    "        loss = criterion(output, target).cpu().detach()\n",
    "        total_loss += loss\n",
    "        batch_idx +=1\n",
    "        batch_size = X.shape[0]\n",
    "        ppl.append(np.exp(loss / (batch_size * (seqlen-skip)))) # update\n",
    "        acc += torch.sum(torch.argmax(prob.cpu().detach(),dim=2) == y.cpu().detach()).float() / torch.FloatTensor([batch_size*(seqlen-skip)])\n",
    "    print('Validation --- Epoch: %d, total loss: %.4f , PPL: %.4f, Acc: %.4f' % (e, total_loss.cpu().detach(), np.mean(ppl), acc/batch_idx))\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 100, loss: 5.2793 , Train PPL: 1.0081, Train Acc: 0.3354\n",
      "Epoch: 0, Batch: 200, loss: 5.4507 , Train PPL: 1.0083, Train Acc: 0.3034\n",
      "Epoch: 0, Batch: 300, loss: 5.4952 , Train PPL: 1.0084, Train Acc: 0.2927\n",
      "Epoch: 0, Batch: 400, loss: 5.3613 , Train PPL: 1.0082, Train Acc: 0.3338\n",
      "Epoch: 0, Batch: 500, loss: 5.5610 , Train PPL: 1.0085, Train Acc: 0.3186\n",
      "Epoch: 0, Batch: 600, loss: 5.5469 , Train PPL: 1.0085, Train Acc: 0.3308\n",
      "Epoch: 0, Batch: 700, loss: 5.4303 , Train PPL: 1.0083, Train Acc: 0.3155\n",
      "Validation --- Epoch: 0, total loss: 310.3686 , PPL: 1.2884, Acc: 0.3404\n",
      "lr = 4\n",
      "wrote model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type TCN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type TC_block. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Chomp1d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 100, loss: 5.0877 , Train PPL: 1.0078, Train Acc: 0.3186\n",
      "Epoch: 1, Batch: 200, loss: 5.3588 , Train PPL: 1.0082, Train Acc: 0.3308\n",
      "Epoch: 1, Batch: 300, loss: 5.4441 , Train PPL: 1.0083, Train Acc: 0.3018\n",
      "Epoch: 1, Batch: 400, loss: 5.1657 , Train PPL: 1.0079, Train Acc: 0.2988\n",
      "Epoch: 1, Batch: 500, loss: 5.1097 , Train PPL: 1.0078, Train Acc: 0.3796\n",
      "Epoch: 1, Batch: 600, loss: 5.2231 , Train PPL: 1.0080, Train Acc: 0.3232\n",
      "Epoch: 1, Batch: 700, loss: 5.3021 , Train PPL: 1.0081, Train Acc: 0.3506\n",
      "Validation --- Epoch: 1, total loss: 308.7462 , PPL: 1.2865, Acc: 0.3243\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 2, Batch: 100, loss: 5.0947 , Train PPL: 1.0078, Train Acc: 0.3582\n",
      "Epoch: 2, Batch: 200, loss: 5.4302 , Train PPL: 1.0083, Train Acc: 0.2957\n",
      "Epoch: 2, Batch: 300, loss: 5.2537 , Train PPL: 1.0080, Train Acc: 0.3384\n",
      "Epoch: 2, Batch: 400, loss: 5.4700 , Train PPL: 1.0084, Train Acc: 0.3095\n",
      "Epoch: 2, Batch: 500, loss: 5.3653 , Train PPL: 1.0082, Train Acc: 0.2546\n",
      "Epoch: 2, Batch: 600, loss: 5.0826 , Train PPL: 1.0078, Train Acc: 0.3659\n",
      "Epoch: 2, Batch: 700, loss: 5.2738 , Train PPL: 1.0081, Train Acc: 0.3277\n",
      "Validation --- Epoch: 2, total loss: 308.4578 , PPL: 1.2866, Acc: 0.3102\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 3, Batch: 100, loss: 4.8942 , Train PPL: 1.0075, Train Acc: 0.3689\n",
      "Epoch: 3, Batch: 200, loss: 5.2230 , Train PPL: 1.0080, Train Acc: 0.3674\n",
      "Epoch: 3, Batch: 300, loss: 5.1398 , Train PPL: 1.0079, Train Acc: 0.3323\n",
      "Epoch: 3, Batch: 400, loss: 5.0251 , Train PPL: 1.0077, Train Acc: 0.3293\n",
      "Epoch: 3, Batch: 500, loss: 5.5520 , Train PPL: 1.0085, Train Acc: 0.2973\n",
      "Epoch: 3, Batch: 600, loss: 5.3173 , Train PPL: 1.0081, Train Acc: 0.2973\n",
      "Epoch: 3, Batch: 700, loss: 5.3184 , Train PPL: 1.0081, Train Acc: 0.3125\n",
      "Validation --- Epoch: 3, total loss: 306.4845 , PPL: 1.2847, Acc: 0.3377\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 4, Batch: 100, loss: 5.1475 , Train PPL: 1.0079, Train Acc: 0.3674\n",
      "Epoch: 4, Batch: 200, loss: 5.1402 , Train PPL: 1.0079, Train Acc: 0.3613\n",
      "Epoch: 4, Batch: 300, loss: 5.1133 , Train PPL: 1.0078, Train Acc: 0.3171\n",
      "Epoch: 4, Batch: 400, loss: 5.2549 , Train PPL: 1.0080, Train Acc: 0.3064\n",
      "Epoch: 4, Batch: 500, loss: 5.1719 , Train PPL: 1.0079, Train Acc: 0.3247\n",
      "Epoch: 4, Batch: 600, loss: 5.3527 , Train PPL: 1.0082, Train Acc: 0.3171\n",
      "Epoch: 4, Batch: 700, loss: 5.3204 , Train PPL: 1.0081, Train Acc: 0.2957\n",
      "Validation --- Epoch: 4, total loss: 305.8726 , PPL: 1.2840, Acc: 0.3430\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 5, Batch: 100, loss: 5.2109 , Train PPL: 1.0080, Train Acc: 0.3018\n",
      "Epoch: 5, Batch: 200, loss: 4.3878 , Train PPL: 1.0067, Train Acc: 0.4421\n",
      "Epoch: 5, Batch: 300, loss: 5.1270 , Train PPL: 1.0078, Train Acc: 0.3293\n",
      "Epoch: 5, Batch: 400, loss: 4.7440 , Train PPL: 1.0073, Train Acc: 0.3796\n",
      "Epoch: 5, Batch: 500, loss: 5.2862 , Train PPL: 1.0081, Train Acc: 0.3308\n",
      "Epoch: 5, Batch: 600, loss: 4.8739 , Train PPL: 1.0075, Train Acc: 0.3430\n",
      "Epoch: 5, Batch: 700, loss: 5.2862 , Train PPL: 1.0081, Train Acc: 0.3155\n",
      "Validation --- Epoch: 5, total loss: 305.2684 , PPL: 1.2831, Acc: 0.3193\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 6, Batch: 100, loss: 5.3334 , Train PPL: 1.0082, Train Acc: 0.3460\n",
      "Epoch: 6, Batch: 200, loss: 5.0806 , Train PPL: 1.0078, Train Acc: 0.3598\n",
      "Epoch: 6, Batch: 300, loss: 5.1449 , Train PPL: 1.0079, Train Acc: 0.3369\n",
      "Epoch: 6, Batch: 400, loss: 5.0165 , Train PPL: 1.0077, Train Acc: 0.3704\n",
      "Epoch: 6, Batch: 500, loss: 4.9167 , Train PPL: 1.0075, Train Acc: 0.3521\n",
      "Epoch: 6, Batch: 600, loss: 5.1270 , Train PPL: 1.0078, Train Acc: 0.3293\n",
      "Epoch: 6, Batch: 700, loss: 5.1978 , Train PPL: 1.0080, Train Acc: 0.2973\n",
      "Validation --- Epoch: 6, total loss: 303.0859 , PPL: 1.2810, Acc: 0.3481\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 7, Batch: 100, loss: 4.9509 , Train PPL: 1.0076, Train Acc: 0.3216\n",
      "Epoch: 7, Batch: 200, loss: 4.6042 , Train PPL: 1.0070, Train Acc: 0.3887\n",
      "Epoch: 7, Batch: 300, loss: 4.6131 , Train PPL: 1.0071, Train Acc: 0.3780\n",
      "Epoch: 7, Batch: 400, loss: 4.8867 , Train PPL: 1.0075, Train Acc: 0.3399\n",
      "Epoch: 7, Batch: 500, loss: 5.4268 , Train PPL: 1.0083, Train Acc: 0.2942\n",
      "Epoch: 7, Batch: 600, loss: 4.8558 , Train PPL: 1.0074, Train Acc: 0.3933\n",
      "Epoch: 7, Batch: 700, loss: 5.0489 , Train PPL: 1.0077, Train Acc: 0.3720\n",
      "Validation --- Epoch: 7, total loss: 302.8813 , PPL: 1.2809, Acc: 0.3311\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 8, Batch: 100, loss: 4.9292 , Train PPL: 1.0075, Train Acc: 0.3430\n",
      "Epoch: 8, Batch: 200, loss: 4.8279 , Train PPL: 1.0074, Train Acc: 0.3750\n",
      "Epoch: 8, Batch: 300, loss: 4.8649 , Train PPL: 1.0074, Train Acc: 0.3460\n",
      "Epoch: 8, Batch: 400, loss: 4.7105 , Train PPL: 1.0072, Train Acc: 0.3613\n",
      "Epoch: 8, Batch: 500, loss: 4.9270 , Train PPL: 1.0075, Train Acc: 0.3369\n",
      "Epoch: 8, Batch: 600, loss: 4.8701 , Train PPL: 1.0075, Train Acc: 0.3750\n",
      "Epoch: 8, Batch: 700, loss: 4.9802 , Train PPL: 1.0076, Train Acc: 0.3415\n",
      "Validation --- Epoch: 8, total loss: 303.5461 , PPL: 1.2818, Acc: 0.3232\n",
      "lr = 4\n",
      "Epoch: 9, Batch: 100, loss: 4.6654 , Train PPL: 1.0071, Train Acc: 0.3887\n",
      "Epoch: 9, Batch: 200, loss: 4.8766 , Train PPL: 1.0075, Train Acc: 0.3399\n",
      "Epoch: 9, Batch: 300, loss: 4.9581 , Train PPL: 1.0076, Train Acc: 0.3293\n",
      "Epoch: 9, Batch: 400, loss: 5.0528 , Train PPL: 1.0077, Train Acc: 0.3308\n",
      "Epoch: 9, Batch: 500, loss: 4.7700 , Train PPL: 1.0073, Train Acc: 0.3491\n",
      "Epoch: 9, Batch: 600, loss: 5.0188 , Train PPL: 1.0077, Train Acc: 0.3704\n",
      "Epoch: 9, Batch: 700, loss: 5.1159 , Train PPL: 1.0078, Train Acc: 0.3262\n",
      "Validation --- Epoch: 9, total loss: 301.7514 , PPL: 1.2799, Acc: 0.3410\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 10, Batch: 100, loss: 4.7230 , Train PPL: 1.0072, Train Acc: 0.3598\n",
      "Epoch: 10, Batch: 200, loss: 4.8613 , Train PPL: 1.0074, Train Acc: 0.3643\n",
      "Epoch: 10, Batch: 300, loss: 4.5896 , Train PPL: 1.0070, Train Acc: 0.3872\n",
      "Epoch: 10, Batch: 400, loss: 4.8693 , Train PPL: 1.0075, Train Acc: 0.3415\n",
      "Epoch: 10, Batch: 500, loss: 4.9590 , Train PPL: 1.0076, Train Acc: 0.3216\n",
      "Epoch: 10, Batch: 600, loss: 5.1710 , Train PPL: 1.0079, Train Acc: 0.3338\n",
      "Epoch: 10, Batch: 700, loss: 5.0743 , Train PPL: 1.0078, Train Acc: 0.3064\n",
      "Validation --- Epoch: 10, total loss: 302.4480 , PPL: 1.2807, Acc: 0.3300\n",
      "lr = 4\n",
      "Epoch: 11, Batch: 100, loss: 4.8274 , Train PPL: 1.0074, Train Acc: 0.3750\n",
      "Epoch: 11, Batch: 200, loss: 4.7058 , Train PPL: 1.0072, Train Acc: 0.3521\n",
      "Epoch: 11, Batch: 300, loss: 4.4318 , Train PPL: 1.0068, Train Acc: 0.3887\n",
      "Epoch: 11, Batch: 400, loss: 4.6433 , Train PPL: 1.0071, Train Acc: 0.3918\n",
      "Epoch: 11, Batch: 500, loss: 4.6980 , Train PPL: 1.0072, Train Acc: 0.3887\n",
      "Epoch: 11, Batch: 600, loss: 5.0083 , Train PPL: 1.0077, Train Acc: 0.3186\n",
      "Epoch: 11, Batch: 700, loss: 5.1863 , Train PPL: 1.0079, Train Acc: 0.3399\n",
      "Validation --- Epoch: 11, total loss: 302.3285 , PPL: 1.2806, Acc: 0.3371\n",
      "lr = 4\n",
      "Epoch: 12, Batch: 100, loss: 4.6944 , Train PPL: 1.0072, Train Acc: 0.3567\n",
      "Epoch: 12, Batch: 200, loss: 4.4131 , Train PPL: 1.0067, Train Acc: 0.3780\n",
      "Epoch: 12, Batch: 300, loss: 4.4349 , Train PPL: 1.0068, Train Acc: 0.3963\n",
      "Epoch: 12, Batch: 400, loss: 4.6171 , Train PPL: 1.0071, Train Acc: 0.4024\n",
      "Epoch: 12, Batch: 500, loss: 4.9721 , Train PPL: 1.0076, Train Acc: 0.3338\n",
      "Epoch: 12, Batch: 600, loss: 4.8868 , Train PPL: 1.0075, Train Acc: 0.3613\n",
      "Epoch: 12, Batch: 700, loss: 4.7669 , Train PPL: 1.0073, Train Acc: 0.3643\n",
      "Validation --- Epoch: 12, total loss: 301.1018 , PPL: 1.2794, Acc: 0.3460\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 13, Batch: 100, loss: 4.7090 , Train PPL: 1.0072, Train Acc: 0.3399\n",
      "Epoch: 13, Batch: 200, loss: 4.7696 , Train PPL: 1.0073, Train Acc: 0.3811\n",
      "Epoch: 13, Batch: 300, loss: 4.8159 , Train PPL: 1.0074, Train Acc: 0.3720\n",
      "Epoch: 13, Batch: 400, loss: 4.7831 , Train PPL: 1.0073, Train Acc: 0.3491\n",
      "Epoch: 13, Batch: 500, loss: 4.8274 , Train PPL: 1.0074, Train Acc: 0.3506\n",
      "Epoch: 13, Batch: 600, loss: 4.6044 , Train PPL: 1.0070, Train Acc: 0.3780\n",
      "Epoch: 13, Batch: 700, loss: 4.6014 , Train PPL: 1.0070, Train Acc: 0.4009\n",
      "Validation --- Epoch: 13, total loss: 301.1453 , PPL: 1.2794, Acc: 0.3451\n",
      "lr = 4\n",
      "Epoch: 14, Batch: 100, loss: 4.5253 , Train PPL: 1.0069, Train Acc: 0.3765\n",
      "Epoch: 14, Batch: 200, loss: 4.7836 , Train PPL: 1.0073, Train Acc: 0.3659\n",
      "Epoch: 14, Batch: 300, loss: 4.7466 , Train PPL: 1.0073, Train Acc: 0.3476\n",
      "Epoch: 14, Batch: 400, loss: 4.7125 , Train PPL: 1.0072, Train Acc: 0.3567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Batch: 500, loss: 4.9416 , Train PPL: 1.0076, Train Acc: 0.2881\n",
      "Epoch: 14, Batch: 600, loss: 4.7736 , Train PPL: 1.0073, Train Acc: 0.3643\n",
      "Epoch: 14, Batch: 700, loss: 4.3876 , Train PPL: 1.0067, Train Acc: 0.3963\n",
      "Validation --- Epoch: 14, total loss: 301.5753 , PPL: 1.2798, Acc: 0.3118\n",
      "lr = 4\n",
      "Epoch: 15, Batch: 100, loss: 4.6768 , Train PPL: 1.0072, Train Acc: 0.3689\n",
      "Epoch: 15, Batch: 200, loss: 4.7835 , Train PPL: 1.0073, Train Acc: 0.3537\n",
      "Epoch: 15, Batch: 300, loss: 4.7732 , Train PPL: 1.0073, Train Acc: 0.3506\n",
      "Epoch: 15, Batch: 400, loss: 4.8680 , Train PPL: 1.0074, Train Acc: 0.3552\n",
      "Epoch: 15, Batch: 500, loss: 4.7886 , Train PPL: 1.0073, Train Acc: 0.3491\n",
      "Epoch: 15, Batch: 600, loss: 4.9769 , Train PPL: 1.0076, Train Acc: 0.3369\n",
      "Epoch: 15, Batch: 700, loss: 4.6148 , Train PPL: 1.0071, Train Acc: 0.3567\n",
      "Validation --- Epoch: 15, total loss: 301.1738 , PPL: 1.2793, Acc: 0.3346\n",
      "lr = 4\n",
      "Epoch: 16, Batch: 100, loss: 4.5137 , Train PPL: 1.0069, Train Acc: 0.3933\n",
      "Epoch: 16, Batch: 200, loss: 4.7701 , Train PPL: 1.0073, Train Acc: 0.3140\n",
      "Epoch: 16, Batch: 300, loss: 4.2987 , Train PPL: 1.0066, Train Acc: 0.4177\n",
      "Epoch: 16, Batch: 400, loss: 4.8548 , Train PPL: 1.0074, Train Acc: 0.3354\n",
      "Epoch: 16, Batch: 500, loss: 4.8054 , Train PPL: 1.0074, Train Acc: 0.3933\n",
      "Epoch: 16, Batch: 600, loss: 4.8116 , Train PPL: 1.0074, Train Acc: 0.3338\n",
      "Epoch: 16, Batch: 700, loss: 4.6225 , Train PPL: 1.0071, Train Acc: 0.3628\n",
      "Validation --- Epoch: 16, total loss: 299.8203 , PPL: 1.2781, Acc: 0.3404\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 17, Batch: 100, loss: 4.9326 , Train PPL: 1.0075, Train Acc: 0.3354\n",
      "Epoch: 17, Batch: 200, loss: 4.6899 , Train PPL: 1.0072, Train Acc: 0.3445\n",
      "Epoch: 17, Batch: 300, loss: 4.5536 , Train PPL: 1.0070, Train Acc: 0.4192\n",
      "Epoch: 17, Batch: 400, loss: 4.9234 , Train PPL: 1.0075, Train Acc: 0.3537\n",
      "Epoch: 17, Batch: 500, loss: 4.7385 , Train PPL: 1.0072, Train Acc: 0.3720\n",
      "Epoch: 17, Batch: 600, loss: 4.6149 , Train PPL: 1.0071, Train Acc: 0.3948\n",
      "Epoch: 17, Batch: 700, loss: 4.7175 , Train PPL: 1.0072, Train Acc: 0.3872\n",
      "Validation --- Epoch: 17, total loss: 300.7748 , PPL: 1.2791, Acc: 0.3357\n",
      "lr = 4\n",
      "Epoch: 18, Batch: 100, loss: 4.6662 , Train PPL: 1.0071, Train Acc: 0.3582\n",
      "Epoch: 18, Batch: 200, loss: 4.7463 , Train PPL: 1.0073, Train Acc: 0.3689\n",
      "Epoch: 18, Batch: 300, loss: 4.8047 , Train PPL: 1.0074, Train Acc: 0.3369\n",
      "Epoch: 18, Batch: 400, loss: 4.7889 , Train PPL: 1.0073, Train Acc: 0.3491\n",
      "Epoch: 18, Batch: 500, loss: 4.9190 , Train PPL: 1.0075, Train Acc: 0.2942\n",
      "Epoch: 18, Batch: 600, loss: 4.6304 , Train PPL: 1.0071, Train Acc: 0.3460\n",
      "Epoch: 18, Batch: 700, loss: 4.7825 , Train PPL: 1.0073, Train Acc: 0.3232\n",
      "Validation --- Epoch: 18, total loss: 301.4350 , PPL: 1.2799, Acc: 0.3331\n",
      "lr = 4\n",
      "Epoch: 19, Batch: 100, loss: 4.5060 , Train PPL: 1.0069, Train Acc: 0.3598\n",
      "Epoch: 19, Batch: 200, loss: 4.7476 , Train PPL: 1.0073, Train Acc: 0.3308\n",
      "Epoch: 19, Batch: 300, loss: 4.7648 , Train PPL: 1.0073, Train Acc: 0.3643\n",
      "Epoch: 19, Batch: 400, loss: 4.8855 , Train PPL: 1.0075, Train Acc: 0.3415\n",
      "Epoch: 19, Batch: 500, loss: 4.6057 , Train PPL: 1.0070, Train Acc: 0.4116\n",
      "Epoch: 19, Batch: 600, loss: 4.5996 , Train PPL: 1.0070, Train Acc: 0.3750\n",
      "Epoch: 19, Batch: 700, loss: 4.7813 , Train PPL: 1.0073, Train Acc: 0.3567\n",
      "Validation --- Epoch: 19, total loss: 300.7335 , PPL: 1.2790, Acc: 0.3509\n",
      "lr = 4\n",
      "Epoch: 20, Batch: 100, loss: 4.6497 , Train PPL: 1.0071, Train Acc: 0.3948\n",
      "Epoch: 20, Batch: 200, loss: 4.6993 , Train PPL: 1.0072, Train Acc: 0.3582\n",
      "Epoch: 20, Batch: 300, loss: 4.7474 , Train PPL: 1.0073, Train Acc: 0.3628\n",
      "Epoch: 20, Batch: 400, loss: 4.6451 , Train PPL: 1.0071, Train Acc: 0.3750\n",
      "Epoch: 20, Batch: 500, loss: 4.8356 , Train PPL: 1.0074, Train Acc: 0.3064\n",
      "Epoch: 20, Batch: 600, loss: 4.7726 , Train PPL: 1.0073, Train Acc: 0.3369\n",
      "Epoch: 20, Batch: 700, loss: 4.8761 , Train PPL: 1.0075, Train Acc: 0.3369\n",
      "Validation --- Epoch: 20, total loss: 300.7013 , PPL: 1.2789, Acc: 0.3238\n",
      "lr = 4\n",
      "Epoch: 21, Batch: 100, loss: 4.5923 , Train PPL: 1.0070, Train Acc: 0.3323\n",
      "Epoch: 21, Batch: 200, loss: 4.5462 , Train PPL: 1.0070, Train Acc: 0.3720\n",
      "Epoch: 21, Batch: 300, loss: 4.2320 , Train PPL: 1.0065, Train Acc: 0.4543\n",
      "Epoch: 21, Batch: 400, loss: 4.5724 , Train PPL: 1.0070, Train Acc: 0.3750\n",
      "Epoch: 21, Batch: 500, loss: 4.8686 , Train PPL: 1.0074, Train Acc: 0.3293\n",
      "Epoch: 21, Batch: 600, loss: 4.8775 , Train PPL: 1.0075, Train Acc: 0.3460\n",
      "Epoch: 21, Batch: 700, loss: 4.8365 , Train PPL: 1.0074, Train Acc: 0.3277\n",
      "Validation --- Epoch: 21, total loss: 300.6589 , PPL: 1.2790, Acc: 0.3456\n",
      "lr = 4\n",
      "Epoch: 22, Batch: 100, loss: 4.4725 , Train PPL: 1.0068, Train Acc: 0.3430\n",
      "Epoch: 22, Batch: 200, loss: 4.5854 , Train PPL: 1.0070, Train Acc: 0.3628\n",
      "Epoch: 22, Batch: 300, loss: 4.5487 , Train PPL: 1.0070, Train Acc: 0.3841\n",
      "Epoch: 22, Batch: 400, loss: 4.6208 , Train PPL: 1.0071, Train Acc: 0.3399\n",
      "Epoch: 22, Batch: 500, loss: 4.7465 , Train PPL: 1.0073, Train Acc: 0.3186\n",
      "Epoch: 22, Batch: 600, loss: 4.7543 , Train PPL: 1.0073, Train Acc: 0.3765\n",
      "Epoch: 22, Batch: 700, loss: 4.7080 , Train PPL: 1.0072, Train Acc: 0.3354\n",
      "Validation --- Epoch: 22, total loss: 300.8201 , PPL: 1.2792, Acc: 0.3300\n",
      "lr = 2.0\n",
      "Epoch: 23, Batch: 100, loss: 4.7678 , Train PPL: 1.0073, Train Acc: 0.3369\n",
      "Epoch: 23, Batch: 200, loss: 4.4681 , Train PPL: 1.0068, Train Acc: 0.3613\n",
      "Epoch: 23, Batch: 300, loss: 4.1502 , Train PPL: 1.0063, Train Acc: 0.4360\n",
      "Epoch: 23, Batch: 400, loss: 4.4728 , Train PPL: 1.0068, Train Acc: 0.3826\n",
      "Epoch: 23, Batch: 500, loss: 4.6635 , Train PPL: 1.0071, Train Acc: 0.3887\n",
      "Epoch: 23, Batch: 600, loss: 4.1817 , Train PPL: 1.0064, Train Acc: 0.4421\n",
      "Epoch: 23, Batch: 700, loss: 4.4335 , Train PPL: 1.0068, Train Acc: 0.3994\n",
      "Validation --- Epoch: 23, total loss: 296.9352 , PPL: 1.2751, Acc: 0.3794\n",
      "lr = 2.0\n",
      "wrote model\n",
      "Epoch: 24, Batch: 100, loss: 4.4744 , Train PPL: 1.0068, Train Acc: 0.3811\n",
      "Epoch: 24, Batch: 200, loss: 4.4608 , Train PPL: 1.0068, Train Acc: 0.4299\n",
      "Epoch: 24, Batch: 300, loss: 4.1583 , Train PPL: 1.0064, Train Acc: 0.4405\n",
      "Epoch: 24, Batch: 400, loss: 4.7449 , Train PPL: 1.0073, Train Acc: 0.3399\n",
      "Epoch: 24, Batch: 500, loss: 4.1624 , Train PPL: 1.0064, Train Acc: 0.4573\n",
      "Epoch: 24, Batch: 600, loss: 4.5207 , Train PPL: 1.0069, Train Acc: 0.3521\n",
      "Epoch: 24, Batch: 700, loss: 4.4492 , Train PPL: 1.0068, Train Acc: 0.3689\n",
      "Validation --- Epoch: 24, total loss: 296.6084 , PPL: 1.2748, Acc: 0.3806\n",
      "lr = 2.0\n",
      "wrote model\n",
      "Epoch: 25, Batch: 100, loss: 4.3135 , Train PPL: 1.0066, Train Acc: 0.3796\n",
      "Epoch: 25, Batch: 200, loss: 4.6479 , Train PPL: 1.0071, Train Acc: 0.3521\n",
      "Epoch: 25, Batch: 300, loss: 4.4367 , Train PPL: 1.0068, Train Acc: 0.3796\n",
      "Epoch: 25, Batch: 400, loss: 4.6506 , Train PPL: 1.0071, Train Acc: 0.3704\n",
      "Epoch: 25, Batch: 500, loss: 4.5370 , Train PPL: 1.0069, Train Acc: 0.3765\n",
      "Epoch: 25, Batch: 600, loss: 4.5751 , Train PPL: 1.0070, Train Acc: 0.3552\n",
      "Epoch: 25, Batch: 700, loss: 4.4908 , Train PPL: 1.0069, Train Acc: 0.3643\n",
      "Validation --- Epoch: 25, total loss: 296.8671 , PPL: 1.2751, Acc: 0.3781\n",
      "lr = 2.0\n",
      "Epoch: 26, Batch: 100, loss: 4.1754 , Train PPL: 1.0064, Train Acc: 0.4253\n",
      "Epoch: 26, Batch: 200, loss: 4.6174 , Train PPL: 1.0071, Train Acc: 0.3293\n",
      "Epoch: 26, Batch: 300, loss: 4.7277 , Train PPL: 1.0072, Train Acc: 0.3232\n",
      "Epoch: 26, Batch: 400, loss: 4.3553 , Train PPL: 1.0067, Train Acc: 0.3841\n",
      "Epoch: 26, Batch: 500, loss: 4.4906 , Train PPL: 1.0069, Train Acc: 0.3796\n",
      "Epoch: 26, Batch: 600, loss: 4.3218 , Train PPL: 1.0066, Train Acc: 0.3765\n",
      "Epoch: 26, Batch: 700, loss: 4.7642 , Train PPL: 1.0073, Train Acc: 0.3445\n",
      "Validation --- Epoch: 26, total loss: 296.5060 , PPL: 1.2747, Acc: 0.3805\n",
      "lr = 2.0\n",
      "wrote model\n",
      "Epoch: 27, Batch: 100, loss: 4.2611 , Train PPL: 1.0065, Train Acc: 0.4329\n",
      "Epoch: 27, Batch: 200, loss: 4.2839 , Train PPL: 1.0066, Train Acc: 0.3796\n",
      "Epoch: 27, Batch: 300, loss: 4.3741 , Train PPL: 1.0067, Train Acc: 0.3277\n",
      "Epoch: 27, Batch: 400, loss: 4.2105 , Train PPL: 1.0064, Train Acc: 0.4573\n",
      "Epoch: 27, Batch: 500, loss: 4.5154 , Train PPL: 1.0069, Train Acc: 0.3613\n",
      "Epoch: 27, Batch: 600, loss: 4.3443 , Train PPL: 1.0066, Train Acc: 0.3765\n",
      "Epoch: 27, Batch: 700, loss: 4.6081 , Train PPL: 1.0070, Train Acc: 0.3689\n",
      "Validation --- Epoch: 27, total loss: 297.0500 , PPL: 1.2753, Acc: 0.3831\n",
      "lr = 2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Batch: 100, loss: 4.3900 , Train PPL: 1.0067, Train Acc: 0.3552\n",
      "Epoch: 28, Batch: 200, loss: 4.1190 , Train PPL: 1.0063, Train Acc: 0.4375\n",
      "Epoch: 28, Batch: 300, loss: 4.4324 , Train PPL: 1.0068, Train Acc: 0.3659\n",
      "Epoch: 28, Batch: 400, loss: 4.6545 , Train PPL: 1.0071, Train Acc: 0.3643\n",
      "Epoch: 28, Batch: 500, loss: 4.3366 , Train PPL: 1.0066, Train Acc: 0.3674\n",
      "Epoch: 28, Batch: 600, loss: 4.5471 , Train PPL: 1.0070, Train Acc: 0.4253\n",
      "Epoch: 28, Batch: 700, loss: 4.3044 , Train PPL: 1.0066, Train Acc: 0.3902\n",
      "Validation --- Epoch: 28, total loss: 297.1898 , PPL: 1.2755, Acc: 0.3811\n",
      "lr = 2.0\n",
      "Epoch: 29, Batch: 100, loss: 3.9129 , Train PPL: 1.0060, Train Acc: 0.4543\n",
      "Epoch: 29, Batch: 200, loss: 4.2277 , Train PPL: 1.0065, Train Acc: 0.4207\n",
      "Epoch: 29, Batch: 300, loss: 4.5261 , Train PPL: 1.0069, Train Acc: 0.3476\n",
      "Epoch: 29, Batch: 400, loss: 4.1366 , Train PPL: 1.0063, Train Acc: 0.4451\n",
      "Epoch: 29, Batch: 500, loss: 4.4972 , Train PPL: 1.0069, Train Acc: 0.3476\n",
      "Epoch: 29, Batch: 600, loss: 4.1435 , Train PPL: 1.0063, Train Acc: 0.4451\n",
      "Epoch: 29, Batch: 700, loss: 4.2828 , Train PPL: 1.0066, Train Acc: 0.4299\n",
      "Validation --- Epoch: 29, total loss: 296.8600 , PPL: 1.2751, Acc: 0.3794\n",
      "lr = 2.0\n",
      "Epoch: 30, Batch: 100, loss: 4.2549 , Train PPL: 1.0065, Train Acc: 0.4101\n",
      "Epoch: 30, Batch: 200, loss: 4.0786 , Train PPL: 1.0062, Train Acc: 0.5137\n",
      "Epoch: 30, Batch: 300, loss: 4.1747 , Train PPL: 1.0064, Train Acc: 0.3887\n",
      "Epoch: 30, Batch: 400, loss: 4.3076 , Train PPL: 1.0066, Train Acc: 0.3796\n",
      "Epoch: 30, Batch: 500, loss: 4.2899 , Train PPL: 1.0066, Train Acc: 0.3704\n",
      "Epoch: 30, Batch: 600, loss: 4.5224 , Train PPL: 1.0069, Train Acc: 0.3857\n",
      "Epoch: 30, Batch: 700, loss: 4.3779 , Train PPL: 1.0067, Train Acc: 0.3720\n",
      "Validation --- Epoch: 30, total loss: 296.9875 , PPL: 1.2754, Acc: 0.3840\n",
      "lr = 2.0\n",
      "Epoch: 31, Batch: 100, loss: 4.1559 , Train PPL: 1.0064, Train Acc: 0.4162\n",
      "Epoch: 31, Batch: 200, loss: 4.4464 , Train PPL: 1.0068, Train Acc: 0.3338\n",
      "Epoch: 31, Batch: 300, loss: 4.2238 , Train PPL: 1.0065, Train Acc: 0.4131\n",
      "Epoch: 31, Batch: 400, loss: 4.1926 , Train PPL: 1.0064, Train Acc: 0.4085\n",
      "Epoch: 31, Batch: 500, loss: 4.2899 , Train PPL: 1.0066, Train Acc: 0.4177\n",
      "Epoch: 31, Batch: 600, loss: 4.1062 , Train PPL: 1.0063, Train Acc: 0.4405\n",
      "Epoch: 31, Batch: 700, loss: 4.3951 , Train PPL: 1.0067, Train Acc: 0.4146\n",
      "Validation --- Epoch: 31, total loss: 297.0499 , PPL: 1.2755, Acc: 0.3828\n",
      "lr = 2.0\n",
      "Epoch: 32, Batch: 100, loss: 4.1306 , Train PPL: 1.0063, Train Acc: 0.3826\n",
      "Epoch: 32, Batch: 200, loss: 4.4784 , Train PPL: 1.0069, Train Acc: 0.3918\n",
      "Epoch: 32, Batch: 300, loss: 4.2732 , Train PPL: 1.0065, Train Acc: 0.3887\n",
      "Epoch: 32, Batch: 400, loss: 4.3540 , Train PPL: 1.0067, Train Acc: 0.4070\n",
      "Epoch: 32, Batch: 500, loss: 4.3868 , Train PPL: 1.0067, Train Acc: 0.3963\n",
      "Epoch: 32, Batch: 600, loss: 4.2360 , Train PPL: 1.0065, Train Acc: 0.4040\n",
      "Epoch: 32, Batch: 700, loss: 4.4343 , Train PPL: 1.0068, Train Acc: 0.3765\n",
      "Validation --- Epoch: 32, total loss: 297.0670 , PPL: 1.2754, Acc: 0.3835\n",
      "lr = 1.0\n",
      "Epoch: 33, Batch: 100, loss: 3.9391 , Train PPL: 1.0060, Train Acc: 0.4619\n",
      "Epoch: 33, Batch: 200, loss: 4.4409 , Train PPL: 1.0068, Train Acc: 0.3567\n",
      "Epoch: 33, Batch: 300, loss: 4.2335 , Train PPL: 1.0065, Train Acc: 0.4146\n",
      "Epoch: 33, Batch: 400, loss: 3.9146 , Train PPL: 1.0060, Train Acc: 0.4665\n",
      "Epoch: 33, Batch: 500, loss: 4.4271 , Train PPL: 1.0068, Train Acc: 0.3598\n",
      "Epoch: 33, Batch: 600, loss: 4.3156 , Train PPL: 1.0066, Train Acc: 0.4070\n",
      "Epoch: 33, Batch: 700, loss: 4.1939 , Train PPL: 1.0064, Train Acc: 0.4177\n",
      "Validation --- Epoch: 33, total loss: 296.3570 , PPL: 1.2747, Acc: 0.3909\n",
      "lr = 1.0\n",
      "wrote model\n",
      "Epoch: 34, Batch: 100, loss: 4.4300 , Train PPL: 1.0068, Train Acc: 0.3841\n",
      "Epoch: 34, Batch: 200, loss: 4.4892 , Train PPL: 1.0069, Train Acc: 0.3704\n",
      "Epoch: 34, Batch: 300, loss: 3.9768 , Train PPL: 1.0061, Train Acc: 0.4558\n",
      "Epoch: 34, Batch: 400, loss: 4.3009 , Train PPL: 1.0066, Train Acc: 0.3704\n",
      "Epoch: 34, Batch: 500, loss: 4.3858 , Train PPL: 1.0067, Train Acc: 0.3521\n",
      "Epoch: 34, Batch: 600, loss: 4.5179 , Train PPL: 1.0069, Train Acc: 0.3643\n",
      "Epoch: 34, Batch: 700, loss: 4.3460 , Train PPL: 1.0066, Train Acc: 0.3780\n",
      "Validation --- Epoch: 34, total loss: 296.4420 , PPL: 1.2748, Acc: 0.3906\n",
      "lr = 1.0\n",
      "Epoch: 35, Batch: 100, loss: 4.0638 , Train PPL: 1.0062, Train Acc: 0.4482\n",
      "Epoch: 35, Batch: 200, loss: 3.8940 , Train PPL: 1.0060, Train Acc: 0.5152\n",
      "Epoch: 35, Batch: 300, loss: 3.9993 , Train PPL: 1.0061, Train Acc: 0.4573\n",
      "Epoch: 35, Batch: 400, loss: 4.0296 , Train PPL: 1.0062, Train Acc: 0.4314\n",
      "Epoch: 35, Batch: 500, loss: 4.4060 , Train PPL: 1.0067, Train Acc: 0.3948\n",
      "Epoch: 35, Batch: 600, loss: 3.8897 , Train PPL: 1.0059, Train Acc: 0.4527\n",
      "Epoch: 35, Batch: 700, loss: 4.3382 , Train PPL: 1.0066, Train Acc: 0.3765\n",
      "Validation --- Epoch: 35, total loss: 296.4248 , PPL: 1.2748, Acc: 0.3919\n",
      "lr = 1.0\n",
      "Epoch: 36, Batch: 100, loss: 4.2390 , Train PPL: 1.0065, Train Acc: 0.4024\n",
      "Epoch: 36, Batch: 200, loss: 4.2710 , Train PPL: 1.0065, Train Acc: 0.3415\n",
      "Epoch: 36, Batch: 300, loss: 4.2821 , Train PPL: 1.0065, Train Acc: 0.3887\n",
      "Epoch: 36, Batch: 400, loss: 4.2063 , Train PPL: 1.0064, Train Acc: 0.4070\n",
      "Epoch: 36, Batch: 500, loss: 4.2906 , Train PPL: 1.0066, Train Acc: 0.4070\n",
      "Epoch: 36, Batch: 600, loss: 4.2884 , Train PPL: 1.0066, Train Acc: 0.3918\n",
      "Epoch: 36, Batch: 700, loss: 4.3734 , Train PPL: 1.0067, Train Acc: 0.3720\n",
      "Validation --- Epoch: 36, total loss: 297.1409 , PPL: 1.2755, Acc: 0.3889\n",
      "lr = 1.0\n",
      "Epoch: 37, Batch: 100, loss: 4.4567 , Train PPL: 1.0068, Train Acc: 0.3720\n",
      "Epoch: 37, Batch: 200, loss: 4.4967 , Train PPL: 1.0069, Train Acc: 0.3841\n",
      "Epoch: 37, Batch: 300, loss: 4.4152 , Train PPL: 1.0068, Train Acc: 0.3460\n",
      "Epoch: 37, Batch: 400, loss: 4.3384 , Train PPL: 1.0066, Train Acc: 0.3902\n",
      "Epoch: 37, Batch: 500, loss: 4.0667 , Train PPL: 1.0062, Train Acc: 0.4116\n",
      "Epoch: 37, Batch: 600, loss: 4.1115 , Train PPL: 1.0063, Train Acc: 0.4787\n",
      "Epoch: 37, Batch: 700, loss: 4.2378 , Train PPL: 1.0065, Train Acc: 0.3674\n",
      "Validation --- Epoch: 37, total loss: 296.7387 , PPL: 1.2751, Acc: 0.3910\n",
      "lr = 1.0\n",
      "Epoch: 38, Batch: 100, loss: 4.3506 , Train PPL: 1.0067, Train Acc: 0.3704\n",
      "Epoch: 38, Batch: 200, loss: 4.0507 , Train PPL: 1.0062, Train Acc: 0.4253\n",
      "Epoch: 38, Batch: 300, loss: 4.3673 , Train PPL: 1.0067, Train Acc: 0.3933\n",
      "Epoch: 38, Batch: 400, loss: 4.4676 , Train PPL: 1.0068, Train Acc: 0.4024\n",
      "Epoch: 38, Batch: 500, loss: 4.0030 , Train PPL: 1.0061, Train Acc: 0.4070\n",
      "Epoch: 38, Batch: 600, loss: 4.4395 , Train PPL: 1.0068, Train Acc: 0.3430\n",
      "Epoch: 38, Batch: 700, loss: 4.6267 , Train PPL: 1.0071, Train Acc: 0.3415\n",
      "Validation --- Epoch: 38, total loss: 296.7275 , PPL: 1.2751, Acc: 0.3930\n",
      "lr = 1.0\n",
      "Epoch: 39, Batch: 100, loss: 4.1046 , Train PPL: 1.0063, Train Acc: 0.4360\n",
      "Epoch: 39, Batch: 200, loss: 4.3168 , Train PPL: 1.0066, Train Acc: 0.3994\n",
      "Epoch: 39, Batch: 300, loss: 4.3061 , Train PPL: 1.0066, Train Acc: 0.3796\n",
      "Epoch: 39, Batch: 400, loss: 3.9015 , Train PPL: 1.0060, Train Acc: 0.4558\n",
      "Epoch: 39, Batch: 500, loss: 4.3228 , Train PPL: 1.0066, Train Acc: 0.3918\n",
      "Epoch: 39, Batch: 600, loss: 4.3712 , Train PPL: 1.0067, Train Acc: 0.3537\n",
      "Epoch: 39, Batch: 700, loss: 4.1855 , Train PPL: 1.0064, Train Acc: 0.4329\n",
      "Validation --- Epoch: 39, total loss: 297.0526 , PPL: 1.2755, Acc: 0.3933\n",
      "lr = 0.5\n",
      "Epoch: 40, Batch: 100, loss: 4.1733 , Train PPL: 1.0064, Train Acc: 0.3902\n",
      "Epoch: 40, Batch: 200, loss: 4.1707 , Train PPL: 1.0064, Train Acc: 0.4009\n",
      "Epoch: 40, Batch: 300, loss: 4.1158 , Train PPL: 1.0063, Train Acc: 0.4421\n",
      "Epoch: 40, Batch: 400, loss: 4.0989 , Train PPL: 1.0063, Train Acc: 0.4085\n",
      "Epoch: 40, Batch: 500, loss: 4.4142 , Train PPL: 1.0068, Train Acc: 0.3293\n",
      "Epoch: 40, Batch: 600, loss: 4.4412 , Train PPL: 1.0068, Train Acc: 0.4085\n",
      "Epoch: 40, Batch: 700, loss: 4.3172 , Train PPL: 1.0066, Train Acc: 0.3750\n",
      "Validation --- Epoch: 40, total loss: 296.7234 , PPL: 1.2751, Acc: 0.3953\n",
      "lr = 0.5\n",
      "Epoch: 41, Batch: 100, loss: 3.9792 , Train PPL: 1.0061, Train Acc: 0.4390\n",
      "Epoch: 41, Batch: 200, loss: 4.2968 , Train PPL: 1.0066, Train Acc: 0.3857\n",
      "Epoch: 41, Batch: 300, loss: 4.3574 , Train PPL: 1.0067, Train Acc: 0.3933\n",
      "Epoch: 41, Batch: 400, loss: 4.3991 , Train PPL: 1.0067, Train Acc: 0.3598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41, Batch: 500, loss: 3.9291 , Train PPL: 1.0060, Train Acc: 0.4634\n",
      "Epoch: 41, Batch: 600, loss: 4.1889 , Train PPL: 1.0064, Train Acc: 0.4482\n",
      "Epoch: 41, Batch: 700, loss: 4.2821 , Train PPL: 1.0065, Train Acc: 0.3918\n",
      "Validation --- Epoch: 41, total loss: 296.8812 , PPL: 1.2753, Acc: 0.3951\n",
      "lr = 0.5\n",
      "Epoch: 42, Batch: 100, loss: 3.7156 , Train PPL: 1.0057, Train Acc: 0.5015\n",
      "Epoch: 42, Batch: 200, loss: 4.0132 , Train PPL: 1.0061, Train Acc: 0.4573\n",
      "Epoch: 42, Batch: 300, loss: 4.2099 , Train PPL: 1.0064, Train Acc: 0.4619\n",
      "Epoch: 42, Batch: 400, loss: 3.9605 , Train PPL: 1.0061, Train Acc: 0.4619\n",
      "Epoch: 42, Batch: 500, loss: 4.1345 , Train PPL: 1.0063, Train Acc: 0.4299\n",
      "Epoch: 42, Batch: 600, loss: 4.3663 , Train PPL: 1.0067, Train Acc: 0.3918\n",
      "Epoch: 42, Batch: 700, loss: 4.1077 , Train PPL: 1.0063, Train Acc: 0.4314\n",
      "Validation --- Epoch: 42, total loss: 297.2900 , PPL: 1.2758, Acc: 0.3958\n",
      "lr = 0.5\n",
      "Epoch: 43, Batch: 100, loss: 4.1686 , Train PPL: 1.0064, Train Acc: 0.4009\n",
      "Epoch: 43, Batch: 200, loss: 4.2824 , Train PPL: 1.0065, Train Acc: 0.3689\n",
      "Epoch: 43, Batch: 300, loss: 4.2000 , Train PPL: 1.0064, Train Acc: 0.3887\n",
      "Epoch: 43, Batch: 400, loss: 4.1595 , Train PPL: 1.0064, Train Acc: 0.3979\n",
      "Epoch: 43, Batch: 500, loss: 4.1485 , Train PPL: 1.0063, Train Acc: 0.4390\n",
      "Epoch: 43, Batch: 600, loss: 4.1826 , Train PPL: 1.0064, Train Acc: 0.4604\n",
      "Epoch: 43, Batch: 700, loss: 3.9438 , Train PPL: 1.0060, Train Acc: 0.4634\n",
      "Validation --- Epoch: 43, total loss: 296.8289 , PPL: 1.2752, Acc: 0.3958\n",
      "lr = 0.5\n",
      "Epoch: 44, Batch: 100, loss: 4.1231 , Train PPL: 1.0063, Train Acc: 0.4771\n",
      "Epoch: 44, Batch: 200, loss: 4.1521 , Train PPL: 1.0063, Train Acc: 0.4192\n",
      "Epoch: 44, Batch: 300, loss: 3.9368 , Train PPL: 1.0060, Train Acc: 0.4162\n",
      "Epoch: 44, Batch: 400, loss: 4.1496 , Train PPL: 1.0063, Train Acc: 0.3902\n",
      "Epoch: 44, Batch: 500, loss: 4.1989 , Train PPL: 1.0064, Train Acc: 0.4040\n",
      "Epoch: 44, Batch: 600, loss: 4.1575 , Train PPL: 1.0064, Train Acc: 0.3979\n",
      "Epoch: 44, Batch: 700, loss: 4.2631 , Train PPL: 1.0065, Train Acc: 0.4421\n",
      "Validation --- Epoch: 44, total loss: 297.1265 , PPL: 1.2756, Acc: 0.3970\n",
      "lr = 0.5\n",
      "Epoch: 45, Batch: 100, loss: 4.0162 , Train PPL: 1.0061, Train Acc: 0.4284\n",
      "Epoch: 45, Batch: 200, loss: 4.0565 , Train PPL: 1.0062, Train Acc: 0.4360\n",
      "Epoch: 45, Batch: 300, loss: 4.1450 , Train PPL: 1.0063, Train Acc: 0.3872\n",
      "Epoch: 45, Batch: 400, loss: 4.2647 , Train PPL: 1.0065, Train Acc: 0.3933\n",
      "Epoch: 45, Batch: 500, loss: 4.4520 , Train PPL: 1.0068, Train Acc: 0.3445\n",
      "Epoch: 45, Batch: 600, loss: 4.3371 , Train PPL: 1.0066, Train Acc: 0.3674\n",
      "Epoch: 45, Batch: 700, loss: 4.2226 , Train PPL: 1.0065, Train Acc: 0.4192\n",
      "Validation --- Epoch: 45, total loss: 296.9469 , PPL: 1.2754, Acc: 0.3964\n",
      "lr = 0.25\n",
      "Epoch: 46, Batch: 100, loss: 4.2925 , Train PPL: 1.0066, Train Acc: 0.3811\n",
      "Epoch: 46, Batch: 200, loss: 4.3663 , Train PPL: 1.0067, Train Acc: 0.3369\n",
      "Epoch: 46, Batch: 300, loss: 3.9611 , Train PPL: 1.0061, Train Acc: 0.4680\n",
      "Epoch: 46, Batch: 400, loss: 4.1441 , Train PPL: 1.0063, Train Acc: 0.4085\n",
      "Epoch: 46, Batch: 500, loss: 3.8209 , Train PPL: 1.0058, Train Acc: 0.4573\n",
      "Epoch: 46, Batch: 600, loss: 4.3005 , Train PPL: 1.0066, Train Acc: 0.3857\n",
      "Epoch: 46, Batch: 700, loss: 4.2294 , Train PPL: 1.0065, Train Acc: 0.3338\n",
      "Validation --- Epoch: 46, total loss: 296.9411 , PPL: 1.2754, Acc: 0.3985\n",
      "lr = 0.25\n",
      "Epoch: 47, Batch: 100, loss: 4.2690 , Train PPL: 1.0065, Train Acc: 0.3963\n",
      "Epoch: 47, Batch: 200, loss: 4.4067 , Train PPL: 1.0067, Train Acc: 0.3598\n",
      "Epoch: 47, Batch: 300, loss: 4.1315 , Train PPL: 1.0063, Train Acc: 0.4055\n",
      "Epoch: 47, Batch: 400, loss: 4.2914 , Train PPL: 1.0066, Train Acc: 0.3963\n",
      "Epoch: 47, Batch: 500, loss: 4.3167 , Train PPL: 1.0066, Train Acc: 0.4146\n",
      "Epoch: 47, Batch: 600, loss: 3.9603 , Train PPL: 1.0061, Train Acc: 0.4177\n",
      "Epoch: 47, Batch: 700, loss: 4.0999 , Train PPL: 1.0063, Train Acc: 0.4085\n",
      "Validation --- Epoch: 47, total loss: 297.1115 , PPL: 1.2756, Acc: 0.3993\n",
      "lr = 0.25\n",
      "Epoch: 48, Batch: 100, loss: 4.2647 , Train PPL: 1.0065, Train Acc: 0.4223\n",
      "Epoch: 48, Batch: 200, loss: 4.2083 , Train PPL: 1.0064, Train Acc: 0.4223\n",
      "Epoch: 48, Batch: 300, loss: 4.0859 , Train PPL: 1.0062, Train Acc: 0.4009\n",
      "Epoch: 48, Batch: 400, loss: 4.2357 , Train PPL: 1.0065, Train Acc: 0.3857\n",
      "Epoch: 48, Batch: 500, loss: 3.9408 , Train PPL: 1.0060, Train Acc: 0.4451\n",
      "Epoch: 48, Batch: 600, loss: 4.3713 , Train PPL: 1.0067, Train Acc: 0.4223\n",
      "Epoch: 48, Batch: 700, loss: 4.2353 , Train PPL: 1.0065, Train Acc: 0.3902\n",
      "Validation --- Epoch: 48, total loss: 297.3644 , PPL: 1.2759, Acc: 0.3992\n",
      "lr = 0.25\n",
      "Epoch: 49, Batch: 100, loss: 3.9841 , Train PPL: 1.0061, Train Acc: 0.4329\n",
      "Epoch: 49, Batch: 200, loss: 4.2605 , Train PPL: 1.0065, Train Acc: 0.3811\n",
      "Epoch: 49, Batch: 300, loss: 4.1705 , Train PPL: 1.0064, Train Acc: 0.3659\n",
      "Epoch: 49, Batch: 400, loss: 4.0057 , Train PPL: 1.0061, Train Acc: 0.4893\n",
      "Epoch: 49, Batch: 500, loss: 4.3547 , Train PPL: 1.0067, Train Acc: 0.3704\n",
      "Epoch: 49, Batch: 600, loss: 3.6924 , Train PPL: 1.0056, Train Acc: 0.5000\n",
      "Epoch: 49, Batch: 700, loss: 4.3375 , Train PPL: 1.0066, Train Acc: 0.3780\n",
      "Validation --- Epoch: 49, total loss: 297.1579 , PPL: 1.2756, Acc: 0.3999\n",
      "lr = 0.25\n",
      "Epoch: 50, Batch: 100, loss: 4.3900 , Train PPL: 1.0067, Train Acc: 0.3765\n",
      "Epoch: 50, Batch: 200, loss: 4.3713 , Train PPL: 1.0067, Train Acc: 0.3628\n",
      "Epoch: 50, Batch: 300, loss: 4.2584 , Train PPL: 1.0065, Train Acc: 0.4085\n",
      "Epoch: 50, Batch: 400, loss: 4.2817 , Train PPL: 1.0065, Train Acc: 0.4268\n",
      "Epoch: 50, Batch: 500, loss: 4.2169 , Train PPL: 1.0064, Train Acc: 0.4040\n",
      "Epoch: 50, Batch: 600, loss: 4.0941 , Train PPL: 1.0063, Train Acc: 0.4177\n",
      "Epoch: 50, Batch: 700, loss: 4.2628 , Train PPL: 1.0065, Train Acc: 0.4024\n",
      "Validation --- Epoch: 50, total loss: 297.1500 , PPL: 1.2756, Acc: 0.3993\n",
      "lr = 0.25\n",
      "Epoch: 51, Batch: 100, loss: 3.9155 , Train PPL: 1.0060, Train Acc: 0.4634\n",
      "Epoch: 51, Batch: 200, loss: 4.2431 , Train PPL: 1.0065, Train Acc: 0.3689\n",
      "Epoch: 51, Batch: 300, loss: 4.1808 , Train PPL: 1.0064, Train Acc: 0.3780\n",
      "Epoch: 51, Batch: 400, loss: 3.7834 , Train PPL: 1.0058, Train Acc: 0.4665\n",
      "Epoch: 51, Batch: 500, loss: 4.0774 , Train PPL: 1.0062, Train Acc: 0.4055\n",
      "Epoch: 51, Batch: 600, loss: 4.1010 , Train PPL: 1.0063, Train Acc: 0.4573\n",
      "Epoch: 51, Batch: 700, loss: 4.1525 , Train PPL: 1.0064, Train Acc: 0.4329\n",
      "Validation --- Epoch: 51, total loss: 297.1644 , PPL: 1.2756, Acc: 0.3992\n",
      "lr = 0.125\n",
      "Epoch: 52, Batch: 100, loss: 4.1434 , Train PPL: 1.0063, Train Acc: 0.3994\n",
      "Epoch: 52, Batch: 200, loss: 4.1785 , Train PPL: 1.0064, Train Acc: 0.3765\n",
      "Epoch: 52, Batch: 300, loss: 4.0824 , Train PPL: 1.0062, Train Acc: 0.4131\n",
      "Epoch: 52, Batch: 400, loss: 4.1130 , Train PPL: 1.0063, Train Acc: 0.4116\n",
      "Epoch: 52, Batch: 500, loss: 4.3628 , Train PPL: 1.0067, Train Acc: 0.4009\n",
      "Epoch: 52, Batch: 600, loss: 3.9838 , Train PPL: 1.0061, Train Acc: 0.4726\n",
      "Epoch: 52, Batch: 700, loss: 4.0745 , Train PPL: 1.0062, Train Acc: 0.4253\n",
      "Validation --- Epoch: 52, total loss: 297.2835 , PPL: 1.2758, Acc: 0.3998\n",
      "lr = 0.125\n",
      "Epoch: 53, Batch: 100, loss: 4.3042 , Train PPL: 1.0066, Train Acc: 0.3948\n",
      "Epoch: 53, Batch: 200, loss: 4.1269 , Train PPL: 1.0063, Train Acc: 0.3948\n",
      "Epoch: 53, Batch: 300, loss: 4.0966 , Train PPL: 1.0063, Train Acc: 0.4101\n",
      "Epoch: 53, Batch: 400, loss: 4.1834 , Train PPL: 1.0064, Train Acc: 0.3735\n",
      "Epoch: 53, Batch: 500, loss: 4.2928 , Train PPL: 1.0066, Train Acc: 0.3399\n",
      "Epoch: 53, Batch: 600, loss: 4.0608 , Train PPL: 1.0062, Train Acc: 0.4329\n",
      "Epoch: 53, Batch: 700, loss: 3.9678 , Train PPL: 1.0061, Train Acc: 0.4405\n",
      "Validation --- Epoch: 53, total loss: 297.2661 , PPL: 1.2758, Acc: 0.3997\n",
      "lr = 0.125\n",
      "Epoch: 54, Batch: 100, loss: 4.4774 , Train PPL: 1.0068, Train Acc: 0.3232\n",
      "Epoch: 54, Batch: 200, loss: 4.1626 , Train PPL: 1.0064, Train Acc: 0.4527\n",
      "Epoch: 54, Batch: 300, loss: 3.9011 , Train PPL: 1.0060, Train Acc: 0.4665\n",
      "Epoch: 54, Batch: 400, loss: 4.2418 , Train PPL: 1.0065, Train Acc: 0.3704\n",
      "Epoch: 54, Batch: 500, loss: 4.1232 , Train PPL: 1.0063, Train Acc: 0.4207\n",
      "Epoch: 54, Batch: 600, loss: 4.1576 , Train PPL: 1.0064, Train Acc: 0.4436\n",
      "Epoch: 54, Batch: 700, loss: 4.0736 , Train PPL: 1.0062, Train Acc: 0.4085\n",
      "Validation --- Epoch: 54, total loss: 297.4274 , PPL: 1.2759, Acc: 0.4011\n",
      "lr = 0.125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55, Batch: 100, loss: 4.2196 , Train PPL: 1.0065, Train Acc: 0.4329\n",
      "Epoch: 55, Batch: 200, loss: 3.9239 , Train PPL: 1.0060, Train Acc: 0.4909\n",
      "Epoch: 55, Batch: 300, loss: 3.8913 , Train PPL: 1.0059, Train Acc: 0.4756\n",
      "Epoch: 55, Batch: 400, loss: 4.1402 , Train PPL: 1.0063, Train Acc: 0.3933\n",
      "Epoch: 55, Batch: 500, loss: 4.2246 , Train PPL: 1.0065, Train Acc: 0.3948\n",
      "Epoch: 55, Batch: 600, loss: 4.3056 , Train PPL: 1.0066, Train Acc: 0.3857\n",
      "Epoch: 55, Batch: 700, loss: 4.3487 , Train PPL: 1.0067, Train Acc: 0.3887\n",
      "Validation --- Epoch: 55, total loss: 297.2353 , PPL: 1.2757, Acc: 0.4004\n",
      "lr = 0.125\n",
      "Epoch: 56, Batch: 100, loss: 3.9319 , Train PPL: 1.0060, Train Acc: 0.4863\n",
      "Epoch: 56, Batch: 200, loss: 4.3392 , Train PPL: 1.0066, Train Acc: 0.3582\n",
      "Epoch: 56, Batch: 300, loss: 4.2180 , Train PPL: 1.0065, Train Acc: 0.4451\n",
      "Epoch: 56, Batch: 400, loss: 3.9325 , Train PPL: 1.0060, Train Acc: 0.4405\n",
      "Epoch: 56, Batch: 500, loss: 4.1257 , Train PPL: 1.0063, Train Acc: 0.4405\n",
      "Epoch: 56, Batch: 600, loss: 4.1649 , Train PPL: 1.0064, Train Acc: 0.4482\n",
      "Epoch: 56, Batch: 700, loss: 3.8629 , Train PPL: 1.0059, Train Acc: 0.4634\n",
      "Validation --- Epoch: 56, total loss: 297.3586 , PPL: 1.2759, Acc: 0.4002\n",
      "lr = 0.125\n",
      "Epoch: 57, Batch: 100, loss: 4.2820 , Train PPL: 1.0065, Train Acc: 0.4345\n",
      "Epoch: 57, Batch: 200, loss: 4.1263 , Train PPL: 1.0063, Train Acc: 0.3933\n",
      "Epoch: 57, Batch: 300, loss: 4.3847 , Train PPL: 1.0067, Train Acc: 0.3659\n",
      "Epoch: 57, Batch: 400, loss: 3.9622 , Train PPL: 1.0061, Train Acc: 0.4787\n",
      "Epoch: 57, Batch: 500, loss: 4.1709 , Train PPL: 1.0064, Train Acc: 0.4116\n",
      "Epoch: 57, Batch: 600, loss: 4.3136 , Train PPL: 1.0066, Train Acc: 0.3918\n",
      "Epoch: 57, Batch: 700, loss: 4.1585 , Train PPL: 1.0064, Train Acc: 0.3979\n",
      "Validation --- Epoch: 57, total loss: 297.3583 , PPL: 1.2759, Acc: 0.3997\n",
      "lr = 0.0625\n",
      "Epoch: 58, Batch: 100, loss: 4.2380 , Train PPL: 1.0065, Train Acc: 0.3918\n",
      "Epoch: 58, Batch: 200, loss: 4.1576 , Train PPL: 1.0064, Train Acc: 0.4284\n",
      "Epoch: 58, Batch: 300, loss: 4.3091 , Train PPL: 1.0066, Train Acc: 0.4055\n",
      "Epoch: 58, Batch: 400, loss: 4.0487 , Train PPL: 1.0062, Train Acc: 0.4345\n",
      "Epoch: 58, Batch: 500, loss: 4.1363 , Train PPL: 1.0063, Train Acc: 0.4192\n",
      "Epoch: 58, Batch: 600, loss: 4.1986 , Train PPL: 1.0064, Train Acc: 0.3720\n",
      "Epoch: 58, Batch: 700, loss: 4.1805 , Train PPL: 1.0064, Train Acc: 0.4299\n",
      "Validation --- Epoch: 58, total loss: 297.5019 , PPL: 1.2760, Acc: 0.4009\n",
      "lr = 0.0625\n",
      "Epoch: 59, Batch: 100, loss: 4.0461 , Train PPL: 1.0062, Train Acc: 0.3872\n",
      "Epoch: 59, Batch: 200, loss: 4.1479 , Train PPL: 1.0063, Train Acc: 0.4177\n",
      "Epoch: 59, Batch: 300, loss: 4.1736 , Train PPL: 1.0064, Train Acc: 0.4085\n",
      "Epoch: 59, Batch: 400, loss: 3.9791 , Train PPL: 1.0061, Train Acc: 0.4543\n",
      "Epoch: 59, Batch: 500, loss: 4.1375 , Train PPL: 1.0063, Train Acc: 0.4223\n",
      "Epoch: 59, Batch: 600, loss: 3.9619 , Train PPL: 1.0061, Train Acc: 0.4497\n",
      "Epoch: 59, Batch: 700, loss: 4.1056 , Train PPL: 1.0063, Train Acc: 0.4116\n",
      "Validation --- Epoch: 59, total loss: 297.3784 , PPL: 1.2759, Acc: 0.4005\n",
      "lr = 0.0625\n",
      "Epoch: 60, Batch: 100, loss: 3.9538 , Train PPL: 1.0060, Train Acc: 0.4177\n",
      "Epoch: 60, Batch: 200, loss: 3.9802 , Train PPL: 1.0061, Train Acc: 0.4284\n",
      "Epoch: 60, Batch: 300, loss: 4.0399 , Train PPL: 1.0062, Train Acc: 0.4451\n",
      "Epoch: 60, Batch: 400, loss: 4.1175 , Train PPL: 1.0063, Train Acc: 0.4101\n",
      "Epoch: 60, Batch: 500, loss: 4.1237 , Train PPL: 1.0063, Train Acc: 0.3872\n",
      "Epoch: 60, Batch: 600, loss: 4.2768 , Train PPL: 1.0065, Train Acc: 0.3902\n",
      "Epoch: 60, Batch: 700, loss: 3.9232 , Train PPL: 1.0060, Train Acc: 0.4741\n",
      "Validation --- Epoch: 60, total loss: 297.4459 , PPL: 1.2760, Acc: 0.4004\n",
      "lr = 0.0625\n",
      "Epoch: 61, Batch: 100, loss: 4.0912 , Train PPL: 1.0063, Train Acc: 0.4314\n",
      "Epoch: 61, Batch: 200, loss: 4.4708 , Train PPL: 1.0068, Train Acc: 0.3765\n",
      "Epoch: 61, Batch: 300, loss: 4.1440 , Train PPL: 1.0063, Train Acc: 0.3963\n",
      "Epoch: 61, Batch: 400, loss: 4.2415 , Train PPL: 1.0065, Train Acc: 0.3918\n",
      "Epoch: 61, Batch: 500, loss: 4.2821 , Train PPL: 1.0065, Train Acc: 0.3872\n",
      "Epoch: 61, Batch: 600, loss: 4.2058 , Train PPL: 1.0064, Train Acc: 0.3948\n",
      "Epoch: 61, Batch: 700, loss: 4.2640 , Train PPL: 1.0065, Train Acc: 0.3750\n",
      "Validation --- Epoch: 61, total loss: 297.4568 , PPL: 1.2760, Acc: 0.4009\n",
      "lr = 0.0625\n",
      "Epoch: 62, Batch: 100, loss: 4.2877 , Train PPL: 1.0066, Train Acc: 0.3506\n",
      "Epoch: 62, Batch: 200, loss: 4.3361 , Train PPL: 1.0066, Train Acc: 0.4040\n",
      "Epoch: 62, Batch: 300, loss: 4.0949 , Train PPL: 1.0063, Train Acc: 0.4070\n",
      "Epoch: 62, Batch: 400, loss: 3.8789 , Train PPL: 1.0059, Train Acc: 0.4832\n",
      "Epoch: 62, Batch: 500, loss: 3.9627 , Train PPL: 1.0061, Train Acc: 0.4345\n",
      "Epoch: 62, Batch: 600, loss: 4.0990 , Train PPL: 1.0063, Train Acc: 0.4375\n",
      "Epoch: 62, Batch: 700, loss: 4.3136 , Train PPL: 1.0066, Train Acc: 0.3872\n",
      "Validation --- Epoch: 62, total loss: 297.4521 , PPL: 1.2760, Acc: 0.4004\n",
      "lr = 0.0625\n",
      "Epoch: 63, Batch: 100, loss: 4.2689 , Train PPL: 1.0065, Train Acc: 0.3704\n",
      "Epoch: 63, Batch: 200, loss: 4.0698 , Train PPL: 1.0062, Train Acc: 0.3963\n",
      "Epoch: 63, Batch: 300, loss: 4.3036 , Train PPL: 1.0066, Train Acc: 0.3720\n",
      "Epoch: 63, Batch: 400, loss: 4.2352 , Train PPL: 1.0065, Train Acc: 0.3948\n",
      "Epoch: 63, Batch: 500, loss: 4.3261 , Train PPL: 1.0066, Train Acc: 0.3537\n",
      "Epoch: 63, Batch: 600, loss: 4.2864 , Train PPL: 1.0066, Train Acc: 0.4070\n",
      "Epoch: 63, Batch: 700, loss: 4.1174 , Train PPL: 1.0063, Train Acc: 0.4040\n",
      "Validation --- Epoch: 63, total loss: 297.4987 , PPL: 1.2760, Acc: 0.4008\n",
      "lr = 0.03125\n",
      "Epoch: 64, Batch: 100, loss: 4.2375 , Train PPL: 1.0065, Train Acc: 0.4497\n",
      "Epoch: 64, Batch: 200, loss: 4.2517 , Train PPL: 1.0065, Train Acc: 0.3613\n",
      "Epoch: 64, Batch: 300, loss: 3.7039 , Train PPL: 1.0057, Train Acc: 0.4878\n",
      "Epoch: 64, Batch: 400, loss: 4.3327 , Train PPL: 1.0066, Train Acc: 0.3628\n",
      "Epoch: 64, Batch: 500, loss: 4.2346 , Train PPL: 1.0065, Train Acc: 0.4055\n",
      "Epoch: 64, Batch: 600, loss: 4.2297 , Train PPL: 1.0065, Train Acc: 0.3841\n",
      "Epoch: 64, Batch: 700, loss: 4.0558 , Train PPL: 1.0062, Train Acc: 0.4253\n",
      "Validation --- Epoch: 64, total loss: 297.4547 , PPL: 1.2760, Acc: 0.4002\n",
      "lr = 0.03125\n",
      "Epoch: 65, Batch: 100, loss: 3.9467 , Train PPL: 1.0060, Train Acc: 0.4787\n",
      "Epoch: 65, Batch: 200, loss: 4.3305 , Train PPL: 1.0066, Train Acc: 0.3765\n",
      "Epoch: 65, Batch: 300, loss: 4.2258 , Train PPL: 1.0065, Train Acc: 0.4177\n",
      "Epoch: 65, Batch: 400, loss: 4.2592 , Train PPL: 1.0065, Train Acc: 0.4345\n",
      "Epoch: 65, Batch: 500, loss: 4.2755 , Train PPL: 1.0065, Train Acc: 0.3552\n",
      "Epoch: 65, Batch: 600, loss: 4.2476 , Train PPL: 1.0065, Train Acc: 0.3872\n",
      "Epoch: 65, Batch: 700, loss: 4.1617 , Train PPL: 1.0064, Train Acc: 0.3948\n",
      "Validation --- Epoch: 65, total loss: 297.4989 , PPL: 1.2760, Acc: 0.3999\n",
      "lr = 0.03125\n",
      "Epoch: 66, Batch: 100, loss: 4.1664 , Train PPL: 1.0064, Train Acc: 0.3674\n",
      "Epoch: 66, Batch: 200, loss: 4.1029 , Train PPL: 1.0063, Train Acc: 0.4405\n",
      "Epoch: 66, Batch: 300, loss: 4.3226 , Train PPL: 1.0066, Train Acc: 0.4177\n",
      "Epoch: 66, Batch: 400, loss: 4.2239 , Train PPL: 1.0065, Train Acc: 0.3963\n",
      "Epoch: 66, Batch: 500, loss: 4.1605 , Train PPL: 1.0064, Train Acc: 0.3720\n",
      "Epoch: 66, Batch: 600, loss: 4.1702 , Train PPL: 1.0064, Train Acc: 0.3857\n",
      "Epoch: 66, Batch: 700, loss: 3.9551 , Train PPL: 1.0060, Train Acc: 0.4649\n",
      "Validation --- Epoch: 66, total loss: 297.5041 , PPL: 1.2760, Acc: 0.4008\n",
      "lr = 0.03125\n",
      "Epoch: 67, Batch: 100, loss: 4.1210 , Train PPL: 1.0063, Train Acc: 0.4085\n",
      "Epoch: 67, Batch: 200, loss: 3.9419 , Train PPL: 1.0060, Train Acc: 0.4009\n",
      "Epoch: 67, Batch: 300, loss: 4.2601 , Train PPL: 1.0065, Train Acc: 0.4131\n",
      "Epoch: 67, Batch: 400, loss: 3.7946 , Train PPL: 1.0058, Train Acc: 0.4878\n",
      "Epoch: 67, Batch: 500, loss: 4.0707 , Train PPL: 1.0062, Train Acc: 0.4345\n",
      "Epoch: 67, Batch: 600, loss: 4.2526 , Train PPL: 1.0065, Train Acc: 0.3659\n",
      "Epoch: 67, Batch: 700, loss: 4.2389 , Train PPL: 1.0065, Train Acc: 0.3750\n",
      "Validation --- Epoch: 67, total loss: 297.5105 , PPL: 1.2760, Acc: 0.4005\n",
      "lr = 0.03125\n",
      "Epoch: 68, Batch: 100, loss: 4.1777 , Train PPL: 1.0064, Train Acc: 0.4116\n",
      "Epoch: 68, Batch: 200, loss: 4.1895 , Train PPL: 1.0064, Train Acc: 0.3994\n",
      "Epoch: 68, Batch: 300, loss: 4.2712 , Train PPL: 1.0065, Train Acc: 0.3841\n",
      "Epoch: 68, Batch: 400, loss: 4.3157 , Train PPL: 1.0066, Train Acc: 0.3826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68, Batch: 500, loss: 4.0369 , Train PPL: 1.0062, Train Acc: 0.4238\n",
      "Epoch: 68, Batch: 600, loss: 4.1295 , Train PPL: 1.0063, Train Acc: 0.4497\n",
      "Epoch: 68, Batch: 700, loss: 4.1792 , Train PPL: 1.0064, Train Acc: 0.3979\n",
      "Validation --- Epoch: 68, total loss: 297.5560 , PPL: 1.2761, Acc: 0.4004\n",
      "lr = 0.03125\n",
      "Epoch: 69, Batch: 100, loss: 3.9388 , Train PPL: 1.0060, Train Acc: 0.4771\n",
      "Epoch: 69, Batch: 200, loss: 3.9959 , Train PPL: 1.0061, Train Acc: 0.4466\n",
      "Epoch: 69, Batch: 300, loss: 4.0616 , Train PPL: 1.0062, Train Acc: 0.4024\n",
      "Epoch: 69, Batch: 400, loss: 3.8252 , Train PPL: 1.0058, Train Acc: 0.4878\n",
      "Epoch: 69, Batch: 500, loss: 3.9534 , Train PPL: 1.0060, Train Acc: 0.4543\n",
      "Epoch: 69, Batch: 600, loss: 3.6842 , Train PPL: 1.0056, Train Acc: 0.5152\n",
      "Epoch: 69, Batch: 700, loss: 3.9165 , Train PPL: 1.0060, Train Acc: 0.4710\n",
      "Validation --- Epoch: 69, total loss: 297.5078 , PPL: 1.2760, Acc: 0.4009\n",
      "lr = 0.015625\n",
      "Epoch: 70, Batch: 100, loss: 3.9344 , Train PPL: 1.0060, Train Acc: 0.4451\n",
      "Epoch: 70, Batch: 200, loss: 4.1706 , Train PPL: 1.0064, Train Acc: 0.3750\n",
      "Epoch: 70, Batch: 300, loss: 3.9468 , Train PPL: 1.0060, Train Acc: 0.4466\n",
      "Epoch: 70, Batch: 400, loss: 4.1533 , Train PPL: 1.0064, Train Acc: 0.4162\n",
      "Epoch: 70, Batch: 500, loss: 4.2247 , Train PPL: 1.0065, Train Acc: 0.4116\n",
      "Epoch: 70, Batch: 600, loss: 4.1195 , Train PPL: 1.0063, Train Acc: 0.3994\n",
      "Epoch: 70, Batch: 700, loss: 4.0586 , Train PPL: 1.0062, Train Acc: 0.4299\n",
      "Validation --- Epoch: 70, total loss: 297.5315 , PPL: 1.2760, Acc: 0.4007\n",
      "lr = 0.015625\n",
      "Epoch: 71, Batch: 100, loss: 4.3222 , Train PPL: 1.0066, Train Acc: 0.3796\n",
      "Epoch: 71, Batch: 200, loss: 4.2610 , Train PPL: 1.0065, Train Acc: 0.3750\n",
      "Epoch: 71, Batch: 300, loss: 3.8211 , Train PPL: 1.0058, Train Acc: 0.4604\n",
      "Epoch: 71, Batch: 400, loss: 4.1065 , Train PPL: 1.0063, Train Acc: 0.4527\n",
      "Epoch: 71, Batch: 500, loss: 4.1287 , Train PPL: 1.0063, Train Acc: 0.3887\n",
      "Epoch: 71, Batch: 600, loss: 4.3614 , Train PPL: 1.0067, Train Acc: 0.3933\n",
      "Epoch: 71, Batch: 700, loss: 4.0973 , Train PPL: 1.0063, Train Acc: 0.4573\n",
      "Validation --- Epoch: 71, total loss: 297.5378 , PPL: 1.2761, Acc: 0.4003\n",
      "lr = 0.015625\n",
      "Epoch: 72, Batch: 100, loss: 4.1279 , Train PPL: 1.0063, Train Acc: 0.3979\n",
      "Epoch: 72, Batch: 200, loss: 4.2819 , Train PPL: 1.0065, Train Acc: 0.3689\n",
      "Epoch: 72, Batch: 300, loss: 3.9949 , Train PPL: 1.0061, Train Acc: 0.4253\n",
      "Epoch: 72, Batch: 400, loss: 3.7224 , Train PPL: 1.0057, Train Acc: 0.4741\n",
      "Epoch: 72, Batch: 500, loss: 4.2353 , Train PPL: 1.0065, Train Acc: 0.4131\n",
      "Epoch: 72, Batch: 600, loss: 3.8834 , Train PPL: 1.0059, Train Acc: 0.4482\n",
      "Epoch: 72, Batch: 700, loss: 4.1199 , Train PPL: 1.0063, Train Acc: 0.4695\n",
      "Validation --- Epoch: 72, total loss: 297.5467 , PPL: 1.2761, Acc: 0.4005\n",
      "lr = 0.015625\n",
      "Epoch: 73, Batch: 100, loss: 4.1190 , Train PPL: 1.0063, Train Acc: 0.4024\n",
      "Epoch: 73, Batch: 200, loss: 4.2583 , Train PPL: 1.0065, Train Acc: 0.3750\n",
      "Epoch: 73, Batch: 300, loss: 4.0929 , Train PPL: 1.0063, Train Acc: 0.3918\n",
      "Epoch: 73, Batch: 400, loss: 3.9120 , Train PPL: 1.0060, Train Acc: 0.4985\n",
      "Epoch: 73, Batch: 500, loss: 4.2103 , Train PPL: 1.0064, Train Acc: 0.4146\n",
      "Epoch: 73, Batch: 600, loss: 4.0626 , Train PPL: 1.0062, Train Acc: 0.4466\n",
      "Epoch: 73, Batch: 700, loss: 3.8217 , Train PPL: 1.0058, Train Acc: 0.4802\n",
      "Validation --- Epoch: 73, total loss: 297.5736 , PPL: 1.2761, Acc: 0.4001\n",
      "lr = 0.015625\n",
      "Epoch: 74, Batch: 100, loss: 4.2285 , Train PPL: 1.0065, Train Acc: 0.4070\n",
      "Epoch: 74, Batch: 200, loss: 3.7578 , Train PPL: 1.0057, Train Acc: 0.5488\n",
      "Epoch: 74, Batch: 300, loss: 4.0975 , Train PPL: 1.0063, Train Acc: 0.4101\n",
      "Epoch: 74, Batch: 400, loss: 4.1304 , Train PPL: 1.0063, Train Acc: 0.4314\n",
      "Epoch: 74, Batch: 500, loss: 4.1554 , Train PPL: 1.0064, Train Acc: 0.3735\n",
      "Epoch: 74, Batch: 600, loss: 3.7451 , Train PPL: 1.0057, Train Acc: 0.4451\n",
      "Epoch: 74, Batch: 700, loss: 4.1450 , Train PPL: 1.0063, Train Acc: 0.4009\n",
      "Validation --- Epoch: 74, total loss: 297.5797 , PPL: 1.2761, Acc: 0.4003\n",
      "lr = 0.015625\n",
      "Epoch: 75, Batch: 100, loss: 4.1901 , Train PPL: 1.0064, Train Acc: 0.3491\n",
      "Epoch: 75, Batch: 200, loss: 3.9500 , Train PPL: 1.0060, Train Acc: 0.4726\n",
      "Epoch: 75, Batch: 300, loss: 4.1235 , Train PPL: 1.0063, Train Acc: 0.3887\n",
      "Epoch: 75, Batch: 400, loss: 4.3630 , Train PPL: 1.0067, Train Acc: 0.4040\n",
      "Epoch: 75, Batch: 500, loss: 4.1688 , Train PPL: 1.0064, Train Acc: 0.4390\n",
      "Epoch: 75, Batch: 600, loss: 4.2891 , Train PPL: 1.0066, Train Acc: 0.3720\n",
      "Epoch: 75, Batch: 700, loss: 4.1134 , Train PPL: 1.0063, Train Acc: 0.4055\n",
      "Validation --- Epoch: 75, total loss: 297.5487 , PPL: 1.2761, Acc: 0.4004\n",
      "lr = 0.0078125\n",
      "Epoch: 76, Batch: 100, loss: 3.8737 , Train PPL: 1.0059, Train Acc: 0.4619\n",
      "Epoch: 76, Batch: 200, loss: 4.0363 , Train PPL: 1.0062, Train Acc: 0.4314\n",
      "Epoch: 76, Batch: 300, loss: 4.2194 , Train PPL: 1.0065, Train Acc: 0.4085\n",
      "Epoch: 76, Batch: 400, loss: 4.0452 , Train PPL: 1.0062, Train Acc: 0.4040\n",
      "Epoch: 76, Batch: 500, loss: 4.1032 , Train PPL: 1.0063, Train Acc: 0.4299\n",
      "Epoch: 76, Batch: 600, loss: 4.3291 , Train PPL: 1.0066, Train Acc: 0.3780\n",
      "Epoch: 76, Batch: 700, loss: 3.7671 , Train PPL: 1.0058, Train Acc: 0.4451\n",
      "Validation --- Epoch: 76, total loss: 297.5567 , PPL: 1.2761, Acc: 0.4004\n",
      "lr = 0.0078125\n",
      "Epoch: 77, Batch: 100, loss: 4.3605 , Train PPL: 1.0067, Train Acc: 0.3689\n",
      "Epoch: 77, Batch: 200, loss: 3.8733 , Train PPL: 1.0059, Train Acc: 0.5107\n",
      "Epoch: 77, Batch: 300, loss: 4.3978 , Train PPL: 1.0067, Train Acc: 0.3780\n",
      "Epoch: 77, Batch: 400, loss: 4.1915 , Train PPL: 1.0064, Train Acc: 0.4146\n",
      "Epoch: 77, Batch: 500, loss: 4.1298 , Train PPL: 1.0063, Train Acc: 0.4604\n",
      "Epoch: 77, Batch: 600, loss: 4.1333 , Train PPL: 1.0063, Train Acc: 0.4101\n",
      "Epoch: 77, Batch: 700, loss: 4.2560 , Train PPL: 1.0065, Train Acc: 0.3857\n",
      "Validation --- Epoch: 77, total loss: 297.5717 , PPL: 1.2761, Acc: 0.4004\n",
      "lr = 0.0078125\n",
      "Epoch: 78, Batch: 100, loss: 4.1564 , Train PPL: 1.0064, Train Acc: 0.4390\n",
      "Epoch: 78, Batch: 200, loss: 4.4438 , Train PPL: 1.0068, Train Acc: 0.3613\n",
      "Epoch: 78, Batch: 300, loss: 4.3427 , Train PPL: 1.0066, Train Acc: 0.4146\n",
      "Epoch: 78, Batch: 400, loss: 4.2949 , Train PPL: 1.0066, Train Acc: 0.3918\n",
      "Epoch: 78, Batch: 500, loss: 3.7253 , Train PPL: 1.0057, Train Acc: 0.5838\n",
      "Epoch: 78, Batch: 600, loss: 3.9326 , Train PPL: 1.0060, Train Acc: 0.4787\n",
      "Epoch: 78, Batch: 700, loss: 4.1313 , Train PPL: 1.0063, Train Acc: 0.4024\n",
      "Validation --- Epoch: 78, total loss: 297.5667 , PPL: 1.2761, Acc: 0.4004\n",
      "lr = 0.0078125\n",
      "Epoch: 79, Batch: 100, loss: 4.1969 , Train PPL: 1.0064, Train Acc: 0.3613\n",
      "Epoch: 79, Batch: 200, loss: 3.9065 , Train PPL: 1.0060, Train Acc: 0.4695\n",
      "Epoch: 79, Batch: 300, loss: 4.3121 , Train PPL: 1.0066, Train Acc: 0.4055\n",
      "Epoch: 79, Batch: 400, loss: 4.1076 , Train PPL: 1.0063, Train Acc: 0.4085\n",
      "Epoch: 79, Batch: 500, loss: 3.9518 , Train PPL: 1.0060, Train Acc: 0.4360\n",
      "Epoch: 79, Batch: 600, loss: 3.9507 , Train PPL: 1.0060, Train Acc: 0.4527\n",
      "Epoch: 79, Batch: 700, loss: 4.1842 , Train PPL: 1.0064, Train Acc: 0.3887\n",
      "Validation --- Epoch: 79, total loss: 297.5784 , PPL: 1.2761, Acc: 0.4004\n",
      "lr = 0.0078125\n",
      "Epoch: 80, Batch: 100, loss: 4.1179 , Train PPL: 1.0063, Train Acc: 0.3933\n",
      "Epoch: 80, Batch: 200, loss: 4.2068 , Train PPL: 1.0064, Train Acc: 0.3598\n",
      "Epoch: 80, Batch: 300, loss: 4.2169 , Train PPL: 1.0064, Train Acc: 0.4207\n",
      "Epoch: 80, Batch: 400, loss: 4.2487 , Train PPL: 1.0065, Train Acc: 0.3918\n",
      "Epoch: 80, Batch: 500, loss: 4.2022 , Train PPL: 1.0064, Train Acc: 0.4162\n",
      "Epoch: 80, Batch: 600, loss: 4.3032 , Train PPL: 1.0066, Train Acc: 0.3765\n",
      "Epoch: 80, Batch: 700, loss: 4.0724 , Train PPL: 1.0062, Train Acc: 0.4314\n",
      "Validation --- Epoch: 80, total loss: 297.5740 , PPL: 1.2761, Acc: 0.4005\n",
      "lr = 0.0078125\n",
      "Epoch: 81, Batch: 100, loss: 4.0695 , Train PPL: 1.0062, Train Acc: 0.4558\n",
      "Epoch: 81, Batch: 200, loss: 3.9423 , Train PPL: 1.0060, Train Acc: 0.4832\n",
      "Epoch: 81, Batch: 300, loss: 4.0933 , Train PPL: 1.0063, Train Acc: 0.3872\n",
      "Epoch: 81, Batch: 400, loss: 3.9275 , Train PPL: 1.0060, Train Acc: 0.4573\n",
      "Epoch: 81, Batch: 500, loss: 4.0646 , Train PPL: 1.0062, Train Acc: 0.3948\n",
      "Epoch: 81, Batch: 600, loss: 4.0152 , Train PPL: 1.0061, Train Acc: 0.4848\n",
      "Epoch: 81, Batch: 700, loss: 4.4224 , Train PPL: 1.0068, Train Acc: 0.3506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --- Epoch: 81, total loss: 297.5801 , PPL: 1.2761, Acc: 0.4004\n",
      "lr = 0.00390625\n",
      "Epoch: 82, Batch: 100, loss: 4.4771 , Train PPL: 1.0068, Train Acc: 0.3460\n",
      "Epoch: 82, Batch: 200, loss: 3.8507 , Train PPL: 1.0059, Train Acc: 0.4710\n",
      "Epoch: 82, Batch: 300, loss: 4.3121 , Train PPL: 1.0066, Train Acc: 0.3994\n",
      "Epoch: 82, Batch: 400, loss: 4.0952 , Train PPL: 1.0063, Train Acc: 0.4634\n",
      "Epoch: 82, Batch: 500, loss: 4.0444 , Train PPL: 1.0062, Train Acc: 0.4482\n",
      "Epoch: 82, Batch: 600, loss: 3.9428 , Train PPL: 1.0060, Train Acc: 0.4909\n",
      "Epoch: 82, Batch: 700, loss: 4.0840 , Train PPL: 1.0062, Train Acc: 0.4177\n",
      "Validation --- Epoch: 82, total loss: 297.5741 , PPL: 1.2761, Acc: 0.4004\n",
      "lr = 0.00390625\n",
      "Epoch: 83, Batch: 100, loss: 4.1454 , Train PPL: 1.0063, Train Acc: 0.4162\n",
      "Epoch: 83, Batch: 200, loss: 4.1977 , Train PPL: 1.0064, Train Acc: 0.4451\n",
      "Epoch: 83, Batch: 300, loss: 3.9777 , Train PPL: 1.0061, Train Acc: 0.4085\n",
      "Epoch: 83, Batch: 400, loss: 3.6449 , Train PPL: 1.0056, Train Acc: 0.4588\n",
      "Epoch: 83, Batch: 500, loss: 4.0898 , Train PPL: 1.0063, Train Acc: 0.4101\n",
      "Epoch: 83, Batch: 600, loss: 3.9647 , Train PPL: 1.0061, Train Acc: 0.4116\n",
      "Epoch: 83, Batch: 700, loss: 4.1365 , Train PPL: 1.0063, Train Acc: 0.3963\n",
      "Validation --- Epoch: 83, total loss: 297.5726 , PPL: 1.2761, Acc: 0.4005\n",
      "lr = 0.00390625\n",
      "Epoch: 84, Batch: 100, loss: 4.0922 , Train PPL: 1.0063, Train Acc: 0.4360\n",
      "Epoch: 84, Batch: 200, loss: 4.2220 , Train PPL: 1.0065, Train Acc: 0.4299\n",
      "Epoch: 84, Batch: 300, loss: 4.1285 , Train PPL: 1.0063, Train Acc: 0.4009\n",
      "Epoch: 84, Batch: 400, loss: 3.7506 , Train PPL: 1.0057, Train Acc: 0.5305\n",
      "Epoch: 84, Batch: 500, loss: 4.1494 , Train PPL: 1.0063, Train Acc: 0.4131\n",
      "Epoch: 84, Batch: 600, loss: 3.8510 , Train PPL: 1.0059, Train Acc: 0.4985\n",
      "Epoch: 84, Batch: 700, loss: 4.2125 , Train PPL: 1.0064, Train Acc: 0.4024\n",
      "Validation --- Epoch: 84, total loss: 297.5789 , PPL: 1.2761, Acc: 0.4004\n",
      "lr = 0.00390625\n",
      "Epoch: 85, Batch: 100, loss: 3.8291 , Train PPL: 1.0059, Train Acc: 0.4482\n",
      "Epoch: 85, Batch: 200, loss: 4.2096 , Train PPL: 1.0064, Train Acc: 0.3948\n",
      "Epoch: 85, Batch: 300, loss: 4.2416 , Train PPL: 1.0065, Train Acc: 0.3780\n",
      "Epoch: 85, Batch: 400, loss: 4.0315 , Train PPL: 1.0062, Train Acc: 0.4588\n",
      "Epoch: 85, Batch: 500, loss: 4.3692 , Train PPL: 1.0067, Train Acc: 0.3902\n",
      "Epoch: 85, Batch: 600, loss: 4.1737 , Train PPL: 1.0064, Train Acc: 0.4070\n",
      "Epoch: 85, Batch: 700, loss: 3.9849 , Train PPL: 1.0061, Train Acc: 0.4710\n",
      "Validation --- Epoch: 85, total loss: 297.5815 , PPL: 1.2761, Acc: 0.4003\n",
      "lr = 0.00390625\n",
      "Epoch: 86, Batch: 100, loss: 3.8090 , Train PPL: 1.0058, Train Acc: 0.4863\n",
      "Epoch: 86, Batch: 200, loss: 4.2808 , Train PPL: 1.0065, Train Acc: 0.3643\n",
      "Epoch: 86, Batch: 300, loss: 4.2969 , Train PPL: 1.0066, Train Acc: 0.3918\n",
      "Epoch: 86, Batch: 400, loss: 3.7859 , Train PPL: 1.0058, Train Acc: 0.4543\n",
      "Epoch: 86, Batch: 500, loss: 4.2482 , Train PPL: 1.0065, Train Acc: 0.3552\n",
      "Epoch: 86, Batch: 600, loss: 4.1621 , Train PPL: 1.0064, Train Acc: 0.4284\n",
      "Epoch: 86, Batch: 700, loss: 4.4331 , Train PPL: 1.0068, Train Acc: 0.3369\n",
      "Validation --- Epoch: 86, total loss: 297.5841 , PPL: 1.2761, Acc: 0.4005\n",
      "lr = 0.00390625\n",
      "Epoch: 87, Batch: 100, loss: 4.0475 , Train PPL: 1.0062, Train Acc: 0.3872\n",
      "Epoch: 87, Batch: 200, loss: 3.7441 , Train PPL: 1.0057, Train Acc: 0.4802\n",
      "Epoch: 87, Batch: 300, loss: 4.2092 , Train PPL: 1.0064, Train Acc: 0.4024\n",
      "Epoch: 87, Batch: 400, loss: 3.9528 , Train PPL: 1.0060, Train Acc: 0.4573\n",
      "Epoch: 87, Batch: 500, loss: 4.1932 , Train PPL: 1.0064, Train Acc: 0.4040\n",
      "Epoch: 87, Batch: 600, loss: 4.0483 , Train PPL: 1.0062, Train Acc: 0.4680\n",
      "Epoch: 87, Batch: 700, loss: 4.1293 , Train PPL: 1.0063, Train Acc: 0.4360\n",
      "Validation --- Epoch: 87, total loss: 297.5771 , PPL: 1.2761, Acc: 0.4004\n",
      "lr = 0.001953125\n",
      "Epoch: 88, Batch: 100, loss: 4.1747 , Train PPL: 1.0064, Train Acc: 0.4451\n",
      "Epoch: 88, Batch: 200, loss: 4.0504 , Train PPL: 1.0062, Train Acc: 0.4482\n",
      "Epoch: 88, Batch: 300, loss: 4.0325 , Train PPL: 1.0062, Train Acc: 0.4375\n",
      "Epoch: 88, Batch: 400, loss: 3.7437 , Train PPL: 1.0057, Train Acc: 0.4207\n",
      "Epoch: 88, Batch: 500, loss: 4.3019 , Train PPL: 1.0066, Train Acc: 0.3887\n",
      "Epoch: 88, Batch: 600, loss: 3.7222 , Train PPL: 1.0057, Train Acc: 0.4771\n",
      "Epoch: 88, Batch: 700, loss: 3.8960 , Train PPL: 1.0060, Train Acc: 0.4710\n",
      "Validation --- Epoch: 88, total loss: 297.5821 , PPL: 1.2761, Acc: 0.4003\n",
      "lr = 0.001953125\n",
      "Epoch: 89, Batch: 100, loss: 4.1254 , Train PPL: 1.0063, Train Acc: 0.4253\n",
      "Epoch: 89, Batch: 200, loss: 4.2635 , Train PPL: 1.0065, Train Acc: 0.4040\n",
      "Epoch: 89, Batch: 300, loss: 4.1469 , Train PPL: 1.0063, Train Acc: 0.4040\n",
      "Epoch: 89, Batch: 400, loss: 4.1419 , Train PPL: 1.0063, Train Acc: 0.4101\n",
      "Epoch: 89, Batch: 500, loss: 4.4092 , Train PPL: 1.0067, Train Acc: 0.3750\n",
      "Epoch: 89, Batch: 600, loss: 4.0237 , Train PPL: 1.0062, Train Acc: 0.4116\n",
      "Epoch: 89, Batch: 700, loss: 4.0541 , Train PPL: 1.0062, Train Acc: 0.4131\n",
      "Validation --- Epoch: 89, total loss: 297.5824 , PPL: 1.2761, Acc: 0.4004\n",
      "lr = 0.001953125\n",
      "Epoch: 90, Batch: 100, loss: 4.2035 , Train PPL: 1.0064, Train Acc: 0.3750\n",
      "Epoch: 90, Batch: 200, loss: 4.2688 , Train PPL: 1.0065, Train Acc: 0.4223\n",
      "Epoch: 90, Batch: 300, loss: 4.3715 , Train PPL: 1.0067, Train Acc: 0.3552\n",
      "Epoch: 90, Batch: 400, loss: 4.4322 , Train PPL: 1.0068, Train Acc: 0.3811\n",
      "Epoch: 90, Batch: 500, loss: 3.9379 , Train PPL: 1.0060, Train Acc: 0.3994\n",
      "Epoch: 90, Batch: 600, loss: 4.2421 , Train PPL: 1.0065, Train Acc: 0.3811\n",
      "Epoch: 90, Batch: 700, loss: 4.3241 , Train PPL: 1.0066, Train Acc: 0.3857\n",
      "Validation --- Epoch: 90, total loss: 297.5809 , PPL: 1.2761, Acc: 0.4003\n",
      "lr = 0.001953125\n",
      "Epoch: 91, Batch: 100, loss: 4.2708 , Train PPL: 1.0065, Train Acc: 0.4207\n",
      "Epoch: 91, Batch: 200, loss: 4.2397 , Train PPL: 1.0065, Train Acc: 0.4055\n",
      "Epoch: 91, Batch: 300, loss: 4.0611 , Train PPL: 1.0062, Train Acc: 0.4177\n",
      "Epoch: 91, Batch: 400, loss: 3.7989 , Train PPL: 1.0058, Train Acc: 0.4634\n",
      "Epoch: 91, Batch: 500, loss: 4.0358 , Train PPL: 1.0062, Train Acc: 0.4466\n",
      "Epoch: 91, Batch: 600, loss: 4.1971 , Train PPL: 1.0064, Train Acc: 0.4680\n",
      "Epoch: 91, Batch: 700, loss: 3.6268 , Train PPL: 1.0055, Train Acc: 0.5305\n",
      "Validation --- Epoch: 91, total loss: 297.5843 , PPL: 1.2761, Acc: 0.4003\n",
      "lr = 0.001953125\n",
      "Epoch: 92, Batch: 100, loss: 4.1702 , Train PPL: 1.0064, Train Acc: 0.4375\n",
      "Epoch: 92, Batch: 200, loss: 3.8536 , Train PPL: 1.0059, Train Acc: 0.4741\n",
      "Epoch: 92, Batch: 300, loss: 4.1196 , Train PPL: 1.0063, Train Acc: 0.4116\n",
      "Epoch: 92, Batch: 400, loss: 4.0404 , Train PPL: 1.0062, Train Acc: 0.4299\n",
      "Epoch: 92, Batch: 500, loss: 4.0347 , Train PPL: 1.0062, Train Acc: 0.4680\n",
      "Epoch: 92, Batch: 600, loss: 4.2297 , Train PPL: 1.0065, Train Acc: 0.4329\n",
      "Epoch: 92, Batch: 700, loss: 4.3560 , Train PPL: 1.0067, Train Acc: 0.3841\n",
      "Validation --- Epoch: 92, total loss: 297.5854 , PPL: 1.2761, Acc: 0.4003\n",
      "lr = 0.001953125\n",
      "Epoch: 93, Batch: 100, loss: 4.0451 , Train PPL: 1.0062, Train Acc: 0.4680\n",
      "Epoch: 93, Batch: 200, loss: 4.5133 , Train PPL: 1.0069, Train Acc: 0.3598\n",
      "Epoch: 93, Batch: 300, loss: 4.2155 , Train PPL: 1.0064, Train Acc: 0.4162\n",
      "Epoch: 93, Batch: 400, loss: 4.1238 , Train PPL: 1.0063, Train Acc: 0.4543\n",
      "Epoch: 93, Batch: 500, loss: 3.9642 , Train PPL: 1.0061, Train Acc: 0.4634\n",
      "Epoch: 93, Batch: 600, loss: 4.1075 , Train PPL: 1.0063, Train Acc: 0.4009\n",
      "Epoch: 93, Batch: 700, loss: 3.6994 , Train PPL: 1.0057, Train Acc: 0.4985\n",
      "Validation --- Epoch: 93, total loss: 297.5891 , PPL: 1.2761, Acc: 0.4003\n",
      "lr = 0.0009765625\n",
      "Epoch: 94, Batch: 100, loss: 4.2410 , Train PPL: 1.0065, Train Acc: 0.3750\n",
      "Epoch: 94, Batch: 200, loss: 4.2971 , Train PPL: 1.0066, Train Acc: 0.4329\n",
      "Epoch: 94, Batch: 300, loss: 4.3195 , Train PPL: 1.0066, Train Acc: 0.3796\n",
      "Epoch: 94, Batch: 400, loss: 4.1244 , Train PPL: 1.0063, Train Acc: 0.4345\n",
      "Epoch: 94, Batch: 500, loss: 3.8395 , Train PPL: 1.0059, Train Acc: 0.4893\n",
      "Epoch: 94, Batch: 600, loss: 3.7440 , Train PPL: 1.0057, Train Acc: 0.4604\n",
      "Epoch: 94, Batch: 700, loss: 4.1592 , Train PPL: 1.0064, Train Acc: 0.4268\n",
      "Validation --- Epoch: 94, total loss: 297.5901 , PPL: 1.2761, Acc: 0.4003\n",
      "lr = 0.0009765625\n",
      "Epoch: 95, Batch: 100, loss: 4.0552 , Train PPL: 1.0062, Train Acc: 0.4436\n",
      "Epoch: 95, Batch: 200, loss: 4.2569 , Train PPL: 1.0065, Train Acc: 0.4085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95, Batch: 300, loss: 4.0985 , Train PPL: 1.0063, Train Acc: 0.4253\n",
      "Epoch: 95, Batch: 400, loss: 4.3239 , Train PPL: 1.0066, Train Acc: 0.3521\n",
      "Epoch: 95, Batch: 500, loss: 3.7890 , Train PPL: 1.0058, Train Acc: 0.4680\n",
      "Epoch: 95, Batch: 600, loss: 4.0826 , Train PPL: 1.0062, Train Acc: 0.4360\n",
      "Epoch: 95, Batch: 700, loss: 4.2292 , Train PPL: 1.0065, Train Acc: 0.3811\n",
      "Validation --- Epoch: 95, total loss: 297.5898 , PPL: 1.2761, Acc: 0.4003\n",
      "lr = 0.0009765625\n",
      "Epoch: 96, Batch: 100, loss: 4.2725 , Train PPL: 1.0065, Train Acc: 0.4055\n",
      "Epoch: 96, Batch: 200, loss: 4.3587 , Train PPL: 1.0067, Train Acc: 0.3613\n",
      "Epoch: 96, Batch: 300, loss: 4.2635 , Train PPL: 1.0065, Train Acc: 0.3918\n",
      "Epoch: 96, Batch: 400, loss: 4.0100 , Train PPL: 1.0061, Train Acc: 0.4482\n",
      "Epoch: 96, Batch: 500, loss: 4.0943 , Train PPL: 1.0063, Train Acc: 0.4466\n",
      "Epoch: 96, Batch: 600, loss: 4.0524 , Train PPL: 1.0062, Train Acc: 0.4405\n",
      "Epoch: 96, Batch: 700, loss: 4.2891 , Train PPL: 1.0066, Train Acc: 0.3643\n",
      "Validation --- Epoch: 96, total loss: 297.5901 , PPL: 1.2761, Acc: 0.4004\n",
      "lr = 0.0009765625\n",
      "Epoch: 97, Batch: 100, loss: 4.3455 , Train PPL: 1.0066, Train Acc: 0.3750\n",
      "Epoch: 97, Batch: 200, loss: 4.2573 , Train PPL: 1.0065, Train Acc: 0.3857\n",
      "Epoch: 97, Batch: 300, loss: 4.0306 , Train PPL: 1.0062, Train Acc: 0.4405\n",
      "Epoch: 97, Batch: 400, loss: 4.1667 , Train PPL: 1.0064, Train Acc: 0.4085\n",
      "Epoch: 97, Batch: 500, loss: 4.0565 , Train PPL: 1.0062, Train Acc: 0.4146\n",
      "Epoch: 97, Batch: 600, loss: 4.1329 , Train PPL: 1.0063, Train Acc: 0.4223\n",
      "Epoch: 97, Batch: 700, loss: 4.1525 , Train PPL: 1.0064, Train Acc: 0.4345\n",
      "Validation --- Epoch: 97, total loss: 297.5872 , PPL: 1.2761, Acc: 0.4003\n",
      "lr = 0.0009765625\n",
      "Epoch: 98, Batch: 100, loss: 3.9933 , Train PPL: 1.0061, Train Acc: 0.4299\n",
      "Epoch: 98, Batch: 200, loss: 4.2719 , Train PPL: 1.0065, Train Acc: 0.4101\n",
      "Epoch: 98, Batch: 300, loss: 4.4160 , Train PPL: 1.0068, Train Acc: 0.3704\n",
      "Epoch: 98, Batch: 400, loss: 4.1754 , Train PPL: 1.0064, Train Acc: 0.3857\n",
      "Epoch: 98, Batch: 500, loss: 4.4301 , Train PPL: 1.0068, Train Acc: 0.4345\n",
      "Epoch: 98, Batch: 600, loss: 4.4848 , Train PPL: 1.0069, Train Acc: 0.3110\n",
      "Epoch: 98, Batch: 700, loss: 4.0998 , Train PPL: 1.0063, Train Acc: 0.3765\n",
      "Validation --- Epoch: 98, total loss: 297.5894 , PPL: 1.2761, Acc: 0.4004\n",
      "lr = 0.0009765625\n",
      "Epoch: 99, Batch: 100, loss: 4.2073 , Train PPL: 1.0064, Train Acc: 0.4116\n",
      "Epoch: 99, Batch: 200, loss: 3.9191 , Train PPL: 1.0060, Train Acc: 0.4619\n",
      "Epoch: 99, Batch: 300, loss: 4.1676 , Train PPL: 1.0064, Train Acc: 0.4253\n",
      "Epoch: 99, Batch: 400, loss: 4.1652 , Train PPL: 1.0064, Train Acc: 0.3780\n",
      "Epoch: 99, Batch: 500, loss: 4.0112 , Train PPL: 1.0061, Train Acc: 0.4543\n",
      "Epoch: 99, Batch: 600, loss: 4.1350 , Train PPL: 1.0063, Train Acc: 0.3979\n",
      "Epoch: 99, Batch: 700, loss: 4.1053 , Train PPL: 1.0063, Train Acc: 0.4177\n",
      "Validation --- Epoch: 99, total loss: 297.5904 , PPL: 1.2761, Acc: 0.4003\n",
      "lr = 0.00048828125\n"
     ]
    }
   ],
   "source": [
    "model = TCN(5, [600,600,600,600], kernel=2, dropout=0.5, embedding_size = 600, n_words = n_words,tied=True)\n",
    "model.cuda()\n",
    "model_parameters = filter(lambda x: x.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print('Network with {} parameters'.format(params))\n",
    "print('Receptive field of network is {}'.format(model.receptive_field))\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode=\"min\",patience=5,min_lr=1e-6,factor=0.5)\n",
    "\n",
    "\n",
    "best_vloss = 1e8\n",
    "for e in range(100):\n",
    "    train_loop(e)\n",
    "    validation_loss = validation_loop(e)\n",
    "    scheduler.step(validation_loss)\n",
    "    print('lr = {}'.format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "    if validation_loss < best_vloss:\n",
    "                with open(\"model_5_layers.pt\", 'wb') as f:\n",
    "                    print('wrote model')\n",
    "                    torch.save(model, f)\n",
    "                best_vloss = validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network with 12497801 parameters\n",
      "Receptive field of network is 62\n",
      "Epoch: 0, Batch: 100, loss: 6.6985 , Train PPL: 1.0103, Train Acc: 0.1570\n",
      "Epoch: 0, Batch: 200, loss: 6.2922 , Train PPL: 1.0096, Train Acc: 0.1601\n",
      "Epoch: 0, Batch: 300, loss: 6.3089 , Train PPL: 1.0097, Train Acc: 0.1829\n",
      "Epoch: 0, Batch: 400, loss: 6.2825 , Train PPL: 1.0096, Train Acc: 0.2180\n",
      "Epoch: 0, Batch: 500, loss: 5.9894 , Train PPL: 1.0092, Train Acc: 0.2226\n",
      "Epoch: 0, Batch: 600, loss: 5.4370 , Train PPL: 1.0083, Train Acc: 0.3003\n",
      "Epoch: 0, Batch: 700, loss: 5.8265 , Train PPL: 1.0089, Train Acc: 0.2866\n",
      "Validation --- Epoch: 0, total loss: 332.0558 , PPL: 1.3110, Acc: 0.2905\n",
      "lr = 4\n",
      "wrote model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type TCN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type TC_block. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Chomp1d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 100, loss: 6.1590 , Train PPL: 1.0094, Train Acc: 0.2424\n",
      "Epoch: 1, Batch: 200, loss: 5.8487 , Train PPL: 1.0090, Train Acc: 0.2271\n",
      "Epoch: 1, Batch: 300, loss: 5.8331 , Train PPL: 1.0089, Train Acc: 0.2591\n",
      "Epoch: 1, Batch: 400, loss: 5.5738 , Train PPL: 1.0085, Train Acc: 0.2759\n",
      "Epoch: 1, Batch: 500, loss: 5.5675 , Train PPL: 1.0085, Train Acc: 0.3430\n",
      "Epoch: 1, Batch: 600, loss: 6.0445 , Train PPL: 1.0093, Train Acc: 0.2348\n",
      "Epoch: 1, Batch: 700, loss: 5.8154 , Train PPL: 1.0089, Train Acc: 0.2424\n",
      "Validation --- Epoch: 1, total loss: 320.2897 , PPL: 1.2984, Acc: 0.3085\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 2, Batch: 100, loss: 5.4596 , Train PPL: 1.0084, Train Acc: 0.2942\n",
      "Epoch: 2, Batch: 200, loss: 5.8151 , Train PPL: 1.0089, Train Acc: 0.2591\n",
      "Epoch: 2, Batch: 300, loss: 5.8196 , Train PPL: 1.0089, Train Acc: 0.2607\n",
      "Epoch: 2, Batch: 400, loss: 5.7728 , Train PPL: 1.0088, Train Acc: 0.2729\n",
      "Epoch: 2, Batch: 500, loss: 5.5933 , Train PPL: 1.0086, Train Acc: 0.2835\n",
      "Epoch: 2, Batch: 600, loss: 5.4302 , Train PPL: 1.0083, Train Acc: 0.3323\n",
      "Epoch: 2, Batch: 700, loss: 5.5387 , Train PPL: 1.0085, Train Acc: 0.3049\n",
      "Validation --- Epoch: 2, total loss: 316.5679 , PPL: 1.2949, Acc: 0.2983\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 3, Batch: 100, loss: 5.5743 , Train PPL: 1.0085, Train Acc: 0.2927\n",
      "Epoch: 3, Batch: 200, loss: 5.3123 , Train PPL: 1.0081, Train Acc: 0.3445\n",
      "Epoch: 3, Batch: 300, loss: 5.3270 , Train PPL: 1.0082, Train Acc: 0.3293\n",
      "Epoch: 3, Batch: 400, loss: 5.5545 , Train PPL: 1.0085, Train Acc: 0.2759\n",
      "Epoch: 3, Batch: 500, loss: 5.4041 , Train PPL: 1.0083, Train Acc: 0.2942\n",
      "Epoch: 3, Batch: 600, loss: 5.3815 , Train PPL: 1.0082, Train Acc: 0.2912\n",
      "Epoch: 3, Batch: 700, loss: 5.4131 , Train PPL: 1.0083, Train Acc: 0.3369\n",
      "Validation --- Epoch: 3, total loss: 312.7291 , PPL: 1.2908, Acc: 0.3200\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 4, Batch: 100, loss: 5.3948 , Train PPL: 1.0083, Train Acc: 0.3247\n",
      "Epoch: 4, Batch: 200, loss: 5.4288 , Train PPL: 1.0083, Train Acc: 0.3308\n",
      "Epoch: 4, Batch: 300, loss: 5.2398 , Train PPL: 1.0080, Train Acc: 0.3186\n",
      "Epoch: 4, Batch: 400, loss: 5.0808 , Train PPL: 1.0078, Train Acc: 0.3521\n",
      "Epoch: 4, Batch: 500, loss: 5.5176 , Train PPL: 1.0084, Train Acc: 0.3201\n",
      "Epoch: 4, Batch: 600, loss: 5.5430 , Train PPL: 1.0085, Train Acc: 0.3095\n",
      "Epoch: 4, Batch: 700, loss: 4.9655 , Train PPL: 1.0076, Train Acc: 0.4131\n",
      "Validation --- Epoch: 4, total loss: 312.1159 , PPL: 1.2905, Acc: 0.3145\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 5, Batch: 100, loss: 5.2459 , Train PPL: 1.0080, Train Acc: 0.3308\n",
      "Epoch: 5, Batch: 200, loss: 5.3114 , Train PPL: 1.0081, Train Acc: 0.3384\n",
      "Epoch: 5, Batch: 300, loss: 5.5026 , Train PPL: 1.0084, Train Acc: 0.2957\n",
      "Epoch: 5, Batch: 400, loss: 5.4601 , Train PPL: 1.0084, Train Acc: 0.3323\n",
      "Epoch: 5, Batch: 500, loss: 5.0749 , Train PPL: 1.0078, Train Acc: 0.3095\n",
      "Epoch: 5, Batch: 600, loss: 5.3466 , Train PPL: 1.0082, Train Acc: 0.3171\n",
      "Epoch: 5, Batch: 700, loss: 5.3199 , Train PPL: 1.0081, Train Acc: 0.3232\n",
      "Validation --- Epoch: 5, total loss: 309.3274 , PPL: 1.2875, Acc: 0.3072\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 6, Batch: 100, loss: 5.2050 , Train PPL: 1.0080, Train Acc: 0.3399\n",
      "Epoch: 6, Batch: 200, loss: 5.1058 , Train PPL: 1.0078, Train Acc: 0.3186\n",
      "Epoch: 6, Batch: 300, loss: 5.1068 , Train PPL: 1.0078, Train Acc: 0.3765\n",
      "Epoch: 6, Batch: 400, loss: 5.0322 , Train PPL: 1.0077, Train Acc: 0.3628\n",
      "Epoch: 6, Batch: 500, loss: 4.9852 , Train PPL: 1.0076, Train Acc: 0.3354\n",
      "Epoch: 6, Batch: 600, loss: 5.3205 , Train PPL: 1.0081, Train Acc: 0.3369\n",
      "Epoch: 6, Batch: 700, loss: 4.9298 , Train PPL: 1.0075, Train Acc: 0.3613\n",
      "Validation --- Epoch: 6, total loss: 308.2814 , PPL: 1.2865, Acc: 0.3041\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 7, Batch: 100, loss: 5.2054 , Train PPL: 1.0080, Train Acc: 0.3399\n",
      "Epoch: 7, Batch: 200, loss: 5.0225 , Train PPL: 1.0077, Train Acc: 0.3704\n",
      "Epoch: 7, Batch: 300, loss: 5.1396 , Train PPL: 1.0079, Train Acc: 0.3186\n",
      "Epoch: 7, Batch: 400, loss: 5.2242 , Train PPL: 1.0080, Train Acc: 0.3399\n",
      "Epoch: 7, Batch: 500, loss: 5.5251 , Train PPL: 1.0085, Train Acc: 0.2744\n",
      "Epoch: 7, Batch: 600, loss: 5.1176 , Train PPL: 1.0078, Train Acc: 0.3476\n",
      "Epoch: 7, Batch: 700, loss: 5.2923 , Train PPL: 1.0081, Train Acc: 0.3186\n",
      "Validation --- Epoch: 7, total loss: 308.8668 , PPL: 1.2872, Acc: 0.3022\n",
      "lr = 4\n",
      "Epoch: 8, Batch: 100, loss: 5.0108 , Train PPL: 1.0077, Train Acc: 0.3369\n",
      "Epoch: 8, Batch: 200, loss: 5.1731 , Train PPL: 1.0079, Train Acc: 0.3552\n",
      "Epoch: 8, Batch: 300, loss: 5.2064 , Train PPL: 1.0080, Train Acc: 0.3796\n",
      "Epoch: 8, Batch: 400, loss: 5.1768 , Train PPL: 1.0079, Train Acc: 0.3003\n",
      "Epoch: 8, Batch: 500, loss: 5.4933 , Train PPL: 1.0084, Train Acc: 0.3293\n",
      "Epoch: 8, Batch: 600, loss: 5.4063 , Train PPL: 1.0083, Train Acc: 0.2881\n",
      "Epoch: 8, Batch: 700, loss: 5.3014 , Train PPL: 1.0081, Train Acc: 0.3140\n",
      "Validation --- Epoch: 8, total loss: 305.1636 , PPL: 1.2833, Acc: 0.3230\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 9, Batch: 100, loss: 5.1242 , Train PPL: 1.0078, Train Acc: 0.3293\n",
      "Epoch: 9, Batch: 200, loss: 5.0599 , Train PPL: 1.0077, Train Acc: 0.3659\n",
      "Epoch: 9, Batch: 300, loss: 5.1371 , Train PPL: 1.0079, Train Acc: 0.3125\n",
      "Epoch: 9, Batch: 400, loss: 5.1084 , Train PPL: 1.0078, Train Acc: 0.2988\n",
      "Epoch: 9, Batch: 500, loss: 4.9836 , Train PPL: 1.0076, Train Acc: 0.3598\n",
      "Epoch: 9, Batch: 600, loss: 4.6831 , Train PPL: 1.0072, Train Acc: 0.3994\n",
      "Epoch: 9, Batch: 700, loss: 5.1557 , Train PPL: 1.0079, Train Acc: 0.3262\n",
      "Validation --- Epoch: 9, total loss: 305.6066 , PPL: 1.2838, Acc: 0.3054\n",
      "lr = 4\n",
      "Epoch: 10, Batch: 100, loss: 4.9364 , Train PPL: 1.0076, Train Acc: 0.3933\n",
      "Epoch: 10, Batch: 200, loss: 5.2092 , Train PPL: 1.0080, Train Acc: 0.3232\n",
      "Epoch: 10, Batch: 300, loss: 5.0701 , Train PPL: 1.0078, Train Acc: 0.3521\n",
      "Epoch: 10, Batch: 400, loss: 5.0443 , Train PPL: 1.0077, Train Acc: 0.3613\n",
      "Epoch: 10, Batch: 500, loss: 5.3122 , Train PPL: 1.0081, Train Acc: 0.3338\n",
      "Epoch: 10, Batch: 600, loss: 4.8988 , Train PPL: 1.0075, Train Acc: 0.3598\n",
      "Epoch: 10, Batch: 700, loss: 5.0337 , Train PPL: 1.0077, Train Acc: 0.3125\n",
      "Validation --- Epoch: 10, total loss: 305.3186 , PPL: 1.2838, Acc: 0.3210\n",
      "lr = 4\n",
      "Epoch: 11, Batch: 100, loss: 4.8229 , Train PPL: 1.0074, Train Acc: 0.3293\n",
      "Epoch: 11, Batch: 200, loss: 5.0919 , Train PPL: 1.0078, Train Acc: 0.3567\n",
      "Epoch: 11, Batch: 300, loss: 4.6205 , Train PPL: 1.0071, Train Acc: 0.4024\n",
      "Epoch: 11, Batch: 400, loss: 4.9166 , Train PPL: 1.0075, Train Acc: 0.3369\n",
      "Epoch: 11, Batch: 500, loss: 4.7316 , Train PPL: 1.0072, Train Acc: 0.4055\n",
      "Epoch: 11, Batch: 600, loss: 4.9382 , Train PPL: 1.0076, Train Acc: 0.3155\n",
      "Epoch: 11, Batch: 700, loss: 5.0484 , Train PPL: 1.0077, Train Acc: 0.3140\n",
      "Validation --- Epoch: 11, total loss: 305.7405 , PPL: 1.2842, Acc: 0.3128\n",
      "lr = 4\n",
      "Epoch: 12, Batch: 100, loss: 5.0041 , Train PPL: 1.0077, Train Acc: 0.3354\n",
      "Epoch: 12, Batch: 200, loss: 4.7120 , Train PPL: 1.0072, Train Acc: 0.3659\n",
      "Epoch: 12, Batch: 300, loss: 5.1066 , Train PPL: 1.0078, Train Acc: 0.3171\n",
      "Epoch: 12, Batch: 400, loss: 4.7877 , Train PPL: 1.0073, Train Acc: 0.3826\n",
      "Epoch: 12, Batch: 500, loss: 5.0397 , Train PPL: 1.0077, Train Acc: 0.3720\n",
      "Epoch: 12, Batch: 600, loss: 4.9920 , Train PPL: 1.0076, Train Acc: 0.3857\n",
      "Epoch: 12, Batch: 700, loss: 5.0270 , Train PPL: 1.0077, Train Acc: 0.3323\n",
      "Validation --- Epoch: 12, total loss: 305.4112 , PPL: 1.2838, Acc: 0.3143\n",
      "lr = 4\n",
      "Epoch: 13, Batch: 100, loss: 5.0368 , Train PPL: 1.0077, Train Acc: 0.2896\n",
      "Epoch: 13, Batch: 200, loss: 4.8997 , Train PPL: 1.0075, Train Acc: 0.3277\n",
      "Epoch: 13, Batch: 300, loss: 4.8877 , Train PPL: 1.0075, Train Acc: 0.3323\n",
      "Epoch: 13, Batch: 400, loss: 4.7700 , Train PPL: 1.0073, Train Acc: 0.3628\n",
      "Epoch: 13, Batch: 500, loss: 4.9855 , Train PPL: 1.0076, Train Acc: 0.3140\n",
      "Epoch: 13, Batch: 600, loss: 4.9411 , Train PPL: 1.0076, Train Acc: 0.3338\n",
      "Epoch: 13, Batch: 700, loss: 4.9324 , Train PPL: 1.0075, Train Acc: 0.3430\n",
      "Validation --- Epoch: 13, total loss: 303.7350 , PPL: 1.2821, Acc: 0.3245\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 14, Batch: 100, loss: 5.0329 , Train PPL: 1.0077, Train Acc: 0.3049\n",
      "Epoch: 14, Batch: 200, loss: 5.1769 , Train PPL: 1.0079, Train Acc: 0.3125\n",
      "Epoch: 14, Batch: 300, loss: 4.8577 , Train PPL: 1.0074, Train Acc: 0.3338\n",
      "Epoch: 14, Batch: 400, loss: 4.8713 , Train PPL: 1.0075, Train Acc: 0.3415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Batch: 500, loss: 4.9122 , Train PPL: 1.0075, Train Acc: 0.3354\n",
      "Epoch: 14, Batch: 600, loss: 4.7655 , Train PPL: 1.0073, Train Acc: 0.3674\n",
      "Epoch: 14, Batch: 700, loss: 4.8263 , Train PPL: 1.0074, Train Acc: 0.3643\n",
      "Validation --- Epoch: 14, total loss: 303.7874 , PPL: 1.2823, Acc: 0.3128\n",
      "lr = 4\n",
      "Epoch: 15, Batch: 100, loss: 4.8351 , Train PPL: 1.0074, Train Acc: 0.3079\n",
      "Epoch: 15, Batch: 200, loss: 4.7545 , Train PPL: 1.0073, Train Acc: 0.3933\n",
      "Epoch: 15, Batch: 300, loss: 5.0739 , Train PPL: 1.0078, Train Acc: 0.3460\n",
      "Epoch: 15, Batch: 400, loss: 5.0020 , Train PPL: 1.0077, Train Acc: 0.3003\n",
      "Epoch: 15, Batch: 500, loss: 4.8689 , Train PPL: 1.0074, Train Acc: 0.3186\n",
      "Epoch: 15, Batch: 600, loss: 5.0323 , Train PPL: 1.0077, Train Acc: 0.2790\n",
      "Epoch: 15, Batch: 700, loss: 4.6444 , Train PPL: 1.0071, Train Acc: 0.4146\n",
      "Validation --- Epoch: 15, total loss: 304.2900 , PPL: 1.2829, Acc: 0.3095\n",
      "lr = 4\n",
      "Epoch: 16, Batch: 100, loss: 4.5593 , Train PPL: 1.0070, Train Acc: 0.4284\n",
      "Epoch: 16, Batch: 200, loss: 5.0161 , Train PPL: 1.0077, Train Acc: 0.3125\n",
      "Epoch: 16, Batch: 300, loss: 4.6847 , Train PPL: 1.0072, Train Acc: 0.3689\n",
      "Epoch: 16, Batch: 400, loss: 5.0762 , Train PPL: 1.0078, Train Acc: 0.3415\n",
      "Epoch: 16, Batch: 500, loss: 5.1160 , Train PPL: 1.0078, Train Acc: 0.2729\n",
      "Epoch: 16, Batch: 600, loss: 4.8494 , Train PPL: 1.0074, Train Acc: 0.3034\n",
      "Epoch: 16, Batch: 700, loss: 5.0150 , Train PPL: 1.0077, Train Acc: 0.3079\n",
      "Validation --- Epoch: 16, total loss: 302.4153 , PPL: 1.2810, Acc: 0.3216\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 17, Batch: 100, loss: 4.4041 , Train PPL: 1.0067, Train Acc: 0.4268\n",
      "Epoch: 17, Batch: 200, loss: 4.5515 , Train PPL: 1.0070, Train Acc: 0.3933\n",
      "Epoch: 17, Batch: 300, loss: 4.4290 , Train PPL: 1.0068, Train Acc: 0.4085\n",
      "Epoch: 17, Batch: 400, loss: 4.9578 , Train PPL: 1.0076, Train Acc: 0.3399\n",
      "Epoch: 17, Batch: 500, loss: 4.6650 , Train PPL: 1.0071, Train Acc: 0.3034\n",
      "Epoch: 17, Batch: 600, loss: 4.9441 , Train PPL: 1.0076, Train Acc: 0.3643\n",
      "Epoch: 17, Batch: 700, loss: 4.8430 , Train PPL: 1.0074, Train Acc: 0.3674\n",
      "Validation --- Epoch: 17, total loss: 302.5901 , PPL: 1.2811, Acc: 0.3279\n",
      "lr = 4\n",
      "Epoch: 18, Batch: 100, loss: 4.5720 , Train PPL: 1.0070, Train Acc: 0.3994\n",
      "Epoch: 18, Batch: 200, loss: 4.9015 , Train PPL: 1.0075, Train Acc: 0.3308\n",
      "Epoch: 18, Batch: 300, loss: 4.6074 , Train PPL: 1.0070, Train Acc: 0.3857\n",
      "Epoch: 18, Batch: 400, loss: 4.8418 , Train PPL: 1.0074, Train Acc: 0.3171\n",
      "Epoch: 18, Batch: 500, loss: 4.8896 , Train PPL: 1.0075, Train Acc: 0.2835\n",
      "Epoch: 18, Batch: 600, loss: 4.9437 , Train PPL: 1.0076, Train Acc: 0.3323\n",
      "Epoch: 18, Batch: 700, loss: 4.9268 , Train PPL: 1.0075, Train Acc: 0.3034\n",
      "Validation --- Epoch: 18, total loss: 302.8065 , PPL: 1.2814, Acc: 0.3133\n",
      "lr = 4\n",
      "Epoch: 19, Batch: 100, loss: 4.6452 , Train PPL: 1.0071, Train Acc: 0.3933\n",
      "Epoch: 19, Batch: 200, loss: 4.6431 , Train PPL: 1.0071, Train Acc: 0.3659\n",
      "Epoch: 19, Batch: 300, loss: 4.6152 , Train PPL: 1.0071, Train Acc: 0.3765\n",
      "Epoch: 19, Batch: 400, loss: 4.8156 , Train PPL: 1.0074, Train Acc: 0.3369\n",
      "Epoch: 19, Batch: 500, loss: 4.9472 , Train PPL: 1.0076, Train Acc: 0.3338\n",
      "Epoch: 19, Batch: 600, loss: 4.6606 , Train PPL: 1.0071, Train Acc: 0.3720\n",
      "Epoch: 19, Batch: 700, loss: 4.8658 , Train PPL: 1.0074, Train Acc: 0.3552\n",
      "Validation --- Epoch: 19, total loss: 304.1112 , PPL: 1.2828, Acc: 0.2833\n",
      "lr = 4\n",
      "Epoch: 20, Batch: 100, loss: 4.7377 , Train PPL: 1.0072, Train Acc: 0.3537\n",
      "Epoch: 20, Batch: 200, loss: 4.6936 , Train PPL: 1.0072, Train Acc: 0.3232\n",
      "Epoch: 20, Batch: 300, loss: 4.8631 , Train PPL: 1.0074, Train Acc: 0.2942\n",
      "Epoch: 20, Batch: 400, loss: 4.5479 , Train PPL: 1.0070, Train Acc: 0.3537\n",
      "Epoch: 20, Batch: 500, loss: 4.7033 , Train PPL: 1.0072, Train Acc: 0.3659\n",
      "Epoch: 20, Batch: 600, loss: 4.7617 , Train PPL: 1.0073, Train Acc: 0.3110\n",
      "Epoch: 20, Batch: 700, loss: 4.8442 , Train PPL: 1.0074, Train Acc: 0.3521\n",
      "Validation --- Epoch: 20, total loss: 301.7118 , PPL: 1.2802, Acc: 0.3311\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 21, Batch: 100, loss: 4.4413 , Train PPL: 1.0068, Train Acc: 0.4345\n",
      "Epoch: 21, Batch: 200, loss: 4.5771 , Train PPL: 1.0070, Train Acc: 0.3491\n",
      "Epoch: 21, Batch: 300, loss: 4.8534 , Train PPL: 1.0074, Train Acc: 0.3079\n",
      "Epoch: 21, Batch: 400, loss: 4.4608 , Train PPL: 1.0068, Train Acc: 0.3994\n",
      "Epoch: 21, Batch: 500, loss: 4.7479 , Train PPL: 1.0073, Train Acc: 0.3628\n",
      "Epoch: 21, Batch: 600, loss: 4.9659 , Train PPL: 1.0076, Train Acc: 0.3659\n",
      "Epoch: 21, Batch: 700, loss: 4.8218 , Train PPL: 1.0074, Train Acc: 0.3338\n",
      "Validation --- Epoch: 21, total loss: 302.5915 , PPL: 1.2813, Acc: 0.3213\n",
      "lr = 4\n",
      "Epoch: 22, Batch: 100, loss: 4.1596 , Train PPL: 1.0064, Train Acc: 0.4345\n",
      "Epoch: 22, Batch: 200, loss: 4.6750 , Train PPL: 1.0072, Train Acc: 0.3460\n",
      "Epoch: 22, Batch: 300, loss: 4.7498 , Train PPL: 1.0073, Train Acc: 0.3506\n",
      "Epoch: 22, Batch: 400, loss: 4.7656 , Train PPL: 1.0073, Train Acc: 0.3095\n",
      "Epoch: 22, Batch: 500, loss: 4.6861 , Train PPL: 1.0072, Train Acc: 0.3552\n",
      "Epoch: 22, Batch: 600, loss: 4.9163 , Train PPL: 1.0075, Train Acc: 0.3186\n",
      "Epoch: 22, Batch: 700, loss: 4.4528 , Train PPL: 1.0068, Train Acc: 0.3598\n",
      "Validation --- Epoch: 22, total loss: 302.3601 , PPL: 1.2810, Acc: 0.3303\n",
      "lr = 4\n",
      "Epoch: 23, Batch: 100, loss: 4.4698 , Train PPL: 1.0068, Train Acc: 0.3841\n",
      "Epoch: 23, Batch: 200, loss: 4.5927 , Train PPL: 1.0070, Train Acc: 0.3567\n",
      "Epoch: 23, Batch: 300, loss: 4.5381 , Train PPL: 1.0069, Train Acc: 0.3491\n",
      "Epoch: 23, Batch: 400, loss: 4.6260 , Train PPL: 1.0071, Train Acc: 0.3750\n",
      "Epoch: 23, Batch: 500, loss: 4.6439 , Train PPL: 1.0071, Train Acc: 0.3735\n",
      "Epoch: 23, Batch: 600, loss: 4.4589 , Train PPL: 1.0068, Train Acc: 0.4040\n",
      "Epoch: 23, Batch: 700, loss: 4.6616 , Train PPL: 1.0071, Train Acc: 0.3018\n",
      "Validation --- Epoch: 23, total loss: 300.8931 , PPL: 1.2794, Acc: 0.3342\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 24, Batch: 100, loss: 4.3144 , Train PPL: 1.0066, Train Acc: 0.3887\n",
      "Epoch: 24, Batch: 200, loss: 4.7963 , Train PPL: 1.0073, Train Acc: 0.3049\n",
      "Epoch: 24, Batch: 300, loss: 4.8461 , Train PPL: 1.0074, Train Acc: 0.3003\n",
      "Epoch: 24, Batch: 400, loss: 4.5624 , Train PPL: 1.0070, Train Acc: 0.3659\n",
      "Epoch: 24, Batch: 500, loss: 4.7412 , Train PPL: 1.0073, Train Acc: 0.3095\n",
      "Epoch: 24, Batch: 600, loss: 4.9171 , Train PPL: 1.0075, Train Acc: 0.3399\n",
      "Epoch: 24, Batch: 700, loss: 4.6441 , Train PPL: 1.0071, Train Acc: 0.3506\n",
      "Validation --- Epoch: 24, total loss: 302.2648 , PPL: 1.2808, Acc: 0.3182\n",
      "lr = 4\n",
      "Epoch: 25, Batch: 100, loss: 4.3232 , Train PPL: 1.0066, Train Acc: 0.4527\n",
      "Epoch: 25, Batch: 200, loss: 4.5568 , Train PPL: 1.0070, Train Acc: 0.3811\n",
      "Epoch: 25, Batch: 300, loss: 4.6368 , Train PPL: 1.0071, Train Acc: 0.3979\n",
      "Epoch: 25, Batch: 400, loss: 4.4336 , Train PPL: 1.0068, Train Acc: 0.3933\n",
      "Epoch: 25, Batch: 500, loss: 4.7149 , Train PPL: 1.0072, Train Acc: 0.3826\n",
      "Epoch: 25, Batch: 600, loss: 4.4731 , Train PPL: 1.0068, Train Acc: 0.3857\n",
      "Epoch: 25, Batch: 700, loss: 4.5903 , Train PPL: 1.0070, Train Acc: 0.3780\n",
      "Validation --- Epoch: 25, total loss: 301.0981 , PPL: 1.2796, Acc: 0.3354\n",
      "lr = 4\n",
      "Epoch: 26, Batch: 100, loss: 4.4957 , Train PPL: 1.0069, Train Acc: 0.4101\n",
      "Epoch: 26, Batch: 200, loss: 4.4639 , Train PPL: 1.0068, Train Acc: 0.4207\n",
      "Epoch: 26, Batch: 300, loss: 4.3582 , Train PPL: 1.0067, Train Acc: 0.4268\n",
      "Epoch: 26, Batch: 400, loss: 4.6537 , Train PPL: 1.0071, Train Acc: 0.3567\n",
      "Epoch: 26, Batch: 500, loss: 4.5454 , Train PPL: 1.0070, Train Acc: 0.3811\n",
      "Epoch: 26, Batch: 600, loss: 4.4650 , Train PPL: 1.0068, Train Acc: 0.3613\n",
      "Epoch: 26, Batch: 700, loss: 4.7136 , Train PPL: 1.0072, Train Acc: 0.3521\n",
      "Validation --- Epoch: 26, total loss: 301.1569 , PPL: 1.2798, Acc: 0.3287\n",
      "lr = 4\n",
      "Epoch: 27, Batch: 100, loss: 4.9730 , Train PPL: 1.0076, Train Acc: 0.2759\n",
      "Epoch: 27, Batch: 200, loss: 4.3657 , Train PPL: 1.0067, Train Acc: 0.4009\n",
      "Epoch: 27, Batch: 300, loss: 4.5154 , Train PPL: 1.0069, Train Acc: 0.3491\n",
      "Epoch: 27, Batch: 400, loss: 4.6566 , Train PPL: 1.0071, Train Acc: 0.3384\n",
      "Epoch: 27, Batch: 500, loss: 4.7841 , Train PPL: 1.0073, Train Acc: 0.3323\n",
      "Epoch: 27, Batch: 600, loss: 4.8174 , Train PPL: 1.0074, Train Acc: 0.3018\n",
      "Epoch: 27, Batch: 700, loss: 4.9000 , Train PPL: 1.0075, Train Acc: 0.3003\n",
      "Validation --- Epoch: 27, total loss: 301.7125 , PPL: 1.2804, Acc: 0.3302\n",
      "lr = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Batch: 100, loss: 4.3584 , Train PPL: 1.0067, Train Acc: 0.3857\n",
      "Epoch: 28, Batch: 200, loss: 4.4723 , Train PPL: 1.0068, Train Acc: 0.3521\n",
      "Epoch: 28, Batch: 300, loss: 4.4491 , Train PPL: 1.0068, Train Acc: 0.3582\n",
      "Epoch: 28, Batch: 400, loss: 4.7258 , Train PPL: 1.0072, Train Acc: 0.3552\n",
      "Epoch: 28, Batch: 500, loss: 4.7691 , Train PPL: 1.0073, Train Acc: 0.3247\n",
      "Epoch: 28, Batch: 600, loss: 4.5074 , Train PPL: 1.0069, Train Acc: 0.3933\n",
      "Epoch: 28, Batch: 700, loss: 4.7583 , Train PPL: 1.0073, Train Acc: 0.4101\n",
      "Validation --- Epoch: 28, total loss: 301.6432 , PPL: 1.2804, Acc: 0.3406\n",
      "lr = 4\n",
      "Epoch: 29, Batch: 100, loss: 4.6565 , Train PPL: 1.0071, Train Acc: 0.3323\n",
      "Epoch: 29, Batch: 200, loss: 4.3978 , Train PPL: 1.0067, Train Acc: 0.3735\n",
      "Epoch: 29, Batch: 300, loss: 4.5735 , Train PPL: 1.0070, Train Acc: 0.3216\n",
      "Epoch: 29, Batch: 400, loss: 4.6734 , Train PPL: 1.0071, Train Acc: 0.3247\n",
      "Epoch: 29, Batch: 500, loss: 4.5150 , Train PPL: 1.0069, Train Acc: 0.4375\n",
      "Epoch: 29, Batch: 600, loss: 4.5652 , Train PPL: 1.0070, Train Acc: 0.3872\n",
      "Epoch: 29, Batch: 700, loss: 4.5828 , Train PPL: 1.0070, Train Acc: 0.3201\n",
      "Validation --- Epoch: 29, total loss: 302.1019 , PPL: 1.2807, Acc: 0.3295\n",
      "lr = 2.0\n",
      "Epoch: 30, Batch: 100, loss: 4.7747 , Train PPL: 1.0073, Train Acc: 0.3537\n",
      "Epoch: 30, Batch: 200, loss: 4.4014 , Train PPL: 1.0067, Train Acc: 0.4131\n",
      "Epoch: 30, Batch: 300, loss: 4.3788 , Train PPL: 1.0067, Train Acc: 0.3826\n",
      "Epoch: 30, Batch: 400, loss: 4.4709 , Train PPL: 1.0068, Train Acc: 0.4024\n",
      "Epoch: 30, Batch: 500, loss: 4.3748 , Train PPL: 1.0067, Train Acc: 0.4177\n",
      "Epoch: 30, Batch: 600, loss: 4.5831 , Train PPL: 1.0070, Train Acc: 0.3902\n",
      "Epoch: 30, Batch: 700, loss: 4.3623 , Train PPL: 1.0067, Train Acc: 0.4146\n",
      "Validation --- Epoch: 30, total loss: 298.1917 , PPL: 1.2768, Acc: 0.3573\n",
      "lr = 2.0\n",
      "wrote model\n",
      "Epoch: 31, Batch: 100, loss: 4.3218 , Train PPL: 1.0066, Train Acc: 0.3552\n",
      "Epoch: 31, Batch: 200, loss: 4.5487 , Train PPL: 1.0070, Train Acc: 0.3552\n",
      "Epoch: 31, Batch: 300, loss: 4.4340 , Train PPL: 1.0068, Train Acc: 0.3887\n",
      "Epoch: 31, Batch: 400, loss: 4.3296 , Train PPL: 1.0066, Train Acc: 0.3826\n",
      "Epoch: 31, Batch: 500, loss: 4.3183 , Train PPL: 1.0066, Train Acc: 0.4177\n",
      "Epoch: 31, Batch: 600, loss: 4.4550 , Train PPL: 1.0068, Train Acc: 0.3567\n",
      "Epoch: 31, Batch: 700, loss: 4.1753 , Train PPL: 1.0064, Train Acc: 0.4497\n",
      "Validation --- Epoch: 31, total loss: 297.6779 , PPL: 1.2765, Acc: 0.3771\n",
      "lr = 2.0\n",
      "wrote model\n",
      "Epoch: 32, Batch: 100, loss: 4.5752 , Train PPL: 1.0070, Train Acc: 0.3552\n",
      "Epoch: 32, Batch: 200, loss: 4.2809 , Train PPL: 1.0065, Train Acc: 0.4085\n",
      "Epoch: 32, Batch: 300, loss: 4.5243 , Train PPL: 1.0069, Train Acc: 0.3323\n",
      "Epoch: 32, Batch: 400, loss: 4.5033 , Train PPL: 1.0069, Train Acc: 0.3765\n",
      "Epoch: 32, Batch: 500, loss: 4.5323 , Train PPL: 1.0069, Train Acc: 0.3491\n",
      "Epoch: 32, Batch: 600, loss: 4.5357 , Train PPL: 1.0069, Train Acc: 0.3430\n",
      "Epoch: 32, Batch: 700, loss: 4.4149 , Train PPL: 1.0068, Train Acc: 0.3567\n",
      "Validation --- Epoch: 32, total loss: 296.9085 , PPL: 1.2754, Acc: 0.3751\n",
      "lr = 2.0\n",
      "wrote model\n",
      "Epoch: 33, Batch: 100, loss: 4.0207 , Train PPL: 1.0061, Train Acc: 0.4162\n",
      "Epoch: 33, Batch: 200, loss: 4.4236 , Train PPL: 1.0068, Train Acc: 0.3201\n",
      "Epoch: 33, Batch: 300, loss: 4.3618 , Train PPL: 1.0067, Train Acc: 0.3689\n",
      "Epoch: 33, Batch: 400, loss: 4.5864 , Train PPL: 1.0070, Train Acc: 0.3186\n",
      "Epoch: 33, Batch: 500, loss: 4.3297 , Train PPL: 1.0066, Train Acc: 0.3902\n",
      "Epoch: 33, Batch: 600, loss: 4.4521 , Train PPL: 1.0068, Train Acc: 0.3552\n",
      "Epoch: 33, Batch: 700, loss: 4.2026 , Train PPL: 1.0064, Train Acc: 0.4451\n",
      "Validation --- Epoch: 33, total loss: 297.1776 , PPL: 1.2759, Acc: 0.3771\n",
      "lr = 2.0\n",
      "Epoch: 34, Batch: 100, loss: 4.4245 , Train PPL: 1.0068, Train Acc: 0.3415\n",
      "Epoch: 34, Batch: 200, loss: 4.2803 , Train PPL: 1.0065, Train Acc: 0.3948\n",
      "Epoch: 34, Batch: 300, loss: 4.0202 , Train PPL: 1.0061, Train Acc: 0.4634\n",
      "Epoch: 34, Batch: 400, loss: 4.2244 , Train PPL: 1.0065, Train Acc: 0.4375\n",
      "Epoch: 34, Batch: 500, loss: 4.2542 , Train PPL: 1.0065, Train Acc: 0.3872\n",
      "Epoch: 34, Batch: 600, loss: 4.3450 , Train PPL: 1.0066, Train Acc: 0.3659\n",
      "Epoch: 34, Batch: 700, loss: 4.4787 , Train PPL: 1.0069, Train Acc: 0.3628\n",
      "Validation --- Epoch: 34, total loss: 297.1621 , PPL: 1.2758, Acc: 0.3812\n",
      "lr = 2.0\n",
      "Epoch: 35, Batch: 100, loss: 4.2942 , Train PPL: 1.0066, Train Acc: 0.3826\n",
      "Epoch: 35, Batch: 200, loss: 4.1709 , Train PPL: 1.0064, Train Acc: 0.4710\n",
      "Epoch: 35, Batch: 300, loss: 4.0644 , Train PPL: 1.0062, Train Acc: 0.4787\n",
      "Epoch: 35, Batch: 400, loss: 4.1455 , Train PPL: 1.0063, Train Acc: 0.3933\n",
      "Epoch: 35, Batch: 500, loss: 4.3865 , Train PPL: 1.0067, Train Acc: 0.3963\n",
      "Epoch: 35, Batch: 600, loss: 4.1895 , Train PPL: 1.0064, Train Acc: 0.4055\n",
      "Epoch: 35, Batch: 700, loss: 4.1001 , Train PPL: 1.0063, Train Acc: 0.3887\n",
      "Validation --- Epoch: 35, total loss: 297.2195 , PPL: 1.2758, Acc: 0.3783\n",
      "lr = 2.0\n",
      "Epoch: 36, Batch: 100, loss: 4.4194 , Train PPL: 1.0068, Train Acc: 0.3659\n",
      "Epoch: 36, Batch: 200, loss: 4.3437 , Train PPL: 1.0066, Train Acc: 0.4085\n",
      "Epoch: 36, Batch: 300, loss: 4.4344 , Train PPL: 1.0068, Train Acc: 0.3902\n",
      "Epoch: 36, Batch: 400, loss: 4.2676 , Train PPL: 1.0065, Train Acc: 0.4070\n",
      "Epoch: 36, Batch: 500, loss: 4.2576 , Train PPL: 1.0065, Train Acc: 0.3887\n",
      "Epoch: 36, Batch: 600, loss: 4.0933 , Train PPL: 1.0063, Train Acc: 0.4024\n",
      "Epoch: 36, Batch: 700, loss: 4.3436 , Train PPL: 1.0066, Train Acc: 0.4284\n",
      "Validation --- Epoch: 36, total loss: 297.4256 , PPL: 1.2760, Acc: 0.3797\n",
      "lr = 2.0\n",
      "Epoch: 37, Batch: 100, loss: 4.4206 , Train PPL: 1.0068, Train Acc: 0.3521\n",
      "Epoch: 37, Batch: 200, loss: 4.2521 , Train PPL: 1.0065, Train Acc: 0.4177\n",
      "Epoch: 37, Batch: 300, loss: 4.2568 , Train PPL: 1.0065, Train Acc: 0.3887\n",
      "Epoch: 37, Batch: 400, loss: 4.4218 , Train PPL: 1.0068, Train Acc: 0.4055\n",
      "Epoch: 37, Batch: 500, loss: 4.1249 , Train PPL: 1.0063, Train Acc: 0.4024\n",
      "Epoch: 37, Batch: 600, loss: 4.4494 , Train PPL: 1.0068, Train Acc: 0.3720\n",
      "Epoch: 37, Batch: 700, loss: 4.3750 , Train PPL: 1.0067, Train Acc: 0.3567\n",
      "Validation --- Epoch: 37, total loss: 297.4040 , PPL: 1.2760, Acc: 0.3794\n",
      "lr = 2.0\n",
      "Epoch: 38, Batch: 100, loss: 4.2281 , Train PPL: 1.0065, Train Acc: 0.3948\n",
      "Epoch: 38, Batch: 200, loss: 4.1878 , Train PPL: 1.0064, Train Acc: 0.3979\n",
      "Epoch: 38, Batch: 300, loss: 4.2354 , Train PPL: 1.0065, Train Acc: 0.3963\n",
      "Epoch: 38, Batch: 400, loss: 4.2805 , Train PPL: 1.0065, Train Acc: 0.4055\n",
      "Epoch: 38, Batch: 500, loss: 4.4276 , Train PPL: 1.0068, Train Acc: 0.3552\n",
      "Epoch: 38, Batch: 600, loss: 4.5677 , Train PPL: 1.0070, Train Acc: 0.3338\n",
      "Epoch: 38, Batch: 700, loss: 4.5619 , Train PPL: 1.0070, Train Acc: 0.3399\n",
      "Validation --- Epoch: 38, total loss: 297.0612 , PPL: 1.2757, Acc: 0.3780\n",
      "lr = 1.0\n",
      "Epoch: 39, Batch: 100, loss: 3.9032 , Train PPL: 1.0060, Train Acc: 0.4482\n",
      "Epoch: 39, Batch: 200, loss: 3.9079 , Train PPL: 1.0060, Train Acc: 0.4604\n",
      "Epoch: 39, Batch: 300, loss: 4.2518 , Train PPL: 1.0065, Train Acc: 0.4101\n",
      "Epoch: 39, Batch: 400, loss: 4.3641 , Train PPL: 1.0067, Train Acc: 0.3918\n",
      "Epoch: 39, Batch: 500, loss: 4.4949 , Train PPL: 1.0069, Train Acc: 0.3674\n",
      "Epoch: 39, Batch: 600, loss: 4.2310 , Train PPL: 1.0065, Train Acc: 0.3689\n",
      "Epoch: 39, Batch: 700, loss: 4.2943 , Train PPL: 1.0066, Train Acc: 0.4024\n",
      "Validation --- Epoch: 39, total loss: 296.7123 , PPL: 1.2754, Acc: 0.3891\n",
      "lr = 1.0\n",
      "wrote model\n",
      "Epoch: 40, Batch: 100, loss: 4.3614 , Train PPL: 1.0067, Train Acc: 0.3476\n",
      "Epoch: 40, Batch: 200, loss: 4.5898 , Train PPL: 1.0070, Train Acc: 0.3369\n",
      "Epoch: 40, Batch: 300, loss: 4.5268 , Train PPL: 1.0069, Train Acc: 0.3552\n",
      "Epoch: 40, Batch: 400, loss: 4.0238 , Train PPL: 1.0062, Train Acc: 0.4375\n",
      "Epoch: 40, Batch: 500, loss: 4.5023 , Train PPL: 1.0069, Train Acc: 0.3399\n",
      "Epoch: 40, Batch: 600, loss: 4.2560 , Train PPL: 1.0065, Train Acc: 0.3765\n",
      "Epoch: 40, Batch: 700, loss: 4.2699 , Train PPL: 1.0065, Train Acc: 0.3841\n",
      "Validation --- Epoch: 40, total loss: 296.1678 , PPL: 1.2749, Acc: 0.3920\n",
      "lr = 1.0\n",
      "wrote model\n",
      "Epoch: 41, Batch: 100, loss: 4.2987 , Train PPL: 1.0066, Train Acc: 0.3796\n",
      "Epoch: 41, Batch: 200, loss: 4.5486 , Train PPL: 1.0070, Train Acc: 0.3643\n",
      "Epoch: 41, Batch: 300, loss: 3.9882 , Train PPL: 1.0061, Train Acc: 0.4421\n",
      "Epoch: 41, Batch: 400, loss: 4.3672 , Train PPL: 1.0067, Train Acc: 0.3689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41, Batch: 500, loss: 4.3623 , Train PPL: 1.0067, Train Acc: 0.4238\n",
      "Epoch: 41, Batch: 600, loss: 4.3940 , Train PPL: 1.0067, Train Acc: 0.3689\n",
      "Epoch: 41, Batch: 700, loss: 4.6392 , Train PPL: 1.0071, Train Acc: 0.3338\n",
      "Validation --- Epoch: 41, total loss: 296.4175 , PPL: 1.2752, Acc: 0.3921\n",
      "lr = 1.0\n",
      "Epoch: 42, Batch: 100, loss: 4.3330 , Train PPL: 1.0066, Train Acc: 0.3780\n",
      "Epoch: 42, Batch: 200, loss: 4.2500 , Train PPL: 1.0065, Train Acc: 0.4040\n",
      "Epoch: 42, Batch: 300, loss: 3.5239 , Train PPL: 1.0054, Train Acc: 0.5503\n",
      "Epoch: 42, Batch: 400, loss: 4.0119 , Train PPL: 1.0061, Train Acc: 0.4924\n",
      "Epoch: 42, Batch: 500, loss: 4.3130 , Train PPL: 1.0066, Train Acc: 0.3857\n",
      "Epoch: 42, Batch: 600, loss: 4.4436 , Train PPL: 1.0068, Train Acc: 0.3445\n",
      "Epoch: 42, Batch: 700, loss: 4.0561 , Train PPL: 1.0062, Train Acc: 0.4192\n",
      "Validation --- Epoch: 42, total loss: 296.4901 , PPL: 1.2751, Acc: 0.3884\n",
      "lr = 1.0\n",
      "Epoch: 43, Batch: 100, loss: 4.3157 , Train PPL: 1.0066, Train Acc: 0.3354\n",
      "Epoch: 43, Batch: 200, loss: 4.5045 , Train PPL: 1.0069, Train Acc: 0.3430\n",
      "Epoch: 43, Batch: 300, loss: 4.1920 , Train PPL: 1.0064, Train Acc: 0.4101\n",
      "Epoch: 43, Batch: 400, loss: 4.1321 , Train PPL: 1.0063, Train Acc: 0.4162\n",
      "Epoch: 43, Batch: 500, loss: 4.0673 , Train PPL: 1.0062, Train Acc: 0.4345\n",
      "Epoch: 43, Batch: 600, loss: 4.3499 , Train PPL: 1.0067, Train Acc: 0.3415\n",
      "Epoch: 43, Batch: 700, loss: 4.2560 , Train PPL: 1.0065, Train Acc: 0.3826\n",
      "Validation --- Epoch: 43, total loss: 296.3312 , PPL: 1.2751, Acc: 0.3944\n",
      "lr = 1.0\n",
      "Epoch: 44, Batch: 100, loss: 4.3189 , Train PPL: 1.0066, Train Acc: 0.3415\n",
      "Epoch: 44, Batch: 200, loss: 4.1054 , Train PPL: 1.0063, Train Acc: 0.4619\n",
      "Epoch: 44, Batch: 300, loss: 4.1398 , Train PPL: 1.0063, Train Acc: 0.3613\n",
      "Epoch: 44, Batch: 400, loss: 4.0266 , Train PPL: 1.0062, Train Acc: 0.4070\n",
      "Epoch: 44, Batch: 500, loss: 4.2855 , Train PPL: 1.0066, Train Acc: 0.3918\n",
      "Epoch: 44, Batch: 600, loss: 4.2667 , Train PPL: 1.0065, Train Acc: 0.3552\n",
      "Epoch: 44, Batch: 700, loss: 4.1531 , Train PPL: 1.0064, Train Acc: 0.4024\n",
      "Validation --- Epoch: 44, total loss: 296.8125 , PPL: 1.2755, Acc: 0.3905\n",
      "lr = 1.0\n",
      "Epoch: 45, Batch: 100, loss: 4.1046 , Train PPL: 1.0063, Train Acc: 0.4284\n",
      "Epoch: 45, Batch: 200, loss: 4.0669 , Train PPL: 1.0062, Train Acc: 0.4146\n",
      "Epoch: 45, Batch: 300, loss: 4.3198 , Train PPL: 1.0066, Train Acc: 0.3415\n",
      "Epoch: 45, Batch: 400, loss: 3.8346 , Train PPL: 1.0059, Train Acc: 0.4726\n",
      "Epoch: 45, Batch: 500, loss: 4.1110 , Train PPL: 1.0063, Train Acc: 0.4177\n",
      "Epoch: 45, Batch: 600, loss: 4.1740 , Train PPL: 1.0064, Train Acc: 0.3750\n",
      "Epoch: 45, Batch: 700, loss: 4.4167 , Train PPL: 1.0068, Train Acc: 0.3948\n",
      "Validation --- Epoch: 45, total loss: 296.4268 , PPL: 1.2751, Acc: 0.3898\n",
      "lr = 1.0\n",
      "Epoch: 46, Batch: 100, loss: 4.3906 , Train PPL: 1.0067, Train Acc: 0.3826\n",
      "Epoch: 46, Batch: 200, loss: 4.0349 , Train PPL: 1.0062, Train Acc: 0.4238\n",
      "Epoch: 46, Batch: 300, loss: 4.2615 , Train PPL: 1.0065, Train Acc: 0.3720\n",
      "Epoch: 46, Batch: 400, loss: 4.1320 , Train PPL: 1.0063, Train Acc: 0.4009\n",
      "Epoch: 46, Batch: 500, loss: 4.3690 , Train PPL: 1.0067, Train Acc: 0.3613\n",
      "Epoch: 46, Batch: 600, loss: 4.2327 , Train PPL: 1.0065, Train Acc: 0.3994\n",
      "Epoch: 46, Batch: 700, loss: 4.1245 , Train PPL: 1.0063, Train Acc: 0.4405\n",
      "Validation --- Epoch: 46, total loss: 297.0623 , PPL: 1.2759, Acc: 0.3926\n",
      "lr = 0.5\n",
      "Epoch: 47, Batch: 100, loss: 3.8267 , Train PPL: 1.0059, Train Acc: 0.4787\n",
      "Epoch: 47, Batch: 200, loss: 4.0274 , Train PPL: 1.0062, Train Acc: 0.4588\n",
      "Epoch: 47, Batch: 300, loss: 4.1043 , Train PPL: 1.0063, Train Acc: 0.3933\n",
      "Epoch: 47, Batch: 400, loss: 4.1759 , Train PPL: 1.0064, Train Acc: 0.3506\n",
      "Epoch: 47, Batch: 500, loss: 4.1244 , Train PPL: 1.0063, Train Acc: 0.4299\n",
      "Epoch: 47, Batch: 600, loss: 3.9545 , Train PPL: 1.0060, Train Acc: 0.4162\n",
      "Epoch: 47, Batch: 700, loss: 4.2491 , Train PPL: 1.0065, Train Acc: 0.4299\n",
      "Validation --- Epoch: 47, total loss: 296.7705 , PPL: 1.2756, Acc: 0.3959\n",
      "lr = 0.5\n",
      "Epoch: 48, Batch: 100, loss: 4.2520 , Train PPL: 1.0065, Train Acc: 0.3567\n",
      "Epoch: 48, Batch: 200, loss: 4.1624 , Train PPL: 1.0064, Train Acc: 0.4055\n",
      "Epoch: 48, Batch: 300, loss: 4.4420 , Train PPL: 1.0068, Train Acc: 0.3201\n",
      "Epoch: 48, Batch: 400, loss: 4.1882 , Train PPL: 1.0064, Train Acc: 0.4146\n",
      "Epoch: 48, Batch: 500, loss: 4.1493 , Train PPL: 1.0063, Train Acc: 0.4040\n",
      "Epoch: 48, Batch: 600, loss: 4.1230 , Train PPL: 1.0063, Train Acc: 0.4253\n",
      "Epoch: 48, Batch: 700, loss: 4.1716 , Train PPL: 1.0064, Train Acc: 0.3750\n",
      "Validation --- Epoch: 48, total loss: 296.8712 , PPL: 1.2757, Acc: 0.3967\n",
      "lr = 0.5\n",
      "Epoch: 49, Batch: 100, loss: 4.4184 , Train PPL: 1.0068, Train Acc: 0.3186\n",
      "Epoch: 49, Batch: 200, loss: 4.1896 , Train PPL: 1.0064, Train Acc: 0.3643\n",
      "Epoch: 49, Batch: 300, loss: 4.3021 , Train PPL: 1.0066, Train Acc: 0.4070\n",
      "Epoch: 49, Batch: 400, loss: 4.2269 , Train PPL: 1.0065, Train Acc: 0.3567\n",
      "Epoch: 49, Batch: 500, loss: 4.3640 , Train PPL: 1.0067, Train Acc: 0.3735\n",
      "Epoch: 49, Batch: 600, loss: 4.1144 , Train PPL: 1.0063, Train Acc: 0.4009\n",
      "Epoch: 49, Batch: 700, loss: 3.7611 , Train PPL: 1.0057, Train Acc: 0.5198\n",
      "Validation --- Epoch: 49, total loss: 297.0069 , PPL: 1.2758, Acc: 0.3971\n",
      "lr = 0.5\n",
      "Epoch: 50, Batch: 100, loss: 4.1638 , Train PPL: 1.0064, Train Acc: 0.4085\n",
      "Epoch: 50, Batch: 200, loss: 4.3593 , Train PPL: 1.0067, Train Acc: 0.3659\n",
      "Epoch: 50, Batch: 300, loss: 4.2254 , Train PPL: 1.0065, Train Acc: 0.3857\n",
      "Epoch: 50, Batch: 400, loss: 4.2254 , Train PPL: 1.0065, Train Acc: 0.3765\n",
      "Epoch: 50, Batch: 500, loss: 3.9866 , Train PPL: 1.0061, Train Acc: 0.3857\n",
      "Epoch: 50, Batch: 600, loss: 4.5562 , Train PPL: 1.0070, Train Acc: 0.3262\n",
      "Epoch: 50, Batch: 700, loss: 3.9526 , Train PPL: 1.0060, Train Acc: 0.4665\n",
      "Validation --- Epoch: 50, total loss: 297.0846 , PPL: 1.2760, Acc: 0.3960\n",
      "lr = 0.5\n",
      "Epoch: 51, Batch: 100, loss: 4.1744 , Train PPL: 1.0064, Train Acc: 0.3963\n",
      "Epoch: 51, Batch: 200, loss: 4.2667 , Train PPL: 1.0065, Train Acc: 0.4024\n",
      "Epoch: 51, Batch: 300, loss: 3.6160 , Train PPL: 1.0055, Train Acc: 0.5366\n",
      "Epoch: 51, Batch: 400, loss: 3.9977 , Train PPL: 1.0061, Train Acc: 0.4649\n",
      "Epoch: 51, Batch: 500, loss: 4.2232 , Train PPL: 1.0065, Train Acc: 0.3598\n",
      "Epoch: 51, Batch: 600, loss: 4.1338 , Train PPL: 1.0063, Train Acc: 0.4101\n",
      "Epoch: 51, Batch: 700, loss: 4.2300 , Train PPL: 1.0065, Train Acc: 0.3643\n",
      "Validation --- Epoch: 51, total loss: 297.1760 , PPL: 1.2761, Acc: 0.3992\n",
      "lr = 0.5\n",
      "Epoch: 52, Batch: 100, loss: 4.0042 , Train PPL: 1.0061, Train Acc: 0.4146\n",
      "Epoch: 52, Batch: 200, loss: 4.1622 , Train PPL: 1.0064, Train Acc: 0.4177\n",
      "Epoch: 52, Batch: 300, loss: 3.9947 , Train PPL: 1.0061, Train Acc: 0.4131\n",
      "Epoch: 52, Batch: 400, loss: 4.1465 , Train PPL: 1.0063, Train Acc: 0.4040\n",
      "Epoch: 52, Batch: 500, loss: 4.1589 , Train PPL: 1.0064, Train Acc: 0.3979\n",
      "Epoch: 52, Batch: 600, loss: 4.0854 , Train PPL: 1.0062, Train Acc: 0.3948\n",
      "Epoch: 52, Batch: 700, loss: 4.0036 , Train PPL: 1.0061, Train Acc: 0.4619\n",
      "Validation --- Epoch: 52, total loss: 297.1473 , PPL: 1.2760, Acc: 0.3979\n",
      "lr = 0.25\n",
      "Epoch: 53, Batch: 100, loss: 4.2789 , Train PPL: 1.0065, Train Acc: 0.3338\n",
      "Epoch: 53, Batch: 200, loss: 4.2672 , Train PPL: 1.0065, Train Acc: 0.3902\n",
      "Epoch: 53, Batch: 300, loss: 4.1412 , Train PPL: 1.0063, Train Acc: 0.4284\n",
      "Epoch: 53, Batch: 400, loss: 4.0475 , Train PPL: 1.0062, Train Acc: 0.4436\n",
      "Epoch: 53, Batch: 500, loss: 4.1150 , Train PPL: 1.0063, Train Acc: 0.4314\n",
      "Epoch: 53, Batch: 600, loss: 4.0320 , Train PPL: 1.0062, Train Acc: 0.4451\n",
      "Epoch: 53, Batch: 700, loss: 4.2054 , Train PPL: 1.0064, Train Acc: 0.4436\n",
      "Validation --- Epoch: 53, total loss: 297.2347 , PPL: 1.2761, Acc: 0.4000\n",
      "lr = 0.25\n",
      "Epoch: 54, Batch: 100, loss: 4.2366 , Train PPL: 1.0065, Train Acc: 0.3765\n",
      "Epoch: 54, Batch: 200, loss: 3.9416 , Train PPL: 1.0060, Train Acc: 0.4284\n",
      "Epoch: 54, Batch: 300, loss: 4.0012 , Train PPL: 1.0061, Train Acc: 0.4665\n",
      "Epoch: 54, Batch: 400, loss: 4.3076 , Train PPL: 1.0066, Train Acc: 0.3780\n",
      "Epoch: 54, Batch: 500, loss: 4.0747 , Train PPL: 1.0062, Train Acc: 0.4101\n",
      "Epoch: 54, Batch: 600, loss: 3.9995 , Train PPL: 1.0061, Train Acc: 0.4451\n",
      "Epoch: 54, Batch: 700, loss: 4.3018 , Train PPL: 1.0066, Train Acc: 0.4009\n",
      "Validation --- Epoch: 54, total loss: 297.1257 , PPL: 1.2760, Acc: 0.3985\n",
      "lr = 0.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55, Batch: 100, loss: 4.1387 , Train PPL: 1.0063, Train Acc: 0.4726\n",
      "Epoch: 55, Batch: 200, loss: 3.7647 , Train PPL: 1.0058, Train Acc: 0.4527\n",
      "Epoch: 55, Batch: 300, loss: 4.1869 , Train PPL: 1.0064, Train Acc: 0.3659\n",
      "Epoch: 55, Batch: 400, loss: 4.1730 , Train PPL: 1.0064, Train Acc: 0.4268\n",
      "Epoch: 55, Batch: 500, loss: 4.3303 , Train PPL: 1.0066, Train Acc: 0.3918\n",
      "Epoch: 55, Batch: 600, loss: 4.0653 , Train PPL: 1.0062, Train Acc: 0.4284\n",
      "Epoch: 55, Batch: 700, loss: 4.3785 , Train PPL: 1.0067, Train Acc: 0.3460\n",
      "Validation --- Epoch: 55, total loss: 297.3725 , PPL: 1.2763, Acc: 0.3994\n",
      "lr = 0.25\n",
      "Epoch: 56, Batch: 100, loss: 4.2284 , Train PPL: 1.0065, Train Acc: 0.3796\n",
      "Epoch: 56, Batch: 200, loss: 4.0217 , Train PPL: 1.0061, Train Acc: 0.3857\n",
      "Epoch: 56, Batch: 300, loss: 4.3579 , Train PPL: 1.0067, Train Acc: 0.3674\n",
      "Epoch: 56, Batch: 400, loss: 3.9402 , Train PPL: 1.0060, Train Acc: 0.4238\n",
      "Epoch: 56, Batch: 500, loss: 4.1254 , Train PPL: 1.0063, Train Acc: 0.4238\n",
      "Epoch: 56, Batch: 600, loss: 4.2240 , Train PPL: 1.0065, Train Acc: 0.3887\n",
      "Epoch: 56, Batch: 700, loss: 4.3082 , Train PPL: 1.0066, Train Acc: 0.4085\n",
      "Validation --- Epoch: 56, total loss: 297.3376 , PPL: 1.2762, Acc: 0.3987\n",
      "lr = 0.25\n",
      "Epoch: 57, Batch: 100, loss: 4.2119 , Train PPL: 1.0064, Train Acc: 0.3674\n",
      "Epoch: 57, Batch: 200, loss: 3.9183 , Train PPL: 1.0060, Train Acc: 0.4741\n",
      "Epoch: 57, Batch: 300, loss: 4.4501 , Train PPL: 1.0068, Train Acc: 0.3628\n",
      "Epoch: 57, Batch: 400, loss: 4.1498 , Train PPL: 1.0063, Train Acc: 0.3887\n",
      "Epoch: 57, Batch: 500, loss: 4.1935 , Train PPL: 1.0064, Train Acc: 0.4375\n",
      "Epoch: 57, Batch: 600, loss: 4.2397 , Train PPL: 1.0065, Train Acc: 0.3750\n",
      "Epoch: 57, Batch: 700, loss: 4.1278 , Train PPL: 1.0063, Train Acc: 0.3948\n",
      "Validation --- Epoch: 57, total loss: 297.3700 , PPL: 1.2763, Acc: 0.3999\n",
      "lr = 0.25\n",
      "Epoch: 58, Batch: 100, loss: 3.9679 , Train PPL: 1.0061, Train Acc: 0.4863\n",
      "Epoch: 58, Batch: 200, loss: 4.0516 , Train PPL: 1.0062, Train Acc: 0.4375\n",
      "Epoch: 58, Batch: 300, loss: 3.9347 , Train PPL: 1.0060, Train Acc: 0.4329\n",
      "Epoch: 58, Batch: 400, loss: 3.9594 , Train PPL: 1.0061, Train Acc: 0.4177\n",
      "Epoch: 58, Batch: 500, loss: 4.3694 , Train PPL: 1.0067, Train Acc: 0.3780\n",
      "Epoch: 58, Batch: 600, loss: 4.1478 , Train PPL: 1.0063, Train Acc: 0.4101\n",
      "Epoch: 58, Batch: 700, loss: 4.3030 , Train PPL: 1.0066, Train Acc: 0.4085\n",
      "Validation --- Epoch: 58, total loss: 297.4318 , PPL: 1.2764, Acc: 0.3990\n",
      "lr = 0.125\n",
      "Epoch: 59, Batch: 100, loss: 4.1118 , Train PPL: 1.0063, Train Acc: 0.3765\n",
      "Epoch: 59, Batch: 200, loss: 4.2752 , Train PPL: 1.0065, Train Acc: 0.3902\n",
      "Epoch: 59, Batch: 300, loss: 3.9537 , Train PPL: 1.0060, Train Acc: 0.3933\n",
      "Epoch: 59, Batch: 400, loss: 4.0132 , Train PPL: 1.0061, Train Acc: 0.4162\n",
      "Epoch: 59, Batch: 500, loss: 4.1075 , Train PPL: 1.0063, Train Acc: 0.4253\n",
      "Epoch: 59, Batch: 600, loss: 4.0959 , Train PPL: 1.0063, Train Acc: 0.4192\n",
      "Epoch: 59, Batch: 700, loss: 4.2666 , Train PPL: 1.0065, Train Acc: 0.3399\n",
      "Validation --- Epoch: 59, total loss: 297.4497 , PPL: 1.2764, Acc: 0.4008\n",
      "lr = 0.125\n",
      "Epoch: 60, Batch: 100, loss: 4.2089 , Train PPL: 1.0064, Train Acc: 0.3689\n",
      "Epoch: 60, Batch: 200, loss: 4.2094 , Train PPL: 1.0064, Train Acc: 0.4329\n",
      "Epoch: 60, Batch: 300, loss: 4.1536 , Train PPL: 1.0064, Train Acc: 0.3979\n",
      "Epoch: 60, Batch: 400, loss: 4.1988 , Train PPL: 1.0064, Train Acc: 0.4162\n",
      "Epoch: 60, Batch: 500, loss: 3.7734 , Train PPL: 1.0058, Train Acc: 0.4604\n",
      "Epoch: 60, Batch: 600, loss: 4.1945 , Train PPL: 1.0064, Train Acc: 0.4436\n",
      "Epoch: 60, Batch: 700, loss: 3.9381 , Train PPL: 1.0060, Train Acc: 0.4680\n",
      "Validation --- Epoch: 60, total loss: 297.5190 , PPL: 1.2765, Acc: 0.3998\n",
      "lr = 0.125\n",
      "Epoch: 61, Batch: 100, loss: 3.9552 , Train PPL: 1.0060, Train Acc: 0.4436\n",
      "Epoch: 61, Batch: 200, loss: 3.8805 , Train PPL: 1.0059, Train Acc: 0.4482\n",
      "Epoch: 61, Batch: 300, loss: 4.1078 , Train PPL: 1.0063, Train Acc: 0.4375\n",
      "Epoch: 61, Batch: 400, loss: 3.9038 , Train PPL: 1.0060, Train Acc: 0.4512\n",
      "Epoch: 61, Batch: 500, loss: 4.1916 , Train PPL: 1.0064, Train Acc: 0.3765\n",
      "Epoch: 61, Batch: 600, loss: 4.0264 , Train PPL: 1.0062, Train Acc: 0.4146\n",
      "Epoch: 61, Batch: 700, loss: 4.0662 , Train PPL: 1.0062, Train Acc: 0.4146\n",
      "Validation --- Epoch: 61, total loss: 297.5029 , PPL: 1.2764, Acc: 0.4010\n",
      "lr = 0.125\n",
      "Epoch: 62, Batch: 100, loss: 3.9865 , Train PPL: 1.0061, Train Acc: 0.4253\n",
      "Epoch: 62, Batch: 200, loss: 4.1255 , Train PPL: 1.0063, Train Acc: 0.4558\n",
      "Epoch: 62, Batch: 300, loss: 4.4255 , Train PPL: 1.0068, Train Acc: 0.3430\n",
      "Epoch: 62, Batch: 400, loss: 4.1383 , Train PPL: 1.0063, Train Acc: 0.3887\n",
      "Epoch: 62, Batch: 500, loss: 3.9039 , Train PPL: 1.0060, Train Acc: 0.4832\n",
      "Epoch: 62, Batch: 600, loss: 3.9332 , Train PPL: 1.0060, Train Acc: 0.4497\n",
      "Epoch: 62, Batch: 700, loss: 4.0524 , Train PPL: 1.0062, Train Acc: 0.4314\n",
      "Validation --- Epoch: 62, total loss: 297.4481 , PPL: 1.2764, Acc: 0.4005\n",
      "lr = 0.125\n",
      "Epoch: 63, Batch: 100, loss: 4.3962 , Train PPL: 1.0067, Train Acc: 0.3735\n",
      "Epoch: 63, Batch: 200, loss: 4.3400 , Train PPL: 1.0066, Train Acc: 0.3308\n",
      "Epoch: 63, Batch: 300, loss: 3.7155 , Train PPL: 1.0057, Train Acc: 0.5168\n",
      "Epoch: 63, Batch: 400, loss: 3.7146 , Train PPL: 1.0057, Train Acc: 0.5549\n",
      "Epoch: 63, Batch: 500, loss: 3.9247 , Train PPL: 1.0060, Train Acc: 0.4619\n",
      "Epoch: 63, Batch: 600, loss: 3.9889 , Train PPL: 1.0061, Train Acc: 0.4101\n",
      "Epoch: 63, Batch: 700, loss: 4.4436 , Train PPL: 1.0068, Train Acc: 0.3643\n",
      "Validation --- Epoch: 63, total loss: 297.5161 , PPL: 1.2764, Acc: 0.4005\n",
      "lr = 0.125\n",
      "Epoch: 64, Batch: 100, loss: 4.1040 , Train PPL: 1.0063, Train Acc: 0.4268\n",
      "Epoch: 64, Batch: 200, loss: 3.6759 , Train PPL: 1.0056, Train Acc: 0.4863\n",
      "Epoch: 64, Batch: 300, loss: 4.0866 , Train PPL: 1.0062, Train Acc: 0.3796\n",
      "Epoch: 64, Batch: 400, loss: 3.9458 , Train PPL: 1.0060, Train Acc: 0.4101\n",
      "Epoch: 64, Batch: 500, loss: 3.4317 , Train PPL: 1.0052, Train Acc: 0.5229\n",
      "Epoch: 64, Batch: 600, loss: 3.8816 , Train PPL: 1.0059, Train Acc: 0.4832\n",
      "Epoch: 64, Batch: 700, loss: 4.2802 , Train PPL: 1.0065, Train Acc: 0.4146\n",
      "Validation --- Epoch: 64, total loss: 297.5584 , PPL: 1.2765, Acc: 0.3997\n",
      "lr = 0.0625\n",
      "Epoch: 65, Batch: 100, loss: 4.0833 , Train PPL: 1.0062, Train Acc: 0.3979\n",
      "Epoch: 65, Batch: 200, loss: 4.1201 , Train PPL: 1.0063, Train Acc: 0.4878\n",
      "Epoch: 65, Batch: 300, loss: 4.0010 , Train PPL: 1.0061, Train Acc: 0.4238\n",
      "Epoch: 65, Batch: 400, loss: 4.2174 , Train PPL: 1.0064, Train Acc: 0.3704\n",
      "Epoch: 65, Batch: 500, loss: 4.2662 , Train PPL: 1.0065, Train Acc: 0.3643\n",
      "Epoch: 65, Batch: 600, loss: 3.7492 , Train PPL: 1.0057, Train Acc: 0.4360\n",
      "Epoch: 65, Batch: 700, loss: 4.0096 , Train PPL: 1.0061, Train Acc: 0.4390\n",
      "Validation --- Epoch: 65, total loss: 297.5868 , PPL: 1.2765, Acc: 0.3995\n",
      "lr = 0.0625\n",
      "Epoch: 66, Batch: 100, loss: 4.3488 , Train PPL: 1.0067, Train Acc: 0.3841\n",
      "Epoch: 66, Batch: 200, loss: 4.2820 , Train PPL: 1.0065, Train Acc: 0.3537\n",
      "Epoch: 66, Batch: 300, loss: 4.0719 , Train PPL: 1.0062, Train Acc: 0.4466\n",
      "Epoch: 66, Batch: 400, loss: 4.0496 , Train PPL: 1.0062, Train Acc: 0.4329\n",
      "Epoch: 66, Batch: 500, loss: 4.1538 , Train PPL: 1.0064, Train Acc: 0.4207\n",
      "Epoch: 66, Batch: 600, loss: 3.9436 , Train PPL: 1.0060, Train Acc: 0.3994\n",
      "Epoch: 66, Batch: 700, loss: 4.0622 , Train PPL: 1.0062, Train Acc: 0.4070\n",
      "Validation --- Epoch: 66, total loss: 297.6167 , PPL: 1.2766, Acc: 0.4007\n",
      "lr = 0.0625\n",
      "Epoch: 67, Batch: 100, loss: 4.0586 , Train PPL: 1.0062, Train Acc: 0.3933\n",
      "Epoch: 67, Batch: 200, loss: 4.1710 , Train PPL: 1.0064, Train Acc: 0.3994\n",
      "Epoch: 67, Batch: 300, loss: 4.0521 , Train PPL: 1.0062, Train Acc: 0.4436\n",
      "Epoch: 67, Batch: 400, loss: 3.9463 , Train PPL: 1.0060, Train Acc: 0.4405\n",
      "Epoch: 67, Batch: 500, loss: 4.1736 , Train PPL: 1.0064, Train Acc: 0.4527\n",
      "Epoch: 67, Batch: 600, loss: 3.8974 , Train PPL: 1.0060, Train Acc: 0.4451\n",
      "Epoch: 67, Batch: 700, loss: 4.1769 , Train PPL: 1.0064, Train Acc: 0.4131\n",
      "Validation --- Epoch: 67, total loss: 297.6257 , PPL: 1.2766, Acc: 0.4003\n",
      "lr = 0.0625\n",
      "Epoch: 68, Batch: 100, loss: 4.1678 , Train PPL: 1.0064, Train Acc: 0.3887\n",
      "Epoch: 68, Batch: 200, loss: 4.0726 , Train PPL: 1.0062, Train Acc: 0.3979\n",
      "Epoch: 68, Batch: 300, loss: 4.2206 , Train PPL: 1.0065, Train Acc: 0.3780\n",
      "Epoch: 68, Batch: 400, loss: 4.0302 , Train PPL: 1.0062, Train Acc: 0.4253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68, Batch: 500, loss: 4.3305 , Train PPL: 1.0066, Train Acc: 0.3643\n",
      "Epoch: 68, Batch: 600, loss: 4.0233 , Train PPL: 1.0062, Train Acc: 0.4177\n",
      "Epoch: 68, Batch: 700, loss: 3.7433 , Train PPL: 1.0057, Train Acc: 0.5396\n",
      "Validation --- Epoch: 68, total loss: 297.6108 , PPL: 1.2765, Acc: 0.4002\n",
      "lr = 0.0625\n",
      "Epoch: 69, Batch: 100, loss: 3.9697 , Train PPL: 1.0061, Train Acc: 0.4421\n",
      "Epoch: 69, Batch: 200, loss: 4.2754 , Train PPL: 1.0065, Train Acc: 0.3902\n",
      "Epoch: 69, Batch: 300, loss: 3.9463 , Train PPL: 1.0060, Train Acc: 0.4284\n",
      "Epoch: 69, Batch: 400, loss: 4.0461 , Train PPL: 1.0062, Train Acc: 0.3902\n",
      "Epoch: 69, Batch: 500, loss: 4.4146 , Train PPL: 1.0068, Train Acc: 0.3963\n",
      "Epoch: 69, Batch: 600, loss: 4.1797 , Train PPL: 1.0064, Train Acc: 0.3659\n",
      "Epoch: 69, Batch: 700, loss: 3.8789 , Train PPL: 1.0059, Train Acc: 0.4787\n",
      "Validation --- Epoch: 69, total loss: 297.6034 , PPL: 1.2766, Acc: 0.4010\n",
      "lr = 0.0625\n",
      "Epoch: 70, Batch: 100, loss: 4.2762 , Train PPL: 1.0065, Train Acc: 0.3735\n",
      "Epoch: 70, Batch: 200, loss: 4.0960 , Train PPL: 1.0063, Train Acc: 0.4390\n",
      "Epoch: 70, Batch: 300, loss: 4.0985 , Train PPL: 1.0063, Train Acc: 0.3750\n",
      "Epoch: 70, Batch: 400, loss: 4.0068 , Train PPL: 1.0061, Train Acc: 0.4268\n",
      "Epoch: 70, Batch: 500, loss: 3.8587 , Train PPL: 1.0059, Train Acc: 0.4924\n",
      "Epoch: 70, Batch: 600, loss: 3.7348 , Train PPL: 1.0057, Train Acc: 0.4512\n",
      "Epoch: 70, Batch: 700, loss: 3.8681 , Train PPL: 1.0059, Train Acc: 0.4634\n",
      "Validation --- Epoch: 70, total loss: 297.6652 , PPL: 1.2766, Acc: 0.4005\n",
      "lr = 0.03125\n",
      "Epoch: 71, Batch: 100, loss: 4.0582 , Train PPL: 1.0062, Train Acc: 0.4101\n",
      "Epoch: 71, Batch: 200, loss: 3.9460 , Train PPL: 1.0060, Train Acc: 0.4771\n",
      "Epoch: 71, Batch: 300, loss: 4.3695 , Train PPL: 1.0067, Train Acc: 0.3826\n",
      "Epoch: 71, Batch: 400, loss: 3.7877 , Train PPL: 1.0058, Train Acc: 0.5046\n",
      "Epoch: 71, Batch: 500, loss: 4.3294 , Train PPL: 1.0066, Train Acc: 0.3415\n",
      "Epoch: 71, Batch: 600, loss: 4.2688 , Train PPL: 1.0065, Train Acc: 0.3674\n",
      "Epoch: 71, Batch: 700, loss: 3.8242 , Train PPL: 1.0058, Train Acc: 0.4604\n",
      "Validation --- Epoch: 71, total loss: 297.6541 , PPL: 1.2766, Acc: 0.4006\n",
      "lr = 0.03125\n",
      "Epoch: 72, Batch: 100, loss: 3.9802 , Train PPL: 1.0061, Train Acc: 0.4040\n",
      "Epoch: 72, Batch: 200, loss: 4.3410 , Train PPL: 1.0066, Train Acc: 0.3887\n",
      "Epoch: 72, Batch: 300, loss: 4.1672 , Train PPL: 1.0064, Train Acc: 0.4192\n",
      "Epoch: 72, Batch: 400, loss: 3.7919 , Train PPL: 1.0058, Train Acc: 0.4466\n",
      "Epoch: 72, Batch: 500, loss: 3.9095 , Train PPL: 1.0060, Train Acc: 0.4848\n",
      "Epoch: 72, Batch: 600, loss: 4.1498 , Train PPL: 1.0063, Train Acc: 0.4177\n",
      "Epoch: 72, Batch: 700, loss: 4.1145 , Train PPL: 1.0063, Train Acc: 0.3811\n",
      "Validation --- Epoch: 72, total loss: 297.6177 , PPL: 1.2766, Acc: 0.4005\n",
      "lr = 0.03125\n",
      "Epoch: 73, Batch: 100, loss: 3.9644 , Train PPL: 1.0061, Train Acc: 0.4299\n",
      "Epoch: 73, Batch: 200, loss: 4.1439 , Train PPL: 1.0063, Train Acc: 0.4223\n",
      "Epoch: 73, Batch: 300, loss: 4.2797 , Train PPL: 1.0065, Train Acc: 0.3674\n",
      "Epoch: 73, Batch: 400, loss: 4.0175 , Train PPL: 1.0061, Train Acc: 0.3704\n",
      "Epoch: 73, Batch: 500, loss: 3.8461 , Train PPL: 1.0059, Train Acc: 0.4665\n",
      "Epoch: 73, Batch: 600, loss: 4.0533 , Train PPL: 1.0062, Train Acc: 0.4558\n",
      "Epoch: 73, Batch: 700, loss: 4.2529 , Train PPL: 1.0065, Train Acc: 0.4238\n",
      "Validation --- Epoch: 73, total loss: 297.6432 , PPL: 1.2766, Acc: 0.4008\n",
      "lr = 0.03125\n",
      "Epoch: 74, Batch: 100, loss: 3.6565 , Train PPL: 1.0056, Train Acc: 0.5061\n",
      "Epoch: 74, Batch: 200, loss: 3.9401 , Train PPL: 1.0060, Train Acc: 0.4116\n",
      "Epoch: 74, Batch: 300, loss: 4.0555 , Train PPL: 1.0062, Train Acc: 0.4299\n",
      "Epoch: 74, Batch: 400, loss: 4.0699 , Train PPL: 1.0062, Train Acc: 0.4360\n",
      "Epoch: 74, Batch: 500, loss: 4.1966 , Train PPL: 1.0064, Train Acc: 0.3918\n",
      "Epoch: 74, Batch: 600, loss: 3.7422 , Train PPL: 1.0057, Train Acc: 0.5168\n",
      "Epoch: 74, Batch: 700, loss: 4.1040 , Train PPL: 1.0063, Train Acc: 0.4116\n",
      "Validation --- Epoch: 74, total loss: 297.6696 , PPL: 1.2766, Acc: 0.4004\n",
      "lr = 0.03125\n",
      "Epoch: 75, Batch: 100, loss: 3.9370 , Train PPL: 1.0060, Train Acc: 0.4436\n",
      "Epoch: 75, Batch: 200, loss: 4.2668 , Train PPL: 1.0065, Train Acc: 0.3720\n",
      "Epoch: 75, Batch: 300, loss: 4.2625 , Train PPL: 1.0065, Train Acc: 0.3552\n",
      "Epoch: 75, Batch: 400, loss: 4.2257 , Train PPL: 1.0065, Train Acc: 0.3811\n",
      "Epoch: 75, Batch: 500, loss: 4.0245 , Train PPL: 1.0062, Train Acc: 0.4512\n",
      "Epoch: 75, Batch: 600, loss: 4.0327 , Train PPL: 1.0062, Train Acc: 0.4192\n",
      "Epoch: 75, Batch: 700, loss: 4.0031 , Train PPL: 1.0061, Train Acc: 0.4284\n",
      "Validation --- Epoch: 75, total loss: 297.6935 , PPL: 1.2766, Acc: 0.4003\n",
      "lr = 0.03125\n",
      "Epoch: 76, Batch: 100, loss: 4.1768 , Train PPL: 1.0064, Train Acc: 0.3963\n",
      "Epoch: 76, Batch: 200, loss: 3.8733 , Train PPL: 1.0059, Train Acc: 0.4192\n",
      "Epoch: 76, Batch: 300, loss: 4.2104 , Train PPL: 1.0064, Train Acc: 0.4146\n",
      "Epoch: 76, Batch: 400, loss: 3.8784 , Train PPL: 1.0059, Train Acc: 0.4527\n",
      "Epoch: 76, Batch: 500, loss: 4.2557 , Train PPL: 1.0065, Train Acc: 0.3491\n",
      "Epoch: 76, Batch: 600, loss: 4.1399 , Train PPL: 1.0063, Train Acc: 0.4131\n",
      "Epoch: 76, Batch: 700, loss: 3.8813 , Train PPL: 1.0059, Train Acc: 0.4451\n",
      "Validation --- Epoch: 76, total loss: 297.6466 , PPL: 1.2766, Acc: 0.4006\n",
      "lr = 0.015625\n",
      "Epoch: 77, Batch: 100, loss: 4.1685 , Train PPL: 1.0064, Train Acc: 0.4253\n",
      "Epoch: 77, Batch: 200, loss: 4.4677 , Train PPL: 1.0068, Train Acc: 0.3826\n",
      "Epoch: 77, Batch: 300, loss: 4.1435 , Train PPL: 1.0063, Train Acc: 0.3826\n",
      "Epoch: 77, Batch: 400, loss: 4.0611 , Train PPL: 1.0062, Train Acc: 0.4177\n",
      "Epoch: 77, Batch: 500, loss: 3.9725 , Train PPL: 1.0061, Train Acc: 0.4893\n",
      "Epoch: 77, Batch: 600, loss: 4.2593 , Train PPL: 1.0065, Train Acc: 0.4223\n",
      "Epoch: 77, Batch: 700, loss: 4.1895 , Train PPL: 1.0064, Train Acc: 0.4101\n",
      "Validation --- Epoch: 77, total loss: 297.7166 , PPL: 1.2767, Acc: 0.4006\n",
      "lr = 0.015625\n",
      "Epoch: 78, Batch: 100, loss: 4.3523 , Train PPL: 1.0067, Train Acc: 0.3659\n",
      "Epoch: 78, Batch: 200, loss: 3.9849 , Train PPL: 1.0061, Train Acc: 0.4405\n",
      "Epoch: 78, Batch: 300, loss: 4.1957 , Train PPL: 1.0064, Train Acc: 0.3918\n",
      "Epoch: 78, Batch: 400, loss: 4.1058 , Train PPL: 1.0063, Train Acc: 0.3765\n",
      "Epoch: 78, Batch: 500, loss: 4.2193 , Train PPL: 1.0065, Train Acc: 0.4238\n",
      "Epoch: 78, Batch: 600, loss: 3.8989 , Train PPL: 1.0060, Train Acc: 0.4345\n",
      "Epoch: 78, Batch: 700, loss: 4.2903 , Train PPL: 1.0066, Train Acc: 0.3216\n",
      "Validation --- Epoch: 78, total loss: 297.7490 , PPL: 1.2767, Acc: 0.4006\n",
      "lr = 0.015625\n",
      "Epoch: 79, Batch: 100, loss: 3.5142 , Train PPL: 1.0054, Train Acc: 0.4985\n",
      "Epoch: 79, Batch: 200, loss: 4.2792 , Train PPL: 1.0065, Train Acc: 0.3887\n",
      "Epoch: 79, Batch: 300, loss: 3.9611 , Train PPL: 1.0061, Train Acc: 0.4055\n",
      "Epoch: 79, Batch: 400, loss: 3.8980 , Train PPL: 1.0060, Train Acc: 0.4573\n",
      "Epoch: 79, Batch: 500, loss: 3.9866 , Train PPL: 1.0061, Train Acc: 0.4512\n",
      "Epoch: 79, Batch: 600, loss: 4.1529 , Train PPL: 1.0064, Train Acc: 0.3872\n",
      "Epoch: 79, Batch: 700, loss: 3.8890 , Train PPL: 1.0059, Train Acc: 0.4207\n",
      "Validation --- Epoch: 79, total loss: 297.6929 , PPL: 1.2766, Acc: 0.4007\n",
      "lr = 0.015625\n",
      "Epoch: 80, Batch: 100, loss: 4.1102 , Train PPL: 1.0063, Train Acc: 0.3933\n",
      "Epoch: 80, Batch: 200, loss: 4.1538 , Train PPL: 1.0064, Train Acc: 0.3933\n",
      "Epoch: 80, Batch: 300, loss: 4.0694 , Train PPL: 1.0062, Train Acc: 0.3643\n",
      "Epoch: 80, Batch: 400, loss: 4.1264 , Train PPL: 1.0063, Train Acc: 0.4649\n",
      "Epoch: 80, Batch: 500, loss: 4.1477 , Train PPL: 1.0063, Train Acc: 0.4116\n",
      "Epoch: 80, Batch: 600, loss: 4.1666 , Train PPL: 1.0064, Train Acc: 0.3994\n",
      "Epoch: 80, Batch: 700, loss: 3.9238 , Train PPL: 1.0060, Train Acc: 0.3933\n",
      "Validation --- Epoch: 80, total loss: 297.7134 , PPL: 1.2767, Acc: 0.4009\n",
      "lr = 0.015625\n",
      "Epoch: 81, Batch: 100, loss: 4.1522 , Train PPL: 1.0063, Train Acc: 0.4085\n",
      "Epoch: 81, Batch: 200, loss: 3.9365 , Train PPL: 1.0060, Train Acc: 0.3979\n",
      "Epoch: 81, Batch: 300, loss: 3.7922 , Train PPL: 1.0058, Train Acc: 0.4573\n",
      "Epoch: 81, Batch: 400, loss: 3.8886 , Train PPL: 1.0059, Train Acc: 0.3841\n",
      "Epoch: 81, Batch: 500, loss: 4.3503 , Train PPL: 1.0067, Train Acc: 0.3460\n",
      "Epoch: 81, Batch: 600, loss: 3.8585 , Train PPL: 1.0059, Train Acc: 0.4649\n",
      "Epoch: 81, Batch: 700, loss: 4.2352 , Train PPL: 1.0065, Train Acc: 0.3765\n",
      "Validation --- Epoch: 81, total loss: 297.6889 , PPL: 1.2766, Acc: 0.4005\n",
      "lr = 0.015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82, Batch: 100, loss: 4.2764 , Train PPL: 1.0065, Train Acc: 0.3552\n",
      "Epoch: 82, Batch: 200, loss: 3.8451 , Train PPL: 1.0059, Train Acc: 0.4451\n",
      "Epoch: 82, Batch: 300, loss: 4.0798 , Train PPL: 1.0062, Train Acc: 0.3659\n",
      "Epoch: 82, Batch: 400, loss: 3.9888 , Train PPL: 1.0061, Train Acc: 0.4726\n",
      "Epoch: 82, Batch: 500, loss: 4.3177 , Train PPL: 1.0066, Train Acc: 0.4040\n",
      "Epoch: 82, Batch: 600, loss: 3.9089 , Train PPL: 1.0060, Train Acc: 0.4329\n",
      "Epoch: 82, Batch: 700, loss: 3.9852 , Train PPL: 1.0061, Train Acc: 0.4604\n",
      "Validation --- Epoch: 82, total loss: 297.7132 , PPL: 1.2767, Acc: 0.4013\n",
      "lr = 0.0078125\n",
      "Epoch: 83, Batch: 100, loss: 4.0771 , Train PPL: 1.0062, Train Acc: 0.4162\n",
      "Epoch: 83, Batch: 200, loss: 4.1362 , Train PPL: 1.0063, Train Acc: 0.3857\n",
      "Epoch: 83, Batch: 300, loss: 3.9782 , Train PPL: 1.0061, Train Acc: 0.4482\n",
      "Epoch: 83, Batch: 400, loss: 4.3317 , Train PPL: 1.0066, Train Acc: 0.3491\n",
      "Epoch: 83, Batch: 500, loss: 4.1671 , Train PPL: 1.0064, Train Acc: 0.3872\n",
      "Epoch: 83, Batch: 600, loss: 4.3153 , Train PPL: 1.0066, Train Acc: 0.3796\n",
      "Epoch: 83, Batch: 700, loss: 3.9760 , Train PPL: 1.0061, Train Acc: 0.4512\n",
      "Validation --- Epoch: 83, total loss: 297.7225 , PPL: 1.2767, Acc: 0.4004\n",
      "lr = 0.0078125\n",
      "Epoch: 84, Batch: 100, loss: 3.8093 , Train PPL: 1.0058, Train Acc: 0.4756\n",
      "Epoch: 84, Batch: 200, loss: 4.0296 , Train PPL: 1.0062, Train Acc: 0.4360\n",
      "Epoch: 84, Batch: 300, loss: 3.8193 , Train PPL: 1.0058, Train Acc: 0.4802\n",
      "Epoch: 84, Batch: 400, loss: 3.7723 , Train PPL: 1.0058, Train Acc: 0.4558\n",
      "Epoch: 84, Batch: 500, loss: 4.1199 , Train PPL: 1.0063, Train Acc: 0.3735\n",
      "Epoch: 84, Batch: 600, loss: 4.0943 , Train PPL: 1.0063, Train Acc: 0.3963\n",
      "Epoch: 84, Batch: 700, loss: 3.9862 , Train PPL: 1.0061, Train Acc: 0.4146\n",
      "Validation --- Epoch: 84, total loss: 297.7169 , PPL: 1.2767, Acc: 0.4002\n",
      "lr = 0.0078125\n",
      "Epoch: 85, Batch: 100, loss: 4.2119 , Train PPL: 1.0064, Train Acc: 0.3963\n",
      "Epoch: 85, Batch: 200, loss: 3.8930 , Train PPL: 1.0060, Train Acc: 0.4527\n",
      "Epoch: 85, Batch: 300, loss: 4.2924 , Train PPL: 1.0066, Train Acc: 0.3628\n",
      "Epoch: 85, Batch: 400, loss: 4.4095 , Train PPL: 1.0067, Train Acc: 0.3552\n",
      "Epoch: 85, Batch: 500, loss: 4.3594 , Train PPL: 1.0067, Train Acc: 0.3628\n",
      "Epoch: 85, Batch: 600, loss: 4.3161 , Train PPL: 1.0066, Train Acc: 0.3689\n",
      "Epoch: 85, Batch: 700, loss: 4.0174 , Train PPL: 1.0061, Train Acc: 0.4131\n",
      "Validation --- Epoch: 85, total loss: 297.7310 , PPL: 1.2767, Acc: 0.4005\n",
      "lr = 0.0078125\n",
      "Epoch: 86, Batch: 100, loss: 4.1556 , Train PPL: 1.0064, Train Acc: 0.3887\n",
      "Epoch: 86, Batch: 200, loss: 4.1482 , Train PPL: 1.0063, Train Acc: 0.4162\n",
      "Epoch: 86, Batch: 300, loss: 4.1947 , Train PPL: 1.0064, Train Acc: 0.4116\n",
      "Epoch: 86, Batch: 400, loss: 4.2920 , Train PPL: 1.0066, Train Acc: 0.3796\n",
      "Epoch: 86, Batch: 500, loss: 4.0880 , Train PPL: 1.0063, Train Acc: 0.4009\n",
      "Epoch: 86, Batch: 600, loss: 3.9151 , Train PPL: 1.0060, Train Acc: 0.4527\n",
      "Epoch: 86, Batch: 700, loss: 4.2346 , Train PPL: 1.0065, Train Acc: 0.3415\n",
      "Validation --- Epoch: 86, total loss: 297.7312 , PPL: 1.2767, Acc: 0.4002\n",
      "lr = 0.0078125\n",
      "Epoch: 87, Batch: 100, loss: 3.8448 , Train PPL: 1.0059, Train Acc: 0.4878\n",
      "Epoch: 87, Batch: 200, loss: 3.9412 , Train PPL: 1.0060, Train Acc: 0.4268\n",
      "Epoch: 87, Batch: 300, loss: 4.2079 , Train PPL: 1.0064, Train Acc: 0.4116\n",
      "Epoch: 87, Batch: 400, loss: 4.2201 , Train PPL: 1.0065, Train Acc: 0.4055\n",
      "Epoch: 87, Batch: 500, loss: 4.3788 , Train PPL: 1.0067, Train Acc: 0.3598\n",
      "Epoch: 87, Batch: 600, loss: 3.9762 , Train PPL: 1.0061, Train Acc: 0.4665\n",
      "Epoch: 87, Batch: 700, loss: 4.0960 , Train PPL: 1.0063, Train Acc: 0.4009\n",
      "Validation --- Epoch: 87, total loss: 297.7335 , PPL: 1.2767, Acc: 0.4005\n",
      "lr = 0.0078125\n",
      "Epoch: 88, Batch: 100, loss: 3.9630 , Train PPL: 1.0061, Train Acc: 0.4665\n",
      "Epoch: 88, Batch: 200, loss: 4.0059 , Train PPL: 1.0061, Train Acc: 0.4360\n",
      "Epoch: 88, Batch: 300, loss: 4.1594 , Train PPL: 1.0064, Train Acc: 0.4116\n",
      "Epoch: 88, Batch: 400, loss: 4.0589 , Train PPL: 1.0062, Train Acc: 0.4482\n",
      "Epoch: 88, Batch: 500, loss: 3.9226 , Train PPL: 1.0060, Train Acc: 0.4466\n",
      "Epoch: 88, Batch: 600, loss: 4.1064 , Train PPL: 1.0063, Train Acc: 0.4482\n",
      "Epoch: 88, Batch: 700, loss: 3.9223 , Train PPL: 1.0060, Train Acc: 0.4543\n",
      "Validation --- Epoch: 88, total loss: 297.7319 , PPL: 1.2767, Acc: 0.4003\n",
      "lr = 0.00390625\n",
      "Epoch: 89, Batch: 100, loss: 4.2618 , Train PPL: 1.0065, Train Acc: 0.3857\n",
      "Epoch: 89, Batch: 200, loss: 3.8499 , Train PPL: 1.0059, Train Acc: 0.4390\n",
      "Epoch: 89, Batch: 300, loss: 4.0286 , Train PPL: 1.0062, Train Acc: 0.4223\n",
      "Epoch: 89, Batch: 400, loss: 3.9682 , Train PPL: 1.0061, Train Acc: 0.3872\n",
      "Epoch: 89, Batch: 500, loss: 4.1551 , Train PPL: 1.0064, Train Acc: 0.3841\n",
      "Epoch: 89, Batch: 600, loss: 4.0095 , Train PPL: 1.0061, Train Acc: 0.4436\n",
      "Epoch: 89, Batch: 700, loss: 4.2843 , Train PPL: 1.0066, Train Acc: 0.3613\n",
      "Validation --- Epoch: 89, total loss: 297.7256 , PPL: 1.2767, Acc: 0.4005\n",
      "lr = 0.00390625\n",
      "Epoch: 90, Batch: 100, loss: 4.2902 , Train PPL: 1.0066, Train Acc: 0.3765\n",
      "Epoch: 90, Batch: 200, loss: 4.1911 , Train PPL: 1.0064, Train Acc: 0.3704\n",
      "Epoch: 90, Batch: 300, loss: 4.2681 , Train PPL: 1.0065, Train Acc: 0.3918\n",
      "Epoch: 90, Batch: 400, loss: 3.9535 , Train PPL: 1.0060, Train Acc: 0.4527\n",
      "Epoch: 90, Batch: 500, loss: 4.0706 , Train PPL: 1.0062, Train Acc: 0.4863\n",
      "Epoch: 90, Batch: 600, loss: 4.0237 , Train PPL: 1.0062, Train Acc: 0.4024\n",
      "Epoch: 90, Batch: 700, loss: 4.2501 , Train PPL: 1.0065, Train Acc: 0.3933\n",
      "Validation --- Epoch: 90, total loss: 297.7348 , PPL: 1.2767, Acc: 0.4006\n",
      "lr = 0.00390625\n",
      "Epoch: 91, Batch: 100, loss: 3.8397 , Train PPL: 1.0059, Train Acc: 0.4848\n",
      "Epoch: 91, Batch: 200, loss: 3.9201 , Train PPL: 1.0060, Train Acc: 0.4726\n",
      "Epoch: 91, Batch: 300, loss: 3.9966 , Train PPL: 1.0061, Train Acc: 0.4116\n",
      "Epoch: 91, Batch: 400, loss: 4.1451 , Train PPL: 1.0063, Train Acc: 0.3704\n",
      "Epoch: 91, Batch: 500, loss: 4.0713 , Train PPL: 1.0062, Train Acc: 0.3963\n",
      "Epoch: 91, Batch: 600, loss: 3.8956 , Train PPL: 1.0060, Train Acc: 0.4863\n",
      "Epoch: 91, Batch: 700, loss: 3.9498 , Train PPL: 1.0060, Train Acc: 0.4345\n",
      "Validation --- Epoch: 91, total loss: 297.7347 , PPL: 1.2767, Acc: 0.4005\n",
      "lr = 0.00390625\n",
      "Epoch: 92, Batch: 100, loss: 4.0633 , Train PPL: 1.0062, Train Acc: 0.3628\n",
      "Epoch: 92, Batch: 200, loss: 3.8882 , Train PPL: 1.0059, Train Acc: 0.4284\n",
      "Epoch: 92, Batch: 300, loss: 4.1993 , Train PPL: 1.0064, Train Acc: 0.3796\n",
      "Epoch: 92, Batch: 400, loss: 4.1245 , Train PPL: 1.0063, Train Acc: 0.3872\n",
      "Epoch: 92, Batch: 500, loss: 4.1962 , Train PPL: 1.0064, Train Acc: 0.3308\n",
      "Epoch: 92, Batch: 600, loss: 4.0549 , Train PPL: 1.0062, Train Acc: 0.3735\n",
      "Epoch: 92, Batch: 700, loss: 3.8418 , Train PPL: 1.0059, Train Acc: 0.4405\n",
      "Validation --- Epoch: 92, total loss: 297.7298 , PPL: 1.2767, Acc: 0.4003\n",
      "lr = 0.00390625\n",
      "Epoch: 93, Batch: 100, loss: 4.2502 , Train PPL: 1.0065, Train Acc: 0.3552\n",
      "Epoch: 93, Batch: 200, loss: 4.1361 , Train PPL: 1.0063, Train Acc: 0.3979\n",
      "Epoch: 93, Batch: 300, loss: 3.8892 , Train PPL: 1.0059, Train Acc: 0.4146\n",
      "Epoch: 93, Batch: 400, loss: 4.2527 , Train PPL: 1.0065, Train Acc: 0.3796\n",
      "Epoch: 93, Batch: 500, loss: 4.2861 , Train PPL: 1.0066, Train Acc: 0.3613\n",
      "Epoch: 93, Batch: 600, loss: 4.1335 , Train PPL: 1.0063, Train Acc: 0.4162\n",
      "Epoch: 93, Batch: 700, loss: 4.1111 , Train PPL: 1.0063, Train Acc: 0.3780\n",
      "Validation --- Epoch: 93, total loss: 297.7316 , PPL: 1.2767, Acc: 0.4004\n",
      "lr = 0.00390625\n",
      "Epoch: 94, Batch: 100, loss: 3.9627 , Train PPL: 1.0061, Train Acc: 0.4375\n",
      "Epoch: 94, Batch: 200, loss: 4.0480 , Train PPL: 1.0062, Train Acc: 0.4314\n",
      "Epoch: 94, Batch: 300, loss: 3.8494 , Train PPL: 1.0059, Train Acc: 0.4436\n",
      "Epoch: 94, Batch: 400, loss: 3.9836 , Train PPL: 1.0061, Train Acc: 0.4588\n",
      "Epoch: 94, Batch: 500, loss: 4.0406 , Train PPL: 1.0062, Train Acc: 0.3994\n",
      "Epoch: 94, Batch: 600, loss: 3.8099 , Train PPL: 1.0058, Train Acc: 0.5305\n",
      "Epoch: 94, Batch: 700, loss: 4.3161 , Train PPL: 1.0066, Train Acc: 0.3659\n",
      "Validation --- Epoch: 94, total loss: 297.7406 , PPL: 1.2767, Acc: 0.4005\n",
      "lr = 0.001953125\n",
      "Epoch: 95, Batch: 100, loss: 4.0250 , Train PPL: 1.0062, Train Acc: 0.4009\n",
      "Epoch: 95, Batch: 200, loss: 4.0236 , Train PPL: 1.0062, Train Acc: 0.4009\n",
      "Epoch: 95, Batch: 300, loss: 4.0886 , Train PPL: 1.0063, Train Acc: 0.4893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95, Batch: 400, loss: 4.0616 , Train PPL: 1.0062, Train Acc: 0.4085\n",
      "Epoch: 95, Batch: 500, loss: 3.8787 , Train PPL: 1.0059, Train Acc: 0.4131\n",
      "Epoch: 95, Batch: 600, loss: 3.9578 , Train PPL: 1.0061, Train Acc: 0.4207\n",
      "Epoch: 95, Batch: 700, loss: 4.0128 , Train PPL: 1.0061, Train Acc: 0.4497\n",
      "Validation --- Epoch: 95, total loss: 297.7440 , PPL: 1.2767, Acc: 0.4005\n",
      "lr = 0.001953125\n",
      "Epoch: 96, Batch: 100, loss: 4.3132 , Train PPL: 1.0066, Train Acc: 0.4131\n",
      "Epoch: 96, Batch: 200, loss: 4.0490 , Train PPL: 1.0062, Train Acc: 0.4146\n",
      "Epoch: 96, Batch: 300, loss: 3.9636 , Train PPL: 1.0061, Train Acc: 0.4588\n",
      "Epoch: 96, Batch: 400, loss: 4.1525 , Train PPL: 1.0064, Train Acc: 0.3979\n",
      "Epoch: 96, Batch: 500, loss: 4.2393 , Train PPL: 1.0065, Train Acc: 0.3720\n",
      "Epoch: 96, Batch: 600, loss: 4.1694 , Train PPL: 1.0064, Train Acc: 0.3765\n",
      "Epoch: 96, Batch: 700, loss: 4.2927 , Train PPL: 1.0066, Train Acc: 0.3857\n",
      "Validation --- Epoch: 96, total loss: 297.7403 , PPL: 1.2767, Acc: 0.4004\n",
      "lr = 0.001953125\n",
      "Epoch: 97, Batch: 100, loss: 3.9804 , Train PPL: 1.0061, Train Acc: 0.4253\n",
      "Epoch: 97, Batch: 200, loss: 3.7054 , Train PPL: 1.0057, Train Acc: 0.4787\n",
      "Epoch: 97, Batch: 300, loss: 3.9042 , Train PPL: 1.0060, Train Acc: 0.5030\n",
      "Epoch: 97, Batch: 400, loss: 3.8305 , Train PPL: 1.0059, Train Acc: 0.4787\n",
      "Epoch: 97, Batch: 500, loss: 4.0666 , Train PPL: 1.0062, Train Acc: 0.4101\n",
      "Epoch: 97, Batch: 600, loss: 3.7342 , Train PPL: 1.0057, Train Acc: 0.5412\n",
      "Epoch: 97, Batch: 700, loss: 3.9635 , Train PPL: 1.0061, Train Acc: 0.4360\n",
      "Validation --- Epoch: 97, total loss: 297.7392 , PPL: 1.2767, Acc: 0.4005\n",
      "lr = 0.001953125\n",
      "Epoch: 98, Batch: 100, loss: 4.1012 , Train PPL: 1.0063, Train Acc: 0.3887\n",
      "Epoch: 98, Batch: 200, loss: 4.1401 , Train PPL: 1.0063, Train Acc: 0.3811\n",
      "Epoch: 98, Batch: 300, loss: 3.8888 , Train PPL: 1.0059, Train Acc: 0.4634\n",
      "Epoch: 98, Batch: 400, loss: 4.2988 , Train PPL: 1.0066, Train Acc: 0.3521\n",
      "Epoch: 98, Batch: 500, loss: 4.2490 , Train PPL: 1.0065, Train Acc: 0.3750\n",
      "Epoch: 98, Batch: 600, loss: 4.2606 , Train PPL: 1.0065, Train Acc: 0.4223\n",
      "Epoch: 98, Batch: 700, loss: 3.5432 , Train PPL: 1.0054, Train Acc: 0.5000\n",
      "Validation --- Epoch: 98, total loss: 297.7419 , PPL: 1.2767, Acc: 0.4006\n",
      "lr = 0.001953125\n",
      "Epoch: 99, Batch: 100, loss: 4.0131 , Train PPL: 1.0061, Train Acc: 0.4131\n",
      "Epoch: 99, Batch: 200, loss: 4.1238 , Train PPL: 1.0063, Train Acc: 0.3933\n",
      "Epoch: 99, Batch: 300, loss: 3.8744 , Train PPL: 1.0059, Train Acc: 0.3994\n",
      "Epoch: 99, Batch: 400, loss: 4.3083 , Train PPL: 1.0066, Train Acc: 0.3994\n",
      "Epoch: 99, Batch: 500, loss: 4.1688 , Train PPL: 1.0064, Train Acc: 0.3857\n",
      "Epoch: 99, Batch: 600, loss: 3.6346 , Train PPL: 1.0056, Train Acc: 0.5259\n",
      "Epoch: 99, Batch: 700, loss: 4.1899 , Train PPL: 1.0064, Train Acc: 0.3567\n",
      "Validation --- Epoch: 99, total loss: 297.7448 , PPL: 1.2767, Acc: 0.4005\n",
      "lr = 0.001953125\n"
     ]
    }
   ],
   "source": [
    "model = TCN(4, [600,600,600], kernel=3, dropout=0.5, embedding_size = 600, n_words = n_words,tied=True)\n",
    "model.cuda()\n",
    "model_parameters = filter(lambda x: x.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print('Network with {} parameters'.format(params))\n",
    "print('Receptive field of network is {}'.format(model.receptive_field))\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode=\"min\",patience=5,min_lr=1e-6,factor=0.5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "best_vloss = 1e8\n",
    "for e in range(100):\n",
    "    train_loop(e)\n",
    "    validation_loss = validation_loop(e)\n",
    "    scheduler.step(validation_loss)\n",
    "    print('lr = {}'.format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "    if validation_loss < best_vloss:\n",
    "                with open(\"model_4_layers_k3.pt\", 'wb') as f:\n",
    "                    print('wrote model')\n",
    "                    torch.save(model, f)\n",
    "                best_vloss = validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network with 4633901 parameters\n",
      "Receptive field of network is 62\n",
      "Epoch: 0, Batch: 100, loss: 6.6709 , Train PPL: 1.0102, Train Acc: 0.1524\n",
      "Epoch: 0, Batch: 200, loss: 6.2315 , Train PPL: 1.0095, Train Acc: 0.1936\n",
      "Epoch: 0, Batch: 300, loss: 6.3242 , Train PPL: 1.0097, Train Acc: 0.1966\n",
      "Epoch: 0, Batch: 400, loss: 6.3010 , Train PPL: 1.0097, Train Acc: 0.2454\n",
      "Epoch: 0, Batch: 500, loss: 6.3201 , Train PPL: 1.0097, Train Acc: 0.2149\n",
      "Epoch: 0, Batch: 600, loss: 5.9883 , Train PPL: 1.0092, Train Acc: 0.2287\n",
      "Epoch: 0, Batch: 700, loss: 5.9589 , Train PPL: 1.0091, Train Acc: 0.2439\n",
      "Validation --- Epoch: 0, total loss: 334.4513 , PPL: 1.3136, Acc: 0.2757\n",
      "lr = 4\n",
      "wrote model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type TCN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type TC_block. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Chomp1d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 100, loss: 6.0809 , Train PPL: 1.0093, Train Acc: 0.2622\n",
      "Epoch: 1, Batch: 200, loss: 6.0547 , Train PPL: 1.0093, Train Acc: 0.2591\n",
      "Epoch: 1, Batch: 300, loss: 5.8774 , Train PPL: 1.0090, Train Acc: 0.2942\n",
      "Epoch: 1, Batch: 400, loss: 5.9999 , Train PPL: 1.0092, Train Acc: 0.2180\n",
      "Epoch: 1, Batch: 500, loss: 5.5050 , Train PPL: 1.0084, Train Acc: 0.3277\n",
      "Epoch: 1, Batch: 600, loss: 5.3919 , Train PPL: 1.0083, Train Acc: 0.2927\n",
      "Epoch: 1, Batch: 700, loss: 5.9243 , Train PPL: 1.0091, Train Acc: 0.2713\n",
      "Validation --- Epoch: 1, total loss: 324.7977 , PPL: 1.3034, Acc: 0.2864\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 2, Batch: 100, loss: 5.5558 , Train PPL: 1.0085, Train Acc: 0.2591\n",
      "Epoch: 2, Batch: 200, loss: 5.9567 , Train PPL: 1.0091, Train Acc: 0.2637\n",
      "Epoch: 2, Batch: 300, loss: 5.7342 , Train PPL: 1.0088, Train Acc: 0.2515\n",
      "Epoch: 2, Batch: 400, loss: 5.0695 , Train PPL: 1.0078, Train Acc: 0.3887\n",
      "Epoch: 2, Batch: 500, loss: 5.5945 , Train PPL: 1.0086, Train Acc: 0.3034\n",
      "Epoch: 2, Batch: 600, loss: 5.5008 , Train PPL: 1.0084, Train Acc: 0.2759\n",
      "Epoch: 2, Batch: 700, loss: 5.6651 , Train PPL: 1.0087, Train Acc: 0.2759\n",
      "Validation --- Epoch: 2, total loss: 319.7422 , PPL: 1.2981, Acc: 0.2966\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 3, Batch: 100, loss: 5.3579 , Train PPL: 1.0082, Train Acc: 0.3521\n",
      "Epoch: 3, Batch: 200, loss: 5.2930 , Train PPL: 1.0081, Train Acc: 0.3750\n",
      "Epoch: 3, Batch: 300, loss: 5.5439 , Train PPL: 1.0085, Train Acc: 0.2713\n",
      "Epoch: 3, Batch: 400, loss: 5.3531 , Train PPL: 1.0082, Train Acc: 0.3369\n",
      "Epoch: 3, Batch: 500, loss: 5.7846 , Train PPL: 1.0089, Train Acc: 0.2622\n",
      "Epoch: 3, Batch: 600, loss: 5.4321 , Train PPL: 1.0083, Train Acc: 0.3018\n",
      "Epoch: 3, Batch: 700, loss: 5.4957 , Train PPL: 1.0084, Train Acc: 0.3095\n",
      "Validation --- Epoch: 3, total loss: 318.8760 , PPL: 1.2973, Acc: 0.2729\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 4, Batch: 100, loss: 5.5929 , Train PPL: 1.0086, Train Acc: 0.2851\n",
      "Epoch: 4, Batch: 200, loss: 5.8385 , Train PPL: 1.0089, Train Acc: 0.2759\n",
      "Epoch: 4, Batch: 300, loss: 5.4788 , Train PPL: 1.0084, Train Acc: 0.2957\n",
      "Epoch: 4, Batch: 400, loss: 5.2453 , Train PPL: 1.0080, Train Acc: 0.3460\n",
      "Epoch: 4, Batch: 500, loss: 5.2555 , Train PPL: 1.0080, Train Acc: 0.3018\n",
      "Epoch: 4, Batch: 600, loss: 5.5423 , Train PPL: 1.0085, Train Acc: 0.3003\n",
      "Epoch: 4, Batch: 700, loss: 5.4758 , Train PPL: 1.0084, Train Acc: 0.3018\n",
      "Validation --- Epoch: 4, total loss: 320.3260 , PPL: 1.2992, Acc: 0.2448\n",
      "lr = 4\n",
      "Epoch: 5, Batch: 100, loss: 5.1826 , Train PPL: 1.0079, Train Acc: 0.3140\n",
      "Epoch: 5, Batch: 200, loss: 5.1656 , Train PPL: 1.0079, Train Acc: 0.3521\n",
      "Epoch: 5, Batch: 300, loss: 5.4156 , Train PPL: 1.0083, Train Acc: 0.2912\n",
      "Epoch: 5, Batch: 400, loss: 5.3817 , Train PPL: 1.0082, Train Acc: 0.2713\n",
      "Epoch: 5, Batch: 500, loss: 5.6473 , Train PPL: 1.0086, Train Acc: 0.3155\n",
      "Epoch: 5, Batch: 600, loss: 5.3044 , Train PPL: 1.0081, Train Acc: 0.3293\n",
      "Epoch: 5, Batch: 700, loss: 5.3208 , Train PPL: 1.0081, Train Acc: 0.3110\n",
      "Validation --- Epoch: 5, total loss: 317.2210 , PPL: 1.2958, Acc: 0.2612\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 6, Batch: 100, loss: 5.3116 , Train PPL: 1.0081, Train Acc: 0.2790\n",
      "Epoch: 6, Batch: 200, loss: 5.5001 , Train PPL: 1.0084, Train Acc: 0.2790\n",
      "Epoch: 6, Batch: 300, loss: 5.4364 , Train PPL: 1.0083, Train Acc: 0.3247\n",
      "Epoch: 6, Batch: 400, loss: 5.3689 , Train PPL: 1.0082, Train Acc: 0.2500\n",
      "Epoch: 6, Batch: 500, loss: 5.3408 , Train PPL: 1.0082, Train Acc: 0.2515\n",
      "Epoch: 6, Batch: 600, loss: 5.5795 , Train PPL: 1.0085, Train Acc: 0.3018\n",
      "Epoch: 6, Batch: 700, loss: 5.2694 , Train PPL: 1.0081, Train Acc: 0.3293\n",
      "Validation --- Epoch: 6, total loss: 314.2206 , PPL: 1.2926, Acc: 0.2827\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 7, Batch: 100, loss: 5.4868 , Train PPL: 1.0084, Train Acc: 0.2622\n",
      "Epoch: 7, Batch: 200, loss: 5.3420 , Train PPL: 1.0082, Train Acc: 0.3750\n",
      "Epoch: 7, Batch: 300, loss: 5.1454 , Train PPL: 1.0079, Train Acc: 0.3171\n",
      "Epoch: 7, Batch: 400, loss: 5.0152 , Train PPL: 1.0077, Train Acc: 0.3948\n",
      "Epoch: 7, Batch: 500, loss: 5.1641 , Train PPL: 1.0079, Train Acc: 0.3354\n",
      "Epoch: 7, Batch: 600, loss: 5.0550 , Train PPL: 1.0077, Train Acc: 0.3674\n",
      "Epoch: 7, Batch: 700, loss: 5.2582 , Train PPL: 1.0080, Train Acc: 0.3140\n",
      "Validation --- Epoch: 7, total loss: 317.5098 , PPL: 1.2964, Acc: 0.2540\n",
      "lr = 4\n",
      "Epoch: 8, Batch: 100, loss: 5.2009 , Train PPL: 1.0080, Train Acc: 0.2668\n",
      "Epoch: 8, Batch: 200, loss: 5.1120 , Train PPL: 1.0078, Train Acc: 0.3018\n",
      "Epoch: 8, Batch: 300, loss: 5.3582 , Train PPL: 1.0082, Train Acc: 0.3018\n",
      "Epoch: 8, Batch: 400, loss: 5.5291 , Train PPL: 1.0085, Train Acc: 0.2576\n",
      "Epoch: 8, Batch: 500, loss: 5.1537 , Train PPL: 1.0079, Train Acc: 0.3659\n",
      "Epoch: 8, Batch: 600, loss: 5.2995 , Train PPL: 1.0081, Train Acc: 0.2896\n",
      "Epoch: 8, Batch: 700, loss: 5.5313 , Train PPL: 1.0085, Train Acc: 0.2790\n",
      "Validation --- Epoch: 8, total loss: 313.7868 , PPL: 1.2924, Acc: 0.3012\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 9, Batch: 100, loss: 5.1610 , Train PPL: 1.0079, Train Acc: 0.3750\n",
      "Epoch: 9, Batch: 200, loss: 4.7680 , Train PPL: 1.0073, Train Acc: 0.3826\n",
      "Epoch: 9, Batch: 300, loss: 5.0159 , Train PPL: 1.0077, Train Acc: 0.3750\n",
      "Epoch: 9, Batch: 400, loss: 5.1766 , Train PPL: 1.0079, Train Acc: 0.3262\n",
      "Epoch: 9, Batch: 500, loss: 5.1692 , Train PPL: 1.0079, Train Acc: 0.3125\n",
      "Epoch: 9, Batch: 600, loss: 5.4473 , Train PPL: 1.0083, Train Acc: 0.2759\n",
      "Epoch: 9, Batch: 700, loss: 5.4928 , Train PPL: 1.0084, Train Acc: 0.3323\n",
      "Validation --- Epoch: 9, total loss: 311.3980 , PPL: 1.2899, Acc: 0.2825\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 10, Batch: 100, loss: 5.3140 , Train PPL: 1.0081, Train Acc: 0.2820\n",
      "Epoch: 10, Batch: 200, loss: 4.8239 , Train PPL: 1.0074, Train Acc: 0.3491\n",
      "Epoch: 10, Batch: 300, loss: 5.4076 , Train PPL: 1.0083, Train Acc: 0.2973\n",
      "Epoch: 10, Batch: 400, loss: 5.2950 , Train PPL: 1.0081, Train Acc: 0.2637\n",
      "Epoch: 10, Batch: 500, loss: 5.0523 , Train PPL: 1.0077, Train Acc: 0.3293\n",
      "Epoch: 10, Batch: 600, loss: 5.4719 , Train PPL: 1.0084, Train Acc: 0.2957\n",
      "Epoch: 10, Batch: 700, loss: 5.3930 , Train PPL: 1.0083, Train Acc: 0.2607\n",
      "Validation --- Epoch: 10, total loss: 314.0953 , PPL: 1.2928, Acc: 0.2761\n",
      "lr = 4\n",
      "Epoch: 11, Batch: 100, loss: 4.9475 , Train PPL: 1.0076, Train Acc: 0.3750\n",
      "Epoch: 11, Batch: 200, loss: 5.1740 , Train PPL: 1.0079, Train Acc: 0.3003\n",
      "Epoch: 11, Batch: 300, loss: 5.1372 , Train PPL: 1.0079, Train Acc: 0.3201\n",
      "Epoch: 11, Batch: 400, loss: 4.8147 , Train PPL: 1.0074, Train Acc: 0.3323\n",
      "Epoch: 11, Batch: 500, loss: 5.2899 , Train PPL: 1.0081, Train Acc: 0.2835\n",
      "Epoch: 11, Batch: 600, loss: 4.9992 , Train PPL: 1.0076, Train Acc: 0.3445\n",
      "Epoch: 11, Batch: 700, loss: 5.2725 , Train PPL: 1.0081, Train Acc: 0.2805\n",
      "Validation --- Epoch: 11, total loss: 314.7237 , PPL: 1.2935, Acc: 0.2606\n",
      "lr = 4\n",
      "Epoch: 12, Batch: 100, loss: 5.1252 , Train PPL: 1.0078, Train Acc: 0.3079\n",
      "Epoch: 12, Batch: 200, loss: 4.8107 , Train PPL: 1.0074, Train Acc: 0.3720\n",
      "Epoch: 12, Batch: 300, loss: 5.1299 , Train PPL: 1.0079, Train Acc: 0.3369\n",
      "Epoch: 12, Batch: 400, loss: 4.9428 , Train PPL: 1.0076, Train Acc: 0.3430\n",
      "Epoch: 12, Batch: 500, loss: 4.8103 , Train PPL: 1.0074, Train Acc: 0.3598\n",
      "Epoch: 12, Batch: 600, loss: 5.1688 , Train PPL: 1.0079, Train Acc: 0.3720\n",
      "Epoch: 12, Batch: 700, loss: 5.4174 , Train PPL: 1.0083, Train Acc: 0.2835\n",
      "Validation --- Epoch: 12, total loss: 312.7706 , PPL: 1.2914, Acc: 0.2638\n",
      "lr = 4\n",
      "Epoch: 13, Batch: 100, loss: 5.0255 , Train PPL: 1.0077, Train Acc: 0.3460\n",
      "Epoch: 13, Batch: 200, loss: 5.0373 , Train PPL: 1.0077, Train Acc: 0.3552\n",
      "Epoch: 13, Batch: 300, loss: 5.2202 , Train PPL: 1.0080, Train Acc: 0.3308\n",
      "Epoch: 13, Batch: 400, loss: 4.8021 , Train PPL: 1.0073, Train Acc: 0.3689\n",
      "Epoch: 13, Batch: 500, loss: 4.8105 , Train PPL: 1.0074, Train Acc: 0.3460\n",
      "Epoch: 13, Batch: 600, loss: 5.0584 , Train PPL: 1.0077, Train Acc: 0.3277\n",
      "Epoch: 13, Batch: 700, loss: 5.2547 , Train PPL: 1.0080, Train Acc: 0.3232\n",
      "Validation --- Epoch: 13, total loss: 313.5035 , PPL: 1.2922, Acc: 0.2781\n",
      "lr = 4\n",
      "Epoch: 14, Batch: 100, loss: 4.9487 , Train PPL: 1.0076, Train Acc: 0.3171\n",
      "Epoch: 14, Batch: 200, loss: 5.0279 , Train PPL: 1.0077, Train Acc: 0.3415\n",
      "Epoch: 14, Batch: 300, loss: 5.2152 , Train PPL: 1.0080, Train Acc: 0.3155\n",
      "Epoch: 14, Batch: 400, loss: 5.2345 , Train PPL: 1.0080, Train Acc: 0.2927\n",
      "Epoch: 14, Batch: 500, loss: 5.1747 , Train PPL: 1.0079, Train Acc: 0.2988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Batch: 600, loss: 5.0966 , Train PPL: 1.0078, Train Acc: 0.3034\n",
      "Epoch: 14, Batch: 700, loss: 5.3374 , Train PPL: 1.0082, Train Acc: 0.3110\n",
      "Validation --- Epoch: 14, total loss: 315.6962 , PPL: 1.2945, Acc: 0.2422\n",
      "lr = 4\n",
      "Epoch: 15, Batch: 100, loss: 4.8201 , Train PPL: 1.0074, Train Acc: 0.3872\n",
      "Epoch: 15, Batch: 200, loss: 5.0907 , Train PPL: 1.0078, Train Acc: 0.3155\n",
      "Epoch: 15, Batch: 300, loss: 4.9318 , Train PPL: 1.0075, Train Acc: 0.3735\n",
      "Epoch: 15, Batch: 400, loss: 5.0245 , Train PPL: 1.0077, Train Acc: 0.2957\n",
      "Epoch: 15, Batch: 500, loss: 5.0694 , Train PPL: 1.0078, Train Acc: 0.3399\n",
      "Epoch: 15, Batch: 600, loss: 4.9473 , Train PPL: 1.0076, Train Acc: 0.3506\n",
      "Epoch: 15, Batch: 700, loss: 4.9373 , Train PPL: 1.0076, Train Acc: 0.3277\n",
      "Validation --- Epoch: 15, total loss: 312.8839 , PPL: 1.2917, Acc: 0.2695\n",
      "lr = 2.0\n",
      "Epoch: 16, Batch: 100, loss: 4.9912 , Train PPL: 1.0076, Train Acc: 0.3140\n",
      "Epoch: 16, Batch: 200, loss: 4.8764 , Train PPL: 1.0075, Train Acc: 0.3506\n",
      "Epoch: 16, Batch: 300, loss: 4.6756 , Train PPL: 1.0072, Train Acc: 0.3582\n",
      "Epoch: 16, Batch: 400, loss: 4.9981 , Train PPL: 1.0076, Train Acc: 0.3445\n",
      "Epoch: 16, Batch: 500, loss: 5.0896 , Train PPL: 1.0078, Train Acc: 0.3293\n",
      "Epoch: 16, Batch: 600, loss: 5.1470 , Train PPL: 1.0079, Train Acc: 0.3018\n",
      "Epoch: 16, Batch: 700, loss: 4.9192 , Train PPL: 1.0075, Train Acc: 0.4177\n",
      "Validation --- Epoch: 16, total loss: 305.9474 , PPL: 1.2843, Acc: 0.3190\n",
      "lr = 2.0\n",
      "wrote model\n",
      "Epoch: 17, Batch: 100, loss: 5.1780 , Train PPL: 1.0079, Train Acc: 0.3354\n",
      "Epoch: 17, Batch: 200, loss: 4.9407 , Train PPL: 1.0076, Train Acc: 0.3735\n",
      "Epoch: 17, Batch: 300, loss: 4.9446 , Train PPL: 1.0076, Train Acc: 0.3323\n",
      "Epoch: 17, Batch: 400, loss: 4.9213 , Train PPL: 1.0075, Train Acc: 0.3598\n",
      "Epoch: 17, Batch: 500, loss: 4.9912 , Train PPL: 1.0076, Train Acc: 0.3323\n",
      "Epoch: 17, Batch: 600, loss: 4.8627 , Train PPL: 1.0074, Train Acc: 0.3689\n",
      "Epoch: 17, Batch: 700, loss: 4.8858 , Train PPL: 1.0075, Train Acc: 0.3735\n",
      "Validation --- Epoch: 17, total loss: 306.9026 , PPL: 1.2854, Acc: 0.3096\n",
      "lr = 2.0\n",
      "Epoch: 18, Batch: 100, loss: 4.7349 , Train PPL: 1.0072, Train Acc: 0.3308\n",
      "Epoch: 18, Batch: 200, loss: 4.9811 , Train PPL: 1.0076, Train Acc: 0.2927\n",
      "Epoch: 18, Batch: 300, loss: 4.9886 , Train PPL: 1.0076, Train Acc: 0.3537\n",
      "Epoch: 18, Batch: 400, loss: 4.6811 , Train PPL: 1.0072, Train Acc: 0.3720\n",
      "Epoch: 18, Batch: 500, loss: 4.9415 , Train PPL: 1.0076, Train Acc: 0.3643\n",
      "Epoch: 18, Batch: 600, loss: 4.9886 , Train PPL: 1.0076, Train Acc: 0.3201\n",
      "Epoch: 18, Batch: 700, loss: 4.5646 , Train PPL: 1.0070, Train Acc: 0.4345\n",
      "Validation --- Epoch: 18, total loss: 307.0518 , PPL: 1.2856, Acc: 0.3140\n",
      "lr = 2.0\n",
      "Epoch: 19, Batch: 100, loss: 4.9064 , Train PPL: 1.0075, Train Acc: 0.3262\n",
      "Epoch: 19, Batch: 200, loss: 4.8793 , Train PPL: 1.0075, Train Acc: 0.3049\n",
      "Epoch: 19, Batch: 300, loss: 4.8823 , Train PPL: 1.0075, Train Acc: 0.3720\n",
      "Epoch: 19, Batch: 400, loss: 4.6412 , Train PPL: 1.0071, Train Acc: 0.3918\n",
      "Epoch: 19, Batch: 500, loss: 4.5795 , Train PPL: 1.0070, Train Acc: 0.4055\n",
      "Epoch: 19, Batch: 600, loss: 4.5981 , Train PPL: 1.0070, Train Acc: 0.4101\n",
      "Epoch: 19, Batch: 700, loss: 4.5724 , Train PPL: 1.0070, Train Acc: 0.3933\n",
      "Validation --- Epoch: 19, total loss: 306.2280 , PPL: 1.2847, Acc: 0.3124\n",
      "lr = 2.0\n",
      "Epoch: 20, Batch: 100, loss: 4.9756 , Train PPL: 1.0076, Train Acc: 0.3079\n",
      "Epoch: 20, Batch: 200, loss: 4.9231 , Train PPL: 1.0075, Train Acc: 0.3155\n",
      "Epoch: 20, Batch: 300, loss: 4.6684 , Train PPL: 1.0071, Train Acc: 0.3491\n",
      "Epoch: 20, Batch: 400, loss: 4.8656 , Train PPL: 1.0074, Train Acc: 0.3354\n",
      "Epoch: 20, Batch: 500, loss: 4.8377 , Train PPL: 1.0074, Train Acc: 0.3293\n",
      "Epoch: 20, Batch: 600, loss: 4.7917 , Train PPL: 1.0073, Train Acc: 0.3415\n",
      "Epoch: 20, Batch: 700, loss: 4.9826 , Train PPL: 1.0076, Train Acc: 0.3293\n",
      "Validation --- Epoch: 20, total loss: 304.9495 , PPL: 1.2833, Acc: 0.3298\n",
      "lr = 2.0\n",
      "wrote model\n",
      "Epoch: 21, Batch: 100, loss: 4.7440 , Train PPL: 1.0073, Train Acc: 0.3491\n",
      "Epoch: 21, Batch: 200, loss: 4.9892 , Train PPL: 1.0076, Train Acc: 0.3735\n",
      "Epoch: 21, Batch: 300, loss: 4.7372 , Train PPL: 1.0072, Train Acc: 0.3262\n",
      "Epoch: 21, Batch: 400, loss: 5.1371 , Train PPL: 1.0079, Train Acc: 0.3308\n",
      "Epoch: 21, Batch: 500, loss: 4.7606 , Train PPL: 1.0073, Train Acc: 0.3521\n",
      "Epoch: 21, Batch: 600, loss: 4.8315 , Train PPL: 1.0074, Train Acc: 0.3552\n",
      "Epoch: 21, Batch: 700, loss: 5.0258 , Train PPL: 1.0077, Train Acc: 0.3262\n",
      "Validation --- Epoch: 21, total loss: 305.6152 , PPL: 1.2841, Acc: 0.3335\n",
      "lr = 2.0\n",
      "Epoch: 22, Batch: 100, loss: 4.8060 , Train PPL: 1.0074, Train Acc: 0.3201\n",
      "Epoch: 22, Batch: 200, loss: 4.9062 , Train PPL: 1.0075, Train Acc: 0.2912\n",
      "Epoch: 22, Batch: 300, loss: 4.8407 , Train PPL: 1.0074, Train Acc: 0.3460\n",
      "Epoch: 22, Batch: 400, loss: 5.1454 , Train PPL: 1.0079, Train Acc: 0.3354\n",
      "Epoch: 22, Batch: 500, loss: 4.8233 , Train PPL: 1.0074, Train Acc: 0.3338\n",
      "Epoch: 22, Batch: 600, loss: 4.7489 , Train PPL: 1.0073, Train Acc: 0.3369\n",
      "Epoch: 22, Batch: 700, loss: 4.7367 , Train PPL: 1.0072, Train Acc: 0.4009\n",
      "Validation --- Epoch: 22, total loss: 305.0269 , PPL: 1.2835, Acc: 0.3178\n",
      "lr = 2.0\n",
      "Epoch: 23, Batch: 100, loss: 4.4303 , Train PPL: 1.0068, Train Acc: 0.4101\n",
      "Epoch: 23, Batch: 200, loss: 4.7630 , Train PPL: 1.0073, Train Acc: 0.3155\n",
      "Epoch: 23, Batch: 300, loss: 4.8422 , Train PPL: 1.0074, Train Acc: 0.3476\n",
      "Epoch: 23, Batch: 400, loss: 4.9364 , Train PPL: 1.0076, Train Acc: 0.3857\n",
      "Epoch: 23, Batch: 500, loss: 4.7393 , Train PPL: 1.0073, Train Acc: 0.3552\n",
      "Epoch: 23, Batch: 600, loss: 4.9703 , Train PPL: 1.0076, Train Acc: 0.3399\n",
      "Epoch: 23, Batch: 700, loss: 4.9065 , Train PPL: 1.0075, Train Acc: 0.3552\n",
      "Validation --- Epoch: 23, total loss: 303.8035 , PPL: 1.2820, Acc: 0.3402\n",
      "lr = 2.0\n",
      "wrote model\n",
      "Epoch: 24, Batch: 100, loss: 4.8698 , Train PPL: 1.0075, Train Acc: 0.3262\n",
      "Epoch: 24, Batch: 200, loss: 4.7076 , Train PPL: 1.0072, Train Acc: 0.4024\n",
      "Epoch: 24, Batch: 300, loss: 4.7744 , Train PPL: 1.0073, Train Acc: 0.3537\n",
      "Epoch: 24, Batch: 400, loss: 4.6280 , Train PPL: 1.0071, Train Acc: 0.4070\n",
      "Epoch: 24, Batch: 500, loss: 4.7615 , Train PPL: 1.0073, Train Acc: 0.3323\n",
      "Epoch: 24, Batch: 600, loss: 5.2135 , Train PPL: 1.0080, Train Acc: 0.3232\n",
      "Epoch: 24, Batch: 700, loss: 4.9107 , Train PPL: 1.0075, Train Acc: 0.3720\n",
      "Validation --- Epoch: 24, total loss: 304.8241 , PPL: 1.2833, Acc: 0.3283\n",
      "lr = 2.0\n",
      "Epoch: 25, Batch: 100, loss: 4.5571 , Train PPL: 1.0070, Train Acc: 0.3796\n",
      "Epoch: 25, Batch: 200, loss: 4.5893 , Train PPL: 1.0070, Train Acc: 0.3643\n",
      "Epoch: 25, Batch: 300, loss: 4.8730 , Train PPL: 1.0075, Train Acc: 0.3018\n",
      "Epoch: 25, Batch: 400, loss: 4.7511 , Train PPL: 1.0073, Train Acc: 0.3338\n",
      "Epoch: 25, Batch: 500, loss: 4.7728 , Train PPL: 1.0073, Train Acc: 0.3308\n",
      "Epoch: 25, Batch: 600, loss: 4.7976 , Train PPL: 1.0073, Train Acc: 0.3064\n",
      "Epoch: 25, Batch: 700, loss: 4.9390 , Train PPL: 1.0076, Train Acc: 0.3216\n",
      "Validation --- Epoch: 25, total loss: 304.6102 , PPL: 1.2830, Acc: 0.3276\n",
      "lr = 2.0\n",
      "Epoch: 26, Batch: 100, loss: 4.6639 , Train PPL: 1.0071, Train Acc: 0.3963\n",
      "Epoch: 26, Batch: 200, loss: 4.7032 , Train PPL: 1.0072, Train Acc: 0.3491\n",
      "Epoch: 26, Batch: 300, loss: 4.8506 , Train PPL: 1.0074, Train Acc: 0.3110\n",
      "Epoch: 26, Batch: 400, loss: 4.9143 , Train PPL: 1.0075, Train Acc: 0.3384\n",
      "Epoch: 26, Batch: 500, loss: 4.8193 , Train PPL: 1.0074, Train Acc: 0.3460\n",
      "Epoch: 26, Batch: 600, loss: 4.6119 , Train PPL: 1.0071, Train Acc: 0.4101\n",
      "Epoch: 26, Batch: 700, loss: 5.1332 , Train PPL: 1.0079, Train Acc: 0.2912\n",
      "Validation --- Epoch: 26, total loss: 303.9521 , PPL: 1.2823, Acc: 0.3301\n",
      "lr = 2.0\n",
      "Epoch: 27, Batch: 100, loss: 4.6112 , Train PPL: 1.0071, Train Acc: 0.3598\n",
      "Epoch: 27, Batch: 200, loss: 4.4916 , Train PPL: 1.0069, Train Acc: 0.3567\n",
      "Epoch: 27, Batch: 300, loss: 4.9023 , Train PPL: 1.0075, Train Acc: 0.3308\n",
      "Epoch: 27, Batch: 400, loss: 4.5158 , Train PPL: 1.0069, Train Acc: 0.3994\n",
      "Epoch: 27, Batch: 500, loss: 4.7749 , Train PPL: 1.0073, Train Acc: 0.3826\n",
      "Epoch: 27, Batch: 600, loss: 4.6718 , Train PPL: 1.0071, Train Acc: 0.3659\n",
      "Epoch: 27, Batch: 700, loss: 4.6700 , Train PPL: 1.0071, Train Acc: 0.3796\n",
      "Validation --- Epoch: 27, total loss: 305.0169 , PPL: 1.2835, Acc: 0.3209\n",
      "lr = 2.0\n",
      "Epoch: 28, Batch: 100, loss: 4.4637 , Train PPL: 1.0068, Train Acc: 0.4451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Batch: 200, loss: 4.9484 , Train PPL: 1.0076, Train Acc: 0.3902\n",
      "Epoch: 28, Batch: 300, loss: 4.8479 , Train PPL: 1.0074, Train Acc: 0.3216\n",
      "Epoch: 28, Batch: 400, loss: 4.6737 , Train PPL: 1.0071, Train Acc: 0.3811\n",
      "Epoch: 28, Batch: 500, loss: 4.8912 , Train PPL: 1.0075, Train Acc: 0.3140\n",
      "Epoch: 28, Batch: 600, loss: 4.8870 , Train PPL: 1.0075, Train Acc: 0.3369\n",
      "Epoch: 28, Batch: 700, loss: 4.4344 , Train PPL: 1.0068, Train Acc: 0.3887\n",
      "Validation --- Epoch: 28, total loss: 305.1396 , PPL: 1.2836, Acc: 0.3147\n",
      "lr = 2.0\n",
      "Epoch: 29, Batch: 100, loss: 4.6747 , Train PPL: 1.0072, Train Acc: 0.3979\n",
      "Epoch: 29, Batch: 200, loss: 4.7686 , Train PPL: 1.0073, Train Acc: 0.3430\n",
      "Epoch: 29, Batch: 300, loss: 4.8959 , Train PPL: 1.0075, Train Acc: 0.3308\n",
      "Epoch: 29, Batch: 400, loss: 4.8978 , Train PPL: 1.0075, Train Acc: 0.3689\n",
      "Epoch: 29, Batch: 500, loss: 4.7492 , Train PPL: 1.0073, Train Acc: 0.3537\n",
      "Epoch: 29, Batch: 600, loss: 4.6855 , Train PPL: 1.0072, Train Acc: 0.3537\n",
      "Epoch: 29, Batch: 700, loss: 4.7004 , Train PPL: 1.0072, Train Acc: 0.3582\n",
      "Validation --- Epoch: 29, total loss: 304.2724 , PPL: 1.2827, Acc: 0.3380\n",
      "lr = 1.0\n",
      "Epoch: 30, Batch: 100, loss: 4.5906 , Train PPL: 1.0070, Train Acc: 0.3826\n",
      "Epoch: 30, Batch: 200, loss: 4.4850 , Train PPL: 1.0069, Train Acc: 0.3826\n",
      "Epoch: 30, Batch: 300, loss: 4.6803 , Train PPL: 1.0072, Train Acc: 0.3567\n",
      "Epoch: 30, Batch: 400, loss: 4.7211 , Train PPL: 1.0072, Train Acc: 0.3659\n",
      "Epoch: 30, Batch: 500, loss: 4.6976 , Train PPL: 1.0072, Train Acc: 0.3948\n",
      "Epoch: 30, Batch: 600, loss: 4.6780 , Train PPL: 1.0072, Train Acc: 0.3857\n",
      "Epoch: 30, Batch: 700, loss: 4.6435 , Train PPL: 1.0071, Train Acc: 0.4040\n",
      "Validation --- Epoch: 30, total loss: 302.1513 , PPL: 1.2805, Acc: 0.3507\n",
      "lr = 1.0\n",
      "wrote model\n",
      "Epoch: 31, Batch: 100, loss: 4.3775 , Train PPL: 1.0067, Train Acc: 0.4024\n",
      "Epoch: 31, Batch: 200, loss: 4.9370 , Train PPL: 1.0076, Train Acc: 0.3338\n",
      "Epoch: 31, Batch: 300, loss: 4.6615 , Train PPL: 1.0071, Train Acc: 0.3460\n",
      "Epoch: 31, Batch: 400, loss: 4.7240 , Train PPL: 1.0072, Train Acc: 0.3841\n",
      "Epoch: 31, Batch: 500, loss: 4.5265 , Train PPL: 1.0069, Train Acc: 0.4253\n",
      "Epoch: 31, Batch: 600, loss: 4.7240 , Train PPL: 1.0072, Train Acc: 0.3384\n",
      "Epoch: 31, Batch: 700, loss: 4.5309 , Train PPL: 1.0069, Train Acc: 0.3979\n",
      "Validation --- Epoch: 31, total loss: 301.4479 , PPL: 1.2798, Acc: 0.3567\n",
      "lr = 1.0\n",
      "wrote model\n",
      "Epoch: 32, Batch: 100, loss: 4.4946 , Train PPL: 1.0069, Train Acc: 0.3933\n",
      "Epoch: 32, Batch: 200, loss: 4.6997 , Train PPL: 1.0072, Train Acc: 0.3567\n",
      "Epoch: 32, Batch: 300, loss: 4.7494 , Train PPL: 1.0073, Train Acc: 0.3780\n",
      "Epoch: 32, Batch: 400, loss: 4.8972 , Train PPL: 1.0075, Train Acc: 0.3537\n",
      "Epoch: 32, Batch: 500, loss: 4.7071 , Train PPL: 1.0072, Train Acc: 0.3598\n",
      "Epoch: 32, Batch: 600, loss: 4.8992 , Train PPL: 1.0075, Train Acc: 0.3857\n",
      "Epoch: 32, Batch: 700, loss: 4.6643 , Train PPL: 1.0071, Train Acc: 0.3415\n",
      "Validation --- Epoch: 32, total loss: 301.1288 , PPL: 1.2795, Acc: 0.3618\n",
      "lr = 1.0\n",
      "wrote model\n",
      "Epoch: 33, Batch: 100, loss: 4.6753 , Train PPL: 1.0072, Train Acc: 0.3491\n",
      "Epoch: 33, Batch: 200, loss: 4.5542 , Train PPL: 1.0070, Train Acc: 0.3948\n",
      "Epoch: 33, Batch: 300, loss: 4.7076 , Train PPL: 1.0072, Train Acc: 0.4131\n",
      "Epoch: 33, Batch: 400, loss: 4.7780 , Train PPL: 1.0073, Train Acc: 0.3796\n",
      "Epoch: 33, Batch: 500, loss: 4.6411 , Train PPL: 1.0071, Train Acc: 0.3780\n",
      "Epoch: 33, Batch: 600, loss: 4.6191 , Train PPL: 1.0071, Train Acc: 0.3415\n",
      "Epoch: 33, Batch: 700, loss: 4.6685 , Train PPL: 1.0071, Train Acc: 0.3552\n",
      "Validation --- Epoch: 33, total loss: 301.3778 , PPL: 1.2797, Acc: 0.3593\n",
      "lr = 1.0\n",
      "Epoch: 34, Batch: 100, loss: 4.6488 , Train PPL: 1.0071, Train Acc: 0.4268\n",
      "Epoch: 34, Batch: 200, loss: 4.7893 , Train PPL: 1.0073, Train Acc: 0.3902\n",
      "Epoch: 34, Batch: 300, loss: 4.4339 , Train PPL: 1.0068, Train Acc: 0.3811\n",
      "Epoch: 34, Batch: 400, loss: 4.6079 , Train PPL: 1.0070, Train Acc: 0.3826\n",
      "Epoch: 34, Batch: 500, loss: 4.7149 , Train PPL: 1.0072, Train Acc: 0.3506\n",
      "Epoch: 34, Batch: 600, loss: 4.5285 , Train PPL: 1.0069, Train Acc: 0.4070\n",
      "Epoch: 34, Batch: 700, loss: 4.8913 , Train PPL: 1.0075, Train Acc: 0.3247\n",
      "Validation --- Epoch: 34, total loss: 301.1200 , PPL: 1.2794, Acc: 0.3608\n",
      "lr = 1.0\n",
      "wrote model\n",
      "Epoch: 35, Batch: 100, loss: 4.6127 , Train PPL: 1.0071, Train Acc: 0.3628\n",
      "Epoch: 35, Batch: 200, loss: 4.7241 , Train PPL: 1.0072, Train Acc: 0.3262\n",
      "Epoch: 35, Batch: 300, loss: 4.5939 , Train PPL: 1.0070, Train Acc: 0.3628\n",
      "Epoch: 35, Batch: 400, loss: 4.5386 , Train PPL: 1.0069, Train Acc: 0.4055\n",
      "Epoch: 35, Batch: 500, loss: 4.3266 , Train PPL: 1.0066, Train Acc: 0.4451\n",
      "Epoch: 35, Batch: 600, loss: 4.1524 , Train PPL: 1.0064, Train Acc: 0.4771\n",
      "Epoch: 35, Batch: 700, loss: 4.8039 , Train PPL: 1.0073, Train Acc: 0.3125\n",
      "Validation --- Epoch: 35, total loss: 300.9099 , PPL: 1.2792, Acc: 0.3590\n",
      "lr = 1.0\n",
      "wrote model\n",
      "Epoch: 36, Batch: 100, loss: 4.4235 , Train PPL: 1.0068, Train Acc: 0.3765\n",
      "Epoch: 36, Batch: 200, loss: 4.4319 , Train PPL: 1.0068, Train Acc: 0.4162\n",
      "Epoch: 36, Batch: 300, loss: 4.6856 , Train PPL: 1.0072, Train Acc: 0.3598\n",
      "Epoch: 36, Batch: 400, loss: 4.5092 , Train PPL: 1.0069, Train Acc: 0.3933\n",
      "Epoch: 36, Batch: 500, loss: 4.7457 , Train PPL: 1.0073, Train Acc: 0.3735\n",
      "Epoch: 36, Batch: 600, loss: 4.5717 , Train PPL: 1.0070, Train Acc: 0.3750\n",
      "Epoch: 36, Batch: 700, loss: 4.9883 , Train PPL: 1.0076, Train Acc: 0.3369\n",
      "Validation --- Epoch: 36, total loss: 301.1448 , PPL: 1.2795, Acc: 0.3624\n",
      "lr = 1.0\n",
      "Epoch: 37, Batch: 100, loss: 4.3051 , Train PPL: 1.0066, Train Acc: 0.4223\n",
      "Epoch: 37, Batch: 200, loss: 4.6053 , Train PPL: 1.0070, Train Acc: 0.3125\n",
      "Epoch: 37, Batch: 300, loss: 4.6397 , Train PPL: 1.0071, Train Acc: 0.3857\n",
      "Epoch: 37, Batch: 400, loss: 4.8165 , Train PPL: 1.0074, Train Acc: 0.3598\n",
      "Epoch: 37, Batch: 500, loss: 4.3200 , Train PPL: 1.0066, Train Acc: 0.4085\n",
      "Epoch: 37, Batch: 600, loss: 4.9320 , Train PPL: 1.0075, Train Acc: 0.3430\n",
      "Epoch: 37, Batch: 700, loss: 4.5500 , Train PPL: 1.0070, Train Acc: 0.3963\n",
      "Validation --- Epoch: 37, total loss: 300.9589 , PPL: 1.2793, Acc: 0.3606\n",
      "lr = 1.0\n",
      "Epoch: 38, Batch: 100, loss: 4.8712 , Train PPL: 1.0075, Train Acc: 0.3277\n",
      "Epoch: 38, Batch: 200, loss: 4.3788 , Train PPL: 1.0067, Train Acc: 0.4299\n",
      "Epoch: 38, Batch: 300, loss: 4.4430 , Train PPL: 1.0068, Train Acc: 0.4070\n",
      "Epoch: 38, Batch: 400, loss: 4.5657 , Train PPL: 1.0070, Train Acc: 0.3628\n",
      "Epoch: 38, Batch: 500, loss: 4.6404 , Train PPL: 1.0071, Train Acc: 0.3521\n",
      "Epoch: 38, Batch: 600, loss: 4.7427 , Train PPL: 1.0073, Train Acc: 0.3643\n",
      "Epoch: 38, Batch: 700, loss: 4.3606 , Train PPL: 1.0067, Train Acc: 0.4024\n",
      "Validation --- Epoch: 38, total loss: 300.7341 , PPL: 1.2791, Acc: 0.3637\n",
      "lr = 1.0\n",
      "wrote model\n",
      "Epoch: 39, Batch: 100, loss: 4.2795 , Train PPL: 1.0065, Train Acc: 0.4527\n",
      "Epoch: 39, Batch: 200, loss: 4.4738 , Train PPL: 1.0068, Train Acc: 0.3826\n",
      "Epoch: 39, Batch: 300, loss: 4.8380 , Train PPL: 1.0074, Train Acc: 0.3262\n",
      "Epoch: 39, Batch: 400, loss: 4.4942 , Train PPL: 1.0069, Train Acc: 0.3643\n",
      "Epoch: 39, Batch: 500, loss: 4.7680 , Train PPL: 1.0073, Train Acc: 0.3186\n",
      "Epoch: 39, Batch: 600, loss: 4.5754 , Train PPL: 1.0070, Train Acc: 0.3430\n",
      "Epoch: 39, Batch: 700, loss: 4.6208 , Train PPL: 1.0071, Train Acc: 0.4085\n",
      "Validation --- Epoch: 39, total loss: 301.1549 , PPL: 1.2795, Acc: 0.3649\n",
      "lr = 1.0\n",
      "Epoch: 40, Batch: 100, loss: 4.6112 , Train PPL: 1.0071, Train Acc: 0.3735\n",
      "Epoch: 40, Batch: 200, loss: 4.8714 , Train PPL: 1.0075, Train Acc: 0.3384\n",
      "Epoch: 40, Batch: 300, loss: 4.6480 , Train PPL: 1.0071, Train Acc: 0.3857\n",
      "Epoch: 40, Batch: 400, loss: 4.7470 , Train PPL: 1.0073, Train Acc: 0.3430\n",
      "Epoch: 40, Batch: 500, loss: 4.7455 , Train PPL: 1.0073, Train Acc: 0.3125\n",
      "Epoch: 40, Batch: 600, loss: 4.6462 , Train PPL: 1.0071, Train Acc: 0.4070\n",
      "Epoch: 40, Batch: 700, loss: 4.5505 , Train PPL: 1.0070, Train Acc: 0.3765\n",
      "Validation --- Epoch: 40, total loss: 300.9557 , PPL: 1.2792, Acc: 0.3617\n",
      "lr = 1.0\n",
      "Epoch: 41, Batch: 100, loss: 4.3954 , Train PPL: 1.0067, Train Acc: 0.4055\n",
      "Epoch: 41, Batch: 200, loss: 4.7571 , Train PPL: 1.0073, Train Acc: 0.3506\n",
      "Epoch: 41, Batch: 300, loss: 4.6696 , Train PPL: 1.0071, Train Acc: 0.3247\n",
      "Epoch: 41, Batch: 400, loss: 4.6479 , Train PPL: 1.0071, Train Acc: 0.3537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41, Batch: 500, loss: 4.8474 , Train PPL: 1.0074, Train Acc: 0.3994\n",
      "Epoch: 41, Batch: 600, loss: 4.5539 , Train PPL: 1.0070, Train Acc: 0.3689\n",
      "Epoch: 41, Batch: 700, loss: 4.5167 , Train PPL: 1.0069, Train Acc: 0.4253\n",
      "Validation --- Epoch: 41, total loss: 300.5365 , PPL: 1.2788, Acc: 0.3643\n",
      "lr = 1.0\n",
      "wrote model\n",
      "Epoch: 42, Batch: 100, loss: 4.8114 , Train PPL: 1.0074, Train Acc: 0.3003\n",
      "Epoch: 42, Batch: 200, loss: 4.6496 , Train PPL: 1.0071, Train Acc: 0.3811\n",
      "Epoch: 42, Batch: 300, loss: 4.7148 , Train PPL: 1.0072, Train Acc: 0.3613\n",
      "Epoch: 42, Batch: 400, loss: 4.4732 , Train PPL: 1.0068, Train Acc: 0.4177\n",
      "Epoch: 42, Batch: 500, loss: 4.4821 , Train PPL: 1.0069, Train Acc: 0.4345\n",
      "Epoch: 42, Batch: 600, loss: 4.3064 , Train PPL: 1.0066, Train Acc: 0.4024\n",
      "Epoch: 42, Batch: 700, loss: 4.8553 , Train PPL: 1.0074, Train Acc: 0.3430\n",
      "Validation --- Epoch: 42, total loss: 300.8099 , PPL: 1.2792, Acc: 0.3675\n",
      "lr = 1.0\n",
      "Epoch: 43, Batch: 100, loss: 4.5383 , Train PPL: 1.0069, Train Acc: 0.3735\n",
      "Epoch: 43, Batch: 200, loss: 4.6823 , Train PPL: 1.0072, Train Acc: 0.3598\n",
      "Epoch: 43, Batch: 300, loss: 4.5966 , Train PPL: 1.0070, Train Acc: 0.3613\n",
      "Epoch: 43, Batch: 400, loss: 4.6248 , Train PPL: 1.0071, Train Acc: 0.3552\n",
      "Epoch: 43, Batch: 500, loss: 4.9315 , Train PPL: 1.0075, Train Acc: 0.3140\n",
      "Epoch: 43, Batch: 600, loss: 4.6307 , Train PPL: 1.0071, Train Acc: 0.3491\n",
      "Epoch: 43, Batch: 700, loss: 4.5352 , Train PPL: 1.0069, Train Acc: 0.3598\n",
      "Validation --- Epoch: 43, total loss: 300.8854 , PPL: 1.2793, Acc: 0.3665\n",
      "lr = 1.0\n",
      "Epoch: 44, Batch: 100, loss: 4.7597 , Train PPL: 1.0073, Train Acc: 0.3323\n",
      "Epoch: 44, Batch: 200, loss: 4.4638 , Train PPL: 1.0068, Train Acc: 0.4192\n",
      "Epoch: 44, Batch: 300, loss: 4.5967 , Train PPL: 1.0070, Train Acc: 0.3384\n",
      "Epoch: 44, Batch: 400, loss: 4.7931 , Train PPL: 1.0073, Train Acc: 0.3155\n",
      "Epoch: 44, Batch: 500, loss: 4.5099 , Train PPL: 1.0069, Train Acc: 0.4299\n",
      "Epoch: 44, Batch: 600, loss: 4.5983 , Train PPL: 1.0070, Train Acc: 0.3628\n",
      "Epoch: 44, Batch: 700, loss: 4.8318 , Train PPL: 1.0074, Train Acc: 0.2790\n",
      "Validation --- Epoch: 44, total loss: 300.6972 , PPL: 1.2791, Acc: 0.3656\n",
      "lr = 1.0\n",
      "Epoch: 45, Batch: 100, loss: 4.7073 , Train PPL: 1.0072, Train Acc: 0.3628\n",
      "Epoch: 45, Batch: 200, loss: 4.6620 , Train PPL: 1.0071, Train Acc: 0.3201\n",
      "Epoch: 45, Batch: 300, loss: 4.9408 , Train PPL: 1.0076, Train Acc: 0.3110\n",
      "Epoch: 45, Batch: 400, loss: 4.5769 , Train PPL: 1.0070, Train Acc: 0.3598\n",
      "Epoch: 45, Batch: 500, loss: 4.5767 , Train PPL: 1.0070, Train Acc: 0.3841\n",
      "Epoch: 45, Batch: 600, loss: 4.4233 , Train PPL: 1.0068, Train Acc: 0.3841\n",
      "Epoch: 45, Batch: 700, loss: 3.9168 , Train PPL: 1.0060, Train Acc: 0.4710\n",
      "Validation --- Epoch: 45, total loss: 300.9792 , PPL: 1.2794, Acc: 0.3681\n",
      "lr = 1.0\n",
      "Epoch: 46, Batch: 100, loss: 4.3798 , Train PPL: 1.0067, Train Acc: 0.4177\n",
      "Epoch: 46, Batch: 200, loss: 4.7641 , Train PPL: 1.0073, Train Acc: 0.3567\n",
      "Epoch: 46, Batch: 300, loss: 4.3112 , Train PPL: 1.0066, Train Acc: 0.3826\n",
      "Epoch: 46, Batch: 400, loss: 4.6359 , Train PPL: 1.0071, Train Acc: 0.3689\n",
      "Epoch: 46, Batch: 500, loss: 4.6955 , Train PPL: 1.0072, Train Acc: 0.3308\n",
      "Epoch: 46, Batch: 600, loss: 4.4987 , Train PPL: 1.0069, Train Acc: 0.3765\n",
      "Epoch: 46, Batch: 700, loss: 4.7887 , Train PPL: 1.0073, Train Acc: 0.3277\n",
      "Validation --- Epoch: 46, total loss: 300.6609 , PPL: 1.2790, Acc: 0.3655\n",
      "lr = 1.0\n",
      "Epoch: 47, Batch: 100, loss: 4.8027 , Train PPL: 1.0073, Train Acc: 0.3338\n",
      "Epoch: 47, Batch: 200, loss: 4.5851 , Train PPL: 1.0070, Train Acc: 0.3826\n",
      "Epoch: 47, Batch: 300, loss: 4.4346 , Train PPL: 1.0068, Train Acc: 0.3841\n",
      "Epoch: 47, Batch: 400, loss: 4.7157 , Train PPL: 1.0072, Train Acc: 0.3598\n",
      "Epoch: 47, Batch: 500, loss: 4.5994 , Train PPL: 1.0070, Train Acc: 0.3537\n",
      "Epoch: 47, Batch: 600, loss: 4.4521 , Train PPL: 1.0068, Train Acc: 0.4055\n",
      "Epoch: 47, Batch: 700, loss: 4.5484 , Train PPL: 1.0070, Train Acc: 0.3720\n",
      "Validation --- Epoch: 47, total loss: 300.8906 , PPL: 1.2793, Acc: 0.3669\n",
      "lr = 0.5\n",
      "Epoch: 48, Batch: 100, loss: 4.5710 , Train PPL: 1.0070, Train Acc: 0.3887\n",
      "Epoch: 48, Batch: 200, loss: 4.4748 , Train PPL: 1.0068, Train Acc: 0.3613\n",
      "Epoch: 48, Batch: 300, loss: 4.7660 , Train PPL: 1.0073, Train Acc: 0.3582\n",
      "Epoch: 48, Batch: 400, loss: 4.2896 , Train PPL: 1.0066, Train Acc: 0.4238\n",
      "Epoch: 48, Batch: 500, loss: 4.5086 , Train PPL: 1.0069, Train Acc: 0.3537\n",
      "Epoch: 48, Batch: 600, loss: 4.6566 , Train PPL: 1.0071, Train Acc: 0.3354\n",
      "Epoch: 48, Batch: 700, loss: 4.5345 , Train PPL: 1.0069, Train Acc: 0.3720\n",
      "Validation --- Epoch: 48, total loss: 300.0930 , PPL: 1.2785, Acc: 0.3714\n",
      "lr = 0.5\n",
      "wrote model\n",
      "Epoch: 49, Batch: 100, loss: 4.5629 , Train PPL: 1.0070, Train Acc: 0.4070\n",
      "Epoch: 49, Batch: 200, loss: 4.5576 , Train PPL: 1.0070, Train Acc: 0.3369\n",
      "Epoch: 49, Batch: 300, loss: 4.6634 , Train PPL: 1.0071, Train Acc: 0.3704\n",
      "Epoch: 49, Batch: 400, loss: 4.6995 , Train PPL: 1.0072, Train Acc: 0.3293\n",
      "Epoch: 49, Batch: 500, loss: 4.5457 , Train PPL: 1.0070, Train Acc: 0.3704\n",
      "Epoch: 49, Batch: 600, loss: 4.7190 , Train PPL: 1.0072, Train Acc: 0.3338\n",
      "Epoch: 49, Batch: 700, loss: 4.5956 , Train PPL: 1.0070, Train Acc: 0.3841\n",
      "Validation --- Epoch: 49, total loss: 300.0416 , PPL: 1.2784, Acc: 0.3732\n",
      "lr = 0.5\n",
      "wrote model\n",
      "Epoch: 50, Batch: 100, loss: 4.6590 , Train PPL: 1.0071, Train Acc: 0.3720\n",
      "Epoch: 50, Batch: 200, loss: 4.8224 , Train PPL: 1.0074, Train Acc: 0.3201\n",
      "Epoch: 50, Batch: 300, loss: 4.6022 , Train PPL: 1.0070, Train Acc: 0.3628\n",
      "Epoch: 50, Batch: 400, loss: 4.7305 , Train PPL: 1.0072, Train Acc: 0.3293\n",
      "Epoch: 50, Batch: 500, loss: 4.2796 , Train PPL: 1.0065, Train Acc: 0.4405\n",
      "Epoch: 50, Batch: 600, loss: 4.1987 , Train PPL: 1.0064, Train Acc: 0.4268\n",
      "Epoch: 50, Batch: 700, loss: 4.7400 , Train PPL: 1.0073, Train Acc: 0.3887\n",
      "Validation --- Epoch: 50, total loss: 299.9082 , PPL: 1.2783, Acc: 0.3735\n",
      "lr = 0.5\n",
      "wrote model\n",
      "Epoch: 51, Batch: 100, loss: 4.2974 , Train PPL: 1.0066, Train Acc: 0.4146\n",
      "Epoch: 51, Batch: 200, loss: 4.5141 , Train PPL: 1.0069, Train Acc: 0.4055\n",
      "Epoch: 51, Batch: 300, loss: 4.4318 , Train PPL: 1.0068, Train Acc: 0.4024\n",
      "Epoch: 51, Batch: 400, loss: 4.5851 , Train PPL: 1.0070, Train Acc: 0.3506\n",
      "Epoch: 51, Batch: 500, loss: 4.4573 , Train PPL: 1.0068, Train Acc: 0.3826\n",
      "Epoch: 51, Batch: 600, loss: 4.6182 , Train PPL: 1.0071, Train Acc: 0.3765\n",
      "Epoch: 51, Batch: 700, loss: 4.5565 , Train PPL: 1.0070, Train Acc: 0.3567\n",
      "Validation --- Epoch: 51, total loss: 299.8371 , PPL: 1.2782, Acc: 0.3734\n",
      "lr = 0.5\n",
      "wrote model\n",
      "Epoch: 52, Batch: 100, loss: 4.4194 , Train PPL: 1.0068, Train Acc: 0.3506\n",
      "Epoch: 52, Batch: 200, loss: 4.6626 , Train PPL: 1.0071, Train Acc: 0.3841\n",
      "Epoch: 52, Batch: 300, loss: 4.4789 , Train PPL: 1.0069, Train Acc: 0.3872\n",
      "Epoch: 52, Batch: 400, loss: 4.5699 , Train PPL: 1.0070, Train Acc: 0.3537\n",
      "Epoch: 52, Batch: 500, loss: 4.6590 , Train PPL: 1.0071, Train Acc: 0.3338\n",
      "Epoch: 52, Batch: 600, loss: 4.5225 , Train PPL: 1.0069, Train Acc: 0.4162\n",
      "Epoch: 52, Batch: 700, loss: 4.5563 , Train PPL: 1.0070, Train Acc: 0.3735\n",
      "Validation --- Epoch: 52, total loss: 299.9627 , PPL: 1.2784, Acc: 0.3738\n",
      "lr = 0.5\n",
      "Epoch: 53, Batch: 100, loss: 4.5688 , Train PPL: 1.0070, Train Acc: 0.3354\n",
      "Epoch: 53, Batch: 200, loss: 4.4442 , Train PPL: 1.0068, Train Acc: 0.3689\n",
      "Epoch: 53, Batch: 300, loss: 4.3947 , Train PPL: 1.0067, Train Acc: 0.4588\n",
      "Epoch: 53, Batch: 400, loss: 4.5452 , Train PPL: 1.0070, Train Acc: 0.3659\n",
      "Epoch: 53, Batch: 500, loss: 4.6377 , Train PPL: 1.0071, Train Acc: 0.3369\n",
      "Epoch: 53, Batch: 600, loss: 4.3938 , Train PPL: 1.0067, Train Acc: 0.4588\n",
      "Epoch: 53, Batch: 700, loss: 4.5561 , Train PPL: 1.0070, Train Acc: 0.3704\n",
      "Validation --- Epoch: 53, total loss: 299.8462 , PPL: 1.2781, Acc: 0.3717\n",
      "lr = 0.5\n",
      "Epoch: 54, Batch: 100, loss: 4.5259 , Train PPL: 1.0069, Train Acc: 0.3613\n",
      "Epoch: 54, Batch: 200, loss: 4.6107 , Train PPL: 1.0071, Train Acc: 0.4040\n",
      "Epoch: 54, Batch: 300, loss: 4.5878 , Train PPL: 1.0070, Train Acc: 0.3796\n",
      "Epoch: 54, Batch: 400, loss: 4.4543 , Train PPL: 1.0068, Train Acc: 0.3902\n",
      "Epoch: 54, Batch: 500, loss: 4.4647 , Train PPL: 1.0068, Train Acc: 0.4009\n",
      "Epoch: 54, Batch: 600, loss: 4.6540 , Train PPL: 1.0071, Train Acc: 0.3521\n",
      "Epoch: 54, Batch: 700, loss: 4.5083 , Train PPL: 1.0069, Train Acc: 0.3796\n",
      "Validation --- Epoch: 54, total loss: 299.7566 , PPL: 1.2781, Acc: 0.3744\n",
      "lr = 0.5\n",
      "wrote model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55, Batch: 100, loss: 4.1787 , Train PPL: 1.0064, Train Acc: 0.4360\n",
      "Epoch: 55, Batch: 200, loss: 4.4781 , Train PPL: 1.0068, Train Acc: 0.3659\n",
      "Epoch: 55, Batch: 300, loss: 4.2238 , Train PPL: 1.0065, Train Acc: 0.4466\n",
      "Epoch: 55, Batch: 400, loss: 4.4778 , Train PPL: 1.0068, Train Acc: 0.4101\n",
      "Epoch: 55, Batch: 500, loss: 4.7884 , Train PPL: 1.0073, Train Acc: 0.3186\n",
      "Epoch: 55, Batch: 600, loss: 4.4560 , Train PPL: 1.0068, Train Acc: 0.3780\n",
      "Epoch: 55, Batch: 700, loss: 4.4922 , Train PPL: 1.0069, Train Acc: 0.3613\n",
      "Validation --- Epoch: 55, total loss: 299.8455 , PPL: 1.2783, Acc: 0.3739\n",
      "lr = 0.5\n",
      "Epoch: 56, Batch: 100, loss: 4.6940 , Train PPL: 1.0072, Train Acc: 0.3476\n",
      "Epoch: 56, Batch: 200, loss: 4.5090 , Train PPL: 1.0069, Train Acc: 0.3796\n",
      "Epoch: 56, Batch: 300, loss: 4.1974 , Train PPL: 1.0064, Train Acc: 0.4253\n",
      "Epoch: 56, Batch: 400, loss: 4.5920 , Train PPL: 1.0070, Train Acc: 0.3430\n",
      "Epoch: 56, Batch: 500, loss: 4.6046 , Train PPL: 1.0070, Train Acc: 0.4070\n",
      "Epoch: 56, Batch: 600, loss: 4.6694 , Train PPL: 1.0071, Train Acc: 0.3659\n",
      "Epoch: 56, Batch: 700, loss: 4.5143 , Train PPL: 1.0069, Train Acc: 0.4055\n",
      "Validation --- Epoch: 56, total loss: 300.0516 , PPL: 1.2784, Acc: 0.3739\n",
      "lr = 0.5\n",
      "Epoch: 57, Batch: 100, loss: 4.6562 , Train PPL: 1.0071, Train Acc: 0.3659\n",
      "Epoch: 57, Batch: 200, loss: 4.3168 , Train PPL: 1.0066, Train Acc: 0.4451\n",
      "Epoch: 57, Batch: 300, loss: 4.4476 , Train PPL: 1.0068, Train Acc: 0.3704\n",
      "Epoch: 57, Batch: 400, loss: 4.7771 , Train PPL: 1.0073, Train Acc: 0.3720\n",
      "Epoch: 57, Batch: 500, loss: 4.6051 , Train PPL: 1.0070, Train Acc: 0.3476\n",
      "Epoch: 57, Batch: 600, loss: 4.6946 , Train PPL: 1.0072, Train Acc: 0.3003\n",
      "Epoch: 57, Batch: 700, loss: 4.4626 , Train PPL: 1.0068, Train Acc: 0.4070\n",
      "Validation --- Epoch: 57, total loss: 299.8702 , PPL: 1.2782, Acc: 0.3723\n",
      "lr = 0.5\n",
      "Epoch: 58, Batch: 100, loss: 4.7085 , Train PPL: 1.0072, Train Acc: 0.3491\n",
      "Epoch: 58, Batch: 200, loss: 4.5054 , Train PPL: 1.0069, Train Acc: 0.3902\n",
      "Epoch: 58, Batch: 300, loss: 4.8116 , Train PPL: 1.0074, Train Acc: 0.2942\n",
      "Epoch: 58, Batch: 400, loss: 4.5351 , Train PPL: 1.0069, Train Acc: 0.3354\n",
      "Epoch: 58, Batch: 500, loss: 4.5337 , Train PPL: 1.0069, Train Acc: 0.3796\n",
      "Epoch: 58, Batch: 600, loss: 4.6036 , Train PPL: 1.0070, Train Acc: 0.3887\n",
      "Epoch: 58, Batch: 700, loss: 4.7031 , Train PPL: 1.0072, Train Acc: 0.3750\n",
      "Validation --- Epoch: 58, total loss: 299.7449 , PPL: 1.2781, Acc: 0.3737\n",
      "lr = 0.5\n",
      "wrote model\n",
      "Epoch: 59, Batch: 100, loss: 4.4221 , Train PPL: 1.0068, Train Acc: 0.3582\n",
      "Epoch: 59, Batch: 200, loss: 4.7469 , Train PPL: 1.0073, Train Acc: 0.3476\n",
      "Epoch: 59, Batch: 300, loss: 4.3781 , Train PPL: 1.0067, Train Acc: 0.4284\n",
      "Epoch: 59, Batch: 400, loss: 4.6121 , Train PPL: 1.0071, Train Acc: 0.3765\n",
      "Epoch: 59, Batch: 500, loss: 4.5795 , Train PPL: 1.0070, Train Acc: 0.3582\n",
      "Epoch: 59, Batch: 600, loss: 4.6947 , Train PPL: 1.0072, Train Acc: 0.3399\n",
      "Epoch: 59, Batch: 700, loss: 4.7694 , Train PPL: 1.0073, Train Acc: 0.3460\n",
      "Validation --- Epoch: 59, total loss: 299.8308 , PPL: 1.2782, Acc: 0.3774\n",
      "lr = 0.5\n",
      "Epoch: 60, Batch: 100, loss: 4.4433 , Train PPL: 1.0068, Train Acc: 0.4146\n",
      "Epoch: 60, Batch: 200, loss: 4.1671 , Train PPL: 1.0064, Train Acc: 0.4238\n",
      "Epoch: 60, Batch: 300, loss: 4.6954 , Train PPL: 1.0072, Train Acc: 0.3171\n",
      "Epoch: 60, Batch: 400, loss: 4.4913 , Train PPL: 1.0069, Train Acc: 0.3750\n",
      "Epoch: 60, Batch: 500, loss: 4.6024 , Train PPL: 1.0070, Train Acc: 0.4009\n",
      "Epoch: 60, Batch: 600, loss: 4.3229 , Train PPL: 1.0066, Train Acc: 0.3689\n",
      "Epoch: 60, Batch: 700, loss: 4.4507 , Train PPL: 1.0068, Train Acc: 0.3994\n",
      "Validation --- Epoch: 60, total loss: 300.0625 , PPL: 1.2784, Acc: 0.3754\n",
      "lr = 0.25\n",
      "Epoch: 61, Batch: 100, loss: 4.3549 , Train PPL: 1.0067, Train Acc: 0.3994\n",
      "Epoch: 61, Batch: 200, loss: 4.8100 , Train PPL: 1.0074, Train Acc: 0.3582\n",
      "Epoch: 61, Batch: 300, loss: 4.0729 , Train PPL: 1.0062, Train Acc: 0.4329\n",
      "Epoch: 61, Batch: 400, loss: 4.4844 , Train PPL: 1.0069, Train Acc: 0.3963\n",
      "Epoch: 61, Batch: 500, loss: 4.6353 , Train PPL: 1.0071, Train Acc: 0.3506\n",
      "Epoch: 61, Batch: 600, loss: 4.4441 , Train PPL: 1.0068, Train Acc: 0.4024\n",
      "Epoch: 61, Batch: 700, loss: 4.2007 , Train PPL: 1.0064, Train Acc: 0.4695\n",
      "Validation --- Epoch: 61, total loss: 299.6801 , PPL: 1.2781, Acc: 0.3764\n",
      "lr = 0.25\n",
      "wrote model\n",
      "Epoch: 62, Batch: 100, loss: 4.8169 , Train PPL: 1.0074, Train Acc: 0.3338\n",
      "Epoch: 62, Batch: 200, loss: 4.6319 , Train PPL: 1.0071, Train Acc: 0.3476\n",
      "Epoch: 62, Batch: 300, loss: 4.3468 , Train PPL: 1.0066, Train Acc: 0.4345\n",
      "Epoch: 62, Batch: 400, loss: 4.4350 , Train PPL: 1.0068, Train Acc: 0.3963\n",
      "Epoch: 62, Batch: 500, loss: 4.5632 , Train PPL: 1.0070, Train Acc: 0.3430\n",
      "Epoch: 62, Batch: 600, loss: 4.5269 , Train PPL: 1.0069, Train Acc: 0.3704\n",
      "Epoch: 62, Batch: 700, loss: 4.3319 , Train PPL: 1.0066, Train Acc: 0.4085\n",
      "Validation --- Epoch: 62, total loss: 299.7352 , PPL: 1.2781, Acc: 0.3762\n",
      "lr = 0.25\n",
      "Epoch: 63, Batch: 100, loss: 4.3114 , Train PPL: 1.0066, Train Acc: 0.4436\n",
      "Epoch: 63, Batch: 200, loss: 4.8640 , Train PPL: 1.0074, Train Acc: 0.3247\n",
      "Epoch: 63, Batch: 300, loss: 4.2401 , Train PPL: 1.0065, Train Acc: 0.4268\n",
      "Epoch: 63, Batch: 400, loss: 4.6736 , Train PPL: 1.0071, Train Acc: 0.3186\n",
      "Epoch: 63, Batch: 500, loss: 4.5585 , Train PPL: 1.0070, Train Acc: 0.3948\n",
      "Epoch: 63, Batch: 600, loss: 4.6623 , Train PPL: 1.0071, Train Acc: 0.3613\n",
      "Epoch: 63, Batch: 700, loss: 4.5883 , Train PPL: 1.0070, Train Acc: 0.3460\n",
      "Validation --- Epoch: 63, total loss: 299.6179 , PPL: 1.2780, Acc: 0.3786\n",
      "lr = 0.25\n",
      "wrote model\n",
      "Epoch: 64, Batch: 100, loss: 4.5739 , Train PPL: 1.0070, Train Acc: 0.3598\n",
      "Epoch: 64, Batch: 200, loss: 4.7657 , Train PPL: 1.0073, Train Acc: 0.3125\n",
      "Epoch: 64, Batch: 300, loss: 4.4442 , Train PPL: 1.0068, Train Acc: 0.3887\n",
      "Epoch: 64, Batch: 400, loss: 4.3218 , Train PPL: 1.0066, Train Acc: 0.4192\n",
      "Epoch: 64, Batch: 500, loss: 4.3400 , Train PPL: 1.0066, Train Acc: 0.3979\n",
      "Epoch: 64, Batch: 600, loss: 4.5291 , Train PPL: 1.0069, Train Acc: 0.3735\n",
      "Epoch: 64, Batch: 700, loss: 4.5936 , Train PPL: 1.0070, Train Acc: 0.3506\n",
      "Validation --- Epoch: 64, total loss: 299.6148 , PPL: 1.2780, Acc: 0.3774\n",
      "lr = 0.25\n",
      "wrote model\n",
      "Epoch: 65, Batch: 100, loss: 4.5622 , Train PPL: 1.0070, Train Acc: 0.3841\n",
      "Epoch: 65, Batch: 200, loss: 4.6006 , Train PPL: 1.0070, Train Acc: 0.3506\n",
      "Epoch: 65, Batch: 300, loss: 4.4339 , Train PPL: 1.0068, Train Acc: 0.3735\n",
      "Epoch: 65, Batch: 400, loss: 4.6505 , Train PPL: 1.0071, Train Acc: 0.3537\n",
      "Epoch: 65, Batch: 500, loss: 4.5627 , Train PPL: 1.0070, Train Acc: 0.3780\n",
      "Epoch: 65, Batch: 600, loss: 4.3409 , Train PPL: 1.0066, Train Acc: 0.4497\n",
      "Epoch: 65, Batch: 700, loss: 4.6969 , Train PPL: 1.0072, Train Acc: 0.3887\n",
      "Validation --- Epoch: 65, total loss: 299.6892 , PPL: 1.2781, Acc: 0.3779\n",
      "lr = 0.25\n",
      "Epoch: 66, Batch: 100, loss: 4.7848 , Train PPL: 1.0073, Train Acc: 0.3140\n",
      "Epoch: 66, Batch: 200, loss: 4.5603 , Train PPL: 1.0070, Train Acc: 0.3765\n",
      "Epoch: 66, Batch: 300, loss: 4.6916 , Train PPL: 1.0072, Train Acc: 0.3598\n",
      "Epoch: 66, Batch: 400, loss: 4.2324 , Train PPL: 1.0065, Train Acc: 0.4360\n",
      "Epoch: 66, Batch: 500, loss: 4.5551 , Train PPL: 1.0070, Train Acc: 0.3628\n",
      "Epoch: 66, Batch: 600, loss: 4.3171 , Train PPL: 1.0066, Train Acc: 0.4131\n",
      "Epoch: 66, Batch: 700, loss: 4.7313 , Train PPL: 1.0072, Train Acc: 0.3262\n",
      "Validation --- Epoch: 66, total loss: 299.6527 , PPL: 1.2780, Acc: 0.3777\n",
      "lr = 0.25\n",
      "Epoch: 67, Batch: 100, loss: 4.6484 , Train PPL: 1.0071, Train Acc: 0.3674\n",
      "Epoch: 67, Batch: 200, loss: 4.6338 , Train PPL: 1.0071, Train Acc: 0.3582\n",
      "Epoch: 67, Batch: 300, loss: 4.5323 , Train PPL: 1.0069, Train Acc: 0.3674\n",
      "Epoch: 67, Batch: 400, loss: 4.5059 , Train PPL: 1.0069, Train Acc: 0.3735\n",
      "Epoch: 67, Batch: 500, loss: 3.9904 , Train PPL: 1.0061, Train Acc: 0.4604\n",
      "Epoch: 67, Batch: 600, loss: 4.7693 , Train PPL: 1.0073, Train Acc: 0.3613\n",
      "Epoch: 67, Batch: 700, loss: 4.6543 , Train PPL: 1.0071, Train Acc: 0.3369\n",
      "Validation --- Epoch: 67, total loss: 299.7620 , PPL: 1.2782, Acc: 0.3789\n",
      "lr = 0.25\n",
      "Epoch: 68, Batch: 100, loss: 4.7005 , Train PPL: 1.0072, Train Acc: 0.3384\n",
      "Epoch: 68, Batch: 200, loss: 4.7489 , Train PPL: 1.0073, Train Acc: 0.3491\n",
      "Epoch: 68, Batch: 300, loss: 4.5517 , Train PPL: 1.0070, Train Acc: 0.3841\n",
      "Epoch: 68, Batch: 400, loss: 4.4854 , Train PPL: 1.0069, Train Acc: 0.3918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68, Batch: 500, loss: 4.7121 , Train PPL: 1.0072, Train Acc: 0.3674\n",
      "Epoch: 68, Batch: 600, loss: 4.5101 , Train PPL: 1.0069, Train Acc: 0.3857\n",
      "Epoch: 68, Batch: 700, loss: 4.7485 , Train PPL: 1.0073, Train Acc: 0.3323\n",
      "Validation --- Epoch: 68, total loss: 299.7613 , PPL: 1.2782, Acc: 0.3790\n",
      "lr = 0.25\n",
      "Epoch: 69, Batch: 100, loss: 4.3615 , Train PPL: 1.0067, Train Acc: 0.3918\n",
      "Epoch: 69, Batch: 200, loss: 4.5387 , Train PPL: 1.0069, Train Acc: 0.4268\n",
      "Epoch: 69, Batch: 300, loss: 4.7952 , Train PPL: 1.0073, Train Acc: 0.3582\n",
      "Epoch: 69, Batch: 400, loss: 4.5317 , Train PPL: 1.0069, Train Acc: 0.3720\n",
      "Epoch: 69, Batch: 500, loss: 4.3588 , Train PPL: 1.0067, Train Acc: 0.3613\n",
      "Epoch: 69, Batch: 600, loss: 4.6314 , Train PPL: 1.0071, Train Acc: 0.3613\n",
      "Epoch: 69, Batch: 700, loss: 4.2725 , Train PPL: 1.0065, Train Acc: 0.4527\n",
      "Validation --- Epoch: 69, total loss: 299.6820 , PPL: 1.2781, Acc: 0.3791\n",
      "lr = 0.125\n",
      "Epoch: 70, Batch: 100, loss: 4.1741 , Train PPL: 1.0064, Train Acc: 0.4741\n",
      "Epoch: 70, Batch: 200, loss: 4.4059 , Train PPL: 1.0067, Train Acc: 0.4009\n",
      "Epoch: 70, Batch: 300, loss: 4.3857 , Train PPL: 1.0067, Train Acc: 0.3872\n",
      "Epoch: 70, Batch: 400, loss: 4.5008 , Train PPL: 1.0069, Train Acc: 0.3582\n",
      "Epoch: 70, Batch: 500, loss: 4.9028 , Train PPL: 1.0075, Train Acc: 0.3491\n",
      "Epoch: 70, Batch: 600, loss: 4.5245 , Train PPL: 1.0069, Train Acc: 0.4040\n",
      "Epoch: 70, Batch: 700, loss: 4.4758 , Train PPL: 1.0068, Train Acc: 0.3689\n",
      "Validation --- Epoch: 70, total loss: 299.6280 , PPL: 1.2780, Acc: 0.3787\n",
      "lr = 0.125\n",
      "Epoch: 71, Batch: 100, loss: 4.4907 , Train PPL: 1.0069, Train Acc: 0.3293\n",
      "Epoch: 71, Batch: 200, loss: 4.3338 , Train PPL: 1.0066, Train Acc: 0.4146\n",
      "Epoch: 71, Batch: 300, loss: 4.6988 , Train PPL: 1.0072, Train Acc: 0.3491\n",
      "Epoch: 71, Batch: 400, loss: 4.1988 , Train PPL: 1.0064, Train Acc: 0.4787\n",
      "Epoch: 71, Batch: 500, loss: 4.1044 , Train PPL: 1.0063, Train Acc: 0.4360\n",
      "Epoch: 71, Batch: 600, loss: 4.6799 , Train PPL: 1.0072, Train Acc: 0.3689\n",
      "Epoch: 71, Batch: 700, loss: 4.5698 , Train PPL: 1.0070, Train Acc: 0.3445\n",
      "Validation --- Epoch: 71, total loss: 299.6411 , PPL: 1.2780, Acc: 0.3805\n",
      "lr = 0.125\n",
      "Epoch: 72, Batch: 100, loss: 4.4998 , Train PPL: 1.0069, Train Acc: 0.3628\n",
      "Epoch: 72, Batch: 200, loss: 4.4093 , Train PPL: 1.0067, Train Acc: 0.3598\n",
      "Epoch: 72, Batch: 300, loss: 4.7330 , Train PPL: 1.0072, Train Acc: 0.3171\n",
      "Epoch: 72, Batch: 400, loss: 4.5453 , Train PPL: 1.0070, Train Acc: 0.3582\n",
      "Epoch: 72, Batch: 500, loss: 4.3679 , Train PPL: 1.0067, Train Acc: 0.3750\n",
      "Epoch: 72, Batch: 600, loss: 4.5123 , Train PPL: 1.0069, Train Acc: 0.3826\n",
      "Epoch: 72, Batch: 700, loss: 4.7230 , Train PPL: 1.0072, Train Acc: 0.3506\n",
      "Validation --- Epoch: 72, total loss: 299.6901 , PPL: 1.2781, Acc: 0.3800\n",
      "lr = 0.125\n",
      "Epoch: 73, Batch: 100, loss: 4.5481 , Train PPL: 1.0070, Train Acc: 0.3659\n",
      "Epoch: 73, Batch: 200, loss: 4.5217 , Train PPL: 1.0069, Train Acc: 0.3598\n",
      "Epoch: 73, Batch: 300, loss: 4.4897 , Train PPL: 1.0069, Train Acc: 0.4116\n",
      "Epoch: 73, Batch: 400, loss: 4.1027 , Train PPL: 1.0063, Train Acc: 0.4512\n",
      "Epoch: 73, Batch: 500, loss: 4.7752 , Train PPL: 1.0073, Train Acc: 0.3735\n",
      "Epoch: 73, Batch: 600, loss: 4.5025 , Train PPL: 1.0069, Train Acc: 0.3720\n",
      "Epoch: 73, Batch: 700, loss: 4.4715 , Train PPL: 1.0068, Train Acc: 0.3659\n",
      "Validation --- Epoch: 73, total loss: 299.6308 , PPL: 1.2781, Acc: 0.3792\n",
      "lr = 0.125\n",
      "Epoch: 74, Batch: 100, loss: 4.5177 , Train PPL: 1.0069, Train Acc: 0.3704\n",
      "Epoch: 74, Batch: 200, loss: 4.6211 , Train PPL: 1.0071, Train Acc: 0.3811\n",
      "Epoch: 74, Batch: 300, loss: 4.4156 , Train PPL: 1.0068, Train Acc: 0.3796\n",
      "Epoch: 74, Batch: 400, loss: 4.4612 , Train PPL: 1.0068, Train Acc: 0.3659\n",
      "Epoch: 74, Batch: 500, loss: 4.4809 , Train PPL: 1.0069, Train Acc: 0.3887\n",
      "Epoch: 74, Batch: 600, loss: 4.6530 , Train PPL: 1.0071, Train Acc: 0.3979\n",
      "Epoch: 74, Batch: 700, loss: 4.5951 , Train PPL: 1.0070, Train Acc: 0.3735\n",
      "Validation --- Epoch: 74, total loss: 299.6204 , PPL: 1.2780, Acc: 0.3810\n",
      "lr = 0.125\n",
      "Epoch: 75, Batch: 100, loss: 4.4647 , Train PPL: 1.0068, Train Acc: 0.3765\n",
      "Epoch: 75, Batch: 200, loss: 4.5096 , Train PPL: 1.0069, Train Acc: 0.4085\n",
      "Epoch: 75, Batch: 300, loss: 4.6617 , Train PPL: 1.0071, Train Acc: 0.3460\n",
      "Epoch: 75, Batch: 400, loss: 4.7684 , Train PPL: 1.0073, Train Acc: 0.3384\n",
      "Epoch: 75, Batch: 500, loss: 4.6267 , Train PPL: 1.0071, Train Acc: 0.3155\n",
      "Epoch: 75, Batch: 600, loss: 4.4878 , Train PPL: 1.0069, Train Acc: 0.3887\n",
      "Epoch: 75, Batch: 700, loss: 4.5663 , Train PPL: 1.0070, Train Acc: 0.3872\n",
      "Validation --- Epoch: 75, total loss: 299.7311 , PPL: 1.2781, Acc: 0.3793\n",
      "lr = 0.0625\n",
      "Epoch: 76, Batch: 100, loss: 4.6318 , Train PPL: 1.0071, Train Acc: 0.3598\n",
      "Epoch: 76, Batch: 200, loss: 4.5763 , Train PPL: 1.0070, Train Acc: 0.3537\n",
      "Epoch: 76, Batch: 300, loss: 4.4884 , Train PPL: 1.0069, Train Acc: 0.3689\n",
      "Epoch: 76, Batch: 400, loss: 4.3793 , Train PPL: 1.0067, Train Acc: 0.4040\n",
      "Epoch: 76, Batch: 500, loss: 4.5015 , Train PPL: 1.0069, Train Acc: 0.3338\n",
      "Epoch: 76, Batch: 600, loss: 4.5991 , Train PPL: 1.0070, Train Acc: 0.3521\n",
      "Epoch: 76, Batch: 700, loss: 4.3677 , Train PPL: 1.0067, Train Acc: 0.3826\n",
      "Validation --- Epoch: 76, total loss: 299.6738 , PPL: 1.2781, Acc: 0.3809\n",
      "lr = 0.0625\n",
      "Epoch: 77, Batch: 100, loss: 4.3204 , Train PPL: 1.0066, Train Acc: 0.4162\n",
      "Epoch: 77, Batch: 200, loss: 4.4935 , Train PPL: 1.0069, Train Acc: 0.3780\n",
      "Epoch: 77, Batch: 300, loss: 4.4150 , Train PPL: 1.0068, Train Acc: 0.4162\n",
      "Epoch: 77, Batch: 400, loss: 4.7387 , Train PPL: 1.0072, Train Acc: 0.3841\n",
      "Epoch: 77, Batch: 500, loss: 4.4824 , Train PPL: 1.0069, Train Acc: 0.3826\n",
      "Epoch: 77, Batch: 600, loss: 4.5639 , Train PPL: 1.0070, Train Acc: 0.3765\n",
      "Epoch: 77, Batch: 700, loss: 4.6464 , Train PPL: 1.0071, Train Acc: 0.3262\n",
      "Validation --- Epoch: 77, total loss: 299.6581 , PPL: 1.2781, Acc: 0.3801\n",
      "lr = 0.0625\n",
      "Epoch: 78, Batch: 100, loss: 4.6746 , Train PPL: 1.0072, Train Acc: 0.3933\n",
      "Epoch: 78, Batch: 200, loss: 4.3791 , Train PPL: 1.0067, Train Acc: 0.4253\n",
      "Epoch: 78, Batch: 300, loss: 4.8568 , Train PPL: 1.0074, Train Acc: 0.2896\n",
      "Epoch: 78, Batch: 400, loss: 4.2187 , Train PPL: 1.0065, Train Acc: 0.4680\n",
      "Epoch: 78, Batch: 500, loss: 4.6647 , Train PPL: 1.0071, Train Acc: 0.3354\n",
      "Epoch: 78, Batch: 600, loss: 4.6058 , Train PPL: 1.0070, Train Acc: 0.3537\n",
      "Epoch: 78, Batch: 700, loss: 4.3978 , Train PPL: 1.0067, Train Acc: 0.3933\n",
      "Validation --- Epoch: 78, total loss: 299.6516 , PPL: 1.2781, Acc: 0.3809\n",
      "lr = 0.0625\n",
      "Epoch: 79, Batch: 100, loss: 4.5060 , Train PPL: 1.0069, Train Acc: 0.3933\n",
      "Epoch: 79, Batch: 200, loss: 4.4535 , Train PPL: 1.0068, Train Acc: 0.3750\n",
      "Epoch: 79, Batch: 300, loss: 4.5294 , Train PPL: 1.0069, Train Acc: 0.3460\n",
      "Epoch: 79, Batch: 400, loss: 4.4925 , Train PPL: 1.0069, Train Acc: 0.3659\n",
      "Epoch: 79, Batch: 500, loss: 4.5245 , Train PPL: 1.0069, Train Acc: 0.3628\n",
      "Epoch: 79, Batch: 600, loss: 4.5169 , Train PPL: 1.0069, Train Acc: 0.3643\n",
      "Epoch: 79, Batch: 700, loss: 4.4586 , Train PPL: 1.0068, Train Acc: 0.3857\n",
      "Validation --- Epoch: 79, total loss: 299.6552 , PPL: 1.2781, Acc: 0.3802\n",
      "lr = 0.0625\n",
      "Epoch: 80, Batch: 100, loss: 4.6043 , Train PPL: 1.0070, Train Acc: 0.3643\n",
      "Epoch: 80, Batch: 200, loss: 4.3860 , Train PPL: 1.0067, Train Acc: 0.4253\n",
      "Epoch: 80, Batch: 300, loss: 4.5046 , Train PPL: 1.0069, Train Acc: 0.3857\n",
      "Epoch: 80, Batch: 400, loss: 4.3737 , Train PPL: 1.0067, Train Acc: 0.3613\n",
      "Epoch: 80, Batch: 500, loss: 4.6972 , Train PPL: 1.0072, Train Acc: 0.3506\n",
      "Epoch: 80, Batch: 600, loss: 4.5303 , Train PPL: 1.0069, Train Acc: 0.3963\n",
      "Epoch: 80, Batch: 700, loss: 4.4305 , Train PPL: 1.0068, Train Acc: 0.3887\n",
      "Validation --- Epoch: 80, total loss: 299.6989 , PPL: 1.2781, Acc: 0.3806\n",
      "lr = 0.0625\n",
      "Epoch: 81, Batch: 100, loss: 4.3778 , Train PPL: 1.0067, Train Acc: 0.4009\n",
      "Epoch: 81, Batch: 200, loss: 4.4183 , Train PPL: 1.0068, Train Acc: 0.4085\n",
      "Epoch: 81, Batch: 300, loss: 4.5532 , Train PPL: 1.0070, Train Acc: 0.3613\n",
      "Epoch: 81, Batch: 400, loss: 4.5122 , Train PPL: 1.0069, Train Acc: 0.3689\n",
      "Epoch: 81, Batch: 500, loss: 4.6661 , Train PPL: 1.0071, Train Acc: 0.2957\n",
      "Epoch: 81, Batch: 600, loss: 4.2727 , Train PPL: 1.0065, Train Acc: 0.3979\n",
      "Epoch: 81, Batch: 700, loss: 4.2527 , Train PPL: 1.0065, Train Acc: 0.4131\n",
      "Validation --- Epoch: 81, total loss: 299.6692 , PPL: 1.2781, Acc: 0.3808\n",
      "lr = 0.03125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82, Batch: 100, loss: 4.3462 , Train PPL: 1.0066, Train Acc: 0.4009\n",
      "Epoch: 82, Batch: 200, loss: 4.6456 , Train PPL: 1.0071, Train Acc: 0.3780\n",
      "Epoch: 82, Batch: 300, loss: 4.6252 , Train PPL: 1.0071, Train Acc: 0.3552\n",
      "Epoch: 82, Batch: 400, loss: 4.4814 , Train PPL: 1.0069, Train Acc: 0.3750\n",
      "Epoch: 82, Batch: 500, loss: 4.3755 , Train PPL: 1.0067, Train Acc: 0.3872\n",
      "Epoch: 82, Batch: 600, loss: 4.4722 , Train PPL: 1.0068, Train Acc: 0.3796\n",
      "Epoch: 82, Batch: 700, loss: 4.3463 , Train PPL: 1.0066, Train Acc: 0.4268\n",
      "Validation --- Epoch: 82, total loss: 299.7017 , PPL: 1.2781, Acc: 0.3807\n",
      "lr = 0.03125\n",
      "Epoch: 83, Batch: 100, loss: 4.5346 , Train PPL: 1.0069, Train Acc: 0.4268\n",
      "Epoch: 83, Batch: 200, loss: 4.4275 , Train PPL: 1.0068, Train Acc: 0.3628\n",
      "Epoch: 83, Batch: 300, loss: 4.4590 , Train PPL: 1.0068, Train Acc: 0.4040\n",
      "Epoch: 83, Batch: 400, loss: 4.1500 , Train PPL: 1.0063, Train Acc: 0.4893\n",
      "Epoch: 83, Batch: 500, loss: 4.4660 , Train PPL: 1.0068, Train Acc: 0.3354\n",
      "Epoch: 83, Batch: 600, loss: 4.0722 , Train PPL: 1.0062, Train Acc: 0.4634\n",
      "Epoch: 83, Batch: 700, loss: 4.2431 , Train PPL: 1.0065, Train Acc: 0.4527\n",
      "Validation --- Epoch: 83, total loss: 299.6843 , PPL: 1.2781, Acc: 0.3807\n",
      "lr = 0.03125\n",
      "Epoch: 84, Batch: 100, loss: 4.6406 , Train PPL: 1.0071, Train Acc: 0.3674\n",
      "Epoch: 84, Batch: 200, loss: 4.4286 , Train PPL: 1.0068, Train Acc: 0.4085\n",
      "Epoch: 84, Batch: 300, loss: 4.3536 , Train PPL: 1.0067, Train Acc: 0.4238\n",
      "Epoch: 84, Batch: 400, loss: 4.3269 , Train PPL: 1.0066, Train Acc: 0.4421\n",
      "Epoch: 84, Batch: 500, loss: 4.6902 , Train PPL: 1.0072, Train Acc: 0.3323\n",
      "Epoch: 84, Batch: 600, loss: 4.2813 , Train PPL: 1.0065, Train Acc: 0.3613\n",
      "Epoch: 84, Batch: 700, loss: 4.3445 , Train PPL: 1.0066, Train Acc: 0.4085\n",
      "Validation --- Epoch: 84, total loss: 299.6546 , PPL: 1.2781, Acc: 0.3810\n",
      "lr = 0.03125\n",
      "Epoch: 85, Batch: 100, loss: 4.3617 , Train PPL: 1.0067, Train Acc: 0.3979\n",
      "Epoch: 85, Batch: 200, loss: 4.6473 , Train PPL: 1.0071, Train Acc: 0.3506\n",
      "Epoch: 85, Batch: 300, loss: 4.5834 , Train PPL: 1.0070, Train Acc: 0.3948\n",
      "Epoch: 85, Batch: 400, loss: 4.3775 , Train PPL: 1.0067, Train Acc: 0.3857\n",
      "Epoch: 85, Batch: 500, loss: 4.5076 , Train PPL: 1.0069, Train Acc: 0.3720\n",
      "Epoch: 85, Batch: 600, loss: 4.2955 , Train PPL: 1.0066, Train Acc: 0.3872\n",
      "Epoch: 85, Batch: 700, loss: 4.6778 , Train PPL: 1.0072, Train Acc: 0.3430\n",
      "Validation --- Epoch: 85, total loss: 299.6544 , PPL: 1.2781, Acc: 0.3805\n",
      "lr = 0.03125\n",
      "Epoch: 86, Batch: 100, loss: 4.6187 , Train PPL: 1.0071, Train Acc: 0.3598\n",
      "Epoch: 86, Batch: 200, loss: 4.3577 , Train PPL: 1.0067, Train Acc: 0.3582\n",
      "Epoch: 86, Batch: 300, loss: 4.5220 , Train PPL: 1.0069, Train Acc: 0.3765\n",
      "Epoch: 86, Batch: 400, loss: 4.4635 , Train PPL: 1.0068, Train Acc: 0.3857\n",
      "Epoch: 86, Batch: 500, loss: 4.6316 , Train PPL: 1.0071, Train Acc: 0.3201\n",
      "Epoch: 86, Batch: 600, loss: 4.6280 , Train PPL: 1.0071, Train Acc: 0.3796\n",
      "Epoch: 86, Batch: 700, loss: 3.9514 , Train PPL: 1.0060, Train Acc: 0.4924\n",
      "Validation --- Epoch: 86, total loss: 299.6977 , PPL: 1.2781, Acc: 0.3807\n",
      "lr = 0.03125\n",
      "Epoch: 87, Batch: 100, loss: 4.5350 , Train PPL: 1.0069, Train Acc: 0.4101\n",
      "Epoch: 87, Batch: 200, loss: 4.3320 , Train PPL: 1.0066, Train Acc: 0.3887\n",
      "Epoch: 87, Batch: 300, loss: 4.5534 , Train PPL: 1.0070, Train Acc: 0.3765\n",
      "Epoch: 87, Batch: 400, loss: 4.4477 , Train PPL: 1.0068, Train Acc: 0.3963\n",
      "Epoch: 87, Batch: 500, loss: 4.5681 , Train PPL: 1.0070, Train Acc: 0.3506\n",
      "Epoch: 87, Batch: 600, loss: 4.7554 , Train PPL: 1.0073, Train Acc: 0.3598\n",
      "Epoch: 87, Batch: 700, loss: 4.4570 , Train PPL: 1.0068, Train Acc: 0.4162\n",
      "Validation --- Epoch: 87, total loss: 299.6649 , PPL: 1.2781, Acc: 0.3804\n",
      "lr = 0.015625\n",
      "Epoch: 88, Batch: 100, loss: 4.3256 , Train PPL: 1.0066, Train Acc: 0.4207\n",
      "Epoch: 88, Batch: 200, loss: 4.3987 , Train PPL: 1.0067, Train Acc: 0.4131\n",
      "Epoch: 88, Batch: 300, loss: 4.3801 , Train PPL: 1.0067, Train Acc: 0.4040\n",
      "Epoch: 88, Batch: 400, loss: 4.4852 , Train PPL: 1.0069, Train Acc: 0.3857\n",
      "Epoch: 88, Batch: 500, loss: 4.5537 , Train PPL: 1.0070, Train Acc: 0.3537\n",
      "Epoch: 88, Batch: 600, loss: 4.7372 , Train PPL: 1.0072, Train Acc: 0.3537\n",
      "Epoch: 88, Batch: 700, loss: 4.6363 , Train PPL: 1.0071, Train Acc: 0.3689\n",
      "Validation --- Epoch: 88, total loss: 299.6849 , PPL: 1.2781, Acc: 0.3810\n",
      "lr = 0.015625\n",
      "Epoch: 89, Batch: 100, loss: 4.5679 , Train PPL: 1.0070, Train Acc: 0.3887\n",
      "Epoch: 89, Batch: 200, loss: 4.4072 , Train PPL: 1.0067, Train Acc: 0.3933\n",
      "Epoch: 89, Batch: 300, loss: 4.4861 , Train PPL: 1.0069, Train Acc: 0.4009\n",
      "Epoch: 89, Batch: 400, loss: 4.4268 , Train PPL: 1.0068, Train Acc: 0.3659\n",
      "Epoch: 89, Batch: 500, loss: 4.5704 , Train PPL: 1.0070, Train Acc: 0.3613\n",
      "Epoch: 89, Batch: 600, loss: 4.6965 , Train PPL: 1.0072, Train Acc: 0.3430\n",
      "Epoch: 89, Batch: 700, loss: 4.5665 , Train PPL: 1.0070, Train Acc: 0.3415\n",
      "Validation --- Epoch: 89, total loss: 299.6666 , PPL: 1.2781, Acc: 0.3809\n",
      "lr = 0.015625\n",
      "Epoch: 90, Batch: 100, loss: 4.4502 , Train PPL: 1.0068, Train Acc: 0.3994\n",
      "Epoch: 90, Batch: 200, loss: 4.3469 , Train PPL: 1.0066, Train Acc: 0.3796\n",
      "Epoch: 90, Batch: 300, loss: 4.5502 , Train PPL: 1.0070, Train Acc: 0.3659\n",
      "Epoch: 90, Batch: 400, loss: 4.5501 , Train PPL: 1.0070, Train Acc: 0.4101\n",
      "Epoch: 90, Batch: 500, loss: 4.3519 , Train PPL: 1.0067, Train Acc: 0.4527\n",
      "Epoch: 90, Batch: 600, loss: 4.4109 , Train PPL: 1.0067, Train Acc: 0.3796\n",
      "Epoch: 90, Batch: 700, loss: 4.4873 , Train PPL: 1.0069, Train Acc: 0.3689\n",
      "Validation --- Epoch: 90, total loss: 299.6829 , PPL: 1.2781, Acc: 0.3814\n",
      "lr = 0.015625\n",
      "Epoch: 91, Batch: 100, loss: 4.5756 , Train PPL: 1.0070, Train Acc: 0.3567\n",
      "Epoch: 91, Batch: 200, loss: 4.7134 , Train PPL: 1.0072, Train Acc: 0.3567\n",
      "Epoch: 91, Batch: 300, loss: 4.5330 , Train PPL: 1.0069, Train Acc: 0.3902\n",
      "Epoch: 91, Batch: 400, loss: 4.7564 , Train PPL: 1.0073, Train Acc: 0.3125\n",
      "Epoch: 91, Batch: 500, loss: 4.2152 , Train PPL: 1.0064, Train Acc: 0.4924\n",
      "Epoch: 91, Batch: 600, loss: 4.5013 , Train PPL: 1.0069, Train Acc: 0.3628\n",
      "Epoch: 91, Batch: 700, loss: 4.2997 , Train PPL: 1.0066, Train Acc: 0.3979\n",
      "Validation --- Epoch: 91, total loss: 299.6691 , PPL: 1.2781, Acc: 0.3810\n",
      "lr = 0.015625\n",
      "Epoch: 92, Batch: 100, loss: 4.4994 , Train PPL: 1.0069, Train Acc: 0.3979\n",
      "Epoch: 92, Batch: 200, loss: 4.7339 , Train PPL: 1.0072, Train Acc: 0.3598\n",
      "Epoch: 92, Batch: 300, loss: 4.5254 , Train PPL: 1.0069, Train Acc: 0.3293\n",
      "Epoch: 92, Batch: 400, loss: 4.3162 , Train PPL: 1.0066, Train Acc: 0.4588\n",
      "Epoch: 92, Batch: 500, loss: 4.1118 , Train PPL: 1.0063, Train Acc: 0.4192\n",
      "Epoch: 92, Batch: 600, loss: 4.3268 , Train PPL: 1.0066, Train Acc: 0.4040\n",
      "Epoch: 92, Batch: 700, loss: 4.6889 , Train PPL: 1.0072, Train Acc: 0.3704\n",
      "Validation --- Epoch: 92, total loss: 299.6624 , PPL: 1.2781, Acc: 0.3805\n",
      "lr = 0.015625\n",
      "Epoch: 93, Batch: 100, loss: 4.3588 , Train PPL: 1.0067, Train Acc: 0.4253\n",
      "Epoch: 93, Batch: 200, loss: 4.4446 , Train PPL: 1.0068, Train Acc: 0.4070\n",
      "Epoch: 93, Batch: 300, loss: 4.4808 , Train PPL: 1.0069, Train Acc: 0.4116\n",
      "Epoch: 93, Batch: 400, loss: 4.4455 , Train PPL: 1.0068, Train Acc: 0.3780\n",
      "Epoch: 93, Batch: 500, loss: 4.6351 , Train PPL: 1.0071, Train Acc: 0.3369\n",
      "Epoch: 93, Batch: 600, loss: 4.1804 , Train PPL: 1.0064, Train Acc: 0.4070\n",
      "Epoch: 93, Batch: 700, loss: 4.5257 , Train PPL: 1.0069, Train Acc: 0.3841\n",
      "Validation --- Epoch: 93, total loss: 299.6745 , PPL: 1.2781, Acc: 0.3811\n",
      "lr = 0.0078125\n",
      "Epoch: 94, Batch: 100, loss: 4.6661 , Train PPL: 1.0071, Train Acc: 0.3354\n",
      "Epoch: 94, Batch: 200, loss: 4.5401 , Train PPL: 1.0069, Train Acc: 0.3430\n",
      "Epoch: 94, Batch: 300, loss: 4.7765 , Train PPL: 1.0073, Train Acc: 0.3232\n",
      "Epoch: 94, Batch: 400, loss: 4.5404 , Train PPL: 1.0069, Train Acc: 0.3659\n",
      "Epoch: 94, Batch: 500, loss: 4.7065 , Train PPL: 1.0072, Train Acc: 0.3659\n",
      "Epoch: 94, Batch: 600, loss: 4.3458 , Train PPL: 1.0066, Train Acc: 0.3841\n",
      "Epoch: 94, Batch: 700, loss: 4.4150 , Train PPL: 1.0068, Train Acc: 0.4116\n",
      "Validation --- Epoch: 94, total loss: 299.6710 , PPL: 1.2781, Acc: 0.3811\n",
      "lr = 0.0078125\n",
      "Epoch: 95, Batch: 100, loss: 4.3943 , Train PPL: 1.0067, Train Acc: 0.4375\n",
      "Epoch: 95, Batch: 200, loss: 4.4023 , Train PPL: 1.0067, Train Acc: 0.3841\n",
      "Epoch: 95, Batch: 300, loss: 4.1893 , Train PPL: 1.0064, Train Acc: 0.4451\n",
      "Epoch: 95, Batch: 400, loss: 4.6930 , Train PPL: 1.0072, Train Acc: 0.3415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95, Batch: 500, loss: 4.4364 , Train PPL: 1.0068, Train Acc: 0.3979\n",
      "Epoch: 95, Batch: 600, loss: 4.4400 , Train PPL: 1.0068, Train Acc: 0.3689\n",
      "Epoch: 95, Batch: 700, loss: 4.5127 , Train PPL: 1.0069, Train Acc: 0.3780\n",
      "Validation --- Epoch: 95, total loss: 299.6606 , PPL: 1.2781, Acc: 0.3813\n",
      "lr = 0.0078125\n",
      "Epoch: 96, Batch: 100, loss: 4.7801 , Train PPL: 1.0073, Train Acc: 0.3140\n",
      "Epoch: 96, Batch: 200, loss: 4.4531 , Train PPL: 1.0068, Train Acc: 0.4070\n",
      "Epoch: 96, Batch: 300, loss: 4.6568 , Train PPL: 1.0071, Train Acc: 0.3506\n",
      "Epoch: 96, Batch: 400, loss: 4.5526 , Train PPL: 1.0070, Train Acc: 0.3476\n",
      "Epoch: 96, Batch: 500, loss: 4.5860 , Train PPL: 1.0070, Train Acc: 0.3674\n",
      "Epoch: 96, Batch: 600, loss: 4.2010 , Train PPL: 1.0064, Train Acc: 0.3643\n",
      "Epoch: 96, Batch: 700, loss: 4.2710 , Train PPL: 1.0065, Train Acc: 0.4268\n",
      "Validation --- Epoch: 96, total loss: 299.6624 , PPL: 1.2781, Acc: 0.3813\n",
      "lr = 0.0078125\n",
      "Epoch: 97, Batch: 100, loss: 4.6833 , Train PPL: 1.0072, Train Acc: 0.3598\n",
      "Epoch: 97, Batch: 200, loss: 4.5592 , Train PPL: 1.0070, Train Acc: 0.4055\n",
      "Epoch: 97, Batch: 300, loss: 4.4951 , Train PPL: 1.0069, Train Acc: 0.4009\n",
      "Epoch: 97, Batch: 400, loss: 4.3444 , Train PPL: 1.0066, Train Acc: 0.3933\n",
      "Epoch: 97, Batch: 500, loss: 4.2412 , Train PPL: 1.0065, Train Acc: 0.4665\n",
      "Epoch: 97, Batch: 600, loss: 4.5711 , Train PPL: 1.0070, Train Acc: 0.3582\n",
      "Epoch: 97, Batch: 700, loss: 4.1831 , Train PPL: 1.0064, Train Acc: 0.4284\n",
      "Validation --- Epoch: 97, total loss: 299.6649 , PPL: 1.2781, Acc: 0.3814\n",
      "lr = 0.0078125\n",
      "Epoch: 98, Batch: 100, loss: 4.3544 , Train PPL: 1.0067, Train Acc: 0.3994\n",
      "Epoch: 98, Batch: 200, loss: 4.6041 , Train PPL: 1.0070, Train Acc: 0.3659\n",
      "Epoch: 98, Batch: 300, loss: 4.2260 , Train PPL: 1.0065, Train Acc: 0.4131\n",
      "Epoch: 98, Batch: 400, loss: 4.4976 , Train PPL: 1.0069, Train Acc: 0.3887\n",
      "Epoch: 98, Batch: 500, loss: 4.3236 , Train PPL: 1.0066, Train Acc: 0.3857\n",
      "Epoch: 98, Batch: 600, loss: 4.4875 , Train PPL: 1.0069, Train Acc: 0.3826\n",
      "Epoch: 98, Batch: 700, loss: 4.5043 , Train PPL: 1.0069, Train Acc: 0.3765\n",
      "Validation --- Epoch: 98, total loss: 299.6604 , PPL: 1.2781, Acc: 0.3811\n",
      "lr = 0.0078125\n",
      "Epoch: 99, Batch: 100, loss: 4.5452 , Train PPL: 1.0070, Train Acc: 0.3796\n",
      "Epoch: 99, Batch: 200, loss: 4.4254 , Train PPL: 1.0068, Train Acc: 0.4116\n",
      "Epoch: 99, Batch: 300, loss: 4.4732 , Train PPL: 1.0068, Train Acc: 0.4070\n",
      "Epoch: 99, Batch: 400, loss: 4.1048 , Train PPL: 1.0063, Train Acc: 0.4817\n",
      "Epoch: 99, Batch: 500, loss: 4.5228 , Train PPL: 1.0069, Train Acc: 0.3720\n",
      "Epoch: 99, Batch: 600, loss: 4.0573 , Train PPL: 1.0062, Train Acc: 0.4329\n",
      "Epoch: 99, Batch: 700, loss: 4.2648 , Train PPL: 1.0065, Train Acc: 0.4040\n",
      "Validation --- Epoch: 99, total loss: 299.6868 , PPL: 1.2781, Acc: 0.3812\n",
      "lr = 0.00390625\n"
     ]
    }
   ],
   "source": [
    "model = TCN(4, [300,300,300], kernel=3, dropout=0.5, embedding_size = 300, n_words = n_words,tied=True,embedding=word2vec)\n",
    "model.cuda()\n",
    "model_parameters = filter(lambda x: x.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print('Network with {} parameters'.format(params))\n",
    "print('Receptive field of network is {}'.format(model.receptive_field))\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode=\"min\",patience=5,min_lr=1e-6,factor=0.5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "best_vloss = 1e8\n",
    "for e in range(100):\n",
    "    train_loop(e)\n",
    "    validation_loss = validation_loop(e)\n",
    "    scheduler.step(validation_loss)\n",
    "    print('lr = {}'.format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "    if validation_loss < best_vloss:\n",
    "                with open(\"model_4_layers_k3_word2vec.pt\", 'wb') as f:\n",
    "                    print('wrote model')\n",
    "                    torch.save(model, f)\n",
    "                best_vloss = validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network with 12497801 parameters\n",
      "Receptive field of network is 62\n",
      "Epoch: 0, Batch: 100, loss: 6.5419 , Train PPL: 1.0100, Train Acc: 0.1540\n",
      "Epoch: 0, Batch: 200, loss: 6.3844 , Train PPL: 1.0098, Train Acc: 0.1936\n",
      "Epoch: 0, Batch: 300, loss: 6.2338 , Train PPL: 1.0095, Train Acc: 0.2073\n",
      "Epoch: 0, Batch: 400, loss: 6.0998 , Train PPL: 1.0093, Train Acc: 0.2363\n",
      "Epoch: 0, Batch: 500, loss: 6.2956 , Train PPL: 1.0096, Train Acc: 0.2210\n",
      "Epoch: 0, Batch: 600, loss: 6.0092 , Train PPL: 1.0092, Train Acc: 0.2561\n",
      "Epoch: 0, Batch: 700, loss: 6.1426 , Train PPL: 1.0094, Train Acc: 0.2378\n",
      "Validation --- Epoch: 0, total loss: 330.8627 , PPL: 1.3098, Acc: 0.2785\n",
      "lr = 4\n",
      "wrote model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type TCN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type TC_block. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Chomp1d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 100, loss: 5.9273 , Train PPL: 1.0091, Train Acc: 0.2378\n",
      "Epoch: 1, Batch: 200, loss: 5.8736 , Train PPL: 1.0090, Train Acc: 0.2622\n",
      "Epoch: 1, Batch: 300, loss: 5.9605 , Train PPL: 1.0091, Train Acc: 0.2378\n",
      "Epoch: 1, Batch: 400, loss: 5.9158 , Train PPL: 1.0091, Train Acc: 0.2515\n",
      "Epoch: 1, Batch: 500, loss: 5.9485 , Train PPL: 1.0091, Train Acc: 0.3003\n",
      "Epoch: 1, Batch: 600, loss: 5.3175 , Train PPL: 1.0081, Train Acc: 0.3643\n",
      "Epoch: 1, Batch: 700, loss: 5.7331 , Train PPL: 1.0088, Train Acc: 0.2866\n",
      "Validation --- Epoch: 1, total loss: 319.0126 , PPL: 1.2970, Acc: 0.3228\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 2, Batch: 100, loss: 5.6604 , Train PPL: 1.0087, Train Acc: 0.2820\n",
      "Epoch: 2, Batch: 200, loss: 5.8650 , Train PPL: 1.0090, Train Acc: 0.2957\n",
      "Epoch: 2, Batch: 300, loss: 5.6612 , Train PPL: 1.0087, Train Acc: 0.2866\n",
      "Epoch: 2, Batch: 400, loss: 5.0549 , Train PPL: 1.0077, Train Acc: 0.3354\n",
      "Epoch: 2, Batch: 500, loss: 5.9892 , Train PPL: 1.0092, Train Acc: 0.2652\n",
      "Epoch: 2, Batch: 600, loss: 5.3933 , Train PPL: 1.0083, Train Acc: 0.3171\n",
      "Epoch: 2, Batch: 700, loss: 5.6422 , Train PPL: 1.0086, Train Acc: 0.2561\n",
      "Validation --- Epoch: 2, total loss: 314.6036 , PPL: 1.2931, Acc: 0.3173\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 3, Batch: 100, loss: 5.4273 , Train PPL: 1.0083, Train Acc: 0.3460\n",
      "Epoch: 3, Batch: 200, loss: 5.2437 , Train PPL: 1.0080, Train Acc: 0.3399\n",
      "Epoch: 3, Batch: 300, loss: 5.4113 , Train PPL: 1.0083, Train Acc: 0.3064\n",
      "Epoch: 3, Batch: 400, loss: 5.6343 , Train PPL: 1.0086, Train Acc: 0.3186\n",
      "Epoch: 3, Batch: 500, loss: 5.4812 , Train PPL: 1.0084, Train Acc: 0.3537\n",
      "Epoch: 3, Batch: 600, loss: 5.2187 , Train PPL: 1.0080, Train Acc: 0.3277\n",
      "Epoch: 3, Batch: 700, loss: 5.3666 , Train PPL: 1.0082, Train Acc: 0.2866\n",
      "Validation --- Epoch: 3, total loss: 311.8390 , PPL: 1.2900, Acc: 0.3177\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 4, Batch: 100, loss: 5.0070 , Train PPL: 1.0077, Train Acc: 0.3552\n",
      "Epoch: 4, Batch: 200, loss: 5.0936 , Train PPL: 1.0078, Train Acc: 0.3735\n",
      "Epoch: 4, Batch: 300, loss: 5.2474 , Train PPL: 1.0080, Train Acc: 0.3780\n",
      "Epoch: 4, Batch: 400, loss: 5.1670 , Train PPL: 1.0079, Train Acc: 0.2988\n",
      "Epoch: 4, Batch: 500, loss: 5.5262 , Train PPL: 1.0085, Train Acc: 0.2744\n",
      "Epoch: 4, Batch: 600, loss: 5.2977 , Train PPL: 1.0081, Train Acc: 0.3003\n",
      "Epoch: 4, Batch: 700, loss: 5.3156 , Train PPL: 1.0081, Train Acc: 0.3171\n",
      "Validation --- Epoch: 4, total loss: 307.9366 , PPL: 1.2858, Acc: 0.3412\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 5, Batch: 100, loss: 5.1686 , Train PPL: 1.0079, Train Acc: 0.3506\n",
      "Epoch: 5, Batch: 200, loss: 4.8621 , Train PPL: 1.0074, Train Acc: 0.4238\n",
      "Epoch: 5, Batch: 300, loss: 5.2159 , Train PPL: 1.0080, Train Acc: 0.3476\n",
      "Epoch: 5, Batch: 400, loss: 5.5812 , Train PPL: 1.0085, Train Acc: 0.3095\n",
      "Epoch: 5, Batch: 500, loss: 5.2754 , Train PPL: 1.0081, Train Acc: 0.3293\n",
      "Epoch: 5, Batch: 600, loss: 5.3950 , Train PPL: 1.0083, Train Acc: 0.3140\n",
      "Epoch: 5, Batch: 700, loss: 5.2894 , Train PPL: 1.0081, Train Acc: 0.3460\n",
      "Validation --- Epoch: 5, total loss: 306.1994 , PPL: 1.2842, Acc: 0.3378\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 6, Batch: 100, loss: 4.8055 , Train PPL: 1.0074, Train Acc: 0.3643\n",
      "Epoch: 6, Batch: 200, loss: 5.0482 , Train PPL: 1.0077, Train Acc: 0.3384\n",
      "Epoch: 6, Batch: 300, loss: 5.0963 , Train PPL: 1.0078, Train Acc: 0.3079\n",
      "Epoch: 6, Batch: 400, loss: 5.3211 , Train PPL: 1.0081, Train Acc: 0.3857\n",
      "Epoch: 6, Batch: 500, loss: 5.2320 , Train PPL: 1.0080, Train Acc: 0.3659\n",
      "Epoch: 6, Batch: 600, loss: 5.0209 , Train PPL: 1.0077, Train Acc: 0.3750\n",
      "Epoch: 6, Batch: 700, loss: 5.3428 , Train PPL: 1.0082, Train Acc: 0.2835\n",
      "Validation --- Epoch: 6, total loss: 303.5392 , PPL: 1.2814, Acc: 0.3525\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 7, Batch: 100, loss: 4.9816 , Train PPL: 1.0076, Train Acc: 0.3582\n",
      "Epoch: 7, Batch: 200, loss: 4.9075 , Train PPL: 1.0075, Train Acc: 0.3704\n",
      "Epoch: 7, Batch: 300, loss: 4.8993 , Train PPL: 1.0075, Train Acc: 0.3750\n",
      "Epoch: 7, Batch: 400, loss: 5.0500 , Train PPL: 1.0077, Train Acc: 0.3720\n",
      "Epoch: 7, Batch: 500, loss: 5.0838 , Train PPL: 1.0078, Train Acc: 0.3476\n",
      "Epoch: 7, Batch: 600, loss: 4.9336 , Train PPL: 1.0075, Train Acc: 0.3735\n",
      "Epoch: 7, Batch: 700, loss: 5.0579 , Train PPL: 1.0077, Train Acc: 0.3262\n",
      "Validation --- Epoch: 7, total loss: 304.0929 , PPL: 1.2821, Acc: 0.3427\n",
      "lr = 4\n",
      "Epoch: 8, Batch: 100, loss: 5.2641 , Train PPL: 1.0081, Train Acc: 0.3034\n",
      "Epoch: 8, Batch: 200, loss: 5.1854 , Train PPL: 1.0079, Train Acc: 0.2942\n",
      "Epoch: 8, Batch: 300, loss: 5.0621 , Train PPL: 1.0077, Train Acc: 0.2973\n",
      "Epoch: 8, Batch: 400, loss: 5.1531 , Train PPL: 1.0079, Train Acc: 0.3034\n",
      "Epoch: 8, Batch: 500, loss: 4.8695 , Train PPL: 1.0075, Train Acc: 0.3872\n",
      "Epoch: 8, Batch: 600, loss: 5.0859 , Train PPL: 1.0078, Train Acc: 0.3506\n",
      "Epoch: 8, Batch: 700, loss: 5.1254 , Train PPL: 1.0078, Train Acc: 0.3979\n",
      "Validation --- Epoch: 8, total loss: 303.1054 , PPL: 1.2813, Acc: 0.3400\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 9, Batch: 100, loss: 4.9029 , Train PPL: 1.0075, Train Acc: 0.3476\n",
      "Epoch: 9, Batch: 200, loss: 5.1375 , Train PPL: 1.0079, Train Acc: 0.3140\n",
      "Epoch: 9, Batch: 300, loss: 5.0507 , Train PPL: 1.0077, Train Acc: 0.3186\n",
      "Epoch: 9, Batch: 400, loss: 5.2552 , Train PPL: 1.0080, Train Acc: 0.3415\n",
      "Epoch: 9, Batch: 500, loss: 4.6573 , Train PPL: 1.0071, Train Acc: 0.4009\n",
      "Epoch: 9, Batch: 600, loss: 4.9558 , Train PPL: 1.0076, Train Acc: 0.3643\n",
      "Epoch: 9, Batch: 700, loss: 5.1745 , Train PPL: 1.0079, Train Acc: 0.3354\n",
      "Validation --- Epoch: 9, total loss: 301.8198 , PPL: 1.2798, Acc: 0.3465\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 10, Batch: 100, loss: 5.1964 , Train PPL: 1.0080, Train Acc: 0.2896\n",
      "Epoch: 10, Batch: 200, loss: 4.9767 , Train PPL: 1.0076, Train Acc: 0.3034\n",
      "Epoch: 10, Batch: 300, loss: 5.2919 , Train PPL: 1.0081, Train Acc: 0.2881\n",
      "Epoch: 10, Batch: 400, loss: 4.9035 , Train PPL: 1.0075, Train Acc: 0.3521\n",
      "Epoch: 10, Batch: 500, loss: 5.2861 , Train PPL: 1.0081, Train Acc: 0.3399\n",
      "Epoch: 10, Batch: 600, loss: 4.6653 , Train PPL: 1.0071, Train Acc: 0.4146\n",
      "Epoch: 10, Batch: 700, loss: 4.9215 , Train PPL: 1.0075, Train Acc: 0.3857\n",
      "Validation --- Epoch: 10, total loss: 301.1988 , PPL: 1.2794, Acc: 0.3406\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 11, Batch: 100, loss: 4.9149 , Train PPL: 1.0075, Train Acc: 0.3460\n",
      "Epoch: 11, Batch: 200, loss: 4.8714 , Train PPL: 1.0075, Train Acc: 0.3994\n",
      "Epoch: 11, Batch: 300, loss: 4.9276 , Train PPL: 1.0075, Train Acc: 0.3521\n",
      "Epoch: 11, Batch: 400, loss: 4.8373 , Train PPL: 1.0074, Train Acc: 0.3399\n",
      "Epoch: 11, Batch: 500, loss: 5.0206 , Train PPL: 1.0077, Train Acc: 0.3277\n",
      "Epoch: 11, Batch: 600, loss: 5.1497 , Train PPL: 1.0079, Train Acc: 0.3155\n",
      "Epoch: 11, Batch: 700, loss: 4.8774 , Train PPL: 1.0075, Train Acc: 0.3354\n",
      "Validation --- Epoch: 11, total loss: 300.9594 , PPL: 1.2792, Acc: 0.3412\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 12, Batch: 100, loss: 4.8253 , Train PPL: 1.0074, Train Acc: 0.3643\n",
      "Epoch: 12, Batch: 200, loss: 4.8719 , Train PPL: 1.0075, Train Acc: 0.3140\n",
      "Epoch: 12, Batch: 300, loss: 5.0201 , Train PPL: 1.0077, Train Acc: 0.3399\n",
      "Epoch: 12, Batch: 400, loss: 5.2296 , Train PPL: 1.0080, Train Acc: 0.3384\n",
      "Epoch: 12, Batch: 500, loss: 4.9048 , Train PPL: 1.0075, Train Acc: 0.3704\n",
      "Epoch: 12, Batch: 600, loss: 4.8243 , Train PPL: 1.0074, Train Acc: 0.3537\n",
      "Epoch: 12, Batch: 700, loss: 4.9562 , Train PPL: 1.0076, Train Acc: 0.3460\n",
      "Validation --- Epoch: 12, total loss: 300.6186 , PPL: 1.2790, Acc: 0.3524\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 13, Batch: 100, loss: 4.6923 , Train PPL: 1.0072, Train Acc: 0.3582\n",
      "Epoch: 13, Batch: 200, loss: 4.5440 , Train PPL: 1.0070, Train Acc: 0.3552\n",
      "Epoch: 13, Batch: 300, loss: 4.7964 , Train PPL: 1.0073, Train Acc: 0.3445\n",
      "Epoch: 13, Batch: 400, loss: 4.9924 , Train PPL: 1.0076, Train Acc: 0.3140\n",
      "Epoch: 13, Batch: 500, loss: 4.7137 , Train PPL: 1.0072, Train Acc: 0.4024\n",
      "Epoch: 13, Batch: 600, loss: 4.7435 , Train PPL: 1.0073, Train Acc: 0.3674\n",
      "Epoch: 13, Batch: 700, loss: 4.6960 , Train PPL: 1.0072, Train Acc: 0.3430\n",
      "Validation --- Epoch: 13, total loss: 301.1320 , PPL: 1.2795, Acc: 0.3532\n",
      "lr = 4\n",
      "Epoch: 14, Batch: 100, loss: 4.9884 , Train PPL: 1.0076, Train Acc: 0.3780\n",
      "Epoch: 14, Batch: 200, loss: 5.1959 , Train PPL: 1.0080, Train Acc: 0.3110\n",
      "Epoch: 14, Batch: 300, loss: 4.9112 , Train PPL: 1.0075, Train Acc: 0.3338\n",
      "Epoch: 14, Batch: 400, loss: 5.1103 , Train PPL: 1.0078, Train Acc: 0.3476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Batch: 500, loss: 4.8003 , Train PPL: 1.0073, Train Acc: 0.3902\n",
      "Epoch: 14, Batch: 600, loss: 4.6251 , Train PPL: 1.0071, Train Acc: 0.3247\n",
      "Epoch: 14, Batch: 700, loss: 4.7220 , Train PPL: 1.0072, Train Acc: 0.3476\n",
      "Validation --- Epoch: 14, total loss: 300.3836 , PPL: 1.2787, Acc: 0.3494\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 15, Batch: 100, loss: 4.3844 , Train PPL: 1.0067, Train Acc: 0.3872\n",
      "Epoch: 15, Batch: 200, loss: 4.9037 , Train PPL: 1.0075, Train Acc: 0.3476\n",
      "Epoch: 15, Batch: 300, loss: 4.4512 , Train PPL: 1.0068, Train Acc: 0.4345\n",
      "Epoch: 15, Batch: 400, loss: 4.3315 , Train PPL: 1.0066, Train Acc: 0.3826\n",
      "Epoch: 15, Batch: 500, loss: 4.9630 , Train PPL: 1.0076, Train Acc: 0.3338\n",
      "Epoch: 15, Batch: 600, loss: 5.0294 , Train PPL: 1.0077, Train Acc: 0.3232\n",
      "Epoch: 15, Batch: 700, loss: 4.6638 , Train PPL: 1.0071, Train Acc: 0.3735\n",
      "Validation --- Epoch: 15, total loss: 299.2794 , PPL: 1.2775, Acc: 0.3477\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 16, Batch: 100, loss: 4.1521 , Train PPL: 1.0063, Train Acc: 0.4299\n",
      "Epoch: 16, Batch: 200, loss: 4.9701 , Train PPL: 1.0076, Train Acc: 0.3293\n",
      "Epoch: 16, Batch: 300, loss: 4.7462 , Train PPL: 1.0073, Train Acc: 0.3415\n",
      "Epoch: 16, Batch: 400, loss: 4.5974 , Train PPL: 1.0070, Train Acc: 0.3506\n",
      "Epoch: 16, Batch: 500, loss: 4.7611 , Train PPL: 1.0073, Train Acc: 0.3674\n",
      "Epoch: 16, Batch: 600, loss: 4.7337 , Train PPL: 1.0072, Train Acc: 0.3354\n",
      "Epoch: 16, Batch: 700, loss: 4.7124 , Train PPL: 1.0072, Train Acc: 0.3659\n",
      "Validation --- Epoch: 16, total loss: 299.5637 , PPL: 1.2780, Acc: 0.3557\n",
      "lr = 4\n",
      "Epoch: 17, Batch: 100, loss: 4.9183 , Train PPL: 1.0075, Train Acc: 0.3216\n",
      "Epoch: 17, Batch: 200, loss: 4.7050 , Train PPL: 1.0072, Train Acc: 0.3567\n",
      "Epoch: 17, Batch: 300, loss: 4.8666 , Train PPL: 1.0074, Train Acc: 0.3582\n",
      "Epoch: 17, Batch: 400, loss: 4.8580 , Train PPL: 1.0074, Train Acc: 0.3308\n",
      "Epoch: 17, Batch: 500, loss: 4.7554 , Train PPL: 1.0073, Train Acc: 0.3765\n",
      "Epoch: 17, Batch: 600, loss: 4.6668 , Train PPL: 1.0071, Train Acc: 0.3872\n",
      "Epoch: 17, Batch: 700, loss: 4.8296 , Train PPL: 1.0074, Train Acc: 0.3323\n",
      "Validation --- Epoch: 17, total loss: 298.9700 , PPL: 1.2772, Acc: 0.3542\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 18, Batch: 100, loss: 4.7236 , Train PPL: 1.0072, Train Acc: 0.3521\n",
      "Epoch: 18, Batch: 200, loss: 4.5561 , Train PPL: 1.0070, Train Acc: 0.3963\n",
      "Epoch: 18, Batch: 300, loss: 4.6384 , Train PPL: 1.0071, Train Acc: 0.3750\n",
      "Epoch: 18, Batch: 400, loss: 4.6558 , Train PPL: 1.0071, Train Acc: 0.3506\n",
      "Epoch: 18, Batch: 500, loss: 4.8950 , Train PPL: 1.0075, Train Acc: 0.3506\n",
      "Epoch: 18, Batch: 600, loss: 4.9415 , Train PPL: 1.0076, Train Acc: 0.3034\n",
      "Epoch: 18, Batch: 700, loss: 4.6181 , Train PPL: 1.0071, Train Acc: 0.3552\n",
      "Validation --- Epoch: 18, total loss: 298.9685 , PPL: 1.2773, Acc: 0.3361\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 19, Batch: 100, loss: 4.6508 , Train PPL: 1.0071, Train Acc: 0.3338\n",
      "Epoch: 19, Batch: 200, loss: 4.8685 , Train PPL: 1.0074, Train Acc: 0.3582\n",
      "Epoch: 19, Batch: 300, loss: 5.1196 , Train PPL: 1.0078, Train Acc: 0.2927\n",
      "Epoch: 19, Batch: 400, loss: 4.5718 , Train PPL: 1.0070, Train Acc: 0.3826\n",
      "Epoch: 19, Batch: 500, loss: 4.8400 , Train PPL: 1.0074, Train Acc: 0.3445\n",
      "Epoch: 19, Batch: 600, loss: 4.7375 , Train PPL: 1.0072, Train Acc: 0.3567\n",
      "Epoch: 19, Batch: 700, loss: 4.9275 , Train PPL: 1.0075, Train Acc: 0.3552\n",
      "Validation --- Epoch: 19, total loss: 300.0328 , PPL: 1.2784, Acc: 0.3461\n",
      "lr = 4\n",
      "Epoch: 20, Batch: 100, loss: 4.2773 , Train PPL: 1.0065, Train Acc: 0.3963\n",
      "Epoch: 20, Batch: 200, loss: 4.7042 , Train PPL: 1.0072, Train Acc: 0.3765\n",
      "Epoch: 20, Batch: 300, loss: 4.6928 , Train PPL: 1.0072, Train Acc: 0.3201\n",
      "Epoch: 20, Batch: 400, loss: 4.3890 , Train PPL: 1.0067, Train Acc: 0.4009\n",
      "Epoch: 20, Batch: 500, loss: 4.6538 , Train PPL: 1.0071, Train Acc: 0.3476\n",
      "Epoch: 20, Batch: 600, loss: 4.9518 , Train PPL: 1.0076, Train Acc: 0.3369\n",
      "Epoch: 20, Batch: 700, loss: 4.8911 , Train PPL: 1.0075, Train Acc: 0.3110\n",
      "Validation --- Epoch: 20, total loss: 298.9557 , PPL: 1.2773, Acc: 0.3546\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 21, Batch: 100, loss: 4.9597 , Train PPL: 1.0076, Train Acc: 0.2820\n",
      "Epoch: 21, Batch: 200, loss: 4.6379 , Train PPL: 1.0071, Train Acc: 0.3354\n",
      "Epoch: 21, Batch: 300, loss: 4.5391 , Train PPL: 1.0069, Train Acc: 0.3948\n",
      "Epoch: 21, Batch: 400, loss: 4.6139 , Train PPL: 1.0071, Train Acc: 0.3735\n",
      "Epoch: 21, Batch: 500, loss: 4.8607 , Train PPL: 1.0074, Train Acc: 0.3430\n",
      "Epoch: 21, Batch: 600, loss: 4.7479 , Train PPL: 1.0073, Train Acc: 0.3689\n",
      "Epoch: 21, Batch: 700, loss: 4.8087 , Train PPL: 1.0074, Train Acc: 0.3430\n",
      "Validation --- Epoch: 21, total loss: 299.2823 , PPL: 1.2778, Acc: 0.3556\n",
      "lr = 4\n",
      "Epoch: 22, Batch: 100, loss: 4.9742 , Train PPL: 1.0076, Train Acc: 0.2988\n",
      "Epoch: 22, Batch: 200, loss: 4.5557 , Train PPL: 1.0070, Train Acc: 0.3491\n",
      "Epoch: 22, Batch: 300, loss: 4.6240 , Train PPL: 1.0071, Train Acc: 0.3171\n",
      "Epoch: 22, Batch: 400, loss: 4.5890 , Train PPL: 1.0070, Train Acc: 0.3750\n",
      "Epoch: 22, Batch: 500, loss: 4.5784 , Train PPL: 1.0070, Train Acc: 0.3537\n",
      "Epoch: 22, Batch: 600, loss: 4.5033 , Train PPL: 1.0069, Train Acc: 0.4177\n",
      "Epoch: 22, Batch: 700, loss: 4.8898 , Train PPL: 1.0075, Train Acc: 0.2866\n",
      "Validation --- Epoch: 22, total loss: 300.4061 , PPL: 1.2789, Acc: 0.3573\n",
      "lr = 4\n",
      "Epoch: 23, Batch: 100, loss: 4.3482 , Train PPL: 1.0067, Train Acc: 0.4009\n",
      "Epoch: 23, Batch: 200, loss: 4.7774 , Train PPL: 1.0073, Train Acc: 0.3277\n",
      "Epoch: 23, Batch: 300, loss: 4.7311 , Train PPL: 1.0072, Train Acc: 0.3506\n",
      "Epoch: 23, Batch: 400, loss: 4.4066 , Train PPL: 1.0067, Train Acc: 0.4253\n",
      "Epoch: 23, Batch: 500, loss: 4.6924 , Train PPL: 1.0072, Train Acc: 0.3476\n",
      "Epoch: 23, Batch: 600, loss: 4.6611 , Train PPL: 1.0071, Train Acc: 0.3308\n",
      "Epoch: 23, Batch: 700, loss: 4.7330 , Train PPL: 1.0072, Train Acc: 0.3689\n",
      "Validation --- Epoch: 23, total loss: 299.1544 , PPL: 1.2775, Acc: 0.3545\n",
      "lr = 2.0\n",
      "Epoch: 24, Batch: 100, loss: 4.4322 , Train PPL: 1.0068, Train Acc: 0.3720\n",
      "Epoch: 24, Batch: 200, loss: 4.1789 , Train PPL: 1.0064, Train Acc: 0.4375\n",
      "Epoch: 24, Batch: 300, loss: 4.5092 , Train PPL: 1.0069, Train Acc: 0.3537\n",
      "Epoch: 24, Batch: 400, loss: 4.6413 , Train PPL: 1.0071, Train Acc: 0.3765\n",
      "Epoch: 24, Batch: 500, loss: 4.2980 , Train PPL: 1.0066, Train Acc: 0.4558\n",
      "Epoch: 24, Batch: 600, loss: 4.5087 , Train PPL: 1.0069, Train Acc: 0.3963\n",
      "Epoch: 24, Batch: 700, loss: 4.4116 , Train PPL: 1.0067, Train Acc: 0.3659\n",
      "Validation --- Epoch: 24, total loss: 295.5481 , PPL: 1.2741, Acc: 0.3860\n",
      "lr = 2.0\n",
      "wrote model\n",
      "Epoch: 25, Batch: 100, loss: 4.0280 , Train PPL: 1.0062, Train Acc: 0.4726\n",
      "Epoch: 25, Batch: 200, loss: 4.4650 , Train PPL: 1.0068, Train Acc: 0.3826\n",
      "Epoch: 25, Batch: 300, loss: 4.8228 , Train PPL: 1.0074, Train Acc: 0.3399\n",
      "Epoch: 25, Batch: 400, loss: 4.4232 , Train PPL: 1.0068, Train Acc: 0.4070\n",
      "Epoch: 25, Batch: 500, loss: 4.4543 , Train PPL: 1.0068, Train Acc: 0.4543\n",
      "Epoch: 25, Batch: 600, loss: 4.1939 , Train PPL: 1.0064, Train Acc: 0.4360\n",
      "Epoch: 25, Batch: 700, loss: 3.9435 , Train PPL: 1.0060, Train Acc: 0.4421\n",
      "Validation --- Epoch: 25, total loss: 295.9534 , PPL: 1.2746, Acc: 0.3864\n",
      "lr = 2.0\n",
      "Epoch: 26, Batch: 100, loss: 4.3961 , Train PPL: 1.0067, Train Acc: 0.3491\n",
      "Epoch: 26, Batch: 200, loss: 4.0570 , Train PPL: 1.0062, Train Acc: 0.4497\n",
      "Epoch: 26, Batch: 300, loss: 4.5092 , Train PPL: 1.0069, Train Acc: 0.3277\n",
      "Epoch: 26, Batch: 400, loss: 4.3024 , Train PPL: 1.0066, Train Acc: 0.4497\n",
      "Epoch: 26, Batch: 500, loss: 4.2476 , Train PPL: 1.0065, Train Acc: 0.4101\n",
      "Epoch: 26, Batch: 600, loss: 4.5563 , Train PPL: 1.0070, Train Acc: 0.3598\n",
      "Epoch: 26, Batch: 700, loss: 4.5936 , Train PPL: 1.0070, Train Acc: 0.3796\n",
      "Validation --- Epoch: 26, total loss: 295.6910 , PPL: 1.2743, Acc: 0.3875\n",
      "lr = 2.0\n",
      "Epoch: 27, Batch: 100, loss: 4.1738 , Train PPL: 1.0064, Train Acc: 0.4146\n",
      "Epoch: 27, Batch: 200, loss: 4.5969 , Train PPL: 1.0070, Train Acc: 0.3689\n",
      "Epoch: 27, Batch: 300, loss: 4.3715 , Train PPL: 1.0067, Train Acc: 0.3720\n",
      "Epoch: 27, Batch: 400, loss: 4.4752 , Train PPL: 1.0068, Train Acc: 0.3338\n",
      "Epoch: 27, Batch: 500, loss: 4.3823 , Train PPL: 1.0067, Train Acc: 0.3826\n",
      "Epoch: 27, Batch: 600, loss: 4.4572 , Train PPL: 1.0068, Train Acc: 0.3780\n",
      "Epoch: 27, Batch: 700, loss: 4.5133 , Train PPL: 1.0069, Train Acc: 0.3628\n",
      "Validation --- Epoch: 27, total loss: 295.6581 , PPL: 1.2743, Acc: 0.3901\n",
      "lr = 2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Batch: 100, loss: 4.3928 , Train PPL: 1.0067, Train Acc: 0.3979\n",
      "Epoch: 28, Batch: 200, loss: 3.8187 , Train PPL: 1.0058, Train Acc: 0.4832\n",
      "Epoch: 28, Batch: 300, loss: 4.5811 , Train PPL: 1.0070, Train Acc: 0.3430\n",
      "Epoch: 28, Batch: 400, loss: 4.4839 , Train PPL: 1.0069, Train Acc: 0.3262\n",
      "Epoch: 28, Batch: 500, loss: 4.4560 , Train PPL: 1.0068, Train Acc: 0.3567\n",
      "Epoch: 28, Batch: 600, loss: 4.3144 , Train PPL: 1.0066, Train Acc: 0.4116\n",
      "Epoch: 28, Batch: 700, loss: 4.4816 , Train PPL: 1.0069, Train Acc: 0.3369\n",
      "Validation --- Epoch: 28, total loss: 296.0933 , PPL: 1.2748, Acc: 0.3880\n",
      "lr = 2.0\n",
      "Epoch: 29, Batch: 100, loss: 4.1156 , Train PPL: 1.0063, Train Acc: 0.4665\n",
      "Epoch: 29, Batch: 200, loss: 4.0788 , Train PPL: 1.0062, Train Acc: 0.4070\n",
      "Epoch: 29, Batch: 300, loss: 4.5404 , Train PPL: 1.0069, Train Acc: 0.3780\n",
      "Epoch: 29, Batch: 400, loss: 4.3861 , Train PPL: 1.0067, Train Acc: 0.3598\n",
      "Epoch: 29, Batch: 500, loss: 4.3156 , Train PPL: 1.0066, Train Acc: 0.3948\n",
      "Epoch: 29, Batch: 600, loss: 4.6498 , Train PPL: 1.0071, Train Acc: 0.3095\n",
      "Epoch: 29, Batch: 700, loss: 4.3547 , Train PPL: 1.0067, Train Acc: 0.3857\n",
      "Validation --- Epoch: 29, total loss: 296.7469 , PPL: 1.2754, Acc: 0.3828\n",
      "lr = 2.0\n",
      "Epoch: 30, Batch: 100, loss: 4.1911 , Train PPL: 1.0064, Train Acc: 0.4131\n",
      "Epoch: 30, Batch: 200, loss: 4.2398 , Train PPL: 1.0065, Train Acc: 0.4116\n",
      "Epoch: 30, Batch: 300, loss: 4.5078 , Train PPL: 1.0069, Train Acc: 0.3399\n",
      "Epoch: 30, Batch: 400, loss: 4.6777 , Train PPL: 1.0072, Train Acc: 0.3079\n",
      "Epoch: 30, Batch: 500, loss: 4.4270 , Train PPL: 1.0068, Train Acc: 0.3537\n",
      "Epoch: 30, Batch: 600, loss: 4.5273 , Train PPL: 1.0069, Train Acc: 0.3384\n",
      "Epoch: 30, Batch: 700, loss: 4.0222 , Train PPL: 1.0062, Train Acc: 0.4741\n",
      "Validation --- Epoch: 30, total loss: 296.2319 , PPL: 1.2749, Acc: 0.3869\n",
      "lr = 1.0\n",
      "Epoch: 31, Batch: 100, loss: 4.1221 , Train PPL: 1.0063, Train Acc: 0.4451\n",
      "Epoch: 31, Batch: 200, loss: 4.0723 , Train PPL: 1.0062, Train Acc: 0.4360\n",
      "Epoch: 31, Batch: 300, loss: 4.3941 , Train PPL: 1.0067, Train Acc: 0.3720\n",
      "Epoch: 31, Batch: 400, loss: 4.3812 , Train PPL: 1.0067, Train Acc: 0.3704\n",
      "Epoch: 31, Batch: 500, loss: 4.5707 , Train PPL: 1.0070, Train Acc: 0.3293\n",
      "Epoch: 31, Batch: 600, loss: 4.0842 , Train PPL: 1.0062, Train Acc: 0.4040\n",
      "Epoch: 31, Batch: 700, loss: 4.2892 , Train PPL: 1.0066, Train Acc: 0.4207\n",
      "Validation --- Epoch: 31, total loss: 295.5936 , PPL: 1.2743, Acc: 0.3982\n",
      "lr = 1.0\n",
      "Epoch: 32, Batch: 100, loss: 4.3647 , Train PPL: 1.0067, Train Acc: 0.3582\n",
      "Epoch: 32, Batch: 200, loss: 4.1378 , Train PPL: 1.0063, Train Acc: 0.4070\n",
      "Epoch: 32, Batch: 300, loss: 4.5263 , Train PPL: 1.0069, Train Acc: 0.3277\n",
      "Epoch: 32, Batch: 400, loss: 4.2897 , Train PPL: 1.0066, Train Acc: 0.4085\n",
      "Epoch: 32, Batch: 500, loss: 4.1422 , Train PPL: 1.0063, Train Acc: 0.4284\n",
      "Epoch: 32, Batch: 600, loss: 4.1259 , Train PPL: 1.0063, Train Acc: 0.4329\n",
      "Epoch: 32, Batch: 700, loss: 4.0113 , Train PPL: 1.0061, Train Acc: 0.4802\n",
      "Validation --- Epoch: 32, total loss: 295.8442 , PPL: 1.2746, Acc: 0.3967\n",
      "lr = 1.0\n",
      "Epoch: 33, Batch: 100, loss: 3.9309 , Train PPL: 1.0060, Train Acc: 0.4482\n",
      "Epoch: 33, Batch: 200, loss: 3.9747 , Train PPL: 1.0061, Train Acc: 0.4741\n",
      "Epoch: 33, Batch: 300, loss: 4.2382 , Train PPL: 1.0065, Train Acc: 0.4375\n",
      "Epoch: 33, Batch: 400, loss: 4.4344 , Train PPL: 1.0068, Train Acc: 0.3979\n",
      "Epoch: 33, Batch: 500, loss: 4.2563 , Train PPL: 1.0065, Train Acc: 0.4558\n",
      "Epoch: 33, Batch: 600, loss: 4.0928 , Train PPL: 1.0063, Train Acc: 0.4390\n",
      "Epoch: 33, Batch: 700, loss: 4.2724 , Train PPL: 1.0065, Train Acc: 0.3994\n",
      "Validation --- Epoch: 33, total loss: 296.1365 , PPL: 1.2749, Acc: 0.3961\n",
      "lr = 1.0\n",
      "Epoch: 34, Batch: 100, loss: 4.0431 , Train PPL: 1.0062, Train Acc: 0.4253\n",
      "Epoch: 34, Batch: 200, loss: 4.1210 , Train PPL: 1.0063, Train Acc: 0.3735\n",
      "Epoch: 34, Batch: 300, loss: 4.2860 , Train PPL: 1.0066, Train Acc: 0.3918\n",
      "Epoch: 34, Batch: 400, loss: 4.1658 , Train PPL: 1.0064, Train Acc: 0.4116\n",
      "Epoch: 34, Batch: 500, loss: 4.2921 , Train PPL: 1.0066, Train Acc: 0.3979\n",
      "Epoch: 34, Batch: 600, loss: 4.5148 , Train PPL: 1.0069, Train Acc: 0.3491\n",
      "Epoch: 34, Batch: 700, loss: 4.3835 , Train PPL: 1.0067, Train Acc: 0.3872\n",
      "Validation --- Epoch: 34, total loss: 296.4500 , PPL: 1.2753, Acc: 0.3982\n",
      "lr = 1.0\n",
      "Epoch: 35, Batch: 100, loss: 4.1481 , Train PPL: 1.0063, Train Acc: 0.4207\n",
      "Epoch: 35, Batch: 200, loss: 4.2785 , Train PPL: 1.0065, Train Acc: 0.3765\n",
      "Epoch: 35, Batch: 300, loss: 4.1034 , Train PPL: 1.0063, Train Acc: 0.4695\n",
      "Epoch: 35, Batch: 400, loss: 4.1713 , Train PPL: 1.0064, Train Acc: 0.4146\n",
      "Epoch: 35, Batch: 500, loss: 4.2589 , Train PPL: 1.0065, Train Acc: 0.3979\n",
      "Epoch: 35, Batch: 600, loss: 4.4438 , Train PPL: 1.0068, Train Acc: 0.3659\n",
      "Epoch: 35, Batch: 700, loss: 3.9968 , Train PPL: 1.0061, Train Acc: 0.4619\n",
      "Validation --- Epoch: 35, total loss: 296.0494 , PPL: 1.2748, Acc: 0.3975\n",
      "lr = 1.0\n",
      "Epoch: 36, Batch: 100, loss: 3.9322 , Train PPL: 1.0060, Train Acc: 0.4604\n",
      "Epoch: 36, Batch: 200, loss: 4.1844 , Train PPL: 1.0064, Train Acc: 0.3857\n",
      "Epoch: 36, Batch: 300, loss: 4.0810 , Train PPL: 1.0062, Train Acc: 0.4284\n",
      "Epoch: 36, Batch: 400, loss: 4.4987 , Train PPL: 1.0069, Train Acc: 0.3430\n",
      "Epoch: 36, Batch: 500, loss: 4.1965 , Train PPL: 1.0064, Train Acc: 0.3811\n",
      "Epoch: 36, Batch: 600, loss: 4.4773 , Train PPL: 1.0068, Train Acc: 0.3659\n",
      "Epoch: 36, Batch: 700, loss: 4.1544 , Train PPL: 1.0064, Train Acc: 0.4146\n",
      "Validation --- Epoch: 36, total loss: 296.4605 , PPL: 1.2754, Acc: 0.3960\n",
      "lr = 0.5\n",
      "Epoch: 37, Batch: 100, loss: 4.2627 , Train PPL: 1.0065, Train Acc: 0.3674\n",
      "Epoch: 37, Batch: 200, loss: 4.2335 , Train PPL: 1.0065, Train Acc: 0.3826\n",
      "Epoch: 37, Batch: 300, loss: 4.2814 , Train PPL: 1.0065, Train Acc: 0.3979\n",
      "Epoch: 37, Batch: 400, loss: 3.9864 , Train PPL: 1.0061, Train Acc: 0.4345\n",
      "Epoch: 37, Batch: 500, loss: 4.1833 , Train PPL: 1.0064, Train Acc: 0.3765\n",
      "Epoch: 37, Batch: 600, loss: 4.1087 , Train PPL: 1.0063, Train Acc: 0.4162\n",
      "Epoch: 37, Batch: 700, loss: 4.3714 , Train PPL: 1.0067, Train Acc: 0.3598\n",
      "Validation --- Epoch: 37, total loss: 296.5151 , PPL: 1.2754, Acc: 0.3994\n",
      "lr = 0.5\n",
      "Epoch: 38, Batch: 100, loss: 4.3036 , Train PPL: 1.0066, Train Acc: 0.4055\n",
      "Epoch: 38, Batch: 200, loss: 4.3018 , Train PPL: 1.0066, Train Acc: 0.3994\n",
      "Epoch: 38, Batch: 300, loss: 4.0161 , Train PPL: 1.0061, Train Acc: 0.4131\n",
      "Epoch: 38, Batch: 400, loss: 3.8982 , Train PPL: 1.0060, Train Acc: 0.5122\n",
      "Epoch: 38, Batch: 500, loss: 3.8265 , Train PPL: 1.0059, Train Acc: 0.5366\n",
      "Epoch: 38, Batch: 600, loss: 3.6817 , Train PPL: 1.0056, Train Acc: 0.5198\n",
      "Epoch: 38, Batch: 700, loss: 4.2018 , Train PPL: 1.0064, Train Acc: 0.4177\n",
      "Validation --- Epoch: 38, total loss: 296.4414 , PPL: 1.2753, Acc: 0.4004\n",
      "lr = 0.5\n",
      "Epoch: 39, Batch: 100, loss: 4.3126 , Train PPL: 1.0066, Train Acc: 0.4192\n",
      "Epoch: 39, Batch: 200, loss: 4.0270 , Train PPL: 1.0062, Train Acc: 0.4543\n",
      "Epoch: 39, Batch: 300, loss: 3.9487 , Train PPL: 1.0060, Train Acc: 0.4223\n",
      "Epoch: 39, Batch: 400, loss: 4.2626 , Train PPL: 1.0065, Train Acc: 0.3628\n",
      "Epoch: 39, Batch: 500, loss: 4.2039 , Train PPL: 1.0064, Train Acc: 0.3918\n",
      "Epoch: 39, Batch: 600, loss: 4.2206 , Train PPL: 1.0065, Train Acc: 0.3643\n",
      "Epoch: 39, Batch: 700, loss: 4.1328 , Train PPL: 1.0063, Train Acc: 0.4268\n",
      "Validation --- Epoch: 39, total loss: 296.5392 , PPL: 1.2754, Acc: 0.4005\n",
      "lr = 0.5\n",
      "Epoch: 40, Batch: 100, loss: 3.9898 , Train PPL: 1.0061, Train Acc: 0.4451\n",
      "Epoch: 40, Batch: 200, loss: 4.1040 , Train PPL: 1.0063, Train Acc: 0.4085\n",
      "Epoch: 40, Batch: 300, loss: 4.1363 , Train PPL: 1.0063, Train Acc: 0.3994\n",
      "Epoch: 40, Batch: 400, loss: 4.1652 , Train PPL: 1.0064, Train Acc: 0.4497\n",
      "Epoch: 40, Batch: 500, loss: 4.3456 , Train PPL: 1.0066, Train Acc: 0.3902\n",
      "Epoch: 40, Batch: 600, loss: 4.1004 , Train PPL: 1.0063, Train Acc: 0.4131\n",
      "Epoch: 40, Batch: 700, loss: 3.7718 , Train PPL: 1.0058, Train Acc: 0.4619\n",
      "Validation --- Epoch: 40, total loss: 296.5605 , PPL: 1.2755, Acc: 0.4012\n",
      "lr = 0.5\n",
      "Epoch: 41, Batch: 100, loss: 4.1569 , Train PPL: 1.0064, Train Acc: 0.4131\n",
      "Epoch: 41, Batch: 200, loss: 4.1054 , Train PPL: 1.0063, Train Acc: 0.3659\n",
      "Epoch: 41, Batch: 300, loss: 4.3071 , Train PPL: 1.0066, Train Acc: 0.3582\n",
      "Epoch: 41, Batch: 400, loss: 4.4568 , Train PPL: 1.0068, Train Acc: 0.3674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41, Batch: 500, loss: 4.1016 , Train PPL: 1.0063, Train Acc: 0.4360\n",
      "Epoch: 41, Batch: 600, loss: 3.9855 , Train PPL: 1.0061, Train Acc: 0.4558\n",
      "Epoch: 41, Batch: 700, loss: 3.7811 , Train PPL: 1.0058, Train Acc: 0.4893\n",
      "Validation --- Epoch: 41, total loss: 296.9861 , PPL: 1.2759, Acc: 0.3988\n",
      "lr = 0.5\n",
      "Epoch: 42, Batch: 100, loss: 4.1104 , Train PPL: 1.0063, Train Acc: 0.3765\n",
      "Epoch: 42, Batch: 200, loss: 4.0471 , Train PPL: 1.0062, Train Acc: 0.4527\n",
      "Epoch: 42, Batch: 300, loss: 3.9806 , Train PPL: 1.0061, Train Acc: 0.4909\n",
      "Epoch: 42, Batch: 400, loss: 4.1736 , Train PPL: 1.0064, Train Acc: 0.4024\n",
      "Epoch: 42, Batch: 500, loss: 3.9789 , Train PPL: 1.0061, Train Acc: 0.3979\n",
      "Epoch: 42, Batch: 600, loss: 3.9221 , Train PPL: 1.0060, Train Acc: 0.4787\n",
      "Epoch: 42, Batch: 700, loss: 4.0533 , Train PPL: 1.0062, Train Acc: 0.4405\n",
      "Validation --- Epoch: 42, total loss: 296.9651 , PPL: 1.2759, Acc: 0.3999\n",
      "lr = 0.25\n",
      "Epoch: 43, Batch: 100, loss: 4.3854 , Train PPL: 1.0067, Train Acc: 0.3552\n",
      "Epoch: 43, Batch: 200, loss: 3.8539 , Train PPL: 1.0059, Train Acc: 0.4436\n",
      "Epoch: 43, Batch: 300, loss: 4.2220 , Train PPL: 1.0065, Train Acc: 0.3902\n",
      "Epoch: 43, Batch: 400, loss: 4.2977 , Train PPL: 1.0066, Train Acc: 0.3796\n",
      "Epoch: 43, Batch: 500, loss: 4.0746 , Train PPL: 1.0062, Train Acc: 0.4024\n",
      "Epoch: 43, Batch: 600, loss: 4.0832 , Train PPL: 1.0062, Train Acc: 0.4268\n",
      "Epoch: 43, Batch: 700, loss: 4.0668 , Train PPL: 1.0062, Train Acc: 0.4207\n",
      "Validation --- Epoch: 43, total loss: 297.0421 , PPL: 1.2760, Acc: 0.4028\n",
      "lr = 0.25\n",
      "Epoch: 44, Batch: 100, loss: 3.8937 , Train PPL: 1.0060, Train Acc: 0.4573\n",
      "Epoch: 44, Batch: 200, loss: 4.1373 , Train PPL: 1.0063, Train Acc: 0.3918\n",
      "Epoch: 44, Batch: 300, loss: 4.1798 , Train PPL: 1.0064, Train Acc: 0.3811\n",
      "Epoch: 44, Batch: 400, loss: 3.8674 , Train PPL: 1.0059, Train Acc: 0.4588\n",
      "Epoch: 44, Batch: 500, loss: 3.9849 , Train PPL: 1.0061, Train Acc: 0.4527\n",
      "Epoch: 44, Batch: 600, loss: 4.1985 , Train PPL: 1.0064, Train Acc: 0.4405\n",
      "Epoch: 44, Batch: 700, loss: 4.0628 , Train PPL: 1.0062, Train Acc: 0.4512\n",
      "Validation --- Epoch: 44, total loss: 297.0612 , PPL: 1.2760, Acc: 0.4017\n",
      "lr = 0.25\n",
      "Epoch: 45, Batch: 100, loss: 4.0224 , Train PPL: 1.0062, Train Acc: 0.4405\n",
      "Epoch: 45, Batch: 200, loss: 4.2965 , Train PPL: 1.0066, Train Acc: 0.3872\n",
      "Epoch: 45, Batch: 300, loss: 4.1541 , Train PPL: 1.0064, Train Acc: 0.3979\n",
      "Epoch: 45, Batch: 400, loss: 4.4470 , Train PPL: 1.0068, Train Acc: 0.3476\n",
      "Epoch: 45, Batch: 500, loss: 4.0747 , Train PPL: 1.0062, Train Acc: 0.3720\n",
      "Epoch: 45, Batch: 600, loss: 4.0168 , Train PPL: 1.0061, Train Acc: 0.4345\n",
      "Epoch: 45, Batch: 700, loss: 4.0452 , Train PPL: 1.0062, Train Acc: 0.4238\n",
      "Validation --- Epoch: 45, total loss: 297.1202 , PPL: 1.2761, Acc: 0.4040\n",
      "lr = 0.25\n",
      "Epoch: 46, Batch: 100, loss: 3.9844 , Train PPL: 1.0061, Train Acc: 0.3994\n",
      "Epoch: 46, Batch: 200, loss: 3.7487 , Train PPL: 1.0057, Train Acc: 0.4787\n",
      "Epoch: 46, Batch: 300, loss: 4.2232 , Train PPL: 1.0065, Train Acc: 0.3841\n",
      "Epoch: 46, Batch: 400, loss: 4.3736 , Train PPL: 1.0067, Train Acc: 0.3613\n",
      "Epoch: 46, Batch: 500, loss: 4.0360 , Train PPL: 1.0062, Train Acc: 0.4131\n",
      "Epoch: 46, Batch: 600, loss: 3.7731 , Train PPL: 1.0058, Train Acc: 0.5000\n",
      "Epoch: 46, Batch: 700, loss: 4.1372 , Train PPL: 1.0063, Train Acc: 0.4314\n",
      "Validation --- Epoch: 46, total loss: 297.2099 , PPL: 1.2762, Acc: 0.4031\n",
      "lr = 0.25\n",
      "Epoch: 47, Batch: 100, loss: 4.1387 , Train PPL: 1.0063, Train Acc: 0.3872\n",
      "Epoch: 47, Batch: 200, loss: 4.3699 , Train PPL: 1.0067, Train Acc: 0.3552\n",
      "Epoch: 47, Batch: 300, loss: 4.1916 , Train PPL: 1.0064, Train Acc: 0.3979\n",
      "Epoch: 47, Batch: 400, loss: 3.8426 , Train PPL: 1.0059, Train Acc: 0.4299\n",
      "Epoch: 47, Batch: 500, loss: 3.7972 , Train PPL: 1.0058, Train Acc: 0.4558\n",
      "Epoch: 47, Batch: 600, loss: 4.4504 , Train PPL: 1.0068, Train Acc: 0.3308\n",
      "Epoch: 47, Batch: 700, loss: 4.2190 , Train PPL: 1.0065, Train Acc: 0.3841\n",
      "Validation --- Epoch: 47, total loss: 297.4913 , PPL: 1.2765, Acc: 0.4023\n",
      "lr = 0.25\n",
      "Epoch: 48, Batch: 100, loss: 4.0015 , Train PPL: 1.0061, Train Acc: 0.4726\n",
      "Epoch: 48, Batch: 200, loss: 4.1572 , Train PPL: 1.0064, Train Acc: 0.4177\n",
      "Epoch: 48, Batch: 300, loss: 4.1299 , Train PPL: 1.0063, Train Acc: 0.3933\n",
      "Epoch: 48, Batch: 400, loss: 4.2575 , Train PPL: 1.0065, Train Acc: 0.3628\n",
      "Epoch: 48, Batch: 500, loss: 3.7391 , Train PPL: 1.0057, Train Acc: 0.5320\n",
      "Epoch: 48, Batch: 600, loss: 4.2260 , Train PPL: 1.0065, Train Acc: 0.3735\n",
      "Epoch: 48, Batch: 700, loss: 4.0693 , Train PPL: 1.0062, Train Acc: 0.3948\n",
      "Validation --- Epoch: 48, total loss: 297.3065 , PPL: 1.2763, Acc: 0.4024\n",
      "lr = 0.125\n",
      "Epoch: 49, Batch: 100, loss: 4.1176 , Train PPL: 1.0063, Train Acc: 0.3735\n",
      "Epoch: 49, Batch: 200, loss: 4.1247 , Train PPL: 1.0063, Train Acc: 0.4207\n",
      "Epoch: 49, Batch: 300, loss: 4.1761 , Train PPL: 1.0064, Train Acc: 0.4131\n",
      "Epoch: 49, Batch: 400, loss: 4.0251 , Train PPL: 1.0062, Train Acc: 0.4009\n",
      "Epoch: 49, Batch: 500, loss: 4.2511 , Train PPL: 1.0065, Train Acc: 0.4329\n",
      "Epoch: 49, Batch: 600, loss: 3.9917 , Train PPL: 1.0061, Train Acc: 0.4756\n",
      "Epoch: 49, Batch: 700, loss: 4.0625 , Train PPL: 1.0062, Train Acc: 0.4070\n",
      "Validation --- Epoch: 49, total loss: 297.4084 , PPL: 1.2764, Acc: 0.4032\n",
      "lr = 0.125\n",
      "Epoch: 50, Batch: 100, loss: 4.0644 , Train PPL: 1.0062, Train Acc: 0.4101\n",
      "Epoch: 50, Batch: 200, loss: 3.7849 , Train PPL: 1.0058, Train Acc: 0.5061\n",
      "Epoch: 50, Batch: 300, loss: 4.0761 , Train PPL: 1.0062, Train Acc: 0.4299\n",
      "Epoch: 50, Batch: 400, loss: 4.1080 , Train PPL: 1.0063, Train Acc: 0.4451\n",
      "Epoch: 50, Batch: 500, loss: 4.0579 , Train PPL: 1.0062, Train Acc: 0.4253\n",
      "Epoch: 50, Batch: 600, loss: 4.1922 , Train PPL: 1.0064, Train Acc: 0.4207\n",
      "Epoch: 50, Batch: 700, loss: 3.7700 , Train PPL: 1.0058, Train Acc: 0.5091\n",
      "Validation --- Epoch: 50, total loss: 297.3298 , PPL: 1.2763, Acc: 0.4032\n",
      "lr = 0.125\n",
      "Epoch: 51, Batch: 100, loss: 4.0800 , Train PPL: 1.0062, Train Acc: 0.4741\n",
      "Epoch: 51, Batch: 200, loss: 4.0648 , Train PPL: 1.0062, Train Acc: 0.3902\n",
      "Epoch: 51, Batch: 300, loss: 3.9060 , Train PPL: 1.0060, Train Acc: 0.4497\n",
      "Epoch: 51, Batch: 400, loss: 4.1709 , Train PPL: 1.0064, Train Acc: 0.4024\n",
      "Epoch: 51, Batch: 500, loss: 4.0824 , Train PPL: 1.0062, Train Acc: 0.4466\n",
      "Epoch: 51, Batch: 600, loss: 4.1107 , Train PPL: 1.0063, Train Acc: 0.4421\n",
      "Epoch: 51, Batch: 700, loss: 3.9582 , Train PPL: 1.0061, Train Acc: 0.4436\n",
      "Validation --- Epoch: 51, total loss: 297.4761 , PPL: 1.2765, Acc: 0.4041\n",
      "lr = 0.125\n",
      "Epoch: 52, Batch: 100, loss: 4.0951 , Train PPL: 1.0063, Train Acc: 0.3994\n",
      "Epoch: 52, Batch: 200, loss: 4.2563 , Train PPL: 1.0065, Train Acc: 0.3918\n",
      "Epoch: 52, Batch: 300, loss: 3.9340 , Train PPL: 1.0060, Train Acc: 0.4604\n",
      "Epoch: 52, Batch: 400, loss: 4.0633 , Train PPL: 1.0062, Train Acc: 0.4085\n",
      "Epoch: 52, Batch: 500, loss: 3.7590 , Train PPL: 1.0057, Train Acc: 0.4649\n",
      "Epoch: 52, Batch: 600, loss: 4.0701 , Train PPL: 1.0062, Train Acc: 0.4451\n",
      "Epoch: 52, Batch: 700, loss: 4.3145 , Train PPL: 1.0066, Train Acc: 0.4223\n",
      "Validation --- Epoch: 52, total loss: 297.3954 , PPL: 1.2764, Acc: 0.4038\n",
      "lr = 0.125\n",
      "Epoch: 53, Batch: 100, loss: 4.0527 , Train PPL: 1.0062, Train Acc: 0.3933\n",
      "Epoch: 53, Batch: 200, loss: 4.2135 , Train PPL: 1.0064, Train Acc: 0.3659\n",
      "Epoch: 53, Batch: 300, loss: 4.1568 , Train PPL: 1.0064, Train Acc: 0.4451\n",
      "Epoch: 53, Batch: 400, loss: 4.0221 , Train PPL: 1.0062, Train Acc: 0.4497\n",
      "Epoch: 53, Batch: 500, loss: 4.2237 , Train PPL: 1.0065, Train Acc: 0.4223\n",
      "Epoch: 53, Batch: 600, loss: 3.9065 , Train PPL: 1.0060, Train Acc: 0.4558\n",
      "Epoch: 53, Batch: 700, loss: 3.8130 , Train PPL: 1.0058, Train Acc: 0.4314\n",
      "Validation --- Epoch: 53, total loss: 297.6308 , PPL: 1.2767, Acc: 0.4037\n",
      "lr = 0.125\n",
      "Epoch: 54, Batch: 100, loss: 4.0866 , Train PPL: 1.0062, Train Acc: 0.4207\n",
      "Epoch: 54, Batch: 200, loss: 4.1538 , Train PPL: 1.0064, Train Acc: 0.3613\n",
      "Epoch: 54, Batch: 300, loss: 4.1556 , Train PPL: 1.0064, Train Acc: 0.4101\n",
      "Epoch: 54, Batch: 400, loss: 3.9463 , Train PPL: 1.0060, Train Acc: 0.4299\n",
      "Epoch: 54, Batch: 500, loss: 3.6256 , Train PPL: 1.0055, Train Acc: 0.4909\n",
      "Epoch: 54, Batch: 600, loss: 4.0362 , Train PPL: 1.0062, Train Acc: 0.4329\n",
      "Epoch: 54, Batch: 700, loss: 4.0760 , Train PPL: 1.0062, Train Acc: 0.4101\n",
      "Validation --- Epoch: 54, total loss: 297.5434 , PPL: 1.2766, Acc: 0.4033\n",
      "lr = 0.0625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55, Batch: 100, loss: 3.8615 , Train PPL: 1.0059, Train Acc: 0.4451\n",
      "Epoch: 55, Batch: 200, loss: 4.0147 , Train PPL: 1.0061, Train Acc: 0.4238\n",
      "Epoch: 55, Batch: 300, loss: 4.1914 , Train PPL: 1.0064, Train Acc: 0.3841\n",
      "Epoch: 55, Batch: 400, loss: 4.0870 , Train PPL: 1.0062, Train Acc: 0.3963\n",
      "Epoch: 55, Batch: 500, loss: 3.9647 , Train PPL: 1.0061, Train Acc: 0.3933\n",
      "Epoch: 55, Batch: 600, loss: 4.0429 , Train PPL: 1.0062, Train Acc: 0.4558\n",
      "Epoch: 55, Batch: 700, loss: 4.0272 , Train PPL: 1.0062, Train Acc: 0.4055\n",
      "Validation --- Epoch: 55, total loss: 297.4913 , PPL: 1.2765, Acc: 0.4039\n",
      "lr = 0.0625\n",
      "Epoch: 56, Batch: 100, loss: 4.4091 , Train PPL: 1.0067, Train Acc: 0.3857\n",
      "Epoch: 56, Batch: 200, loss: 4.2524 , Train PPL: 1.0065, Train Acc: 0.3659\n",
      "Epoch: 56, Batch: 300, loss: 3.7303 , Train PPL: 1.0057, Train Acc: 0.4375\n",
      "Epoch: 56, Batch: 400, loss: 3.9027 , Train PPL: 1.0060, Train Acc: 0.4436\n",
      "Epoch: 56, Batch: 500, loss: 3.8428 , Train PPL: 1.0059, Train Acc: 0.4451\n",
      "Epoch: 56, Batch: 600, loss: 4.3168 , Train PPL: 1.0066, Train Acc: 0.3582\n",
      "Epoch: 56, Batch: 700, loss: 3.9619 , Train PPL: 1.0061, Train Acc: 0.4634\n",
      "Validation --- Epoch: 56, total loss: 297.5306 , PPL: 1.2766, Acc: 0.4046\n",
      "lr = 0.0625\n",
      "Epoch: 57, Batch: 100, loss: 4.2353 , Train PPL: 1.0065, Train Acc: 0.3674\n",
      "Epoch: 57, Batch: 200, loss: 4.0234 , Train PPL: 1.0062, Train Acc: 0.3765\n",
      "Epoch: 57, Batch: 300, loss: 4.2430 , Train PPL: 1.0065, Train Acc: 0.3963\n",
      "Epoch: 57, Batch: 400, loss: 4.1164 , Train PPL: 1.0063, Train Acc: 0.3963\n",
      "Epoch: 57, Batch: 500, loss: 4.0227 , Train PPL: 1.0062, Train Acc: 0.3933\n",
      "Epoch: 57, Batch: 600, loss: 4.0037 , Train PPL: 1.0061, Train Acc: 0.4436\n",
      "Epoch: 57, Batch: 700, loss: 4.1127 , Train PPL: 1.0063, Train Acc: 0.4024\n",
      "Validation --- Epoch: 57, total loss: 297.5924 , PPL: 1.2766, Acc: 0.4046\n",
      "lr = 0.0625\n",
      "Epoch: 58, Batch: 100, loss: 4.3377 , Train PPL: 1.0066, Train Acc: 0.3415\n",
      "Epoch: 58, Batch: 200, loss: 4.1319 , Train PPL: 1.0063, Train Acc: 0.4146\n",
      "Epoch: 58, Batch: 300, loss: 4.1132 , Train PPL: 1.0063, Train Acc: 0.4162\n",
      "Epoch: 58, Batch: 400, loss: 4.1243 , Train PPL: 1.0063, Train Acc: 0.4375\n",
      "Epoch: 58, Batch: 500, loss: 4.1808 , Train PPL: 1.0064, Train Acc: 0.4375\n",
      "Epoch: 58, Batch: 600, loss: 4.0300 , Train PPL: 1.0062, Train Acc: 0.4741\n",
      "Epoch: 58, Batch: 700, loss: 3.9707 , Train PPL: 1.0061, Train Acc: 0.4116\n",
      "Validation --- Epoch: 58, total loss: 297.6208 , PPL: 1.2767, Acc: 0.4043\n",
      "lr = 0.0625\n",
      "Epoch: 59, Batch: 100, loss: 3.9983 , Train PPL: 1.0061, Train Acc: 0.4009\n",
      "Epoch: 59, Batch: 200, loss: 4.0754 , Train PPL: 1.0062, Train Acc: 0.4390\n",
      "Epoch: 59, Batch: 300, loss: 4.3200 , Train PPL: 1.0066, Train Acc: 0.3720\n",
      "Epoch: 59, Batch: 400, loss: 4.0151 , Train PPL: 1.0061, Train Acc: 0.4512\n",
      "Epoch: 59, Batch: 500, loss: 4.3405 , Train PPL: 1.0066, Train Acc: 0.3750\n",
      "Epoch: 59, Batch: 600, loss: 4.2718 , Train PPL: 1.0065, Train Acc: 0.3567\n",
      "Epoch: 59, Batch: 700, loss: 3.9516 , Train PPL: 1.0060, Train Acc: 0.4405\n",
      "Validation --- Epoch: 59, total loss: 297.6571 , PPL: 1.2767, Acc: 0.4038\n",
      "lr = 0.0625\n",
      "Epoch: 60, Batch: 100, loss: 3.9621 , Train PPL: 1.0061, Train Acc: 0.4512\n",
      "Epoch: 60, Batch: 200, loss: 4.2107 , Train PPL: 1.0064, Train Acc: 0.3643\n",
      "Epoch: 60, Batch: 300, loss: 4.1731 , Train PPL: 1.0064, Train Acc: 0.4756\n",
      "Epoch: 60, Batch: 400, loss: 4.1057 , Train PPL: 1.0063, Train Acc: 0.4009\n",
      "Epoch: 60, Batch: 500, loss: 4.2371 , Train PPL: 1.0065, Train Acc: 0.3796\n",
      "Epoch: 60, Batch: 600, loss: 4.1811 , Train PPL: 1.0064, Train Acc: 0.4314\n",
      "Epoch: 60, Batch: 700, loss: 4.1771 , Train PPL: 1.0064, Train Acc: 0.4192\n",
      "Validation --- Epoch: 60, total loss: 297.6618 , PPL: 1.2767, Acc: 0.4046\n",
      "lr = 0.03125\n",
      "Epoch: 61, Batch: 100, loss: 3.9615 , Train PPL: 1.0061, Train Acc: 0.4390\n",
      "Epoch: 61, Batch: 200, loss: 4.2615 , Train PPL: 1.0065, Train Acc: 0.4085\n",
      "Epoch: 61, Batch: 300, loss: 3.7204 , Train PPL: 1.0057, Train Acc: 0.5503\n",
      "Epoch: 61, Batch: 400, loss: 4.0765 , Train PPL: 1.0062, Train Acc: 0.4177\n",
      "Epoch: 61, Batch: 500, loss: 4.1395 , Train PPL: 1.0063, Train Acc: 0.4055\n",
      "Epoch: 61, Batch: 600, loss: 4.0490 , Train PPL: 1.0062, Train Acc: 0.4024\n",
      "Epoch: 61, Batch: 700, loss: 4.3127 , Train PPL: 1.0066, Train Acc: 0.3872\n",
      "Validation --- Epoch: 61, total loss: 297.6419 , PPL: 1.2767, Acc: 0.4043\n",
      "lr = 0.03125\n",
      "Epoch: 62, Batch: 100, loss: 4.1110 , Train PPL: 1.0063, Train Acc: 0.3841\n",
      "Epoch: 62, Batch: 200, loss: 4.0500 , Train PPL: 1.0062, Train Acc: 0.4573\n",
      "Epoch: 62, Batch: 300, loss: 3.8756 , Train PPL: 1.0059, Train Acc: 0.4893\n",
      "Epoch: 62, Batch: 400, loss: 3.8065 , Train PPL: 1.0058, Train Acc: 0.4756\n",
      "Epoch: 62, Batch: 500, loss: 4.0141 , Train PPL: 1.0061, Train Acc: 0.4375\n",
      "Epoch: 62, Batch: 600, loss: 4.0199 , Train PPL: 1.0061, Train Acc: 0.4040\n",
      "Epoch: 62, Batch: 700, loss: 4.1772 , Train PPL: 1.0064, Train Acc: 0.3994\n",
      "Validation --- Epoch: 62, total loss: 297.6284 , PPL: 1.2767, Acc: 0.4037\n",
      "lr = 0.03125\n",
      "Epoch: 63, Batch: 100, loss: 4.2708 , Train PPL: 1.0065, Train Acc: 0.3857\n",
      "Epoch: 63, Batch: 200, loss: 3.9590 , Train PPL: 1.0061, Train Acc: 0.4558\n",
      "Epoch: 63, Batch: 300, loss: 4.0348 , Train PPL: 1.0062, Train Acc: 0.4085\n",
      "Epoch: 63, Batch: 400, loss: 4.2446 , Train PPL: 1.0065, Train Acc: 0.3841\n",
      "Epoch: 63, Batch: 500, loss: 4.0752 , Train PPL: 1.0062, Train Acc: 0.4253\n",
      "Epoch: 63, Batch: 600, loss: 4.0514 , Train PPL: 1.0062, Train Acc: 0.4482\n",
      "Epoch: 63, Batch: 700, loss: 3.9130 , Train PPL: 1.0060, Train Acc: 0.4497\n",
      "Validation --- Epoch: 63, total loss: 297.7334 , PPL: 1.2768, Acc: 0.4041\n",
      "lr = 0.03125\n",
      "Epoch: 64, Batch: 100, loss: 3.9965 , Train PPL: 1.0061, Train Acc: 0.4558\n",
      "Epoch: 64, Batch: 200, loss: 4.2227 , Train PPL: 1.0065, Train Acc: 0.3994\n",
      "Epoch: 64, Batch: 300, loss: 4.1074 , Train PPL: 1.0063, Train Acc: 0.3963\n",
      "Epoch: 64, Batch: 400, loss: 4.0639 , Train PPL: 1.0062, Train Acc: 0.3796\n",
      "Epoch: 64, Batch: 500, loss: 4.1293 , Train PPL: 1.0063, Train Acc: 0.3720\n",
      "Epoch: 64, Batch: 600, loss: 3.9947 , Train PPL: 1.0061, Train Acc: 0.4680\n",
      "Epoch: 64, Batch: 700, loss: 4.1468 , Train PPL: 1.0063, Train Acc: 0.3963\n",
      "Validation --- Epoch: 64, total loss: 297.7294 , PPL: 1.2768, Acc: 0.4040\n",
      "lr = 0.03125\n",
      "Epoch: 65, Batch: 100, loss: 4.0649 , Train PPL: 1.0062, Train Acc: 0.4253\n",
      "Epoch: 65, Batch: 200, loss: 4.0030 , Train PPL: 1.0061, Train Acc: 0.3948\n",
      "Epoch: 65, Batch: 300, loss: 4.0172 , Train PPL: 1.0061, Train Acc: 0.4329\n",
      "Epoch: 65, Batch: 400, loss: 4.2451 , Train PPL: 1.0065, Train Acc: 0.3948\n",
      "Epoch: 65, Batch: 500, loss: 4.1340 , Train PPL: 1.0063, Train Acc: 0.3689\n",
      "Epoch: 65, Batch: 600, loss: 3.9878 , Train PPL: 1.0061, Train Acc: 0.4573\n",
      "Epoch: 65, Batch: 700, loss: 4.2082 , Train PPL: 1.0064, Train Acc: 0.3918\n",
      "Validation --- Epoch: 65, total loss: 297.7043 , PPL: 1.2767, Acc: 0.4044\n",
      "lr = 0.03125\n",
      "Epoch: 66, Batch: 100, loss: 3.9473 , Train PPL: 1.0060, Train Acc: 0.4329\n",
      "Epoch: 66, Batch: 200, loss: 3.9927 , Train PPL: 1.0061, Train Acc: 0.4619\n",
      "Epoch: 66, Batch: 300, loss: 4.2209 , Train PPL: 1.0065, Train Acc: 0.3735\n",
      "Epoch: 66, Batch: 400, loss: 4.0230 , Train PPL: 1.0062, Train Acc: 0.4207\n",
      "Epoch: 66, Batch: 500, loss: 4.3541 , Train PPL: 1.0067, Train Acc: 0.3994\n",
      "Epoch: 66, Batch: 600, loss: 4.0042 , Train PPL: 1.0061, Train Acc: 0.4040\n",
      "Epoch: 66, Batch: 700, loss: 4.0421 , Train PPL: 1.0062, Train Acc: 0.4680\n",
      "Validation --- Epoch: 66, total loss: 297.7333 , PPL: 1.2768, Acc: 0.4045\n",
      "lr = 0.015625\n",
      "Epoch: 67, Batch: 100, loss: 4.2426 , Train PPL: 1.0065, Train Acc: 0.3979\n",
      "Epoch: 67, Batch: 200, loss: 4.1941 , Train PPL: 1.0064, Train Acc: 0.4009\n",
      "Epoch: 67, Batch: 300, loss: 3.8500 , Train PPL: 1.0059, Train Acc: 0.4146\n",
      "Epoch: 67, Batch: 400, loss: 4.0348 , Train PPL: 1.0062, Train Acc: 0.3796\n",
      "Epoch: 67, Batch: 500, loss: 3.5451 , Train PPL: 1.0054, Train Acc: 0.5305\n",
      "Epoch: 67, Batch: 600, loss: 3.7575 , Train PPL: 1.0057, Train Acc: 0.4253\n",
      "Epoch: 67, Batch: 700, loss: 4.0370 , Train PPL: 1.0062, Train Acc: 0.4405\n",
      "Validation --- Epoch: 67, total loss: 297.7047 , PPL: 1.2767, Acc: 0.4042\n",
      "lr = 0.015625\n",
      "Epoch: 68, Batch: 100, loss: 4.1295 , Train PPL: 1.0063, Train Acc: 0.3979\n",
      "Epoch: 68, Batch: 200, loss: 4.0694 , Train PPL: 1.0062, Train Acc: 0.4162\n",
      "Epoch: 68, Batch: 300, loss: 4.1059 , Train PPL: 1.0063, Train Acc: 0.4070\n",
      "Epoch: 68, Batch: 400, loss: 4.0166 , Train PPL: 1.0061, Train Acc: 0.4543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68, Batch: 500, loss: 4.1065 , Train PPL: 1.0063, Train Acc: 0.4466\n",
      "Epoch: 68, Batch: 600, loss: 4.1738 , Train PPL: 1.0064, Train Acc: 0.4345\n",
      "Epoch: 68, Batch: 700, loss: 4.2148 , Train PPL: 1.0064, Train Acc: 0.3902\n",
      "Validation --- Epoch: 68, total loss: 297.7128 , PPL: 1.2767, Acc: 0.4044\n",
      "lr = 0.015625\n",
      "Epoch: 69, Batch: 100, loss: 4.0469 , Train PPL: 1.0062, Train Acc: 0.4146\n",
      "Epoch: 69, Batch: 200, loss: 4.2358 , Train PPL: 1.0065, Train Acc: 0.3628\n",
      "Epoch: 69, Batch: 300, loss: 3.9074 , Train PPL: 1.0060, Train Acc: 0.4314\n",
      "Epoch: 69, Batch: 400, loss: 3.8072 , Train PPL: 1.0058, Train Acc: 0.4665\n",
      "Epoch: 69, Batch: 500, loss: 4.1199 , Train PPL: 1.0063, Train Acc: 0.4375\n",
      "Epoch: 69, Batch: 600, loss: 3.9615 , Train PPL: 1.0061, Train Acc: 0.4573\n",
      "Epoch: 69, Batch: 700, loss: 4.0833 , Train PPL: 1.0062, Train Acc: 0.4466\n",
      "Validation --- Epoch: 69, total loss: 297.7010 , PPL: 1.2767, Acc: 0.4043\n",
      "lr = 0.015625\n",
      "Epoch: 70, Batch: 100, loss: 3.9558 , Train PPL: 1.0060, Train Acc: 0.4314\n",
      "Epoch: 70, Batch: 200, loss: 3.8831 , Train PPL: 1.0059, Train Acc: 0.4695\n",
      "Epoch: 70, Batch: 300, loss: 3.9095 , Train PPL: 1.0060, Train Acc: 0.4543\n",
      "Epoch: 70, Batch: 400, loss: 4.0108 , Train PPL: 1.0061, Train Acc: 0.4299\n",
      "Epoch: 70, Batch: 500, loss: 3.9448 , Train PPL: 1.0060, Train Acc: 0.4497\n",
      "Epoch: 70, Batch: 600, loss: 3.7711 , Train PPL: 1.0058, Train Acc: 0.4954\n",
      "Epoch: 70, Batch: 700, loss: 4.0624 , Train PPL: 1.0062, Train Acc: 0.4390\n",
      "Validation --- Epoch: 70, total loss: 297.7387 , PPL: 1.2768, Acc: 0.4040\n",
      "lr = 0.015625\n",
      "Epoch: 71, Batch: 100, loss: 4.0270 , Train PPL: 1.0062, Train Acc: 0.3887\n",
      "Epoch: 71, Batch: 200, loss: 3.9768 , Train PPL: 1.0061, Train Acc: 0.4543\n",
      "Epoch: 71, Batch: 300, loss: 4.2197 , Train PPL: 1.0065, Train Acc: 0.4070\n",
      "Epoch: 71, Batch: 400, loss: 3.9417 , Train PPL: 1.0060, Train Acc: 0.4726\n",
      "Epoch: 71, Batch: 500, loss: 3.7159 , Train PPL: 1.0057, Train Acc: 0.5305\n",
      "Epoch: 71, Batch: 600, loss: 4.1015 , Train PPL: 1.0063, Train Acc: 0.4146\n",
      "Epoch: 71, Batch: 700, loss: 4.0544 , Train PPL: 1.0062, Train Acc: 0.4466\n",
      "Validation --- Epoch: 71, total loss: 297.7244 , PPL: 1.2768, Acc: 0.4045\n",
      "lr = 0.015625\n",
      "Epoch: 72, Batch: 100, loss: 4.2163 , Train PPL: 1.0064, Train Acc: 0.3857\n",
      "Epoch: 72, Batch: 200, loss: 4.0010 , Train PPL: 1.0061, Train Acc: 0.4131\n",
      "Epoch: 72, Batch: 300, loss: 4.2583 , Train PPL: 1.0065, Train Acc: 0.3780\n",
      "Epoch: 72, Batch: 400, loss: 4.1634 , Train PPL: 1.0064, Train Acc: 0.4040\n",
      "Epoch: 72, Batch: 500, loss: 3.6122 , Train PPL: 1.0055, Train Acc: 0.5335\n",
      "Epoch: 72, Batch: 600, loss: 3.8920 , Train PPL: 1.0060, Train Acc: 0.4726\n",
      "Epoch: 72, Batch: 700, loss: 4.0574 , Train PPL: 1.0062, Train Acc: 0.4436\n",
      "Validation --- Epoch: 72, total loss: 297.7466 , PPL: 1.2768, Acc: 0.4042\n",
      "lr = 0.0078125\n",
      "Epoch: 73, Batch: 100, loss: 4.2479 , Train PPL: 1.0065, Train Acc: 0.4146\n",
      "Epoch: 73, Batch: 200, loss: 4.2379 , Train PPL: 1.0065, Train Acc: 0.3902\n",
      "Epoch: 73, Batch: 300, loss: 4.1932 , Train PPL: 1.0064, Train Acc: 0.4146\n",
      "Epoch: 73, Batch: 400, loss: 4.2164 , Train PPL: 1.0064, Train Acc: 0.3704\n",
      "Epoch: 73, Batch: 500, loss: 4.0694 , Train PPL: 1.0062, Train Acc: 0.3811\n",
      "Epoch: 73, Batch: 600, loss: 4.2529 , Train PPL: 1.0065, Train Acc: 0.3552\n",
      "Epoch: 73, Batch: 700, loss: 4.0819 , Train PPL: 1.0062, Train Acc: 0.3598\n",
      "Validation --- Epoch: 73, total loss: 297.7389 , PPL: 1.2768, Acc: 0.4047\n",
      "lr = 0.0078125\n",
      "Epoch: 74, Batch: 100, loss: 3.9985 , Train PPL: 1.0061, Train Acc: 0.3872\n",
      "Epoch: 74, Batch: 200, loss: 3.9599 , Train PPL: 1.0061, Train Acc: 0.5076\n",
      "Epoch: 74, Batch: 300, loss: 4.1322 , Train PPL: 1.0063, Train Acc: 0.3918\n",
      "Epoch: 74, Batch: 400, loss: 3.5936 , Train PPL: 1.0055, Train Acc: 0.5030\n",
      "Epoch: 74, Batch: 500, loss: 4.0659 , Train PPL: 1.0062, Train Acc: 0.4177\n",
      "Epoch: 74, Batch: 600, loss: 3.8255 , Train PPL: 1.0058, Train Acc: 0.4817\n",
      "Epoch: 74, Batch: 700, loss: 4.0877 , Train PPL: 1.0063, Train Acc: 0.4284\n",
      "Validation --- Epoch: 74, total loss: 297.7415 , PPL: 1.2768, Acc: 0.4043\n",
      "lr = 0.0078125\n",
      "Epoch: 75, Batch: 100, loss: 4.1502 , Train PPL: 1.0063, Train Acc: 0.4131\n",
      "Epoch: 75, Batch: 200, loss: 4.2124 , Train PPL: 1.0064, Train Acc: 0.3918\n",
      "Epoch: 75, Batch: 300, loss: 3.9792 , Train PPL: 1.0061, Train Acc: 0.4329\n",
      "Epoch: 75, Batch: 400, loss: 3.9587 , Train PPL: 1.0061, Train Acc: 0.4512\n",
      "Epoch: 75, Batch: 500, loss: 4.2363 , Train PPL: 1.0065, Train Acc: 0.4268\n",
      "Epoch: 75, Batch: 600, loss: 3.9870 , Train PPL: 1.0061, Train Acc: 0.5091\n",
      "Epoch: 75, Batch: 700, loss: 4.0107 , Train PPL: 1.0061, Train Acc: 0.4314\n",
      "Validation --- Epoch: 75, total loss: 297.7418 , PPL: 1.2768, Acc: 0.4043\n",
      "lr = 0.0078125\n",
      "Epoch: 76, Batch: 100, loss: 4.0511 , Train PPL: 1.0062, Train Acc: 0.4009\n",
      "Epoch: 76, Batch: 200, loss: 4.1587 , Train PPL: 1.0064, Train Acc: 0.4101\n",
      "Epoch: 76, Batch: 300, loss: 3.9227 , Train PPL: 1.0060, Train Acc: 0.4375\n",
      "Epoch: 76, Batch: 400, loss: 4.2281 , Train PPL: 1.0065, Train Acc: 0.3918\n",
      "Epoch: 76, Batch: 500, loss: 4.1899 , Train PPL: 1.0064, Train Acc: 0.3857\n",
      "Epoch: 76, Batch: 600, loss: 4.0882 , Train PPL: 1.0063, Train Acc: 0.4253\n",
      "Epoch: 76, Batch: 700, loss: 4.1012 , Train PPL: 1.0063, Train Acc: 0.4131\n",
      "Validation --- Epoch: 76, total loss: 297.7373 , PPL: 1.2768, Acc: 0.4042\n",
      "lr = 0.0078125\n",
      "Epoch: 77, Batch: 100, loss: 4.1213 , Train PPL: 1.0063, Train Acc: 0.4040\n",
      "Epoch: 77, Batch: 200, loss: 3.7230 , Train PPL: 1.0057, Train Acc: 0.5107\n",
      "Epoch: 77, Batch: 300, loss: 3.9771 , Train PPL: 1.0061, Train Acc: 0.4527\n",
      "Epoch: 77, Batch: 400, loss: 4.0141 , Train PPL: 1.0061, Train Acc: 0.4040\n",
      "Epoch: 77, Batch: 500, loss: 3.9697 , Train PPL: 1.0061, Train Acc: 0.4329\n",
      "Epoch: 77, Batch: 600, loss: 4.1373 , Train PPL: 1.0063, Train Acc: 0.3933\n",
      "Epoch: 77, Batch: 700, loss: 4.1053 , Train PPL: 1.0063, Train Acc: 0.4223\n",
      "Validation --- Epoch: 77, total loss: 297.7497 , PPL: 1.2768, Acc: 0.4043\n",
      "lr = 0.0078125\n",
      "Epoch: 78, Batch: 100, loss: 4.0399 , Train PPL: 1.0062, Train Acc: 0.3765\n",
      "Epoch: 78, Batch: 200, loss: 4.2181 , Train PPL: 1.0065, Train Acc: 0.3872\n",
      "Epoch: 78, Batch: 300, loss: 4.1299 , Train PPL: 1.0063, Train Acc: 0.4131\n",
      "Epoch: 78, Batch: 400, loss: 3.9359 , Train PPL: 1.0060, Train Acc: 0.3979\n",
      "Epoch: 78, Batch: 500, loss: 4.1634 , Train PPL: 1.0064, Train Acc: 0.3598\n",
      "Epoch: 78, Batch: 600, loss: 4.0011 , Train PPL: 1.0061, Train Acc: 0.3841\n",
      "Epoch: 78, Batch: 700, loss: 3.9674 , Train PPL: 1.0061, Train Acc: 0.4512\n",
      "Validation --- Epoch: 78, total loss: 297.7600 , PPL: 1.2768, Acc: 0.4044\n",
      "lr = 0.00390625\n",
      "Epoch: 79, Batch: 100, loss: 4.1371 , Train PPL: 1.0063, Train Acc: 0.4055\n",
      "Epoch: 79, Batch: 200, loss: 4.2571 , Train PPL: 1.0065, Train Acc: 0.3857\n",
      "Epoch: 79, Batch: 300, loss: 4.2921 , Train PPL: 1.0066, Train Acc: 0.3826\n",
      "Epoch: 79, Batch: 400, loss: 4.1833 , Train PPL: 1.0064, Train Acc: 0.3720\n",
      "Epoch: 79, Batch: 500, loss: 4.1899 , Train PPL: 1.0064, Train Acc: 0.3841\n",
      "Epoch: 79, Batch: 600, loss: 3.8857 , Train PPL: 1.0059, Train Acc: 0.4832\n",
      "Epoch: 79, Batch: 700, loss: 4.2404 , Train PPL: 1.0065, Train Acc: 0.3979\n",
      "Validation --- Epoch: 79, total loss: 297.7643 , PPL: 1.2768, Acc: 0.4042\n",
      "lr = 0.00390625\n",
      "Epoch: 80, Batch: 100, loss: 4.3602 , Train PPL: 1.0067, Train Acc: 0.3704\n",
      "Epoch: 80, Batch: 200, loss: 3.7867 , Train PPL: 1.0058, Train Acc: 0.4848\n",
      "Epoch: 80, Batch: 300, loss: 4.0202 , Train PPL: 1.0061, Train Acc: 0.4360\n",
      "Epoch: 80, Batch: 400, loss: 4.2095 , Train PPL: 1.0064, Train Acc: 0.3506\n",
      "Epoch: 80, Batch: 500, loss: 4.3693 , Train PPL: 1.0067, Train Acc: 0.3872\n",
      "Epoch: 80, Batch: 600, loss: 4.3230 , Train PPL: 1.0066, Train Acc: 0.3796\n",
      "Epoch: 80, Batch: 700, loss: 4.1767 , Train PPL: 1.0064, Train Acc: 0.3933\n",
      "Validation --- Epoch: 80, total loss: 297.7553 , PPL: 1.2768, Acc: 0.4043\n",
      "lr = 0.00390625\n",
      "Epoch: 81, Batch: 100, loss: 3.9321 , Train PPL: 1.0060, Train Acc: 0.4253\n",
      "Epoch: 81, Batch: 200, loss: 3.9639 , Train PPL: 1.0061, Train Acc: 0.4512\n",
      "Epoch: 81, Batch: 300, loss: 4.1318 , Train PPL: 1.0063, Train Acc: 0.4345\n",
      "Epoch: 81, Batch: 400, loss: 4.4133 , Train PPL: 1.0068, Train Acc: 0.3826\n",
      "Epoch: 81, Batch: 500, loss: 4.2993 , Train PPL: 1.0066, Train Acc: 0.3918\n",
      "Epoch: 81, Batch: 600, loss: 3.8665 , Train PPL: 1.0059, Train Acc: 0.4573\n",
      "Epoch: 81, Batch: 700, loss: 4.0880 , Train PPL: 1.0063, Train Acc: 0.3918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --- Epoch: 81, total loss: 297.7574 , PPL: 1.2768, Acc: 0.4043\n",
      "lr = 0.00390625\n",
      "Epoch: 82, Batch: 100, loss: 4.3383 , Train PPL: 1.0066, Train Acc: 0.3780\n",
      "Epoch: 82, Batch: 200, loss: 4.0229 , Train PPL: 1.0062, Train Acc: 0.3979\n",
      "Epoch: 82, Batch: 300, loss: 3.6791 , Train PPL: 1.0056, Train Acc: 0.5107\n",
      "Epoch: 82, Batch: 400, loss: 4.0946 , Train PPL: 1.0063, Train Acc: 0.4085\n",
      "Epoch: 82, Batch: 500, loss: 4.1215 , Train PPL: 1.0063, Train Acc: 0.3780\n",
      "Epoch: 82, Batch: 600, loss: 4.1194 , Train PPL: 1.0063, Train Acc: 0.3552\n",
      "Epoch: 82, Batch: 700, loss: 3.9657 , Train PPL: 1.0061, Train Acc: 0.4375\n",
      "Validation --- Epoch: 82, total loss: 297.7550 , PPL: 1.2768, Acc: 0.4044\n",
      "lr = 0.00390625\n",
      "Epoch: 83, Batch: 100, loss: 4.3565 , Train PPL: 1.0067, Train Acc: 0.3857\n",
      "Epoch: 83, Batch: 200, loss: 4.1539 , Train PPL: 1.0064, Train Acc: 0.3979\n",
      "Epoch: 83, Batch: 300, loss: 3.5311 , Train PPL: 1.0054, Train Acc: 0.5274\n",
      "Epoch: 83, Batch: 400, loss: 4.2498 , Train PPL: 1.0065, Train Acc: 0.3963\n",
      "Epoch: 83, Batch: 500, loss: 3.9514 , Train PPL: 1.0060, Train Acc: 0.4085\n",
      "Epoch: 83, Batch: 600, loss: 4.3441 , Train PPL: 1.0066, Train Acc: 0.3720\n",
      "Epoch: 83, Batch: 700, loss: 4.1872 , Train PPL: 1.0064, Train Acc: 0.3796\n",
      "Validation --- Epoch: 83, total loss: 297.7599 , PPL: 1.2768, Acc: 0.4044\n",
      "lr = 0.00390625\n",
      "Epoch: 84, Batch: 100, loss: 4.1635 , Train PPL: 1.0064, Train Acc: 0.4101\n",
      "Epoch: 84, Batch: 200, loss: 3.8298 , Train PPL: 1.0059, Train Acc: 0.4787\n",
      "Epoch: 84, Batch: 300, loss: 4.2467 , Train PPL: 1.0065, Train Acc: 0.3750\n",
      "Epoch: 84, Batch: 400, loss: 3.8897 , Train PPL: 1.0059, Train Acc: 0.5107\n",
      "Epoch: 84, Batch: 500, loss: 4.0058 , Train PPL: 1.0061, Train Acc: 0.4436\n",
      "Epoch: 84, Batch: 600, loss: 4.1331 , Train PPL: 1.0063, Train Acc: 0.4207\n",
      "Epoch: 84, Batch: 700, loss: 3.7332 , Train PPL: 1.0057, Train Acc: 0.5152\n",
      "Validation --- Epoch: 84, total loss: 297.7581 , PPL: 1.2768, Acc: 0.4043\n",
      "lr = 0.001953125\n",
      "Epoch: 85, Batch: 100, loss: 4.1701 , Train PPL: 1.0064, Train Acc: 0.4436\n",
      "Epoch: 85, Batch: 200, loss: 3.8762 , Train PPL: 1.0059, Train Acc: 0.4436\n",
      "Epoch: 85, Batch: 300, loss: 3.9923 , Train PPL: 1.0061, Train Acc: 0.4451\n",
      "Epoch: 85, Batch: 400, loss: 3.9509 , Train PPL: 1.0060, Train Acc: 0.4421\n",
      "Epoch: 85, Batch: 500, loss: 4.1981 , Train PPL: 1.0064, Train Acc: 0.3765\n",
      "Epoch: 85, Batch: 600, loss: 4.2431 , Train PPL: 1.0065, Train Acc: 0.3963\n",
      "Epoch: 85, Batch: 700, loss: 3.6813 , Train PPL: 1.0056, Train Acc: 0.5015\n",
      "Validation --- Epoch: 85, total loss: 297.7581 , PPL: 1.2768, Acc: 0.4043\n",
      "lr = 0.001953125\n",
      "Epoch: 86, Batch: 100, loss: 4.1359 , Train PPL: 1.0063, Train Acc: 0.4162\n",
      "Epoch: 86, Batch: 200, loss: 4.0327 , Train PPL: 1.0062, Train Acc: 0.4116\n",
      "Epoch: 86, Batch: 300, loss: 3.9785 , Train PPL: 1.0061, Train Acc: 0.4162\n",
      "Epoch: 86, Batch: 400, loss: 4.1012 , Train PPL: 1.0063, Train Acc: 0.4207\n",
      "Epoch: 86, Batch: 500, loss: 4.1859 , Train PPL: 1.0064, Train Acc: 0.4436\n",
      "Epoch: 86, Batch: 600, loss: 4.0193 , Train PPL: 1.0061, Train Acc: 0.4360\n",
      "Epoch: 86, Batch: 700, loss: 4.1180 , Train PPL: 1.0063, Train Acc: 0.3918\n",
      "Validation --- Epoch: 86, total loss: 297.7548 , PPL: 1.2768, Acc: 0.4045\n",
      "lr = 0.001953125\n",
      "Epoch: 87, Batch: 100, loss: 4.1701 , Train PPL: 1.0064, Train Acc: 0.4527\n",
      "Epoch: 87, Batch: 200, loss: 3.8729 , Train PPL: 1.0059, Train Acc: 0.4253\n",
      "Epoch: 87, Batch: 300, loss: 3.8358 , Train PPL: 1.0059, Train Acc: 0.4649\n",
      "Epoch: 87, Batch: 400, loss: 4.3540 , Train PPL: 1.0067, Train Acc: 0.3247\n",
      "Epoch: 87, Batch: 500, loss: 3.9140 , Train PPL: 1.0060, Train Acc: 0.4314\n",
      "Epoch: 87, Batch: 600, loss: 4.2289 , Train PPL: 1.0065, Train Acc: 0.3887\n",
      "Epoch: 87, Batch: 700, loss: 4.1417 , Train PPL: 1.0063, Train Acc: 0.4131\n",
      "Validation --- Epoch: 87, total loss: 297.7590 , PPL: 1.2768, Acc: 0.4045\n",
      "lr = 0.001953125\n",
      "Epoch: 88, Batch: 100, loss: 3.8126 , Train PPL: 1.0058, Train Acc: 0.4649\n",
      "Epoch: 88, Batch: 200, loss: 3.8320 , Train PPL: 1.0059, Train Acc: 0.4040\n",
      "Epoch: 88, Batch: 300, loss: 3.7918 , Train PPL: 1.0058, Train Acc: 0.5152\n",
      "Epoch: 88, Batch: 400, loss: 3.4625 , Train PPL: 1.0053, Train Acc: 0.5000\n",
      "Epoch: 88, Batch: 500, loss: 4.1877 , Train PPL: 1.0064, Train Acc: 0.3857\n",
      "Epoch: 88, Batch: 600, loss: 4.0990 , Train PPL: 1.0063, Train Acc: 0.4207\n",
      "Epoch: 88, Batch: 700, loss: 4.4159 , Train PPL: 1.0068, Train Acc: 0.3491\n",
      "Validation --- Epoch: 88, total loss: 297.7553 , PPL: 1.2768, Acc: 0.4046\n",
      "lr = 0.001953125\n",
      "Epoch: 89, Batch: 100, loss: 4.2858 , Train PPL: 1.0066, Train Acc: 0.3826\n",
      "Epoch: 89, Batch: 200, loss: 4.0959 , Train PPL: 1.0063, Train Acc: 0.4390\n",
      "Epoch: 89, Batch: 300, loss: 3.8066 , Train PPL: 1.0058, Train Acc: 0.4436\n",
      "Epoch: 89, Batch: 400, loss: 4.0668 , Train PPL: 1.0062, Train Acc: 0.4253\n",
      "Epoch: 89, Batch: 500, loss: 4.0599 , Train PPL: 1.0062, Train Acc: 0.3811\n",
      "Epoch: 89, Batch: 600, loss: 3.6710 , Train PPL: 1.0056, Train Acc: 0.5107\n",
      "Epoch: 89, Batch: 700, loss: 3.9187 , Train PPL: 1.0060, Train Acc: 0.4665\n",
      "Validation --- Epoch: 89, total loss: 297.7549 , PPL: 1.2768, Acc: 0.4045\n",
      "lr = 0.001953125\n",
      "Epoch: 90, Batch: 100, loss: 3.9969 , Train PPL: 1.0061, Train Acc: 0.4390\n",
      "Epoch: 90, Batch: 200, loss: 4.1130 , Train PPL: 1.0063, Train Acc: 0.4497\n",
      "Epoch: 90, Batch: 300, loss: 4.3292 , Train PPL: 1.0066, Train Acc: 0.3963\n",
      "Epoch: 90, Batch: 400, loss: 4.0097 , Train PPL: 1.0061, Train Acc: 0.4497\n",
      "Epoch: 90, Batch: 500, loss: 4.2630 , Train PPL: 1.0065, Train Acc: 0.4009\n",
      "Epoch: 90, Batch: 600, loss: 3.9171 , Train PPL: 1.0060, Train Acc: 0.4680\n",
      "Epoch: 90, Batch: 700, loss: 3.6276 , Train PPL: 1.0055, Train Acc: 0.5488\n",
      "Validation --- Epoch: 90, total loss: 297.7562 , PPL: 1.2768, Acc: 0.4046\n",
      "lr = 0.0009765625\n",
      "Epoch: 91, Batch: 100, loss: 4.1898 , Train PPL: 1.0064, Train Acc: 0.4040\n",
      "Epoch: 91, Batch: 200, loss: 3.9882 , Train PPL: 1.0061, Train Acc: 0.4421\n",
      "Epoch: 91, Batch: 300, loss: 3.7548 , Train PPL: 1.0057, Train Acc: 0.4924\n",
      "Epoch: 91, Batch: 400, loss: 4.2384 , Train PPL: 1.0065, Train Acc: 0.3918\n",
      "Epoch: 91, Batch: 500, loss: 3.8898 , Train PPL: 1.0059, Train Acc: 0.4649\n",
      "Epoch: 91, Batch: 600, loss: 4.0800 , Train PPL: 1.0062, Train Acc: 0.4253\n",
      "Epoch: 91, Batch: 700, loss: 4.0646 , Train PPL: 1.0062, Train Acc: 0.3872\n",
      "Validation --- Epoch: 91, total loss: 297.7564 , PPL: 1.2768, Acc: 0.4046\n",
      "lr = 0.0009765625\n",
      "Epoch: 92, Batch: 100, loss: 3.9798 , Train PPL: 1.0061, Train Acc: 0.4512\n",
      "Epoch: 92, Batch: 200, loss: 3.9900 , Train PPL: 1.0061, Train Acc: 0.4405\n",
      "Epoch: 92, Batch: 300, loss: 3.9665 , Train PPL: 1.0061, Train Acc: 0.4421\n",
      "Epoch: 92, Batch: 400, loss: 4.3704 , Train PPL: 1.0067, Train Acc: 0.3750\n",
      "Epoch: 92, Batch: 500, loss: 3.8737 , Train PPL: 1.0059, Train Acc: 0.4558\n",
      "Epoch: 92, Batch: 600, loss: 4.0551 , Train PPL: 1.0062, Train Acc: 0.4512\n",
      "Epoch: 92, Batch: 700, loss: 4.2218 , Train PPL: 1.0065, Train Acc: 0.4131\n",
      "Validation --- Epoch: 92, total loss: 297.7596 , PPL: 1.2768, Acc: 0.4047\n",
      "lr = 0.0009765625\n",
      "Epoch: 93, Batch: 100, loss: 4.1652 , Train PPL: 1.0064, Train Acc: 0.4284\n",
      "Epoch: 93, Batch: 200, loss: 4.1974 , Train PPL: 1.0064, Train Acc: 0.3537\n",
      "Epoch: 93, Batch: 300, loss: 3.8972 , Train PPL: 1.0060, Train Acc: 0.4451\n",
      "Epoch: 93, Batch: 400, loss: 4.1011 , Train PPL: 1.0063, Train Acc: 0.3948\n",
      "Epoch: 93, Batch: 500, loss: 3.9474 , Train PPL: 1.0060, Train Acc: 0.4924\n",
      "Epoch: 93, Batch: 600, loss: 3.9902 , Train PPL: 1.0061, Train Acc: 0.4284\n",
      "Epoch: 93, Batch: 700, loss: 4.1375 , Train PPL: 1.0063, Train Acc: 0.4040\n",
      "Validation --- Epoch: 93, total loss: 297.7593 , PPL: 1.2768, Acc: 0.4047\n",
      "lr = 0.0009765625\n",
      "Epoch: 94, Batch: 100, loss: 3.8240 , Train PPL: 1.0058, Train Acc: 0.4527\n",
      "Epoch: 94, Batch: 200, loss: 4.2657 , Train PPL: 1.0065, Train Acc: 0.3780\n",
      "Epoch: 94, Batch: 300, loss: 4.2749 , Train PPL: 1.0065, Train Acc: 0.3232\n",
      "Epoch: 94, Batch: 400, loss: 3.8701 , Train PPL: 1.0059, Train Acc: 0.4710\n",
      "Epoch: 94, Batch: 500, loss: 4.0862 , Train PPL: 1.0062, Train Acc: 0.4329\n",
      "Epoch: 94, Batch: 600, loss: 4.2592 , Train PPL: 1.0065, Train Acc: 0.3399\n",
      "Epoch: 94, Batch: 700, loss: 3.8801 , Train PPL: 1.0059, Train Acc: 0.4558\n",
      "Validation --- Epoch: 94, total loss: 297.7630 , PPL: 1.2768, Acc: 0.4044\n",
      "lr = 0.0009765625\n",
      "Epoch: 95, Batch: 100, loss: 3.9664 , Train PPL: 1.0061, Train Acc: 0.4162\n",
      "Epoch: 95, Batch: 200, loss: 4.0724 , Train PPL: 1.0062, Train Acc: 0.4131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95, Batch: 300, loss: 4.1071 , Train PPL: 1.0063, Train Acc: 0.3491\n",
      "Epoch: 95, Batch: 400, loss: 4.0860 , Train PPL: 1.0062, Train Acc: 0.4192\n",
      "Epoch: 95, Batch: 500, loss: 4.0520 , Train PPL: 1.0062, Train Acc: 0.4421\n",
      "Epoch: 95, Batch: 600, loss: 4.2026 , Train PPL: 1.0064, Train Acc: 0.3811\n",
      "Epoch: 95, Batch: 700, loss: 4.2060 , Train PPL: 1.0064, Train Acc: 0.3704\n",
      "Validation --- Epoch: 95, total loss: 297.7622 , PPL: 1.2768, Acc: 0.4044\n",
      "lr = 0.0009765625\n",
      "Epoch: 96, Batch: 100, loss: 4.3149 , Train PPL: 1.0066, Train Acc: 0.3841\n",
      "Epoch: 96, Batch: 200, loss: 3.9771 , Train PPL: 1.0061, Train Acc: 0.4954\n",
      "Epoch: 96, Batch: 300, loss: 3.9621 , Train PPL: 1.0061, Train Acc: 0.4238\n",
      "Epoch: 96, Batch: 400, loss: 3.9832 , Train PPL: 1.0061, Train Acc: 0.4436\n",
      "Epoch: 96, Batch: 500, loss: 4.3112 , Train PPL: 1.0066, Train Acc: 0.3765\n",
      "Epoch: 96, Batch: 600, loss: 4.0550 , Train PPL: 1.0062, Train Acc: 0.4375\n",
      "Epoch: 96, Batch: 700, loss: 4.0768 , Train PPL: 1.0062, Train Acc: 0.4451\n",
      "Validation --- Epoch: 96, total loss: 297.7636 , PPL: 1.2768, Acc: 0.4046\n",
      "lr = 0.00048828125\n",
      "Epoch: 97, Batch: 100, loss: 4.0950 , Train PPL: 1.0063, Train Acc: 0.4314\n",
      "Epoch: 97, Batch: 200, loss: 4.0969 , Train PPL: 1.0063, Train Acc: 0.4329\n",
      "Epoch: 97, Batch: 300, loss: 4.0004 , Train PPL: 1.0061, Train Acc: 0.4345\n",
      "Epoch: 97, Batch: 400, loss: 3.9638 , Train PPL: 1.0061, Train Acc: 0.4314\n",
      "Epoch: 97, Batch: 500, loss: 4.1291 , Train PPL: 1.0063, Train Acc: 0.3948\n",
      "Epoch: 97, Batch: 600, loss: 4.0923 , Train PPL: 1.0063, Train Acc: 0.4238\n",
      "Epoch: 97, Batch: 700, loss: 3.8878 , Train PPL: 1.0059, Train Acc: 0.4756\n",
      "Validation --- Epoch: 97, total loss: 297.7626 , PPL: 1.2768, Acc: 0.4045\n",
      "lr = 0.00048828125\n",
      "Epoch: 98, Batch: 100, loss: 4.0884 , Train PPL: 1.0063, Train Acc: 0.3994\n",
      "Epoch: 98, Batch: 200, loss: 4.3155 , Train PPL: 1.0066, Train Acc: 0.3765\n",
      "Epoch: 98, Batch: 300, loss: 4.1934 , Train PPL: 1.0064, Train Acc: 0.3796\n",
      "Epoch: 98, Batch: 400, loss: 3.9041 , Train PPL: 1.0060, Train Acc: 0.5290\n",
      "Epoch: 98, Batch: 500, loss: 3.8518 , Train PPL: 1.0059, Train Acc: 0.4588\n",
      "Epoch: 98, Batch: 600, loss: 4.1499 , Train PPL: 1.0063, Train Acc: 0.3780\n",
      "Epoch: 98, Batch: 700, loss: 3.9301 , Train PPL: 1.0060, Train Acc: 0.4710\n",
      "Validation --- Epoch: 98, total loss: 297.7625 , PPL: 1.2768, Acc: 0.4044\n",
      "lr = 0.00048828125\n",
      "Epoch: 99, Batch: 100, loss: 3.8698 , Train PPL: 1.0059, Train Acc: 0.4634\n",
      "Epoch: 99, Batch: 200, loss: 3.8325 , Train PPL: 1.0059, Train Acc: 0.4817\n",
      "Epoch: 99, Batch: 300, loss: 3.8737 , Train PPL: 1.0059, Train Acc: 0.4497\n",
      "Epoch: 99, Batch: 400, loss: 4.1125 , Train PPL: 1.0063, Train Acc: 0.4101\n",
      "Epoch: 99, Batch: 500, loss: 4.2163 , Train PPL: 1.0064, Train Acc: 0.3643\n",
      "Epoch: 99, Batch: 600, loss: 4.2773 , Train PPL: 1.0065, Train Acc: 0.3765\n",
      "Epoch: 99, Batch: 700, loss: 3.8266 , Train PPL: 1.0059, Train Acc: 0.4405\n",
      "Validation --- Epoch: 99, total loss: 297.7635 , PPL: 1.2768, Acc: 0.4046\n",
      "lr = 0.00048828125\n"
     ]
    }
   ],
   "source": [
    "model = TCN(4, [600,600,600], kernel=3, dropout=0.45, embedding_size = 600, n_words = n_words,tied=True)\n",
    "model.cuda()\n",
    "model_parameters = filter(lambda x: x.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print('Network with {} parameters'.format(params))\n",
    "print('Receptive field of network is {}'.format(model.receptive_field))\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode=\"min\",patience=5,min_lr=1e-6,factor=0.5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "best_vloss = 1e8\n",
    "for e in range(100):\n",
    "    train_loop(e)\n",
    "    validation_loss = validation_loop(e)\n",
    "    scheduler.step(validation_loss)\n",
    "    print('lr = {}'.format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "    if validation_loss < best_vloss:\n",
    "                with open(\"model_4_layers_k3_600_filters_2.pt\", 'wb') as f:\n",
    "                    print('wrote model')\n",
    "                    torch.save(model, f)\n",
    "                best_vloss = validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network with 37945601 parameters\n",
      "Receptive field of network is 62\n",
      "Epoch: 0, Batch: 100, loss: 6.6188 , Train PPL: 1.0101, Train Acc: 0.1280\n",
      "Epoch: 0, Batch: 200, loss: 6.3271 , Train PPL: 1.0097, Train Acc: 0.1723\n",
      "Epoch: 0, Batch: 300, loss: 6.0799 , Train PPL: 1.0093, Train Acc: 0.2424\n",
      "Epoch: 0, Batch: 400, loss: 5.8677 , Train PPL: 1.0090, Train Acc: 0.2500\n",
      "Epoch: 0, Batch: 500, loss: 6.1308 , Train PPL: 1.0094, Train Acc: 0.2485\n",
      "Epoch: 0, Batch: 600, loss: 5.6975 , Train PPL: 1.0087, Train Acc: 0.2866\n",
      "Epoch: 0, Batch: 700, loss: 5.9261 , Train PPL: 1.0091, Train Acc: 0.2820\n",
      "Validation --- Epoch: 0, total loss: 328.5562 , PPL: 1.3073, Acc: 0.3034\n",
      "lr = 4\n",
      "wrote model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type TCN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type TC_block. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Chomp1d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 100, loss: 5.5057 , Train PPL: 1.0084, Train Acc: 0.2896\n",
      "Epoch: 1, Batch: 200, loss: 5.8249 , Train PPL: 1.0089, Train Acc: 0.2439\n",
      "Epoch: 1, Batch: 300, loss: 6.0073 , Train PPL: 1.0092, Train Acc: 0.2530\n",
      "Epoch: 1, Batch: 400, loss: 5.5218 , Train PPL: 1.0085, Train Acc: 0.2652\n",
      "Epoch: 1, Batch: 500, loss: 5.7056 , Train PPL: 1.0087, Train Acc: 0.3003\n",
      "Epoch: 1, Batch: 600, loss: 5.6646 , Train PPL: 1.0087, Train Acc: 0.2515\n",
      "Epoch: 1, Batch: 700, loss: 5.5139 , Train PPL: 1.0084, Train Acc: 0.2957\n",
      "Validation --- Epoch: 1, total loss: 317.9680 , PPL: 1.2961, Acc: 0.3198\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 2, Batch: 100, loss: 5.6628 , Train PPL: 1.0087, Train Acc: 0.2896\n",
      "Epoch: 2, Batch: 200, loss: 5.2338 , Train PPL: 1.0080, Train Acc: 0.3369\n",
      "Epoch: 2, Batch: 300, loss: 5.1300 , Train PPL: 1.0079, Train Acc: 0.3506\n",
      "Epoch: 2, Batch: 400, loss: 5.4361 , Train PPL: 1.0083, Train Acc: 0.2973\n",
      "Epoch: 2, Batch: 500, loss: 5.4712 , Train PPL: 1.0084, Train Acc: 0.3415\n",
      "Epoch: 2, Batch: 600, loss: 5.3393 , Train PPL: 1.0082, Train Acc: 0.3171\n",
      "Epoch: 2, Batch: 700, loss: 5.5667 , Train PPL: 1.0085, Train Acc: 0.3034\n",
      "Validation --- Epoch: 2, total loss: 310.4789 , PPL: 1.2885, Acc: 0.3451\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 3, Batch: 100, loss: 5.5287 , Train PPL: 1.0085, Train Acc: 0.3354\n",
      "Epoch: 3, Batch: 200, loss: 5.4460 , Train PPL: 1.0083, Train Acc: 0.2973\n",
      "Epoch: 3, Batch: 300, loss: 5.2452 , Train PPL: 1.0080, Train Acc: 0.3399\n",
      "Epoch: 3, Batch: 400, loss: 5.3185 , Train PPL: 1.0081, Train Acc: 0.3125\n",
      "Epoch: 3, Batch: 500, loss: 5.2502 , Train PPL: 1.0080, Train Acc: 0.3674\n",
      "Epoch: 3, Batch: 600, loss: 5.3921 , Train PPL: 1.0083, Train Acc: 0.3567\n",
      "Epoch: 3, Batch: 700, loss: 5.4192 , Train PPL: 1.0083, Train Acc: 0.3415\n",
      "Validation --- Epoch: 3, total loss: 306.8604 , PPL: 1.2847, Acc: 0.3490\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 4, Batch: 100, loss: 5.3539 , Train PPL: 1.0082, Train Acc: 0.3369\n",
      "Epoch: 4, Batch: 200, loss: 4.9553 , Train PPL: 1.0076, Train Acc: 0.4024\n",
      "Epoch: 4, Batch: 300, loss: 5.2288 , Train PPL: 1.0080, Train Acc: 0.3216\n",
      "Epoch: 4, Batch: 400, loss: 5.4069 , Train PPL: 1.0083, Train Acc: 0.3277\n",
      "Epoch: 4, Batch: 500, loss: 5.1551 , Train PPL: 1.0079, Train Acc: 0.3308\n",
      "Epoch: 4, Batch: 600, loss: 5.5381 , Train PPL: 1.0085, Train Acc: 0.2622\n",
      "Epoch: 4, Batch: 700, loss: 5.3973 , Train PPL: 1.0083, Train Acc: 0.3201\n",
      "Validation --- Epoch: 4, total loss: 304.0750 , PPL: 1.2819, Acc: 0.3602\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 5, Batch: 100, loss: 5.0839 , Train PPL: 1.0078, Train Acc: 0.3735\n",
      "Epoch: 5, Batch: 200, loss: 5.0958 , Train PPL: 1.0078, Train Acc: 0.3445\n",
      "Epoch: 5, Batch: 300, loss: 5.1200 , Train PPL: 1.0078, Train Acc: 0.3613\n",
      "Epoch: 5, Batch: 400, loss: 5.1500 , Train PPL: 1.0079, Train Acc: 0.3704\n",
      "Epoch: 5, Batch: 500, loss: 5.3700 , Train PPL: 1.0082, Train Acc: 0.3308\n",
      "Epoch: 5, Batch: 600, loss: 4.9629 , Train PPL: 1.0076, Train Acc: 0.4314\n",
      "Epoch: 5, Batch: 700, loss: 5.3128 , Train PPL: 1.0081, Train Acc: 0.3064\n",
      "Validation --- Epoch: 5, total loss: 302.2029 , PPL: 1.2801, Acc: 0.3493\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 6, Batch: 100, loss: 4.6919 , Train PPL: 1.0072, Train Acc: 0.3765\n",
      "Epoch: 6, Batch: 200, loss: 5.3262 , Train PPL: 1.0082, Train Acc: 0.3476\n",
      "Epoch: 6, Batch: 300, loss: 5.1693 , Train PPL: 1.0079, Train Acc: 0.3598\n",
      "Epoch: 6, Batch: 400, loss: 5.0139 , Train PPL: 1.0077, Train Acc: 0.3643\n",
      "Epoch: 6, Batch: 500, loss: 5.2328 , Train PPL: 1.0080, Train Acc: 0.3323\n",
      "Epoch: 6, Batch: 600, loss: 4.9484 , Train PPL: 1.0076, Train Acc: 0.3018\n",
      "Epoch: 6, Batch: 700, loss: 5.0104 , Train PPL: 1.0077, Train Acc: 0.3902\n",
      "Validation --- Epoch: 6, total loss: 301.5816 , PPL: 1.2795, Acc: 0.3556\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 7, Batch: 100, loss: 5.0675 , Train PPL: 1.0078, Train Acc: 0.3704\n",
      "Epoch: 7, Batch: 200, loss: 4.6988 , Train PPL: 1.0072, Train Acc: 0.3765\n",
      "Epoch: 7, Batch: 300, loss: 4.8273 , Train PPL: 1.0074, Train Acc: 0.3552\n",
      "Epoch: 7, Batch: 400, loss: 4.9693 , Train PPL: 1.0076, Train Acc: 0.3460\n",
      "Epoch: 7, Batch: 500, loss: 4.9269 , Train PPL: 1.0075, Train Acc: 0.3857\n",
      "Epoch: 7, Batch: 600, loss: 4.9867 , Train PPL: 1.0076, Train Acc: 0.3354\n",
      "Epoch: 7, Batch: 700, loss: 4.4336 , Train PPL: 1.0068, Train Acc: 0.4101\n",
      "Validation --- Epoch: 7, total loss: 299.1585 , PPL: 1.2771, Acc: 0.3600\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 8, Batch: 100, loss: 4.9195 , Train PPL: 1.0075, Train Acc: 0.3201\n",
      "Epoch: 8, Batch: 200, loss: 5.0738 , Train PPL: 1.0078, Train Acc: 0.3247\n",
      "Epoch: 8, Batch: 300, loss: 4.9218 , Train PPL: 1.0075, Train Acc: 0.3963\n",
      "Epoch: 8, Batch: 400, loss: 4.7048 , Train PPL: 1.0072, Train Acc: 0.3521\n",
      "Epoch: 8, Batch: 500, loss: 4.9472 , Train PPL: 1.0076, Train Acc: 0.3704\n",
      "Epoch: 8, Batch: 600, loss: 4.7512 , Train PPL: 1.0073, Train Acc: 0.4055\n",
      "Epoch: 8, Batch: 700, loss: 5.1129 , Train PPL: 1.0078, Train Acc: 0.3552\n",
      "Validation --- Epoch: 8, total loss: 298.2910 , PPL: 1.2764, Acc: 0.3720\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 9, Batch: 100, loss: 4.6057 , Train PPL: 1.0070, Train Acc: 0.3720\n",
      "Epoch: 9, Batch: 200, loss: 5.0328 , Train PPL: 1.0077, Train Acc: 0.3338\n",
      "Epoch: 9, Batch: 300, loss: 5.0394 , Train PPL: 1.0077, Train Acc: 0.3277\n",
      "Epoch: 9, Batch: 400, loss: 4.4531 , Train PPL: 1.0068, Train Acc: 0.4405\n",
      "Epoch: 9, Batch: 500, loss: 4.8281 , Train PPL: 1.0074, Train Acc: 0.3293\n",
      "Epoch: 9, Batch: 600, loss: 5.0885 , Train PPL: 1.0078, Train Acc: 0.3140\n",
      "Epoch: 9, Batch: 700, loss: 4.8144 , Train PPL: 1.0074, Train Acc: 0.3979\n",
      "Validation --- Epoch: 9, total loss: 296.7190 , PPL: 1.2749, Acc: 0.3762\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 10, Batch: 100, loss: 5.0647 , Train PPL: 1.0078, Train Acc: 0.2973\n",
      "Epoch: 10, Batch: 200, loss: 4.8735 , Train PPL: 1.0075, Train Acc: 0.3552\n",
      "Epoch: 10, Batch: 300, loss: 4.9144 , Train PPL: 1.0075, Train Acc: 0.3338\n",
      "Epoch: 10, Batch: 400, loss: 4.9296 , Train PPL: 1.0075, Train Acc: 0.3659\n",
      "Epoch: 10, Batch: 500, loss: 4.7128 , Train PPL: 1.0072, Train Acc: 0.3826\n",
      "Epoch: 10, Batch: 600, loss: 5.0339 , Train PPL: 1.0077, Train Acc: 0.3521\n",
      "Epoch: 10, Batch: 700, loss: 4.6865 , Train PPL: 1.0072, Train Acc: 0.4177\n",
      "Validation --- Epoch: 10, total loss: 296.3898 , PPL: 1.2746, Acc: 0.3742\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 11, Batch: 100, loss: 4.9536 , Train PPL: 1.0076, Train Acc: 0.3277\n",
      "Epoch: 11, Batch: 200, loss: 4.6386 , Train PPL: 1.0071, Train Acc: 0.3598\n",
      "Epoch: 11, Batch: 300, loss: 4.3875 , Train PPL: 1.0067, Train Acc: 0.3948\n",
      "Epoch: 11, Batch: 400, loss: 4.8900 , Train PPL: 1.0075, Train Acc: 0.3765\n",
      "Epoch: 11, Batch: 500, loss: 4.8268 , Train PPL: 1.0074, Train Acc: 0.3567\n",
      "Epoch: 11, Batch: 600, loss: 4.6010 , Train PPL: 1.0070, Train Acc: 0.3902\n",
      "Epoch: 11, Batch: 700, loss: 5.0090 , Train PPL: 1.0077, Train Acc: 0.3308\n",
      "Validation --- Epoch: 11, total loss: 295.6559 , PPL: 1.2740, Acc: 0.3737\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 12, Batch: 100, loss: 4.8717 , Train PPL: 1.0075, Train Acc: 0.3613\n",
      "Epoch: 12, Batch: 200, loss: 4.9222 , Train PPL: 1.0075, Train Acc: 0.3338\n",
      "Epoch: 12, Batch: 300, loss: 5.0347 , Train PPL: 1.0077, Train Acc: 0.3933\n",
      "Epoch: 12, Batch: 400, loss: 4.6328 , Train PPL: 1.0071, Train Acc: 0.3796\n",
      "Epoch: 12, Batch: 500, loss: 4.8591 , Train PPL: 1.0074, Train Acc: 0.3735\n",
      "Epoch: 12, Batch: 600, loss: 4.7997 , Train PPL: 1.0073, Train Acc: 0.3232\n",
      "Epoch: 12, Batch: 700, loss: 4.7733 , Train PPL: 1.0073, Train Acc: 0.3140\n",
      "Validation --- Epoch: 12, total loss: 296.1987 , PPL: 1.2748, Acc: 0.3715\n",
      "lr = 4\n",
      "Epoch: 13, Batch: 100, loss: 4.6819 , Train PPL: 1.0072, Train Acc: 0.3780\n",
      "Epoch: 13, Batch: 200, loss: 4.4631 , Train PPL: 1.0068, Train Acc: 0.3765\n",
      "Epoch: 13, Batch: 300, loss: 4.8005 , Train PPL: 1.0073, Train Acc: 0.3628\n",
      "Epoch: 13, Batch: 400, loss: 4.8652 , Train PPL: 1.0074, Train Acc: 0.3369\n",
      "Epoch: 13, Batch: 500, loss: 4.7574 , Train PPL: 1.0073, Train Acc: 0.3095\n",
      "Epoch: 13, Batch: 600, loss: 4.5270 , Train PPL: 1.0069, Train Acc: 0.3963\n",
      "Epoch: 13, Batch: 700, loss: 4.8691 , Train PPL: 1.0074, Train Acc: 0.3491\n",
      "Validation --- Epoch: 13, total loss: 295.2693 , PPL: 1.2737, Acc: 0.3772\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 14, Batch: 100, loss: 4.4241 , Train PPL: 1.0068, Train Acc: 0.3918\n",
      "Epoch: 14, Batch: 200, loss: 4.7223 , Train PPL: 1.0072, Train Acc: 0.3232\n",
      "Epoch: 14, Batch: 300, loss: 4.5375 , Train PPL: 1.0069, Train Acc: 0.3750\n",
      "Epoch: 14, Batch: 400, loss: 4.3413 , Train PPL: 1.0066, Train Acc: 0.4329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Batch: 500, loss: 4.3117 , Train PPL: 1.0066, Train Acc: 0.4451\n",
      "Epoch: 14, Batch: 600, loss: 4.6746 , Train PPL: 1.0072, Train Acc: 0.3399\n",
      "Epoch: 14, Batch: 700, loss: 4.7082 , Train PPL: 1.0072, Train Acc: 0.3582\n",
      "Validation --- Epoch: 14, total loss: 295.2891 , PPL: 1.2735, Acc: 0.3834\n",
      "lr = 4\n",
      "Epoch: 15, Batch: 100, loss: 3.8884 , Train PPL: 1.0059, Train Acc: 0.4726\n",
      "Epoch: 15, Batch: 200, loss: 4.1853 , Train PPL: 1.0064, Train Acc: 0.4436\n",
      "Epoch: 15, Batch: 300, loss: 4.6337 , Train PPL: 1.0071, Train Acc: 0.3643\n",
      "Epoch: 15, Batch: 400, loss: 4.5683 , Train PPL: 1.0070, Train Acc: 0.3674\n",
      "Epoch: 15, Batch: 500, loss: 4.8975 , Train PPL: 1.0075, Train Acc: 0.2973\n",
      "Epoch: 15, Batch: 600, loss: 4.9057 , Train PPL: 1.0075, Train Acc: 0.3064\n",
      "Epoch: 15, Batch: 700, loss: 4.7426 , Train PPL: 1.0073, Train Acc: 0.3780\n",
      "Validation --- Epoch: 15, total loss: 296.3316 , PPL: 1.2749, Acc: 0.3774\n",
      "lr = 4\n",
      "Epoch: 16, Batch: 100, loss: 4.2540 , Train PPL: 1.0065, Train Acc: 0.4009\n",
      "Epoch: 16, Batch: 200, loss: 4.5740 , Train PPL: 1.0070, Train Acc: 0.3567\n",
      "Epoch: 16, Batch: 300, loss: 4.5872 , Train PPL: 1.0070, Train Acc: 0.3552\n",
      "Epoch: 16, Batch: 400, loss: 4.7184 , Train PPL: 1.0072, Train Acc: 0.3155\n",
      "Epoch: 16, Batch: 500, loss: 4.5974 , Train PPL: 1.0070, Train Acc: 0.3430\n",
      "Epoch: 16, Batch: 600, loss: 4.7114 , Train PPL: 1.0072, Train Acc: 0.3140\n",
      "Epoch: 16, Batch: 700, loss: 4.2464 , Train PPL: 1.0065, Train Acc: 0.4360\n",
      "Validation --- Epoch: 16, total loss: 296.8619 , PPL: 1.2756, Acc: 0.3768\n",
      "lr = 4\n",
      "Epoch: 17, Batch: 100, loss: 4.0781 , Train PPL: 1.0062, Train Acc: 0.4588\n",
      "Epoch: 17, Batch: 200, loss: 4.3829 , Train PPL: 1.0067, Train Acc: 0.3796\n",
      "Epoch: 17, Batch: 300, loss: 4.5373 , Train PPL: 1.0069, Train Acc: 0.3780\n",
      "Epoch: 17, Batch: 400, loss: 4.7978 , Train PPL: 1.0073, Train Acc: 0.3384\n",
      "Epoch: 17, Batch: 500, loss: 4.3390 , Train PPL: 1.0066, Train Acc: 0.4192\n",
      "Epoch: 17, Batch: 600, loss: 4.4286 , Train PPL: 1.0068, Train Acc: 0.3902\n",
      "Epoch: 17, Batch: 700, loss: 4.5539 , Train PPL: 1.0070, Train Acc: 0.3567\n",
      "Validation --- Epoch: 17, total loss: 296.1046 , PPL: 1.2746, Acc: 0.3802\n",
      "lr = 4\n",
      "Epoch: 18, Batch: 100, loss: 4.6416 , Train PPL: 1.0071, Train Acc: 0.3277\n",
      "Epoch: 18, Batch: 200, loss: 4.5906 , Train PPL: 1.0070, Train Acc: 0.3750\n",
      "Epoch: 18, Batch: 300, loss: 4.4591 , Train PPL: 1.0068, Train Acc: 0.3689\n",
      "Epoch: 18, Batch: 400, loss: 4.6212 , Train PPL: 1.0071, Train Acc: 0.3095\n",
      "Epoch: 18, Batch: 500, loss: 4.4517 , Train PPL: 1.0068, Train Acc: 0.3887\n",
      "Epoch: 18, Batch: 600, loss: 4.2717 , Train PPL: 1.0065, Train Acc: 0.3979\n",
      "Epoch: 18, Batch: 700, loss: 4.5480 , Train PPL: 1.0070, Train Acc: 0.3598\n",
      "Validation --- Epoch: 18, total loss: 295.7488 , PPL: 1.2744, Acc: 0.3809\n",
      "lr = 4\n",
      "Epoch: 19, Batch: 100, loss: 4.3917 , Train PPL: 1.0067, Train Acc: 0.3857\n",
      "Epoch: 19, Batch: 200, loss: 4.2873 , Train PPL: 1.0066, Train Acc: 0.3857\n",
      "Epoch: 19, Batch: 300, loss: 3.9873 , Train PPL: 1.0061, Train Acc: 0.4954\n",
      "Epoch: 19, Batch: 400, loss: 4.2233 , Train PPL: 1.0065, Train Acc: 0.4375\n",
      "Epoch: 19, Batch: 500, loss: 4.6175 , Train PPL: 1.0071, Train Acc: 0.3308\n",
      "Epoch: 19, Batch: 600, loss: 4.2031 , Train PPL: 1.0064, Train Acc: 0.4085\n",
      "Epoch: 19, Batch: 700, loss: 3.9993 , Train PPL: 1.0061, Train Acc: 0.4787\n",
      "Validation --- Epoch: 19, total loss: 296.6858 , PPL: 1.2754, Acc: 0.3792\n",
      "lr = 2.0\n",
      "Epoch: 20, Batch: 100, loss: 4.3334 , Train PPL: 1.0066, Train Acc: 0.3994\n",
      "Epoch: 20, Batch: 200, loss: 4.2714 , Train PPL: 1.0065, Train Acc: 0.3948\n",
      "Epoch: 20, Batch: 300, loss: 4.3533 , Train PPL: 1.0067, Train Acc: 0.3613\n",
      "Epoch: 20, Batch: 400, loss: 3.8434 , Train PPL: 1.0059, Train Acc: 0.5030\n",
      "Epoch: 20, Batch: 500, loss: 4.1843 , Train PPL: 1.0064, Train Acc: 0.3780\n",
      "Epoch: 20, Batch: 600, loss: 3.6714 , Train PPL: 1.0056, Train Acc: 0.5640\n",
      "Epoch: 20, Batch: 700, loss: 3.9642 , Train PPL: 1.0061, Train Acc: 0.5030\n",
      "Validation --- Epoch: 20, total loss: 295.5731 , PPL: 1.2743, Acc: 0.3985\n",
      "lr = 2.0\n",
      "Epoch: 21, Batch: 100, loss: 4.0671 , Train PPL: 1.0062, Train Acc: 0.3659\n",
      "Epoch: 21, Batch: 200, loss: 4.1788 , Train PPL: 1.0064, Train Acc: 0.4421\n",
      "Epoch: 21, Batch: 300, loss: 3.8928 , Train PPL: 1.0060, Train Acc: 0.4299\n",
      "Epoch: 21, Batch: 400, loss: 4.1472 , Train PPL: 1.0063, Train Acc: 0.3643\n",
      "Epoch: 21, Batch: 500, loss: 4.1329 , Train PPL: 1.0063, Train Acc: 0.4085\n",
      "Epoch: 21, Batch: 600, loss: 4.3643 , Train PPL: 1.0067, Train Acc: 0.3780\n",
      "Epoch: 21, Batch: 700, loss: 4.2478 , Train PPL: 1.0065, Train Acc: 0.4192\n",
      "Validation --- Epoch: 21, total loss: 296.1239 , PPL: 1.2749, Acc: 0.4016\n",
      "lr = 2.0\n",
      "Epoch: 22, Batch: 100, loss: 4.0743 , Train PPL: 1.0062, Train Acc: 0.3963\n",
      "Epoch: 22, Batch: 200, loss: 4.3097 , Train PPL: 1.0066, Train Acc: 0.3735\n",
      "Epoch: 22, Batch: 300, loss: 3.7000 , Train PPL: 1.0057, Train Acc: 0.5076\n",
      "Epoch: 22, Batch: 400, loss: 4.2460 , Train PPL: 1.0065, Train Acc: 0.3216\n",
      "Epoch: 22, Batch: 500, loss: 4.2296 , Train PPL: 1.0065, Train Acc: 0.4131\n",
      "Epoch: 22, Batch: 600, loss: 4.1416 , Train PPL: 1.0063, Train Acc: 0.3948\n",
      "Epoch: 22, Batch: 700, loss: 4.0210 , Train PPL: 1.0061, Train Acc: 0.4238\n",
      "Validation --- Epoch: 22, total loss: 296.4953 , PPL: 1.2753, Acc: 0.3971\n",
      "lr = 2.0\n",
      "Epoch: 23, Batch: 100, loss: 3.9341 , Train PPL: 1.0060, Train Acc: 0.4116\n",
      "Epoch: 23, Batch: 200, loss: 4.0842 , Train PPL: 1.0062, Train Acc: 0.4146\n",
      "Epoch: 23, Batch: 300, loss: 3.7334 , Train PPL: 1.0057, Train Acc: 0.5061\n",
      "Epoch: 23, Batch: 400, loss: 4.1643 , Train PPL: 1.0064, Train Acc: 0.3567\n",
      "Epoch: 23, Batch: 500, loss: 3.4767 , Train PPL: 1.0053, Train Acc: 0.5015\n",
      "Epoch: 23, Batch: 600, loss: 4.2526 , Train PPL: 1.0065, Train Acc: 0.3796\n",
      "Epoch: 23, Batch: 700, loss: 3.7306 , Train PPL: 1.0057, Train Acc: 0.4466\n",
      "Validation --- Epoch: 23, total loss: 297.2357 , PPL: 1.2762, Acc: 0.3963\n",
      "lr = 2.0\n",
      "Epoch: 24, Batch: 100, loss: 4.1761 , Train PPL: 1.0064, Train Acc: 0.4131\n",
      "Epoch: 24, Batch: 200, loss: 4.1129 , Train PPL: 1.0063, Train Acc: 0.4040\n",
      "Epoch: 24, Batch: 300, loss: 4.0106 , Train PPL: 1.0061, Train Acc: 0.4573\n",
      "Epoch: 24, Batch: 400, loss: 4.3247 , Train PPL: 1.0066, Train Acc: 0.3476\n",
      "Epoch: 24, Batch: 500, loss: 3.8100 , Train PPL: 1.0058, Train Acc: 0.4466\n",
      "Epoch: 24, Batch: 600, loss: 4.2270 , Train PPL: 1.0065, Train Acc: 0.3613\n",
      "Epoch: 24, Batch: 700, loss: 4.0989 , Train PPL: 1.0063, Train Acc: 0.3872\n",
      "Validation --- Epoch: 24, total loss: 297.6918 , PPL: 1.2767, Acc: 0.3991\n",
      "lr = 2.0\n",
      "Epoch: 25, Batch: 100, loss: 3.5685 , Train PPL: 1.0055, Train Acc: 0.4924\n",
      "Epoch: 25, Batch: 200, loss: 3.8662 , Train PPL: 1.0059, Train Acc: 0.4177\n",
      "Epoch: 25, Batch: 300, loss: 4.0170 , Train PPL: 1.0061, Train Acc: 0.3887\n",
      "Epoch: 25, Batch: 400, loss: 3.9244 , Train PPL: 1.0060, Train Acc: 0.4299\n",
      "Epoch: 25, Batch: 500, loss: 4.4961 , Train PPL: 1.0069, Train Acc: 0.3582\n",
      "Epoch: 25, Batch: 600, loss: 3.9942 , Train PPL: 1.0061, Train Acc: 0.4482\n",
      "Epoch: 25, Batch: 700, loss: 3.9508 , Train PPL: 1.0060, Train Acc: 0.3796\n",
      "Validation --- Epoch: 25, total loss: 298.4733 , PPL: 1.2776, Acc: 0.4032\n",
      "lr = 1.0\n",
      "Epoch: 26, Batch: 100, loss: 3.6629 , Train PPL: 1.0056, Train Acc: 0.4527\n",
      "Epoch: 26, Batch: 200, loss: 4.1224 , Train PPL: 1.0063, Train Acc: 0.3994\n",
      "Epoch: 26, Batch: 300, loss: 3.7837 , Train PPL: 1.0058, Train Acc: 0.4329\n",
      "Epoch: 26, Batch: 400, loss: 3.9932 , Train PPL: 1.0061, Train Acc: 0.4116\n",
      "Epoch: 26, Batch: 500, loss: 4.0781 , Train PPL: 1.0062, Train Acc: 0.4040\n",
      "Epoch: 26, Batch: 600, loss: 4.0598 , Train PPL: 1.0062, Train Acc: 0.3994\n",
      "Epoch: 26, Batch: 700, loss: 3.8508 , Train PPL: 1.0059, Train Acc: 0.4588\n",
      "Validation --- Epoch: 26, total loss: 298.9599 , PPL: 1.2782, Acc: 0.4046\n",
      "lr = 1.0\n",
      "Epoch: 27, Batch: 100, loss: 4.1451 , Train PPL: 1.0063, Train Acc: 0.3826\n",
      "Epoch: 27, Batch: 200, loss: 4.3007 , Train PPL: 1.0066, Train Acc: 0.3354\n",
      "Epoch: 27, Batch: 300, loss: 3.8452 , Train PPL: 1.0059, Train Acc: 0.4451\n",
      "Epoch: 27, Batch: 400, loss: 3.9789 , Train PPL: 1.0061, Train Acc: 0.4604\n",
      "Epoch: 27, Batch: 500, loss: 3.6252 , Train PPL: 1.0055, Train Acc: 0.5137\n",
      "Epoch: 27, Batch: 600, loss: 3.7764 , Train PPL: 1.0058, Train Acc: 0.4527\n",
      "Epoch: 27, Batch: 700, loss: 3.9976 , Train PPL: 1.0061, Train Acc: 0.3811\n",
      "Validation --- Epoch: 27, total loss: 299.6066 , PPL: 1.2789, Acc: 0.4080\n",
      "lr = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Batch: 100, loss: 3.9145 , Train PPL: 1.0060, Train Acc: 0.4253\n",
      "Epoch: 28, Batch: 200, loss: 3.8448 , Train PPL: 1.0059, Train Acc: 0.4649\n",
      "Epoch: 28, Batch: 300, loss: 3.8934 , Train PPL: 1.0060, Train Acc: 0.4284\n",
      "Epoch: 28, Batch: 400, loss: 3.9923 , Train PPL: 1.0061, Train Acc: 0.3979\n",
      "Epoch: 28, Batch: 500, loss: 3.5158 , Train PPL: 1.0054, Train Acc: 0.5655\n",
      "Epoch: 28, Batch: 600, loss: 3.3992 , Train PPL: 1.0052, Train Acc: 0.5091\n",
      "Epoch: 28, Batch: 700, loss: 3.8541 , Train PPL: 1.0059, Train Acc: 0.4085\n",
      "Validation --- Epoch: 28, total loss: 299.7286 , PPL: 1.2789, Acc: 0.4059\n",
      "lr = 1.0\n",
      "Epoch: 29, Batch: 100, loss: 3.2703 , Train PPL: 1.0050, Train Acc: 0.5503\n",
      "Epoch: 29, Batch: 200, loss: 3.4933 , Train PPL: 1.0053, Train Acc: 0.4421\n",
      "Epoch: 29, Batch: 300, loss: 3.9117 , Train PPL: 1.0060, Train Acc: 0.4146\n",
      "Epoch: 29, Batch: 400, loss: 3.5925 , Train PPL: 1.0055, Train Acc: 0.5076\n",
      "Epoch: 29, Batch: 500, loss: 3.9342 , Train PPL: 1.0060, Train Acc: 0.3994\n",
      "Epoch: 29, Batch: 600, loss: 3.4271 , Train PPL: 1.0052, Train Acc: 0.5030\n",
      "Epoch: 29, Batch: 700, loss: 3.7487 , Train PPL: 1.0057, Train Acc: 0.4253\n",
      "Validation --- Epoch: 29, total loss: 300.4784 , PPL: 1.2798, Acc: 0.4049\n",
      "lr = 1.0\n",
      "Epoch: 30, Batch: 100, loss: 3.9000 , Train PPL: 1.0060, Train Acc: 0.4604\n",
      "Epoch: 30, Batch: 200, loss: 3.9106 , Train PPL: 1.0060, Train Acc: 0.3704\n",
      "Epoch: 30, Batch: 300, loss: 3.9949 , Train PPL: 1.0061, Train Acc: 0.4055\n",
      "Epoch: 30, Batch: 400, loss: 3.8082 , Train PPL: 1.0058, Train Acc: 0.4192\n",
      "Epoch: 30, Batch: 500, loss: 3.2390 , Train PPL: 1.0049, Train Acc: 0.5716\n",
      "Epoch: 30, Batch: 600, loss: 3.9506 , Train PPL: 1.0060, Train Acc: 0.3841\n",
      "Epoch: 30, Batch: 700, loss: 3.9434 , Train PPL: 1.0060, Train Acc: 0.3613\n",
      "Validation --- Epoch: 30, total loss: 300.5145 , PPL: 1.2798, Acc: 0.4061\n",
      "lr = 1.0\n",
      "Epoch: 31, Batch: 100, loss: 3.6581 , Train PPL: 1.0056, Train Acc: 0.4436\n",
      "Epoch: 31, Batch: 200, loss: 3.9328 , Train PPL: 1.0060, Train Acc: 0.4284\n",
      "Epoch: 31, Batch: 300, loss: 3.8592 , Train PPL: 1.0059, Train Acc: 0.4680\n",
      "Epoch: 31, Batch: 400, loss: 3.3778 , Train PPL: 1.0052, Train Acc: 0.5152\n",
      "Epoch: 31, Batch: 500, loss: 3.5520 , Train PPL: 1.0054, Train Acc: 0.5030\n",
      "Epoch: 31, Batch: 600, loss: 3.7208 , Train PPL: 1.0057, Train Acc: 0.4192\n",
      "Epoch: 31, Batch: 700, loss: 3.7086 , Train PPL: 1.0057, Train Acc: 0.4466\n",
      "Validation --- Epoch: 31, total loss: 301.0960 , PPL: 1.2805, Acc: 0.4054\n",
      "lr = 0.5\n",
      "Epoch: 32, Batch: 100, loss: 3.7284 , Train PPL: 1.0057, Train Acc: 0.4345\n",
      "Epoch: 32, Batch: 200, loss: 3.8954 , Train PPL: 1.0060, Train Acc: 0.4284\n",
      "Epoch: 32, Batch: 300, loss: 3.7518 , Train PPL: 1.0057, Train Acc: 0.4726\n",
      "Epoch: 32, Batch: 400, loss: 3.6889 , Train PPL: 1.0056, Train Acc: 0.4466\n",
      "Epoch: 32, Batch: 500, loss: 3.8142 , Train PPL: 1.0058, Train Acc: 0.4360\n",
      "Epoch: 32, Batch: 600, loss: 3.9495 , Train PPL: 1.0060, Train Acc: 0.3918\n",
      "Epoch: 32, Batch: 700, loss: 3.5108 , Train PPL: 1.0054, Train Acc: 0.4802\n",
      "Validation --- Epoch: 32, total loss: 301.3883 , PPL: 1.2808, Acc: 0.4071\n",
      "lr = 0.5\n",
      "Epoch: 33, Batch: 100, loss: 3.6415 , Train PPL: 1.0056, Train Acc: 0.4665\n",
      "Epoch: 33, Batch: 200, loss: 3.3874 , Train PPL: 1.0052, Train Acc: 0.5183\n",
      "Epoch: 33, Batch: 300, loss: 3.7262 , Train PPL: 1.0057, Train Acc: 0.4177\n",
      "Epoch: 33, Batch: 400, loss: 3.6240 , Train PPL: 1.0055, Train Acc: 0.4405\n",
      "Epoch: 33, Batch: 500, loss: 3.9091 , Train PPL: 1.0060, Train Acc: 0.4207\n",
      "Epoch: 33, Batch: 600, loss: 3.9788 , Train PPL: 1.0061, Train Acc: 0.4101\n",
      "Epoch: 33, Batch: 700, loss: 3.5190 , Train PPL: 1.0054, Train Acc: 0.4665\n",
      "Validation --- Epoch: 33, total loss: 301.6504 , PPL: 1.2811, Acc: 0.4086\n",
      "lr = 0.5\n",
      "Epoch: 34, Batch: 100, loss: 3.5850 , Train PPL: 1.0055, Train Acc: 0.4756\n",
      "Epoch: 34, Batch: 200, loss: 3.5189 , Train PPL: 1.0054, Train Acc: 0.4954\n",
      "Epoch: 34, Batch: 300, loss: 3.9236 , Train PPL: 1.0060, Train Acc: 0.4116\n",
      "Epoch: 34, Batch: 400, loss: 3.8235 , Train PPL: 1.0058, Train Acc: 0.4238\n",
      "Epoch: 34, Batch: 500, loss: 3.8923 , Train PPL: 1.0060, Train Acc: 0.4360\n",
      "Epoch: 34, Batch: 600, loss: 3.8712 , Train PPL: 1.0059, Train Acc: 0.4421\n",
      "Epoch: 34, Batch: 700, loss: 4.0213 , Train PPL: 1.0061, Train Acc: 0.3902\n",
      "Validation --- Epoch: 34, total loss: 302.0809 , PPL: 1.2816, Acc: 0.4081\n",
      "lr = 0.5\n",
      "Epoch: 35, Batch: 100, loss: 3.9733 , Train PPL: 1.0061, Train Acc: 0.4040\n",
      "Epoch: 35, Batch: 200, loss: 3.5914 , Train PPL: 1.0055, Train Acc: 0.5274\n",
      "Epoch: 35, Batch: 300, loss: 3.4616 , Train PPL: 1.0053, Train Acc: 0.4924\n",
      "Epoch: 35, Batch: 400, loss: 3.4375 , Train PPL: 1.0053, Train Acc: 0.5213\n",
      "Epoch: 35, Batch: 500, loss: 3.7731 , Train PPL: 1.0058, Train Acc: 0.3979\n",
      "Epoch: 35, Batch: 600, loss: 3.6451 , Train PPL: 1.0056, Train Acc: 0.4268\n",
      "Epoch: 35, Batch: 700, loss: 3.5147 , Train PPL: 1.0054, Train Acc: 0.4939\n",
      "Validation --- Epoch: 35, total loss: 302.2472 , PPL: 1.2818, Acc: 0.4093\n",
      "lr = 0.5\n",
      "Epoch: 36, Batch: 100, loss: 3.4132 , Train PPL: 1.0052, Train Acc: 0.5183\n",
      "Epoch: 36, Batch: 200, loss: 3.8023 , Train PPL: 1.0058, Train Acc: 0.4512\n",
      "Epoch: 36, Batch: 300, loss: 3.6298 , Train PPL: 1.0055, Train Acc: 0.4680\n",
      "Epoch: 36, Batch: 400, loss: 3.6398 , Train PPL: 1.0056, Train Acc: 0.4451\n",
      "Epoch: 36, Batch: 500, loss: 3.5611 , Train PPL: 1.0054, Train Acc: 0.4253\n",
      "Epoch: 36, Batch: 600, loss: 3.8261 , Train PPL: 1.0058, Train Acc: 0.4070\n",
      "Epoch: 36, Batch: 700, loss: 3.4708 , Train PPL: 1.0053, Train Acc: 0.4497\n",
      "Validation --- Epoch: 36, total loss: 302.6790 , PPL: 1.2822, Acc: 0.4082\n",
      "lr = 0.5\n",
      "Epoch: 37, Batch: 100, loss: 3.6787 , Train PPL: 1.0056, Train Acc: 0.4665\n",
      "Epoch: 37, Batch: 200, loss: 3.6100 , Train PPL: 1.0055, Train Acc: 0.5000\n",
      "Epoch: 37, Batch: 300, loss: 3.8387 , Train PPL: 1.0059, Train Acc: 0.4177\n",
      "Epoch: 37, Batch: 400, loss: 3.6676 , Train PPL: 1.0056, Train Acc: 0.4162\n",
      "Epoch: 37, Batch: 500, loss: 4.1144 , Train PPL: 1.0063, Train Acc: 0.3110\n",
      "Epoch: 37, Batch: 600, loss: 3.6805 , Train PPL: 1.0056, Train Acc: 0.4527\n",
      "Epoch: 37, Batch: 700, loss: 3.6927 , Train PPL: 1.0056, Train Acc: 0.4299\n",
      "Validation --- Epoch: 37, total loss: 302.7545 , PPL: 1.2823, Acc: 0.4080\n",
      "lr = 0.25\n",
      "Epoch: 38, Batch: 100, loss: 3.7014 , Train PPL: 1.0057, Train Acc: 0.4131\n",
      "Epoch: 38, Batch: 200, loss: 3.4158 , Train PPL: 1.0052, Train Acc: 0.4878\n",
      "Epoch: 38, Batch: 300, loss: 3.9630 , Train PPL: 1.0061, Train Acc: 0.3857\n",
      "Epoch: 38, Batch: 400, loss: 3.5410 , Train PPL: 1.0054, Train Acc: 0.4726\n",
      "Epoch: 38, Batch: 500, loss: 3.6943 , Train PPL: 1.0056, Train Acc: 0.4268\n",
      "Epoch: 38, Batch: 600, loss: 3.7091 , Train PPL: 1.0057, Train Acc: 0.4223\n",
      "Epoch: 38, Batch: 700, loss: 3.8394 , Train PPL: 1.0059, Train Acc: 0.4192\n",
      "Validation --- Epoch: 38, total loss: 302.9970 , PPL: 1.2826, Acc: 0.4075\n",
      "lr = 0.25\n",
      "Epoch: 39, Batch: 100, loss: 3.7392 , Train PPL: 1.0057, Train Acc: 0.3491\n",
      "Epoch: 39, Batch: 200, loss: 3.7721 , Train PPL: 1.0058, Train Acc: 0.4177\n",
      "Epoch: 39, Batch: 300, loss: 3.7002 , Train PPL: 1.0057, Train Acc: 0.4162\n",
      "Epoch: 39, Batch: 400, loss: 3.5828 , Train PPL: 1.0055, Train Acc: 0.4909\n",
      "Epoch: 39, Batch: 500, loss: 3.3619 , Train PPL: 1.0051, Train Acc: 0.5290\n",
      "Epoch: 39, Batch: 600, loss: 3.4907 , Train PPL: 1.0053, Train Acc: 0.4421\n",
      "Epoch: 39, Batch: 700, loss: 3.9393 , Train PPL: 1.0060, Train Acc: 0.3979\n",
      "Validation --- Epoch: 39, total loss: 303.0856 , PPL: 1.2827, Acc: 0.4089\n",
      "lr = 0.25\n",
      "Epoch: 40, Batch: 100, loss: 3.7215 , Train PPL: 1.0057, Train Acc: 0.4405\n",
      "Epoch: 40, Batch: 200, loss: 3.4396 , Train PPL: 1.0053, Train Acc: 0.5549\n",
      "Epoch: 40, Batch: 300, loss: 3.4082 , Train PPL: 1.0052, Train Acc: 0.5015\n",
      "Epoch: 40, Batch: 400, loss: 3.7395 , Train PPL: 1.0057, Train Acc: 0.4558\n",
      "Epoch: 40, Batch: 500, loss: 3.9976 , Train PPL: 1.0061, Train Acc: 0.3567\n",
      "Epoch: 40, Batch: 600, loss: 3.5582 , Train PPL: 1.0054, Train Acc: 0.4909\n",
      "Epoch: 40, Batch: 700, loss: 3.3439 , Train PPL: 1.0051, Train Acc: 0.5274\n",
      "Validation --- Epoch: 40, total loss: 303.4339 , PPL: 1.2831, Acc: 0.4083\n",
      "lr = 0.25\n",
      "Epoch: 41, Batch: 100, loss: 3.8670 , Train PPL: 1.0059, Train Acc: 0.3933\n",
      "Epoch: 41, Batch: 200, loss: 3.6920 , Train PPL: 1.0056, Train Acc: 0.4695\n",
      "Epoch: 41, Batch: 300, loss: 3.9272 , Train PPL: 1.0060, Train Acc: 0.3841\n",
      "Epoch: 41, Batch: 400, loss: 3.6800 , Train PPL: 1.0056, Train Acc: 0.4558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41, Batch: 500, loss: 3.5210 , Train PPL: 1.0054, Train Acc: 0.4482\n",
      "Epoch: 41, Batch: 600, loss: 3.6976 , Train PPL: 1.0057, Train Acc: 0.4345\n",
      "Epoch: 41, Batch: 700, loss: 3.8223 , Train PPL: 1.0058, Train Acc: 0.4131\n",
      "Validation --- Epoch: 41, total loss: 303.6558 , PPL: 1.2833, Acc: 0.4083\n",
      "lr = 0.25\n",
      "Epoch: 42, Batch: 100, loss: 3.4541 , Train PPL: 1.0053, Train Acc: 0.4741\n",
      "Epoch: 42, Batch: 200, loss: 3.6088 , Train PPL: 1.0055, Train Acc: 0.4466\n",
      "Epoch: 42, Batch: 300, loss: 3.6988 , Train PPL: 1.0057, Train Acc: 0.4299\n",
      "Epoch: 42, Batch: 400, loss: 3.7122 , Train PPL: 1.0057, Train Acc: 0.4040\n",
      "Epoch: 42, Batch: 500, loss: 3.6581 , Train PPL: 1.0056, Train Acc: 0.4619\n",
      "Epoch: 42, Batch: 600, loss: 3.7213 , Train PPL: 1.0057, Train Acc: 0.4954\n",
      "Epoch: 42, Batch: 700, loss: 3.8601 , Train PPL: 1.0059, Train Acc: 0.3948\n",
      "Validation --- Epoch: 42, total loss: 303.8425 , PPL: 1.2835, Acc: 0.4088\n",
      "lr = 0.25\n",
      "Epoch: 43, Batch: 100, loss: 3.7242 , Train PPL: 1.0057, Train Acc: 0.4421\n",
      "Epoch: 43, Batch: 200, loss: 3.6908 , Train PPL: 1.0056, Train Acc: 0.4284\n",
      "Epoch: 43, Batch: 300, loss: 3.4647 , Train PPL: 1.0053, Train Acc: 0.5442\n",
      "Epoch: 43, Batch: 400, loss: 3.3625 , Train PPL: 1.0051, Train Acc: 0.5579\n",
      "Epoch: 43, Batch: 500, loss: 3.3972 , Train PPL: 1.0052, Train Acc: 0.5198\n",
      "Epoch: 43, Batch: 600, loss: 3.7917 , Train PPL: 1.0058, Train Acc: 0.4024\n",
      "Epoch: 43, Batch: 700, loss: 3.6051 , Train PPL: 1.0055, Train Acc: 0.4665\n",
      "Validation --- Epoch: 43, total loss: 303.9118 , PPL: 1.2836, Acc: 0.4083\n",
      "lr = 0.125\n",
      "Epoch: 44, Batch: 100, loss: 3.7165 , Train PPL: 1.0057, Train Acc: 0.4863\n",
      "Epoch: 44, Batch: 200, loss: 3.4220 , Train PPL: 1.0052, Train Acc: 0.4680\n",
      "Epoch: 44, Batch: 300, loss: 3.6386 , Train PPL: 1.0056, Train Acc: 0.4848\n",
      "Epoch: 44, Batch: 400, loss: 3.4140 , Train PPL: 1.0052, Train Acc: 0.5107\n",
      "Epoch: 44, Batch: 500, loss: 4.1208 , Train PPL: 1.0063, Train Acc: 0.3430\n",
      "Epoch: 44, Batch: 600, loss: 3.9531 , Train PPL: 1.0060, Train Acc: 0.4238\n",
      "Epoch: 44, Batch: 700, loss: 3.9782 , Train PPL: 1.0061, Train Acc: 0.3750\n",
      "Validation --- Epoch: 44, total loss: 304.1032 , PPL: 1.2838, Acc: 0.4087\n",
      "lr = 0.125\n",
      "Epoch: 45, Batch: 100, loss: 3.5237 , Train PPL: 1.0054, Train Acc: 0.4893\n",
      "Epoch: 45, Batch: 200, loss: 3.8683 , Train PPL: 1.0059, Train Acc: 0.4345\n",
      "Epoch: 45, Batch: 300, loss: 3.3121 , Train PPL: 1.0051, Train Acc: 0.5137\n",
      "Epoch: 45, Batch: 400, loss: 3.3984 , Train PPL: 1.0052, Train Acc: 0.4680\n",
      "Epoch: 45, Batch: 500, loss: 3.7355 , Train PPL: 1.0057, Train Acc: 0.3872\n",
      "Epoch: 45, Batch: 600, loss: 3.4138 , Train PPL: 1.0052, Train Acc: 0.5351\n",
      "Epoch: 45, Batch: 700, loss: 3.7771 , Train PPL: 1.0058, Train Acc: 0.4238\n",
      "Validation --- Epoch: 45, total loss: 304.1393 , PPL: 1.2838, Acc: 0.4088\n",
      "lr = 0.125\n",
      "Epoch: 46, Batch: 100, loss: 3.5592 , Train PPL: 1.0054, Train Acc: 0.4527\n",
      "Epoch: 46, Batch: 200, loss: 3.5113 , Train PPL: 1.0054, Train Acc: 0.4680\n",
      "Epoch: 46, Batch: 300, loss: 3.5718 , Train PPL: 1.0055, Train Acc: 0.4710\n",
      "Epoch: 46, Batch: 400, loss: 3.5381 , Train PPL: 1.0054, Train Acc: 0.4497\n",
      "Epoch: 46, Batch: 500, loss: 3.3750 , Train PPL: 1.0052, Train Acc: 0.5244\n",
      "Epoch: 46, Batch: 600, loss: 3.9211 , Train PPL: 1.0060, Train Acc: 0.4253\n",
      "Epoch: 46, Batch: 700, loss: 3.2580 , Train PPL: 1.0050, Train Acc: 0.5671\n",
      "Validation --- Epoch: 46, total loss: 304.2143 , PPL: 1.2839, Acc: 0.4090\n",
      "lr = 0.125\n",
      "Epoch: 47, Batch: 100, loss: 3.6375 , Train PPL: 1.0056, Train Acc: 0.4314\n",
      "Epoch: 47, Batch: 200, loss: 3.5129 , Train PPL: 1.0054, Train Acc: 0.4985\n",
      "Epoch: 47, Batch: 300, loss: 3.5126 , Train PPL: 1.0054, Train Acc: 0.5107\n",
      "Epoch: 47, Batch: 400, loss: 3.6816 , Train PPL: 1.0056, Train Acc: 0.5061\n",
      "Epoch: 47, Batch: 500, loss: 3.4112 , Train PPL: 1.0052, Train Acc: 0.4451\n",
      "Epoch: 47, Batch: 600, loss: 3.7394 , Train PPL: 1.0057, Train Acc: 0.4314\n",
      "Epoch: 47, Batch: 700, loss: 3.5971 , Train PPL: 1.0055, Train Acc: 0.4527\n",
      "Validation --- Epoch: 47, total loss: 304.3865 , PPL: 1.2841, Acc: 0.4083\n",
      "lr = 0.125\n",
      "Epoch: 48, Batch: 100, loss: 3.4100 , Train PPL: 1.0052, Train Acc: 0.4573\n",
      "Epoch: 48, Batch: 200, loss: 3.8159 , Train PPL: 1.0058, Train Acc: 0.4314\n",
      "Epoch: 48, Batch: 300, loss: 3.8271 , Train PPL: 1.0059, Train Acc: 0.4024\n",
      "Epoch: 48, Batch: 400, loss: 3.8269 , Train PPL: 1.0059, Train Acc: 0.4253\n",
      "Epoch: 48, Batch: 500, loss: 3.8301 , Train PPL: 1.0059, Train Acc: 0.3857\n",
      "Epoch: 48, Batch: 600, loss: 3.2381 , Train PPL: 1.0049, Train Acc: 0.5335\n",
      "Epoch: 48, Batch: 700, loss: 3.9474 , Train PPL: 1.0060, Train Acc: 0.3994\n",
      "Validation --- Epoch: 48, total loss: 304.4127 , PPL: 1.2841, Acc: 0.4093\n",
      "lr = 0.125\n",
      "Epoch: 49, Batch: 100, loss: 3.7293 , Train PPL: 1.0057, Train Acc: 0.4573\n",
      "Epoch: 49, Batch: 200, loss: 3.6247 , Train PPL: 1.0055, Train Acc: 0.4588\n",
      "Epoch: 49, Batch: 300, loss: 3.7565 , Train PPL: 1.0057, Train Acc: 0.3963\n",
      "Epoch: 49, Batch: 400, loss: 3.5395 , Train PPL: 1.0054, Train Acc: 0.4390\n",
      "Epoch: 49, Batch: 500, loss: 3.7278 , Train PPL: 1.0057, Train Acc: 0.4253\n",
      "Epoch: 49, Batch: 600, loss: 3.5627 , Train PPL: 1.0054, Train Acc: 0.5534\n",
      "Epoch: 49, Batch: 700, loss: 3.6618 , Train PPL: 1.0056, Train Acc: 0.4207\n",
      "Validation --- Epoch: 49, total loss: 304.3417 , PPL: 1.2840, Acc: 0.4086\n",
      "lr = 0.0625\n",
      "Epoch: 50, Batch: 100, loss: 4.0529 , Train PPL: 1.0062, Train Acc: 0.4116\n",
      "Epoch: 50, Batch: 200, loss: 3.6158 , Train PPL: 1.0055, Train Acc: 0.4558\n",
      "Epoch: 50, Batch: 300, loss: 3.6947 , Train PPL: 1.0056, Train Acc: 0.4360\n",
      "Epoch: 50, Batch: 400, loss: 3.5509 , Train PPL: 1.0054, Train Acc: 0.4543\n",
      "Epoch: 50, Batch: 500, loss: 3.7220 , Train PPL: 1.0057, Train Acc: 0.4284\n",
      "Epoch: 50, Batch: 600, loss: 3.6207 , Train PPL: 1.0055, Train Acc: 0.4588\n",
      "Epoch: 50, Batch: 700, loss: 3.5992 , Train PPL: 1.0055, Train Acc: 0.4146\n",
      "Validation --- Epoch: 50, total loss: 304.4655 , PPL: 1.2842, Acc: 0.4097\n",
      "lr = 0.0625\n",
      "Epoch: 51, Batch: 100, loss: 3.4762 , Train PPL: 1.0053, Train Acc: 0.4954\n",
      "Epoch: 51, Batch: 200, loss: 3.8441 , Train PPL: 1.0059, Train Acc: 0.3933\n",
      "Epoch: 51, Batch: 300, loss: 3.6304 , Train PPL: 1.0055, Train Acc: 0.4649\n",
      "Epoch: 51, Batch: 400, loss: 3.6914 , Train PPL: 1.0056, Train Acc: 0.3994\n",
      "Epoch: 51, Batch: 500, loss: 3.3861 , Train PPL: 1.0052, Train Acc: 0.4985\n",
      "Epoch: 51, Batch: 600, loss: 3.5199 , Train PPL: 1.0054, Train Acc: 0.4543\n",
      "Epoch: 51, Batch: 700, loss: 3.2703 , Train PPL: 1.0050, Train Acc: 0.5137\n",
      "Validation --- Epoch: 51, total loss: 304.4091 , PPL: 1.2841, Acc: 0.4090\n",
      "lr = 0.0625\n",
      "Epoch: 52, Batch: 100, loss: 3.2846 , Train PPL: 1.0050, Train Acc: 0.5396\n",
      "Epoch: 52, Batch: 200, loss: 3.7204 , Train PPL: 1.0057, Train Acc: 0.3918\n",
      "Epoch: 52, Batch: 300, loss: 3.8165 , Train PPL: 1.0058, Train Acc: 0.3887\n",
      "Epoch: 52, Batch: 400, loss: 3.7143 , Train PPL: 1.0057, Train Acc: 0.4436\n",
      "Epoch: 52, Batch: 500, loss: 3.5751 , Train PPL: 1.0055, Train Acc: 0.4466\n",
      "Epoch: 52, Batch: 600, loss: 3.6130 , Train PPL: 1.0055, Train Acc: 0.4207\n",
      "Epoch: 52, Batch: 700, loss: 3.8241 , Train PPL: 1.0058, Train Acc: 0.4345\n",
      "Validation --- Epoch: 52, total loss: 304.5234 , PPL: 1.2843, Acc: 0.4091\n",
      "lr = 0.0625\n",
      "Epoch: 53, Batch: 100, loss: 3.7249 , Train PPL: 1.0057, Train Acc: 0.4238\n",
      "Epoch: 53, Batch: 200, loss: 3.6838 , Train PPL: 1.0056, Train Acc: 0.4131\n",
      "Epoch: 53, Batch: 300, loss: 3.7333 , Train PPL: 1.0057, Train Acc: 0.4085\n",
      "Epoch: 53, Batch: 400, loss: 3.6337 , Train PPL: 1.0056, Train Acc: 0.3948\n",
      "Epoch: 53, Batch: 500, loss: 3.5418 , Train PPL: 1.0054, Train Acc: 0.5351\n",
      "Epoch: 53, Batch: 600, loss: 3.8141 , Train PPL: 1.0058, Train Acc: 0.4131\n",
      "Epoch: 53, Batch: 700, loss: 3.7950 , Train PPL: 1.0058, Train Acc: 0.3994\n",
      "Validation --- Epoch: 53, total loss: 304.5917 , PPL: 1.2843, Acc: 0.4088\n",
      "lr = 0.0625\n",
      "Epoch: 54, Batch: 100, loss: 3.5715 , Train PPL: 1.0055, Train Acc: 0.4893\n",
      "Epoch: 54, Batch: 200, loss: 3.5754 , Train PPL: 1.0055, Train Acc: 0.4497\n",
      "Epoch: 54, Batch: 300, loss: 3.6037 , Train PPL: 1.0055, Train Acc: 0.4345\n",
      "Epoch: 54, Batch: 400, loss: 3.5257 , Train PPL: 1.0054, Train Acc: 0.5168\n",
      "Epoch: 54, Batch: 500, loss: 3.6509 , Train PPL: 1.0056, Train Acc: 0.4695\n",
      "Epoch: 54, Batch: 600, loss: 3.6037 , Train PPL: 1.0055, Train Acc: 0.4512\n",
      "Epoch: 54, Batch: 700, loss: 3.7999 , Train PPL: 1.0058, Train Acc: 0.4223\n",
      "Validation --- Epoch: 54, total loss: 304.5864 , PPL: 1.2843, Acc: 0.4086\n",
      "lr = 0.0625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55, Batch: 100, loss: 3.3802 , Train PPL: 1.0052, Train Acc: 0.5290\n",
      "Epoch: 55, Batch: 200, loss: 3.5500 , Train PPL: 1.0054, Train Acc: 0.4284\n",
      "Epoch: 55, Batch: 300, loss: 3.6853 , Train PPL: 1.0056, Train Acc: 0.4741\n",
      "Epoch: 55, Batch: 400, loss: 3.5997 , Train PPL: 1.0055, Train Acc: 0.4024\n",
      "Epoch: 55, Batch: 500, loss: 3.6116 , Train PPL: 1.0055, Train Acc: 0.4573\n",
      "Epoch: 55, Batch: 600, loss: 3.4481 , Train PPL: 1.0053, Train Acc: 0.5152\n",
      "Epoch: 55, Batch: 700, loss: 3.6963 , Train PPL: 1.0057, Train Acc: 0.4512\n",
      "Validation --- Epoch: 55, total loss: 304.6096 , PPL: 1.2844, Acc: 0.4094\n",
      "lr = 0.03125\n",
      "Epoch: 56, Batch: 100, loss: 3.4541 , Train PPL: 1.0053, Train Acc: 0.5335\n",
      "Epoch: 56, Batch: 200, loss: 3.5377 , Train PPL: 1.0054, Train Acc: 0.5152\n",
      "Epoch: 56, Batch: 300, loss: 3.6184 , Train PPL: 1.0055, Train Acc: 0.4314\n",
      "Epoch: 56, Batch: 400, loss: 3.4039 , Train PPL: 1.0052, Train Acc: 0.5335\n",
      "Epoch: 56, Batch: 500, loss: 3.8714 , Train PPL: 1.0059, Train Acc: 0.3979\n",
      "Epoch: 56, Batch: 600, loss: 3.4049 , Train PPL: 1.0052, Train Acc: 0.4924\n",
      "Epoch: 56, Batch: 700, loss: 3.3872 , Train PPL: 1.0052, Train Acc: 0.5030\n",
      "Validation --- Epoch: 56, total loss: 304.6755 , PPL: 1.2844, Acc: 0.4093\n",
      "lr = 0.03125\n",
      "Epoch: 57, Batch: 100, loss: 3.5157 , Train PPL: 1.0054, Train Acc: 0.4954\n",
      "Epoch: 57, Batch: 200, loss: 3.8021 , Train PPL: 1.0058, Train Acc: 0.3887\n",
      "Epoch: 57, Batch: 300, loss: 3.7323 , Train PPL: 1.0057, Train Acc: 0.4192\n",
      "Epoch: 57, Batch: 400, loss: 3.8784 , Train PPL: 1.0059, Train Acc: 0.4070\n",
      "Epoch: 57, Batch: 500, loss: 3.6810 , Train PPL: 1.0056, Train Acc: 0.4268\n",
      "Epoch: 57, Batch: 600, loss: 3.3018 , Train PPL: 1.0050, Train Acc: 0.5061\n",
      "Epoch: 57, Batch: 700, loss: 3.6205 , Train PPL: 1.0055, Train Acc: 0.4329\n",
      "Validation --- Epoch: 57, total loss: 304.7018 , PPL: 1.2845, Acc: 0.4093\n",
      "lr = 0.03125\n",
      "Epoch: 58, Batch: 100, loss: 3.4554 , Train PPL: 1.0053, Train Acc: 0.4482\n",
      "Epoch: 58, Batch: 200, loss: 3.4621 , Train PPL: 1.0053, Train Acc: 0.4954\n",
      "Epoch: 58, Batch: 300, loss: 3.6408 , Train PPL: 1.0056, Train Acc: 0.4345\n",
      "Epoch: 58, Batch: 400, loss: 4.0976 , Train PPL: 1.0063, Train Acc: 0.3948\n",
      "Epoch: 58, Batch: 500, loss: 3.6146 , Train PPL: 1.0055, Train Acc: 0.4497\n",
      "Epoch: 58, Batch: 600, loss: 3.7565 , Train PPL: 1.0057, Train Acc: 0.4223\n",
      "Epoch: 58, Batch: 700, loss: 3.6422 , Train PPL: 1.0056, Train Acc: 0.4207\n",
      "Validation --- Epoch: 58, total loss: 304.7266 , PPL: 1.2845, Acc: 0.4088\n",
      "lr = 0.03125\n",
      "Epoch: 59, Batch: 100, loss: 3.5020 , Train PPL: 1.0054, Train Acc: 0.5061\n",
      "Epoch: 59, Batch: 200, loss: 3.6918 , Train PPL: 1.0056, Train Acc: 0.5015\n",
      "Epoch: 59, Batch: 300, loss: 3.7112 , Train PPL: 1.0057, Train Acc: 0.4284\n",
      "Epoch: 59, Batch: 400, loss: 3.6534 , Train PPL: 1.0056, Train Acc: 0.4299\n",
      "Epoch: 59, Batch: 500, loss: 3.5412 , Train PPL: 1.0054, Train Acc: 0.4787\n",
      "Epoch: 59, Batch: 600, loss: 3.8229 , Train PPL: 1.0058, Train Acc: 0.3826\n",
      "Epoch: 59, Batch: 700, loss: 3.7447 , Train PPL: 1.0057, Train Acc: 0.3872\n",
      "Validation --- Epoch: 59, total loss: 304.7254 , PPL: 1.2845, Acc: 0.4086\n",
      "lr = 0.03125\n",
      "Epoch: 60, Batch: 100, loss: 3.5529 , Train PPL: 1.0054, Train Acc: 0.4848\n",
      "Epoch: 60, Batch: 200, loss: 3.7876 , Train PPL: 1.0058, Train Acc: 0.3659\n",
      "Epoch: 60, Batch: 300, loss: 3.9038 , Train PPL: 1.0060, Train Acc: 0.3750\n",
      "Epoch: 60, Batch: 400, loss: 3.5414 , Train PPL: 1.0054, Train Acc: 0.4939\n",
      "Epoch: 60, Batch: 500, loss: 3.8429 , Train PPL: 1.0059, Train Acc: 0.3994\n",
      "Epoch: 60, Batch: 600, loss: 3.7601 , Train PPL: 1.0057, Train Acc: 0.4421\n",
      "Epoch: 60, Batch: 700, loss: 3.6607 , Train PPL: 1.0056, Train Acc: 0.4878\n",
      "Validation --- Epoch: 60, total loss: 304.7998 , PPL: 1.2846, Acc: 0.4090\n",
      "lr = 0.03125\n",
      "Epoch: 61, Batch: 100, loss: 3.5289 , Train PPL: 1.0054, Train Acc: 0.4939\n",
      "Epoch: 61, Batch: 200, loss: 3.7085 , Train PPL: 1.0057, Train Acc: 0.4223\n",
      "Epoch: 61, Batch: 300, loss: 3.3332 , Train PPL: 1.0051, Train Acc: 0.5168\n",
      "Epoch: 61, Batch: 400, loss: 3.5533 , Train PPL: 1.0054, Train Acc: 0.4604\n",
      "Epoch: 61, Batch: 500, loss: 3.5248 , Train PPL: 1.0054, Train Acc: 0.4863\n",
      "Epoch: 61, Batch: 600, loss: 3.6475 , Train PPL: 1.0056, Train Acc: 0.4680\n",
      "Epoch: 61, Batch: 700, loss: 3.9058 , Train PPL: 1.0060, Train Acc: 0.4101\n",
      "Validation --- Epoch: 61, total loss: 304.7698 , PPL: 1.2845, Acc: 0.4090\n",
      "lr = 0.015625\n",
      "Epoch: 62, Batch: 100, loss: 3.6326 , Train PPL: 1.0056, Train Acc: 0.4512\n",
      "Epoch: 62, Batch: 200, loss: 3.5445 , Train PPL: 1.0054, Train Acc: 0.4649\n",
      "Epoch: 62, Batch: 300, loss: 3.6824 , Train PPL: 1.0056, Train Acc: 0.4223\n",
      "Epoch: 62, Batch: 400, loss: 3.4054 , Train PPL: 1.0052, Train Acc: 0.4710\n",
      "Epoch: 62, Batch: 500, loss: 3.7152 , Train PPL: 1.0057, Train Acc: 0.4131\n",
      "Epoch: 62, Batch: 600, loss: 3.6785 , Train PPL: 1.0056, Train Acc: 0.4649\n",
      "Epoch: 62, Batch: 700, loss: 3.5574 , Train PPL: 1.0054, Train Acc: 0.4238\n",
      "Validation --- Epoch: 62, total loss: 304.8073 , PPL: 1.2846, Acc: 0.4089\n",
      "lr = 0.015625\n",
      "Epoch: 63, Batch: 100, loss: 3.7204 , Train PPL: 1.0057, Train Acc: 0.4024\n",
      "Epoch: 63, Batch: 200, loss: 3.4910 , Train PPL: 1.0053, Train Acc: 0.4543\n",
      "Epoch: 63, Batch: 300, loss: 3.7977 , Train PPL: 1.0058, Train Acc: 0.4512\n",
      "Epoch: 63, Batch: 400, loss: 3.5818 , Train PPL: 1.0055, Train Acc: 0.4512\n",
      "Epoch: 63, Batch: 500, loss: 3.4684 , Train PPL: 1.0053, Train Acc: 0.4939\n",
      "Epoch: 63, Batch: 600, loss: 3.5707 , Train PPL: 1.0055, Train Acc: 0.4680\n",
      "Epoch: 63, Batch: 700, loss: 3.2928 , Train PPL: 1.0050, Train Acc: 0.5290\n",
      "Validation --- Epoch: 63, total loss: 304.8220 , PPL: 1.2846, Acc: 0.4092\n",
      "lr = 0.015625\n",
      "Epoch: 64, Batch: 100, loss: 3.4294 , Train PPL: 1.0052, Train Acc: 0.4878\n",
      "Epoch: 64, Batch: 200, loss: 3.6723 , Train PPL: 1.0056, Train Acc: 0.4375\n",
      "Epoch: 64, Batch: 300, loss: 3.4140 , Train PPL: 1.0052, Train Acc: 0.5152\n",
      "Epoch: 64, Batch: 400, loss: 3.6400 , Train PPL: 1.0056, Train Acc: 0.4771\n",
      "Epoch: 64, Batch: 500, loss: 3.6614 , Train PPL: 1.0056, Train Acc: 0.4421\n",
      "Epoch: 64, Batch: 600, loss: 3.3773 , Train PPL: 1.0052, Train Acc: 0.4924\n",
      "Epoch: 64, Batch: 700, loss: 3.4500 , Train PPL: 1.0053, Train Acc: 0.4909\n",
      "Validation --- Epoch: 64, total loss: 304.8216 , PPL: 1.2846, Acc: 0.4094\n",
      "lr = 0.015625\n",
      "Epoch: 65, Batch: 100, loss: 3.4909 , Train PPL: 1.0053, Train Acc: 0.4771\n",
      "Epoch: 65, Batch: 200, loss: 3.7051 , Train PPL: 1.0057, Train Acc: 0.4009\n",
      "Epoch: 65, Batch: 300, loss: 3.8032 , Train PPL: 1.0058, Train Acc: 0.4634\n",
      "Epoch: 65, Batch: 400, loss: 3.6408 , Train PPL: 1.0056, Train Acc: 0.4878\n",
      "Epoch: 65, Batch: 500, loss: 3.6498 , Train PPL: 1.0056, Train Acc: 0.4497\n",
      "Epoch: 65, Batch: 600, loss: 3.7514 , Train PPL: 1.0057, Train Acc: 0.4192\n",
      "Epoch: 65, Batch: 700, loss: 3.7684 , Train PPL: 1.0058, Train Acc: 0.3720\n",
      "Validation --- Epoch: 65, total loss: 304.8060 , PPL: 1.2846, Acc: 0.4092\n",
      "lr = 0.015625\n",
      "Epoch: 66, Batch: 100, loss: 3.6196 , Train PPL: 1.0055, Train Acc: 0.4314\n",
      "Epoch: 66, Batch: 200, loss: 3.8355 , Train PPL: 1.0059, Train Acc: 0.3704\n",
      "Epoch: 66, Batch: 300, loss: 3.3107 , Train PPL: 1.0051, Train Acc: 0.4924\n",
      "Epoch: 66, Batch: 400, loss: 3.6041 , Train PPL: 1.0055, Train Acc: 0.5046\n",
      "Epoch: 66, Batch: 500, loss: 3.3920 , Train PPL: 1.0052, Train Acc: 0.5137\n",
      "Epoch: 66, Batch: 600, loss: 3.4512 , Train PPL: 1.0053, Train Acc: 0.4466\n",
      "Epoch: 66, Batch: 700, loss: 3.6944 , Train PPL: 1.0056, Train Acc: 0.4512\n",
      "Validation --- Epoch: 66, total loss: 304.8445 , PPL: 1.2846, Acc: 0.4086\n",
      "lr = 0.015625\n",
      "Epoch: 67, Batch: 100, loss: 3.5352 , Train PPL: 1.0054, Train Acc: 0.4573\n",
      "Epoch: 67, Batch: 200, loss: 3.7564 , Train PPL: 1.0057, Train Acc: 0.3994\n",
      "Epoch: 67, Batch: 300, loss: 3.7406 , Train PPL: 1.0057, Train Acc: 0.4024\n",
      "Epoch: 67, Batch: 400, loss: 3.4115 , Train PPL: 1.0052, Train Acc: 0.4863\n",
      "Epoch: 67, Batch: 500, loss: 3.4087 , Train PPL: 1.0052, Train Acc: 0.4909\n",
      "Epoch: 67, Batch: 600, loss: 3.3375 , Train PPL: 1.0051, Train Acc: 0.5381\n",
      "Epoch: 67, Batch: 700, loss: 3.6737 , Train PPL: 1.0056, Train Acc: 0.4497\n",
      "Validation --- Epoch: 67, total loss: 304.8621 , PPL: 1.2846, Acc: 0.4088\n",
      "lr = 0.0078125\n",
      "Epoch: 68, Batch: 100, loss: 3.5063 , Train PPL: 1.0054, Train Acc: 0.4588\n",
      "Epoch: 68, Batch: 200, loss: 3.3119 , Train PPL: 1.0051, Train Acc: 0.5381\n",
      "Epoch: 68, Batch: 300, loss: 3.6208 , Train PPL: 1.0055, Train Acc: 0.4162\n",
      "Epoch: 68, Batch: 400, loss: 3.6078 , Train PPL: 1.0055, Train Acc: 0.4482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68, Batch: 500, loss: 3.6659 , Train PPL: 1.0056, Train Acc: 0.4024\n",
      "Epoch: 68, Batch: 600, loss: 3.4049 , Train PPL: 1.0052, Train Acc: 0.4878\n",
      "Epoch: 68, Batch: 700, loss: 3.3603 , Train PPL: 1.0051, Train Acc: 0.5564\n",
      "Validation --- Epoch: 68, total loss: 304.8464 , PPL: 1.2846, Acc: 0.4086\n",
      "lr = 0.0078125\n",
      "Epoch: 69, Batch: 100, loss: 3.3471 , Train PPL: 1.0051, Train Acc: 0.5640\n",
      "Epoch: 69, Batch: 200, loss: 3.8510 , Train PPL: 1.0059, Train Acc: 0.4101\n",
      "Epoch: 69, Batch: 300, loss: 3.6821 , Train PPL: 1.0056, Train Acc: 0.4314\n",
      "Epoch: 69, Batch: 400, loss: 3.8262 , Train PPL: 1.0058, Train Acc: 0.3841\n",
      "Epoch: 69, Batch: 500, loss: 3.5242 , Train PPL: 1.0054, Train Acc: 0.4497\n",
      "Epoch: 69, Batch: 600, loss: 3.2034 , Train PPL: 1.0049, Train Acc: 0.5610\n",
      "Epoch: 69, Batch: 700, loss: 3.8129 , Train PPL: 1.0058, Train Acc: 0.3841\n",
      "Validation --- Epoch: 69, total loss: 304.8475 , PPL: 1.2846, Acc: 0.4086\n",
      "lr = 0.0078125\n",
      "Epoch: 70, Batch: 100, loss: 3.5061 , Train PPL: 1.0054, Train Acc: 0.4726\n",
      "Epoch: 70, Batch: 200, loss: 3.4150 , Train PPL: 1.0052, Train Acc: 0.5335\n",
      "Epoch: 70, Batch: 300, loss: 3.6858 , Train PPL: 1.0056, Train Acc: 0.4405\n",
      "Epoch: 70, Batch: 400, loss: 3.5955 , Train PPL: 1.0055, Train Acc: 0.4512\n",
      "Epoch: 70, Batch: 500, loss: 3.5937 , Train PPL: 1.0055, Train Acc: 0.4284\n",
      "Epoch: 70, Batch: 600, loss: 3.9222 , Train PPL: 1.0060, Train Acc: 0.3902\n",
      "Epoch: 70, Batch: 700, loss: 3.6594 , Train PPL: 1.0056, Train Acc: 0.4314\n",
      "Validation --- Epoch: 70, total loss: 304.8599 , PPL: 1.2846, Acc: 0.4088\n",
      "lr = 0.0078125\n",
      "Epoch: 71, Batch: 100, loss: 3.6570 , Train PPL: 1.0056, Train Acc: 0.4466\n",
      "Epoch: 71, Batch: 200, loss: 3.7808 , Train PPL: 1.0058, Train Acc: 0.4558\n",
      "Epoch: 71, Batch: 300, loss: 3.6484 , Train PPL: 1.0056, Train Acc: 0.4665\n",
      "Epoch: 71, Batch: 400, loss: 3.7321 , Train PPL: 1.0057, Train Acc: 0.4390\n",
      "Epoch: 71, Batch: 500, loss: 3.5684 , Train PPL: 1.0055, Train Acc: 0.4299\n",
      "Epoch: 71, Batch: 600, loss: 3.4398 , Train PPL: 1.0053, Train Acc: 0.4299\n",
      "Epoch: 71, Batch: 700, loss: 3.8789 , Train PPL: 1.0059, Train Acc: 0.3765\n",
      "Validation --- Epoch: 71, total loss: 304.8586 , PPL: 1.2846, Acc: 0.4087\n",
      "lr = 0.0078125\n",
      "Epoch: 72, Batch: 100, loss: 3.7887 , Train PPL: 1.0058, Train Acc: 0.4466\n",
      "Epoch: 72, Batch: 200, loss: 3.5433 , Train PPL: 1.0054, Train Acc: 0.4619\n",
      "Epoch: 72, Batch: 300, loss: 3.7309 , Train PPL: 1.0057, Train Acc: 0.4009\n",
      "Epoch: 72, Batch: 400, loss: 3.5105 , Train PPL: 1.0054, Train Acc: 0.4649\n",
      "Epoch: 72, Batch: 500, loss: 3.7411 , Train PPL: 1.0057, Train Acc: 0.4512\n",
      "Epoch: 72, Batch: 600, loss: 3.5554 , Train PPL: 1.0054, Train Acc: 0.4756\n",
      "Epoch: 72, Batch: 700, loss: 4.0539 , Train PPL: 1.0062, Train Acc: 0.3780\n",
      "Validation --- Epoch: 72, total loss: 304.8604 , PPL: 1.2846, Acc: 0.4087\n",
      "lr = 0.0078125\n",
      "Epoch: 73, Batch: 100, loss: 3.4914 , Train PPL: 1.0053, Train Acc: 0.4970\n",
      "Epoch: 73, Batch: 200, loss: 3.3776 , Train PPL: 1.0052, Train Acc: 0.5290\n",
      "Epoch: 73, Batch: 300, loss: 3.8298 , Train PPL: 1.0059, Train Acc: 0.4451\n",
      "Epoch: 73, Batch: 400, loss: 3.5070 , Train PPL: 1.0054, Train Acc: 0.4756\n",
      "Epoch: 73, Batch: 500, loss: 3.6293 , Train PPL: 1.0055, Train Acc: 0.4527\n",
      "Epoch: 73, Batch: 600, loss: 3.5266 , Train PPL: 1.0054, Train Acc: 0.4497\n",
      "Epoch: 73, Batch: 700, loss: 3.4393 , Train PPL: 1.0053, Train Acc: 0.5534\n",
      "Validation --- Epoch: 73, total loss: 304.8868 , PPL: 1.2847, Acc: 0.4089\n",
      "lr = 0.00390625\n",
      "Epoch: 74, Batch: 100, loss: 3.5722 , Train PPL: 1.0055, Train Acc: 0.4588\n",
      "Epoch: 74, Batch: 200, loss: 3.2057 , Train PPL: 1.0049, Train Acc: 0.4954\n",
      "Epoch: 74, Batch: 300, loss: 3.6992 , Train PPL: 1.0057, Train Acc: 0.4451\n",
      "Epoch: 74, Batch: 400, loss: 3.7621 , Train PPL: 1.0058, Train Acc: 0.3979\n",
      "Epoch: 74, Batch: 500, loss: 3.5147 , Train PPL: 1.0054, Train Acc: 0.4588\n",
      "Epoch: 74, Batch: 600, loss: 3.9165 , Train PPL: 1.0060, Train Acc: 0.3887\n",
      "Epoch: 74, Batch: 700, loss: 3.5074 , Train PPL: 1.0054, Train Acc: 0.4421\n",
      "Validation --- Epoch: 74, total loss: 304.8869 , PPL: 1.2847, Acc: 0.4087\n",
      "lr = 0.00390625\n",
      "Epoch: 75, Batch: 100, loss: 3.6308 , Train PPL: 1.0056, Train Acc: 0.4573\n",
      "Epoch: 75, Batch: 200, loss: 3.2942 , Train PPL: 1.0050, Train Acc: 0.5381\n",
      "Epoch: 75, Batch: 300, loss: 3.4112 , Train PPL: 1.0052, Train Acc: 0.5168\n",
      "Epoch: 75, Batch: 400, loss: 3.8023 , Train PPL: 1.0058, Train Acc: 0.4223\n",
      "Epoch: 75, Batch: 500, loss: 3.6968 , Train PPL: 1.0057, Train Acc: 0.4192\n",
      "Epoch: 75, Batch: 600, loss: 3.7018 , Train PPL: 1.0057, Train Acc: 0.4131\n",
      "Epoch: 75, Batch: 700, loss: 3.7544 , Train PPL: 1.0057, Train Acc: 0.3963\n",
      "Validation --- Epoch: 75, total loss: 304.8963 , PPL: 1.2847, Acc: 0.4088\n",
      "lr = 0.00390625\n",
      "Epoch: 76, Batch: 100, loss: 3.6580 , Train PPL: 1.0056, Train Acc: 0.4070\n",
      "Epoch: 76, Batch: 200, loss: 3.4939 , Train PPL: 1.0053, Train Acc: 0.4619\n",
      "Epoch: 76, Batch: 300, loss: 3.6989 , Train PPL: 1.0057, Train Acc: 0.4405\n",
      "Epoch: 76, Batch: 400, loss: 3.8507 , Train PPL: 1.0059, Train Acc: 0.4146\n",
      "Epoch: 76, Batch: 500, loss: 3.5991 , Train PPL: 1.0055, Train Acc: 0.4390\n",
      "Epoch: 76, Batch: 600, loss: 3.4953 , Train PPL: 1.0053, Train Acc: 0.4421\n",
      "Epoch: 76, Batch: 700, loss: 3.5761 , Train PPL: 1.0055, Train Acc: 0.4223\n",
      "Validation --- Epoch: 76, total loss: 304.8854 , PPL: 1.2847, Acc: 0.4089\n",
      "lr = 0.00390625\n",
      "Epoch: 77, Batch: 100, loss: 3.6487 , Train PPL: 1.0056, Train Acc: 0.4436\n",
      "Epoch: 77, Batch: 200, loss: 3.6978 , Train PPL: 1.0057, Train Acc: 0.4192\n",
      "Epoch: 77, Batch: 300, loss: 3.6325 , Train PPL: 1.0056, Train Acc: 0.4787\n",
      "Epoch: 77, Batch: 400, loss: 3.8345 , Train PPL: 1.0059, Train Acc: 0.3750\n",
      "Epoch: 77, Batch: 500, loss: 3.7156 , Train PPL: 1.0057, Train Acc: 0.4345\n",
      "Epoch: 77, Batch: 600, loss: 3.4997 , Train PPL: 1.0053, Train Acc: 0.5198\n",
      "Epoch: 77, Batch: 700, loss: 3.8442 , Train PPL: 1.0059, Train Acc: 0.4390\n",
      "Validation --- Epoch: 77, total loss: 304.8858 , PPL: 1.2847, Acc: 0.4088\n",
      "lr = 0.00390625\n",
      "Epoch: 78, Batch: 100, loss: 3.3827 , Train PPL: 1.0052, Train Acc: 0.4802\n",
      "Epoch: 78, Batch: 200, loss: 3.6197 , Train PPL: 1.0055, Train Acc: 0.4162\n",
      "Epoch: 78, Batch: 300, loss: 3.4503 , Train PPL: 1.0053, Train Acc: 0.4924\n",
      "Epoch: 78, Batch: 400, loss: 3.6868 , Train PPL: 1.0056, Train Acc: 0.4238\n",
      "Epoch: 78, Batch: 500, loss: 3.8188 , Train PPL: 1.0058, Train Acc: 0.3552\n",
      "Epoch: 78, Batch: 600, loss: 3.7996 , Train PPL: 1.0058, Train Acc: 0.4375\n",
      "Epoch: 78, Batch: 700, loss: 3.9405 , Train PPL: 1.0060, Train Acc: 0.4314\n",
      "Validation --- Epoch: 78, total loss: 304.8914 , PPL: 1.2847, Acc: 0.4087\n",
      "lr = 0.00390625\n",
      "Epoch: 79, Batch: 100, loss: 3.6600 , Train PPL: 1.0056, Train Acc: 0.4131\n",
      "Epoch: 79, Batch: 200, loss: 3.6635 , Train PPL: 1.0056, Train Acc: 0.4482\n",
      "Epoch: 79, Batch: 300, loss: 3.5914 , Train PPL: 1.0055, Train Acc: 0.4314\n",
      "Epoch: 79, Batch: 400, loss: 3.4354 , Train PPL: 1.0053, Train Acc: 0.4893\n",
      "Epoch: 79, Batch: 500, loss: 3.5036 , Train PPL: 1.0054, Train Acc: 0.4924\n",
      "Epoch: 79, Batch: 600, loss: 3.3154 , Train PPL: 1.0051, Train Acc: 0.5335\n",
      "Epoch: 79, Batch: 700, loss: 3.6837 , Train PPL: 1.0056, Train Acc: 0.4466\n",
      "Validation --- Epoch: 79, total loss: 304.8964 , PPL: 1.2847, Acc: 0.4089\n",
      "lr = 0.001953125\n",
      "Epoch: 80, Batch: 100, loss: 3.6225 , Train PPL: 1.0055, Train Acc: 0.4924\n",
      "Epoch: 80, Batch: 200, loss: 3.6230 , Train PPL: 1.0055, Train Acc: 0.4558\n",
      "Epoch: 80, Batch: 300, loss: 3.6960 , Train PPL: 1.0057, Train Acc: 0.4146\n",
      "Epoch: 80, Batch: 400, loss: 3.5664 , Train PPL: 1.0055, Train Acc: 0.4573\n",
      "Epoch: 80, Batch: 500, loss: 3.7768 , Train PPL: 1.0058, Train Acc: 0.4497\n",
      "Epoch: 80, Batch: 600, loss: 3.8270 , Train PPL: 1.0059, Train Acc: 0.4009\n",
      "Epoch: 80, Batch: 700, loss: 3.8303 , Train PPL: 1.0059, Train Acc: 0.4162\n",
      "Validation --- Epoch: 80, total loss: 304.8931 , PPL: 1.2847, Acc: 0.4087\n",
      "lr = 0.001953125\n",
      "Epoch: 81, Batch: 100, loss: 3.4908 , Train PPL: 1.0053, Train Acc: 0.4802\n",
      "Epoch: 81, Batch: 200, loss: 3.2914 , Train PPL: 1.0050, Train Acc: 0.5625\n",
      "Epoch: 81, Batch: 300, loss: 3.1703 , Train PPL: 1.0048, Train Acc: 0.5015\n",
      "Epoch: 81, Batch: 400, loss: 3.6067 , Train PPL: 1.0055, Train Acc: 0.4192\n",
      "Epoch: 81, Batch: 500, loss: 3.4607 , Train PPL: 1.0053, Train Acc: 0.4832\n",
      "Epoch: 81, Batch: 600, loss: 3.5250 , Train PPL: 1.0054, Train Acc: 0.4756\n",
      "Epoch: 81, Batch: 700, loss: 3.5921 , Train PPL: 1.0055, Train Acc: 0.4634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --- Epoch: 81, total loss: 304.8944 , PPL: 1.2847, Acc: 0.4087\n",
      "lr = 0.001953125\n",
      "Epoch: 82, Batch: 100, loss: 3.7093 , Train PPL: 1.0057, Train Acc: 0.4665\n",
      "Epoch: 82, Batch: 200, loss: 3.6441 , Train PPL: 1.0056, Train Acc: 0.4741\n",
      "Epoch: 82, Batch: 300, loss: 3.5269 , Train PPL: 1.0054, Train Acc: 0.4482\n",
      "Epoch: 82, Batch: 400, loss: 3.4404 , Train PPL: 1.0053, Train Acc: 0.5152\n",
      "Epoch: 82, Batch: 500, loss: 3.9217 , Train PPL: 1.0060, Train Acc: 0.4345\n",
      "Epoch: 82, Batch: 600, loss: 3.6755 , Train PPL: 1.0056, Train Acc: 0.4558\n",
      "Epoch: 82, Batch: 700, loss: 3.3789 , Train PPL: 1.0052, Train Acc: 0.5061\n",
      "Validation --- Epoch: 82, total loss: 304.8973 , PPL: 1.2847, Acc: 0.4087\n",
      "lr = 0.001953125\n",
      "Epoch: 83, Batch: 100, loss: 3.4672 , Train PPL: 1.0053, Train Acc: 0.4787\n",
      "Epoch: 83, Batch: 200, loss: 3.6949 , Train PPL: 1.0056, Train Acc: 0.4497\n",
      "Epoch: 83, Batch: 300, loss: 3.4228 , Train PPL: 1.0052, Train Acc: 0.5061\n",
      "Epoch: 83, Batch: 400, loss: 3.5663 , Train PPL: 1.0055, Train Acc: 0.3994\n",
      "Epoch: 83, Batch: 500, loss: 3.4277 , Train PPL: 1.0052, Train Acc: 0.4924\n",
      "Epoch: 83, Batch: 600, loss: 3.5942 , Train PPL: 1.0055, Train Acc: 0.4848\n",
      "Epoch: 83, Batch: 700, loss: 3.5114 , Train PPL: 1.0054, Train Acc: 0.4710\n",
      "Validation --- Epoch: 83, total loss: 304.8971 , PPL: 1.2847, Acc: 0.4088\n",
      "lr = 0.001953125\n",
      "Epoch: 84, Batch: 100, loss: 3.7441 , Train PPL: 1.0057, Train Acc: 0.4588\n",
      "Epoch: 84, Batch: 200, loss: 3.4718 , Train PPL: 1.0053, Train Acc: 0.4649\n",
      "Epoch: 84, Batch: 300, loss: 3.7491 , Train PPL: 1.0057, Train Acc: 0.4268\n",
      "Epoch: 84, Batch: 400, loss: 3.6099 , Train PPL: 1.0055, Train Acc: 0.4573\n",
      "Epoch: 84, Batch: 500, loss: 3.7512 , Train PPL: 1.0057, Train Acc: 0.4253\n",
      "Epoch: 84, Batch: 600, loss: 3.7105 , Train PPL: 1.0057, Train Acc: 0.3994\n",
      "Epoch: 84, Batch: 700, loss: 3.7645 , Train PPL: 1.0058, Train Acc: 0.4665\n",
      "Validation --- Epoch: 84, total loss: 304.8947 , PPL: 1.2847, Acc: 0.4088\n",
      "lr = 0.001953125\n",
      "Epoch: 85, Batch: 100, loss: 3.7820 , Train PPL: 1.0058, Train Acc: 0.4162\n",
      "Epoch: 85, Batch: 200, loss: 3.5854 , Train PPL: 1.0055, Train Acc: 0.4848\n",
      "Epoch: 85, Batch: 300, loss: 3.5518 , Train PPL: 1.0054, Train Acc: 0.4588\n",
      "Epoch: 85, Batch: 400, loss: 3.4791 , Train PPL: 1.0053, Train Acc: 0.4680\n",
      "Epoch: 85, Batch: 500, loss: 3.6307 , Train PPL: 1.0055, Train Acc: 0.5030\n",
      "Epoch: 85, Batch: 600, loss: 3.6619 , Train PPL: 1.0056, Train Acc: 0.4558\n",
      "Epoch: 85, Batch: 700, loss: 3.3747 , Train PPL: 1.0052, Train Acc: 0.5290\n",
      "Validation --- Epoch: 85, total loss: 304.8998 , PPL: 1.2847, Acc: 0.4088\n",
      "lr = 0.0009765625\n",
      "Epoch: 86, Batch: 100, loss: 3.8396 , Train PPL: 1.0059, Train Acc: 0.4116\n",
      "Epoch: 86, Batch: 200, loss: 3.7423 , Train PPL: 1.0057, Train Acc: 0.4375\n",
      "Epoch: 86, Batch: 300, loss: 3.7937 , Train PPL: 1.0058, Train Acc: 0.3689\n",
      "Epoch: 86, Batch: 400, loss: 3.6450 , Train PPL: 1.0056, Train Acc: 0.4055\n",
      "Epoch: 86, Batch: 500, loss: 3.4055 , Train PPL: 1.0052, Train Acc: 0.5000\n",
      "Epoch: 86, Batch: 600, loss: 3.4242 , Train PPL: 1.0052, Train Acc: 0.4710\n",
      "Epoch: 86, Batch: 700, loss: 3.4202 , Train PPL: 1.0052, Train Acc: 0.5274\n",
      "Validation --- Epoch: 86, total loss: 304.8995 , PPL: 1.2847, Acc: 0.4088\n",
      "lr = 0.0009765625\n",
      "Epoch: 87, Batch: 100, loss: 3.5819 , Train PPL: 1.0055, Train Acc: 0.4436\n",
      "Epoch: 87, Batch: 200, loss: 3.5025 , Train PPL: 1.0054, Train Acc: 0.4878\n",
      "Epoch: 87, Batch: 300, loss: 3.6138 , Train PPL: 1.0055, Train Acc: 0.4009\n",
      "Epoch: 87, Batch: 400, loss: 3.8617 , Train PPL: 1.0059, Train Acc: 0.4619\n",
      "Epoch: 87, Batch: 500, loss: 3.4163 , Train PPL: 1.0052, Train Acc: 0.4893\n",
      "Epoch: 87, Batch: 600, loss: 3.6625 , Train PPL: 1.0056, Train Acc: 0.4207\n",
      "Epoch: 87, Batch: 700, loss: 3.4652 , Train PPL: 1.0053, Train Acc: 0.5152\n",
      "Validation --- Epoch: 87, total loss: 304.9003 , PPL: 1.2847, Acc: 0.4088\n",
      "lr = 0.0009765625\n",
      "Epoch: 88, Batch: 100, loss: 3.7850 , Train PPL: 1.0058, Train Acc: 0.3872\n",
      "Epoch: 88, Batch: 200, loss: 3.5192 , Train PPL: 1.0054, Train Acc: 0.4680\n",
      "Epoch: 88, Batch: 300, loss: 3.9087 , Train PPL: 1.0060, Train Acc: 0.3872\n",
      "Epoch: 88, Batch: 400, loss: 3.3216 , Train PPL: 1.0051, Train Acc: 0.5335\n",
      "Epoch: 88, Batch: 500, loss: 3.2324 , Train PPL: 1.0049, Train Acc: 0.5168\n",
      "Epoch: 88, Batch: 600, loss: 3.4679 , Train PPL: 1.0053, Train Acc: 0.4802\n",
      "Epoch: 88, Batch: 700, loss: 3.6058 , Train PPL: 1.0055, Train Acc: 0.4970\n",
      "Validation --- Epoch: 88, total loss: 304.9002 , PPL: 1.2847, Acc: 0.4088\n",
      "lr = 0.0009765625\n",
      "Epoch: 89, Batch: 100, loss: 3.5987 , Train PPL: 1.0055, Train Acc: 0.4436\n",
      "Epoch: 89, Batch: 200, loss: 3.5371 , Train PPL: 1.0054, Train Acc: 0.4466\n",
      "Epoch: 89, Batch: 300, loss: 3.7961 , Train PPL: 1.0058, Train Acc: 0.4207\n",
      "Epoch: 89, Batch: 400, loss: 3.8735 , Train PPL: 1.0059, Train Acc: 0.4070\n",
      "Epoch: 89, Batch: 500, loss: 3.6313 , Train PPL: 1.0056, Train Acc: 0.4543\n",
      "Epoch: 89, Batch: 600, loss: 3.4797 , Train PPL: 1.0053, Train Acc: 0.5274\n",
      "Epoch: 89, Batch: 700, loss: 3.5646 , Train PPL: 1.0054, Train Acc: 0.4649\n",
      "Validation --- Epoch: 89, total loss: 304.9010 , PPL: 1.2847, Acc: 0.4088\n",
      "lr = 0.0009765625\n",
      "Epoch: 90, Batch: 100, loss: 3.3313 , Train PPL: 1.0051, Train Acc: 0.5107\n",
      "Epoch: 90, Batch: 200, loss: 3.3509 , Train PPL: 1.0051, Train Acc: 0.5366\n",
      "Epoch: 90, Batch: 300, loss: 3.4504 , Train PPL: 1.0053, Train Acc: 0.4848\n",
      "Epoch: 90, Batch: 400, loss: 3.4759 , Train PPL: 1.0053, Train Acc: 0.4573\n",
      "Epoch: 90, Batch: 500, loss: 3.9663 , Train PPL: 1.0061, Train Acc: 0.3811\n",
      "Epoch: 90, Batch: 600, loss: 3.2335 , Train PPL: 1.0049, Train Acc: 0.5488\n",
      "Epoch: 90, Batch: 700, loss: 3.5944 , Train PPL: 1.0055, Train Acc: 0.4329\n",
      "Validation --- Epoch: 90, total loss: 304.9006 , PPL: 1.2847, Acc: 0.4088\n",
      "lr = 0.0009765625\n",
      "Epoch: 91, Batch: 100, loss: 3.6402 , Train PPL: 1.0056, Train Acc: 0.4207\n",
      "Epoch: 91, Batch: 200, loss: 3.5814 , Train PPL: 1.0055, Train Acc: 0.5137\n",
      "Epoch: 91, Batch: 300, loss: 3.6419 , Train PPL: 1.0056, Train Acc: 0.4497\n",
      "Epoch: 91, Batch: 400, loss: 3.7849 , Train PPL: 1.0058, Train Acc: 0.4085\n",
      "Epoch: 91, Batch: 500, loss: 3.3493 , Train PPL: 1.0051, Train Acc: 0.5137\n",
      "Epoch: 91, Batch: 600, loss: 3.5395 , Train PPL: 1.0054, Train Acc: 0.4421\n",
      "Epoch: 91, Batch: 700, loss: 3.6346 , Train PPL: 1.0056, Train Acc: 0.4131\n",
      "Validation --- Epoch: 91, total loss: 304.9015 , PPL: 1.2847, Acc: 0.4089\n",
      "lr = 0.00048828125\n",
      "Epoch: 92, Batch: 100, loss: 3.5715 , Train PPL: 1.0055, Train Acc: 0.4649\n",
      "Epoch: 92, Batch: 200, loss: 3.4887 , Train PPL: 1.0053, Train Acc: 0.5030\n",
      "Epoch: 92, Batch: 300, loss: 3.8468 , Train PPL: 1.0059, Train Acc: 0.4238\n",
      "Epoch: 92, Batch: 400, loss: 3.8108 , Train PPL: 1.0058, Train Acc: 0.3796\n",
      "Epoch: 92, Batch: 500, loss: 3.5835 , Train PPL: 1.0055, Train Acc: 0.4817\n",
      "Epoch: 92, Batch: 600, loss: 3.6093 , Train PPL: 1.0055, Train Acc: 0.4619\n",
      "Epoch: 92, Batch: 700, loss: 3.4788 , Train PPL: 1.0053, Train Acc: 0.4588\n",
      "Validation --- Epoch: 92, total loss: 304.9026 , PPL: 1.2847, Acc: 0.4089\n",
      "lr = 0.00048828125\n",
      "Epoch: 93, Batch: 100, loss: 3.5224 , Train PPL: 1.0054, Train Acc: 0.4649\n",
      "Epoch: 93, Batch: 200, loss: 3.8351 , Train PPL: 1.0059, Train Acc: 0.4390\n",
      "Epoch: 93, Batch: 300, loss: 3.5330 , Train PPL: 1.0054, Train Acc: 0.4299\n",
      "Epoch: 93, Batch: 400, loss: 3.3239 , Train PPL: 1.0051, Train Acc: 0.5503\n",
      "Epoch: 93, Batch: 500, loss: 3.5603 , Train PPL: 1.0054, Train Acc: 0.4634\n",
      "Epoch: 93, Batch: 600, loss: 3.3284 , Train PPL: 1.0051, Train Acc: 0.4985\n",
      "Epoch: 93, Batch: 700, loss: 3.1237 , Train PPL: 1.0048, Train Acc: 0.5305\n",
      "Validation --- Epoch: 93, total loss: 304.9026 , PPL: 1.2847, Acc: 0.4089\n",
      "lr = 0.00048828125\n",
      "Epoch: 94, Batch: 100, loss: 3.6218 , Train PPL: 1.0055, Train Acc: 0.4421\n",
      "Epoch: 94, Batch: 200, loss: 3.3135 , Train PPL: 1.0051, Train Acc: 0.5335\n",
      "Epoch: 94, Batch: 300, loss: 3.2105 , Train PPL: 1.0049, Train Acc: 0.5808\n",
      "Epoch: 94, Batch: 400, loss: 3.4691 , Train PPL: 1.0053, Train Acc: 0.4482\n",
      "Epoch: 94, Batch: 500, loss: 3.7040 , Train PPL: 1.0057, Train Acc: 0.4284\n",
      "Epoch: 94, Batch: 600, loss: 3.6366 , Train PPL: 1.0056, Train Acc: 0.4451\n",
      "Epoch: 94, Batch: 700, loss: 3.7230 , Train PPL: 1.0057, Train Acc: 0.4527\n",
      "Validation --- Epoch: 94, total loss: 304.9027 , PPL: 1.2847, Acc: 0.4089\n",
      "lr = 0.00048828125\n",
      "Epoch: 95, Batch: 100, loss: 3.5013 , Train PPL: 1.0054, Train Acc: 0.4512\n",
      "Epoch: 95, Batch: 200, loss: 3.4366 , Train PPL: 1.0053, Train Acc: 0.4695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95, Batch: 300, loss: 3.3859 , Train PPL: 1.0052, Train Acc: 0.5046\n",
      "Epoch: 95, Batch: 400, loss: 3.5669 , Train PPL: 1.0055, Train Acc: 0.4634\n",
      "Epoch: 95, Batch: 500, loss: 3.4019 , Train PPL: 1.0052, Train Acc: 0.5198\n",
      "Epoch: 95, Batch: 600, loss: 3.6852 , Train PPL: 1.0056, Train Acc: 0.3857\n",
      "Epoch: 95, Batch: 700, loss: 3.4447 , Train PPL: 1.0053, Train Acc: 0.4893\n",
      "Validation --- Epoch: 95, total loss: 304.9020 , PPL: 1.2847, Acc: 0.4088\n",
      "lr = 0.00048828125\n",
      "Epoch: 96, Batch: 100, loss: 3.7878 , Train PPL: 1.0058, Train Acc: 0.4238\n",
      "Epoch: 96, Batch: 200, loss: 3.3712 , Train PPL: 1.0052, Train Acc: 0.5290\n",
      "Epoch: 96, Batch: 300, loss: 3.6095 , Train PPL: 1.0055, Train Acc: 0.4314\n",
      "Epoch: 96, Batch: 400, loss: 3.5345 , Train PPL: 1.0054, Train Acc: 0.4512\n",
      "Epoch: 96, Batch: 500, loss: 3.6079 , Train PPL: 1.0055, Train Acc: 0.4527\n",
      "Epoch: 96, Batch: 600, loss: 3.4366 , Train PPL: 1.0053, Train Acc: 0.4756\n",
      "Epoch: 96, Batch: 700, loss: 3.4851 , Train PPL: 1.0053, Train Acc: 0.4482\n",
      "Validation --- Epoch: 96, total loss: 304.9020 , PPL: 1.2847, Acc: 0.4089\n",
      "lr = 0.00048828125\n",
      "Epoch: 97, Batch: 100, loss: 3.3336 , Train PPL: 1.0051, Train Acc: 0.5152\n",
      "Epoch: 97, Batch: 200, loss: 3.4638 , Train PPL: 1.0053, Train Acc: 0.4832\n",
      "Epoch: 97, Batch: 300, loss: 3.6867 , Train PPL: 1.0056, Train Acc: 0.4375\n",
      "Epoch: 97, Batch: 400, loss: 3.4286 , Train PPL: 1.0052, Train Acc: 0.4970\n",
      "Epoch: 97, Batch: 500, loss: 3.7807 , Train PPL: 1.0058, Train Acc: 0.4329\n",
      "Epoch: 97, Batch: 600, loss: 3.6513 , Train PPL: 1.0056, Train Acc: 0.4451\n",
      "Epoch: 97, Batch: 700, loss: 3.6653 , Train PPL: 1.0056, Train Acc: 0.4314\n",
      "Validation --- Epoch: 97, total loss: 304.9021 , PPL: 1.2847, Acc: 0.4089\n",
      "lr = 0.000244140625\n",
      "Epoch: 98, Batch: 100, loss: 3.4902 , Train PPL: 1.0053, Train Acc: 0.4299\n",
      "Epoch: 98, Batch: 200, loss: 3.4788 , Train PPL: 1.0053, Train Acc: 0.5030\n",
      "Epoch: 98, Batch: 300, loss: 3.4214 , Train PPL: 1.0052, Train Acc: 0.4878\n",
      "Epoch: 98, Batch: 400, loss: 3.6978 , Train PPL: 1.0057, Train Acc: 0.4131\n",
      "Epoch: 98, Batch: 500, loss: 3.8994 , Train PPL: 1.0060, Train Acc: 0.4192\n",
      "Epoch: 98, Batch: 600, loss: 3.5896 , Train PPL: 1.0055, Train Acc: 0.5229\n",
      "Epoch: 98, Batch: 700, loss: 3.4031 , Train PPL: 1.0052, Train Acc: 0.5046\n",
      "Validation --- Epoch: 98, total loss: 304.9019 , PPL: 1.2847, Acc: 0.4089\n",
      "lr = 0.000244140625\n",
      "Epoch: 99, Batch: 100, loss: 3.8704 , Train PPL: 1.0059, Train Acc: 0.3841\n",
      "Epoch: 99, Batch: 200, loss: 3.9480 , Train PPL: 1.0060, Train Acc: 0.3887\n",
      "Epoch: 99, Batch: 300, loss: 3.5921 , Train PPL: 1.0055, Train Acc: 0.4710\n",
      "Epoch: 99, Batch: 400, loss: 3.5467 , Train PPL: 1.0054, Train Acc: 0.4223\n",
      "Epoch: 99, Batch: 500, loss: 3.6449 , Train PPL: 1.0056, Train Acc: 0.4162\n",
      "Epoch: 99, Batch: 600, loss: 3.2871 , Train PPL: 1.0050, Train Acc: 0.5366\n",
      "Epoch: 99, Batch: 700, loss: 3.7324 , Train PPL: 1.0057, Train Acc: 0.4329\n",
      "Validation --- Epoch: 99, total loss: 304.9019 , PPL: 1.2847, Acc: 0.4089\n",
      "lr = 0.000244140625\n"
     ]
    }
   ],
   "source": [
    "model = TCN(4, [1200,1200,1200], kernel=3, dropout=0.45, embedding_size = 1200, n_words = n_words,tied=True)\n",
    "model.cuda()\n",
    "model_parameters = filter(lambda x: x.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print('Network with {} parameters'.format(params))\n",
    "print('Receptive field of network is {}'.format(model.receptive_field))\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode=\"min\",patience=5,min_lr=1e-6,factor=0.5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "best_vloss = 1e8\n",
    "for e in range(100):\n",
    "    train_loop(e)\n",
    "    validation_loss = validation_loop(e)\n",
    "    scheduler.step(validation_loss)\n",
    "    print('lr = {}'.format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "    if validation_loss < best_vloss:\n",
    "                with open(\"model_4_layers_k3_1200_filters_2.pt\", 'wb') as f:\n",
    "                    print('wrote model')\n",
    "                    torch.save(model, f)\n",
    "                best_vloss = validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network with 93151601 parameters\n",
      "Receptive field of network is 30\n",
      "Epoch: 0, Batch: 100, loss: 6.4636 , Train PPL: 1.0099, Train Acc: 0.1753\n",
      "Epoch: 0, Batch: 200, loss: 6.5999 , Train PPL: 1.0101, Train Acc: 0.1601\n",
      "Epoch: 0, Batch: 300, loss: 6.2316 , Train PPL: 1.0095, Train Acc: 0.2088\n",
      "Epoch: 0, Batch: 400, loss: 5.8660 , Train PPL: 1.0090, Train Acc: 0.2637\n",
      "Epoch: 0, Batch: 500, loss: 6.1481 , Train PPL: 1.0094, Train Acc: 0.2439\n",
      "Epoch: 0, Batch: 600, loss: 5.8979 , Train PPL: 1.0090, Train Acc: 0.2927\n",
      "Epoch: 0, Batch: 700, loss: 5.4474 , Train PPL: 1.0083, Train Acc: 0.2973\n",
      "Validation --- Epoch: 0, total loss: 328.2361 , PPL: 1.3071, Acc: 0.2930\n",
      "lr = 4\n",
      "wrote model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type TCN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type TC_block. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Chomp1d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 100, loss: 6.0499 , Train PPL: 1.0093, Train Acc: 0.2713\n",
      "Epoch: 1, Batch: 200, loss: 5.7102 , Train PPL: 1.0087, Train Acc: 0.2927\n",
      "Epoch: 1, Batch: 300, loss: 5.7158 , Train PPL: 1.0088, Train Acc: 0.2881\n",
      "Epoch: 1, Batch: 400, loss: 5.8043 , Train PPL: 1.0089, Train Acc: 0.2759\n",
      "Epoch: 1, Batch: 500, loss: 5.5635 , Train PPL: 1.0085, Train Acc: 0.2851\n",
      "Epoch: 1, Batch: 600, loss: 5.5518 , Train PPL: 1.0085, Train Acc: 0.3369\n",
      "Epoch: 1, Batch: 700, loss: 5.5929 , Train PPL: 1.0086, Train Acc: 0.3110\n",
      "Validation --- Epoch: 1, total loss: 314.9261 , PPL: 1.2929, Acc: 0.3331\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 2, Batch: 100, loss: 5.6616 , Train PPL: 1.0087, Train Acc: 0.2988\n",
      "Epoch: 2, Batch: 200, loss: 5.3535 , Train PPL: 1.0082, Train Acc: 0.3811\n",
      "Epoch: 2, Batch: 300, loss: 5.5667 , Train PPL: 1.0085, Train Acc: 0.3110\n",
      "Epoch: 2, Batch: 400, loss: 5.7474 , Train PPL: 1.0088, Train Acc: 0.2805\n",
      "Epoch: 2, Batch: 500, loss: 5.5693 , Train PPL: 1.0085, Train Acc: 0.3308\n",
      "Epoch: 2, Batch: 600, loss: 5.2974 , Train PPL: 1.0081, Train Acc: 0.3369\n",
      "Epoch: 2, Batch: 700, loss: 5.3735 , Train PPL: 1.0082, Train Acc: 0.2820\n",
      "Validation --- Epoch: 2, total loss: 309.6695 , PPL: 1.2877, Acc: 0.3432\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 3, Batch: 100, loss: 5.1742 , Train PPL: 1.0079, Train Acc: 0.3613\n",
      "Epoch: 3, Batch: 200, loss: 5.4363 , Train PPL: 1.0083, Train Acc: 0.3338\n",
      "Epoch: 3, Batch: 300, loss: 5.0999 , Train PPL: 1.0078, Train Acc: 0.3552\n",
      "Epoch: 3, Batch: 400, loss: 5.1322 , Train PPL: 1.0079, Train Acc: 0.3704\n",
      "Epoch: 3, Batch: 500, loss: 5.3937 , Train PPL: 1.0083, Train Acc: 0.3095\n",
      "Epoch: 3, Batch: 600, loss: 5.0682 , Train PPL: 1.0078, Train Acc: 0.3841\n",
      "Epoch: 3, Batch: 700, loss: 5.2526 , Train PPL: 1.0080, Train Acc: 0.3338\n",
      "Validation --- Epoch: 3, total loss: 305.6405 , PPL: 1.2834, Acc: 0.3490\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 4, Batch: 100, loss: 4.9985 , Train PPL: 1.0076, Train Acc: 0.3780\n",
      "Epoch: 4, Batch: 200, loss: 4.8615 , Train PPL: 1.0074, Train Acc: 0.3735\n",
      "Epoch: 4, Batch: 300, loss: 4.6948 , Train PPL: 1.0072, Train Acc: 0.4360\n",
      "Epoch: 4, Batch: 400, loss: 4.9922 , Train PPL: 1.0076, Train Acc: 0.3765\n",
      "Epoch: 4, Batch: 500, loss: 5.1244 , Train PPL: 1.0078, Train Acc: 0.3582\n",
      "Epoch: 4, Batch: 600, loss: 5.1522 , Train PPL: 1.0079, Train Acc: 0.3506\n",
      "Epoch: 4, Batch: 700, loss: 5.1483 , Train PPL: 1.0079, Train Acc: 0.3262\n",
      "Validation --- Epoch: 4, total loss: 302.9875 , PPL: 1.2808, Acc: 0.3585\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 5, Batch: 100, loss: 4.9393 , Train PPL: 1.0076, Train Acc: 0.3155\n",
      "Epoch: 5, Batch: 200, loss: 5.0369 , Train PPL: 1.0077, Train Acc: 0.3384\n",
      "Epoch: 5, Batch: 300, loss: 5.1423 , Train PPL: 1.0079, Train Acc: 0.3399\n",
      "Epoch: 5, Batch: 400, loss: 4.9232 , Train PPL: 1.0075, Train Acc: 0.3811\n",
      "Epoch: 5, Batch: 500, loss: 5.0252 , Train PPL: 1.0077, Train Acc: 0.3506\n",
      "Epoch: 5, Batch: 600, loss: 4.8478 , Train PPL: 1.0074, Train Acc: 0.3963\n",
      "Epoch: 5, Batch: 700, loss: 5.1981 , Train PPL: 1.0080, Train Acc: 0.3430\n",
      "Validation --- Epoch: 5, total loss: 300.4077 , PPL: 1.2785, Acc: 0.3671\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 6, Batch: 100, loss: 4.8317 , Train PPL: 1.0074, Train Acc: 0.3521\n",
      "Epoch: 6, Batch: 200, loss: 4.5490 , Train PPL: 1.0070, Train Acc: 0.3674\n",
      "Epoch: 6, Batch: 300, loss: 5.0376 , Train PPL: 1.0077, Train Acc: 0.3354\n",
      "Epoch: 6, Batch: 400, loss: 5.2271 , Train PPL: 1.0080, Train Acc: 0.3384\n",
      "Epoch: 6, Batch: 500, loss: 5.2299 , Train PPL: 1.0080, Train Acc: 0.2973\n",
      "Epoch: 6, Batch: 600, loss: 4.5423 , Train PPL: 1.0069, Train Acc: 0.4497\n",
      "Epoch: 6, Batch: 700, loss: 4.9389 , Train PPL: 1.0076, Train Acc: 0.3796\n",
      "Validation --- Epoch: 6, total loss: 298.7332 , PPL: 1.2767, Acc: 0.3701\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 7, Batch: 100, loss: 5.0475 , Train PPL: 1.0077, Train Acc: 0.3399\n",
      "Epoch: 7, Batch: 200, loss: 5.0019 , Train PPL: 1.0077, Train Acc: 0.3445\n",
      "Epoch: 7, Batch: 300, loss: 4.7317 , Train PPL: 1.0072, Train Acc: 0.3460\n",
      "Epoch: 7, Batch: 400, loss: 4.9340 , Train PPL: 1.0075, Train Acc: 0.3598\n",
      "Epoch: 7, Batch: 500, loss: 4.7941 , Train PPL: 1.0073, Train Acc: 0.4040\n",
      "Epoch: 7, Batch: 600, loss: 5.0308 , Train PPL: 1.0077, Train Acc: 0.3262\n",
      "Epoch: 7, Batch: 700, loss: 4.9824 , Train PPL: 1.0076, Train Acc: 0.3552\n",
      "Validation --- Epoch: 7, total loss: 298.1881 , PPL: 1.2763, Acc: 0.3707\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 8, Batch: 100, loss: 4.6807 , Train PPL: 1.0072, Train Acc: 0.3887\n",
      "Epoch: 8, Batch: 200, loss: 4.7742 , Train PPL: 1.0073, Train Acc: 0.4223\n",
      "Epoch: 8, Batch: 300, loss: 4.8223 , Train PPL: 1.0074, Train Acc: 0.4009\n",
      "Epoch: 8, Batch: 400, loss: 4.9206 , Train PPL: 1.0075, Train Acc: 0.3460\n",
      "Epoch: 8, Batch: 500, loss: 4.6628 , Train PPL: 1.0071, Train Acc: 0.3887\n",
      "Epoch: 8, Batch: 600, loss: 4.8459 , Train PPL: 1.0074, Train Acc: 0.3552\n",
      "Epoch: 8, Batch: 700, loss: 4.7982 , Train PPL: 1.0073, Train Acc: 0.3491\n",
      "Validation --- Epoch: 8, total loss: 297.3973 , PPL: 1.2755, Acc: 0.3760\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 9, Batch: 100, loss: 4.3832 , Train PPL: 1.0067, Train Acc: 0.4726\n",
      "Epoch: 9, Batch: 200, loss: 4.7260 , Train PPL: 1.0072, Train Acc: 0.3476\n",
      "Epoch: 9, Batch: 300, loss: 5.1040 , Train PPL: 1.0078, Train Acc: 0.3018\n",
      "Epoch: 9, Batch: 400, loss: 4.6405 , Train PPL: 1.0071, Train Acc: 0.3537\n",
      "Epoch: 9, Batch: 500, loss: 4.3503 , Train PPL: 1.0067, Train Acc: 0.4588\n",
      "Epoch: 9, Batch: 600, loss: 4.7368 , Train PPL: 1.0072, Train Acc: 0.4024\n",
      "Epoch: 9, Batch: 700, loss: 4.3552 , Train PPL: 1.0067, Train Acc: 0.4268\n",
      "Validation --- Epoch: 9, total loss: 296.4224 , PPL: 1.2747, Acc: 0.3841\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 10, Batch: 100, loss: 4.4424 , Train PPL: 1.0068, Train Acc: 0.4421\n",
      "Epoch: 10, Batch: 200, loss: 4.9320 , Train PPL: 1.0075, Train Acc: 0.3110\n",
      "Epoch: 10, Batch: 300, loss: 4.7295 , Train PPL: 1.0072, Train Acc: 0.3384\n",
      "Epoch: 10, Batch: 400, loss: 4.6625 , Train PPL: 1.0071, Train Acc: 0.3369\n",
      "Epoch: 10, Batch: 500, loss: 4.9591 , Train PPL: 1.0076, Train Acc: 0.3491\n",
      "Epoch: 10, Batch: 600, loss: 4.8139 , Train PPL: 1.0074, Train Acc: 0.3476\n",
      "Epoch: 10, Batch: 700, loss: 4.6799 , Train PPL: 1.0072, Train Acc: 0.3506\n",
      "Validation --- Epoch: 10, total loss: 297.1820 , PPL: 1.2756, Acc: 0.3814\n",
      "lr = 4\n",
      "Epoch: 11, Batch: 100, loss: 4.0221 , Train PPL: 1.0062, Train Acc: 0.4924\n",
      "Epoch: 11, Batch: 200, loss: 4.5245 , Train PPL: 1.0069, Train Acc: 0.3826\n",
      "Epoch: 11, Batch: 300, loss: 4.7197 , Train PPL: 1.0072, Train Acc: 0.3506\n",
      "Epoch: 11, Batch: 400, loss: 4.2906 , Train PPL: 1.0066, Train Acc: 0.4070\n",
      "Epoch: 11, Batch: 500, loss: 4.2341 , Train PPL: 1.0065, Train Acc: 0.3963\n",
      "Epoch: 11, Batch: 600, loss: 4.7142 , Train PPL: 1.0072, Train Acc: 0.3445\n",
      "Epoch: 11, Batch: 700, loss: 4.6263 , Train PPL: 1.0071, Train Acc: 0.3567\n",
      "Validation --- Epoch: 11, total loss: 296.5508 , PPL: 1.2752, Acc: 0.3815\n",
      "lr = 4\n",
      "Epoch: 12, Batch: 100, loss: 4.4088 , Train PPL: 1.0067, Train Acc: 0.3826\n",
      "Epoch: 12, Batch: 200, loss: 4.1649 , Train PPL: 1.0064, Train Acc: 0.4314\n",
      "Epoch: 12, Batch: 300, loss: 4.3393 , Train PPL: 1.0066, Train Acc: 0.4146\n",
      "Epoch: 12, Batch: 400, loss: 4.4744 , Train PPL: 1.0068, Train Acc: 0.3140\n",
      "Epoch: 12, Batch: 500, loss: 3.9352 , Train PPL: 1.0060, Train Acc: 0.5305\n",
      "Epoch: 12, Batch: 600, loss: 4.0784 , Train PPL: 1.0062, Train Acc: 0.4771\n",
      "Epoch: 12, Batch: 700, loss: 4.0042 , Train PPL: 1.0061, Train Acc: 0.4512\n",
      "Validation --- Epoch: 12, total loss: 297.9275 , PPL: 1.2765, Acc: 0.3857\n",
      "lr = 4\n",
      "Epoch: 13, Batch: 100, loss: 4.4868 , Train PPL: 1.0069, Train Acc: 0.3445\n",
      "Epoch: 13, Batch: 200, loss: 4.4462 , Train PPL: 1.0068, Train Acc: 0.3613\n",
      "Epoch: 13, Batch: 300, loss: 4.3367 , Train PPL: 1.0066, Train Acc: 0.4024\n",
      "Epoch: 13, Batch: 400, loss: 4.0698 , Train PPL: 1.0062, Train Acc: 0.4649\n",
      "Epoch: 13, Batch: 500, loss: 4.3388 , Train PPL: 1.0066, Train Acc: 0.3948\n",
      "Epoch: 13, Batch: 600, loss: 4.0786 , Train PPL: 1.0062, Train Acc: 0.4421\n",
      "Epoch: 13, Batch: 700, loss: 4.3912 , Train PPL: 1.0067, Train Acc: 0.3948\n",
      "Validation --- Epoch: 13, total loss: 297.4845 , PPL: 1.2761, Acc: 0.3895\n",
      "lr = 4\n",
      "Epoch: 14, Batch: 100, loss: 4.3425 , Train PPL: 1.0066, Train Acc: 0.3841\n",
      "Epoch: 14, Batch: 200, loss: 4.2233 , Train PPL: 1.0065, Train Acc: 0.4009\n",
      "Epoch: 14, Batch: 300, loss: 4.3512 , Train PPL: 1.0067, Train Acc: 0.3720\n",
      "Epoch: 14, Batch: 400, loss: 4.1785 , Train PPL: 1.0064, Train Acc: 0.4085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Batch: 500, loss: 4.4728 , Train PPL: 1.0068, Train Acc: 0.3384\n",
      "Epoch: 14, Batch: 600, loss: 4.4490 , Train PPL: 1.0068, Train Acc: 0.3171\n",
      "Epoch: 14, Batch: 700, loss: 4.7832 , Train PPL: 1.0073, Train Acc: 0.3186\n",
      "Validation --- Epoch: 14, total loss: 298.4163 , PPL: 1.2771, Acc: 0.3917\n",
      "lr = 4\n",
      "Epoch: 15, Batch: 100, loss: 3.9747 , Train PPL: 1.0061, Train Acc: 0.4329\n",
      "Epoch: 15, Batch: 200, loss: 3.8776 , Train PPL: 1.0059, Train Acc: 0.4207\n",
      "Epoch: 15, Batch: 300, loss: 3.9523 , Train PPL: 1.0060, Train Acc: 0.4070\n",
      "Epoch: 15, Batch: 400, loss: 4.1202 , Train PPL: 1.0063, Train Acc: 0.3841\n",
      "Epoch: 15, Batch: 500, loss: 4.1186 , Train PPL: 1.0063, Train Acc: 0.4405\n",
      "Epoch: 15, Batch: 600, loss: 4.3947 , Train PPL: 1.0067, Train Acc: 0.3552\n",
      "Epoch: 15, Batch: 700, loss: 3.9131 , Train PPL: 1.0060, Train Acc: 0.4390\n",
      "Validation --- Epoch: 15, total loss: 299.5661 , PPL: 1.2785, Acc: 0.3787\n",
      "lr = 2.0\n",
      "Epoch: 16, Batch: 100, loss: 3.6768 , Train PPL: 1.0056, Train Acc: 0.4695\n",
      "Epoch: 16, Batch: 200, loss: 4.0616 , Train PPL: 1.0062, Train Acc: 0.3918\n",
      "Epoch: 16, Batch: 300, loss: 3.6834 , Train PPL: 1.0056, Train Acc: 0.4878\n",
      "Epoch: 16, Batch: 400, loss: 4.1356 , Train PPL: 1.0063, Train Acc: 0.4375\n",
      "Epoch: 16, Batch: 500, loss: 4.0687 , Train PPL: 1.0062, Train Acc: 0.3613\n",
      "Epoch: 16, Batch: 600, loss: 4.2949 , Train PPL: 1.0066, Train Acc: 0.3826\n",
      "Epoch: 16, Batch: 700, loss: 4.0693 , Train PPL: 1.0062, Train Acc: 0.4223\n",
      "Validation --- Epoch: 16, total loss: 300.8076 , PPL: 1.2800, Acc: 0.3992\n",
      "lr = 2.0\n",
      "Epoch: 17, Batch: 100, loss: 4.0229 , Train PPL: 1.0062, Train Acc: 0.3887\n",
      "Epoch: 17, Batch: 200, loss: 3.9665 , Train PPL: 1.0061, Train Acc: 0.3750\n",
      "Epoch: 17, Batch: 300, loss: 3.7359 , Train PPL: 1.0057, Train Acc: 0.4314\n",
      "Epoch: 17, Batch: 400, loss: 3.7129 , Train PPL: 1.0057, Train Acc: 0.4436\n",
      "Epoch: 17, Batch: 500, loss: 3.9227 , Train PPL: 1.0060, Train Acc: 0.4405\n",
      "Epoch: 17, Batch: 600, loss: 4.1353 , Train PPL: 1.0063, Train Acc: 0.3613\n",
      "Epoch: 17, Batch: 700, loss: 3.6296 , Train PPL: 1.0055, Train Acc: 0.4924\n",
      "Validation --- Epoch: 17, total loss: 302.0453 , PPL: 1.2813, Acc: 0.3993\n",
      "lr = 2.0\n",
      "Epoch: 18, Batch: 100, loss: 3.5607 , Train PPL: 1.0054, Train Acc: 0.4756\n",
      "Epoch: 18, Batch: 200, loss: 4.1234 , Train PPL: 1.0063, Train Acc: 0.4116\n",
      "Epoch: 18, Batch: 300, loss: 3.7005 , Train PPL: 1.0057, Train Acc: 0.4223\n",
      "Epoch: 18, Batch: 400, loss: 4.1903 , Train PPL: 1.0064, Train Acc: 0.3567\n",
      "Epoch: 18, Batch: 500, loss: 4.0085 , Train PPL: 1.0061, Train Acc: 0.4162\n",
      "Epoch: 18, Batch: 600, loss: 3.8217 , Train PPL: 1.0058, Train Acc: 0.4421\n",
      "Epoch: 18, Batch: 700, loss: 3.5245 , Train PPL: 1.0054, Train Acc: 0.4802\n",
      "Validation --- Epoch: 18, total loss: 302.5122 , PPL: 1.2819, Acc: 0.4002\n",
      "lr = 2.0\n",
      "Epoch: 19, Batch: 100, loss: 3.9367 , Train PPL: 1.0060, Train Acc: 0.4009\n",
      "Epoch: 19, Batch: 200, loss: 3.3464 , Train PPL: 1.0051, Train Acc: 0.4939\n",
      "Epoch: 19, Batch: 300, loss: 3.5288 , Train PPL: 1.0054, Train Acc: 0.4375\n",
      "Epoch: 19, Batch: 400, loss: 3.6422 , Train PPL: 1.0056, Train Acc: 0.4466\n",
      "Epoch: 19, Batch: 500, loss: 3.8536 , Train PPL: 1.0059, Train Acc: 0.4116\n",
      "Epoch: 19, Batch: 600, loss: 4.0440 , Train PPL: 1.0062, Train Acc: 0.3811\n",
      "Epoch: 19, Batch: 700, loss: 4.0480 , Train PPL: 1.0062, Train Acc: 0.3674\n",
      "Validation --- Epoch: 19, total loss: 304.8862 , PPL: 1.2846, Acc: 0.3994\n",
      "lr = 2.0\n",
      "Epoch: 20, Batch: 100, loss: 3.7924 , Train PPL: 1.0058, Train Acc: 0.3659\n",
      "Epoch: 20, Batch: 200, loss: 3.6847 , Train PPL: 1.0056, Train Acc: 0.4299\n",
      "Epoch: 20, Batch: 300, loss: 3.9230 , Train PPL: 1.0060, Train Acc: 0.3887\n",
      "Epoch: 20, Batch: 400, loss: 3.7049 , Train PPL: 1.0057, Train Acc: 0.4787\n",
      "Epoch: 20, Batch: 500, loss: 3.5305 , Train PPL: 1.0054, Train Acc: 0.5030\n",
      "Epoch: 20, Batch: 600, loss: 3.7795 , Train PPL: 1.0058, Train Acc: 0.4497\n",
      "Epoch: 20, Batch: 700, loss: 3.4485 , Train PPL: 1.0053, Train Acc: 0.4924\n",
      "Validation --- Epoch: 20, total loss: 305.2324 , PPL: 1.2849, Acc: 0.3956\n",
      "lr = 2.0\n",
      "Epoch: 21, Batch: 100, loss: 3.3336 , Train PPL: 1.0051, Train Acc: 0.4878\n",
      "Epoch: 21, Batch: 200, loss: 3.7324 , Train PPL: 1.0057, Train Acc: 0.3963\n",
      "Epoch: 21, Batch: 300, loss: 3.7761 , Train PPL: 1.0058, Train Acc: 0.3811\n",
      "Epoch: 21, Batch: 400, loss: 3.6118 , Train PPL: 1.0055, Train Acc: 0.4893\n",
      "Epoch: 21, Batch: 500, loss: 3.6133 , Train PPL: 1.0055, Train Acc: 0.4680\n",
      "Epoch: 21, Batch: 600, loss: 3.6974 , Train PPL: 1.0057, Train Acc: 0.4116\n",
      "Epoch: 21, Batch: 700, loss: 3.7841 , Train PPL: 1.0058, Train Acc: 0.4162\n",
      "Validation --- Epoch: 21, total loss: 306.8900 , PPL: 1.2869, Acc: 0.3977\n",
      "lr = 1.0\n",
      "Epoch: 22, Batch: 100, loss: 3.6158 , Train PPL: 1.0055, Train Acc: 0.4238\n",
      "Epoch: 22, Batch: 200, loss: 3.6281 , Train PPL: 1.0055, Train Acc: 0.4665\n",
      "Epoch: 22, Batch: 300, loss: 3.3543 , Train PPL: 1.0051, Train Acc: 0.4848\n",
      "Epoch: 22, Batch: 400, loss: 3.6667 , Train PPL: 1.0056, Train Acc: 0.4543\n",
      "Epoch: 22, Batch: 500, loss: 3.4840 , Train PPL: 1.0053, Train Acc: 0.4299\n",
      "Epoch: 22, Batch: 600, loss: 3.7524 , Train PPL: 1.0057, Train Acc: 0.3887\n",
      "Epoch: 22, Batch: 700, loss: 3.7145 , Train PPL: 1.0057, Train Acc: 0.4070\n",
      "Validation --- Epoch: 22, total loss: 309.0186 , PPL: 1.2892, Acc: 0.4004\n",
      "lr = 1.0\n",
      "Epoch: 23, Batch: 100, loss: 3.5666 , Train PPL: 1.0055, Train Acc: 0.4070\n",
      "Epoch: 23, Batch: 200, loss: 3.3508 , Train PPL: 1.0051, Train Acc: 0.4726\n",
      "Epoch: 23, Batch: 300, loss: 3.4190 , Train PPL: 1.0052, Train Acc: 0.4573\n",
      "Epoch: 23, Batch: 400, loss: 3.6007 , Train PPL: 1.0055, Train Acc: 0.4665\n",
      "Epoch: 23, Batch: 500, loss: 3.6035 , Train PPL: 1.0055, Train Acc: 0.4329\n",
      "Epoch: 23, Batch: 600, loss: 3.6772 , Train PPL: 1.0056, Train Acc: 0.3826\n",
      "Epoch: 23, Batch: 700, loss: 3.6372 , Train PPL: 1.0056, Train Acc: 0.4421\n",
      "Validation --- Epoch: 23, total loss: 310.1785 , PPL: 1.2904, Acc: 0.4014\n",
      "lr = 1.0\n",
      "Epoch: 24, Batch: 100, loss: 3.5037 , Train PPL: 1.0054, Train Acc: 0.4588\n",
      "Epoch: 24, Batch: 200, loss: 3.6932 , Train PPL: 1.0056, Train Acc: 0.3948\n",
      "Epoch: 24, Batch: 300, loss: 3.2437 , Train PPL: 1.0050, Train Acc: 0.5015\n",
      "Epoch: 24, Batch: 400, loss: 3.6441 , Train PPL: 1.0056, Train Acc: 0.4131\n",
      "Epoch: 24, Batch: 500, loss: 3.4546 , Train PPL: 1.0053, Train Acc: 0.4665\n",
      "Epoch: 24, Batch: 600, loss: 3.4024 , Train PPL: 1.0052, Train Acc: 0.4497\n",
      "Epoch: 24, Batch: 700, loss: 3.6495 , Train PPL: 1.0056, Train Acc: 0.4146\n",
      "Validation --- Epoch: 24, total loss: 310.4915 , PPL: 1.2907, Acc: 0.3991\n",
      "lr = 1.0\n",
      "Epoch: 25, Batch: 100, loss: 3.3294 , Train PPL: 1.0051, Train Acc: 0.4284\n",
      "Epoch: 25, Batch: 200, loss: 3.8291 , Train PPL: 1.0059, Train Acc: 0.4162\n",
      "Epoch: 25, Batch: 300, loss: 3.0774 , Train PPL: 1.0047, Train Acc: 0.5640\n",
      "Epoch: 25, Batch: 400, loss: 3.1685 , Train PPL: 1.0048, Train Acc: 0.4543\n",
      "Epoch: 25, Batch: 500, loss: 3.7355 , Train PPL: 1.0057, Train Acc: 0.3902\n",
      "Epoch: 25, Batch: 600, loss: 3.1749 , Train PPL: 1.0049, Train Acc: 0.5152\n",
      "Epoch: 25, Batch: 700, loss: 3.3631 , Train PPL: 1.0051, Train Acc: 0.4741\n",
      "Validation --- Epoch: 25, total loss: 311.6020 , PPL: 1.2919, Acc: 0.4010\n",
      "lr = 1.0\n",
      "Epoch: 26, Batch: 100, loss: 3.6782 , Train PPL: 1.0056, Train Acc: 0.4101\n",
      "Epoch: 26, Batch: 200, loss: 3.3454 , Train PPL: 1.0051, Train Acc: 0.4863\n",
      "Epoch: 26, Batch: 300, loss: 3.3027 , Train PPL: 1.0050, Train Acc: 0.4649\n",
      "Epoch: 26, Batch: 400, loss: 3.3175 , Train PPL: 1.0051, Train Acc: 0.4802\n",
      "Epoch: 26, Batch: 500, loss: 2.9121 , Train PPL: 1.0044, Train Acc: 0.5671\n",
      "Epoch: 26, Batch: 600, loss: 3.4481 , Train PPL: 1.0053, Train Acc: 0.4985\n",
      "Epoch: 26, Batch: 700, loss: 3.4221 , Train PPL: 1.0052, Train Acc: 0.4345\n",
      "Validation --- Epoch: 26, total loss: 312.9957 , PPL: 1.2935, Acc: 0.4011\n",
      "lr = 1.0\n",
      "Epoch: 27, Batch: 100, loss: 3.7456 , Train PPL: 1.0057, Train Acc: 0.3994\n",
      "Epoch: 27, Batch: 200, loss: 3.0530 , Train PPL: 1.0047, Train Acc: 0.5412\n",
      "Epoch: 27, Batch: 300, loss: 3.5442 , Train PPL: 1.0054, Train Acc: 0.3857\n",
      "Epoch: 27, Batch: 400, loss: 3.0978 , Train PPL: 1.0047, Train Acc: 0.4954\n",
      "Epoch: 27, Batch: 500, loss: 2.8031 , Train PPL: 1.0043, Train Acc: 0.5534\n",
      "Epoch: 27, Batch: 600, loss: 3.4848 , Train PPL: 1.0053, Train Acc: 0.4162\n",
      "Epoch: 27, Batch: 700, loss: 3.5009 , Train PPL: 1.0054, Train Acc: 0.4131\n",
      "Validation --- Epoch: 27, total loss: 313.1435 , PPL: 1.2937, Acc: 0.4012\n",
      "lr = 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Batch: 100, loss: 3.4143 , Train PPL: 1.0052, Train Acc: 0.4436\n",
      "Epoch: 28, Batch: 200, loss: 3.1178 , Train PPL: 1.0048, Train Acc: 0.5534\n",
      "Epoch: 28, Batch: 300, loss: 3.3090 , Train PPL: 1.0051, Train Acc: 0.4345\n",
      "Epoch: 28, Batch: 400, loss: 3.1607 , Train PPL: 1.0048, Train Acc: 0.4771\n",
      "Epoch: 28, Batch: 500, loss: 3.4021 , Train PPL: 1.0052, Train Acc: 0.4405\n",
      "Epoch: 28, Batch: 600, loss: 3.6909 , Train PPL: 1.0056, Train Acc: 0.3796\n",
      "Epoch: 28, Batch: 700, loss: 3.5678 , Train PPL: 1.0055, Train Acc: 0.4177\n",
      "Validation --- Epoch: 28, total loss: 314.3120 , PPL: 1.2949, Acc: 0.4010\n",
      "lr = 0.5\n",
      "Epoch: 29, Batch: 100, loss: 3.1667 , Train PPL: 1.0048, Train Acc: 0.5000\n",
      "Epoch: 29, Batch: 200, loss: 2.9772 , Train PPL: 1.0045, Train Acc: 0.4848\n",
      "Epoch: 29, Batch: 300, loss: 3.3247 , Train PPL: 1.0051, Train Acc: 0.4665\n",
      "Epoch: 29, Batch: 400, loss: 3.2969 , Train PPL: 1.0050, Train Acc: 0.4360\n",
      "Epoch: 29, Batch: 500, loss: 3.1048 , Train PPL: 1.0047, Train Acc: 0.5000\n",
      "Epoch: 29, Batch: 600, loss: 3.5403 , Train PPL: 1.0054, Train Acc: 0.4375\n",
      "Epoch: 29, Batch: 700, loss: 3.2463 , Train PPL: 1.0050, Train Acc: 0.4970\n",
      "Validation --- Epoch: 29, total loss: 314.3531 , PPL: 1.2949, Acc: 0.4020\n",
      "lr = 0.5\n",
      "Epoch: 30, Batch: 100, loss: 3.5137 , Train PPL: 1.0054, Train Acc: 0.4482\n",
      "Epoch: 30, Batch: 200, loss: 3.0714 , Train PPL: 1.0047, Train Acc: 0.5777\n",
      "Epoch: 30, Batch: 300, loss: 3.6247 , Train PPL: 1.0055, Train Acc: 0.3735\n",
      "Epoch: 30, Batch: 400, loss: 3.3786 , Train PPL: 1.0052, Train Acc: 0.4421\n",
      "Epoch: 30, Batch: 500, loss: 3.4730 , Train PPL: 1.0053, Train Acc: 0.4024\n",
      "Epoch: 30, Batch: 600, loss: 3.6446 , Train PPL: 1.0056, Train Acc: 0.4436\n",
      "Epoch: 30, Batch: 700, loss: 3.4004 , Train PPL: 1.0052, Train Acc: 0.4817\n",
      "Validation --- Epoch: 30, total loss: 315.3410 , PPL: 1.2961, Acc: 0.4008\n",
      "lr = 0.5\n",
      "Epoch: 31, Batch: 100, loss: 3.2298 , Train PPL: 1.0049, Train Acc: 0.4695\n",
      "Epoch: 31, Batch: 200, loss: 3.0419 , Train PPL: 1.0046, Train Acc: 0.4848\n",
      "Epoch: 31, Batch: 300, loss: 3.4487 , Train PPL: 1.0053, Train Acc: 0.4421\n",
      "Epoch: 31, Batch: 400, loss: 3.2213 , Train PPL: 1.0049, Train Acc: 0.5091\n",
      "Epoch: 31, Batch: 500, loss: 3.2518 , Train PPL: 1.0050, Train Acc: 0.4604\n",
      "Epoch: 31, Batch: 600, loss: 2.9167 , Train PPL: 1.0045, Train Acc: 0.5412\n",
      "Epoch: 31, Batch: 700, loss: 3.3915 , Train PPL: 1.0052, Train Acc: 0.4314\n",
      "Validation --- Epoch: 31, total loss: 315.9705 , PPL: 1.2968, Acc: 0.4017\n",
      "lr = 0.5\n",
      "Epoch: 32, Batch: 100, loss: 3.0978 , Train PPL: 1.0047, Train Acc: 0.5061\n",
      "Epoch: 32, Batch: 200, loss: 3.4705 , Train PPL: 1.0053, Train Acc: 0.4116\n",
      "Epoch: 32, Batch: 300, loss: 2.7383 , Train PPL: 1.0042, Train Acc: 0.5427\n",
      "Epoch: 32, Batch: 400, loss: 3.2758 , Train PPL: 1.0050, Train Acc: 0.4680\n",
      "Epoch: 32, Batch: 500, loss: 3.0961 , Train PPL: 1.0047, Train Acc: 0.5320\n",
      "Epoch: 32, Batch: 600, loss: 3.2376 , Train PPL: 1.0049, Train Acc: 0.4375\n",
      "Epoch: 32, Batch: 700, loss: 3.4062 , Train PPL: 1.0052, Train Acc: 0.4238\n",
      "Validation --- Epoch: 32, total loss: 315.9533 , PPL: 1.2968, Acc: 0.4006\n",
      "lr = 0.5\n",
      "Epoch: 33, Batch: 100, loss: 3.0941 , Train PPL: 1.0047, Train Acc: 0.5290\n",
      "Epoch: 33, Batch: 200, loss: 3.2526 , Train PPL: 1.0050, Train Acc: 0.5000\n",
      "Epoch: 33, Batch: 300, loss: 3.0362 , Train PPL: 1.0046, Train Acc: 0.5396\n",
      "Epoch: 33, Batch: 400, loss: 2.8873 , Train PPL: 1.0044, Train Acc: 0.6037\n",
      "Epoch: 33, Batch: 500, loss: 2.9799 , Train PPL: 1.0046, Train Acc: 0.5152\n",
      "Epoch: 33, Batch: 600, loss: 3.3274 , Train PPL: 1.0051, Train Acc: 0.4146\n",
      "Epoch: 33, Batch: 700, loss: 3.4236 , Train PPL: 1.0052, Train Acc: 0.4421\n",
      "Validation --- Epoch: 33, total loss: 316.6606 , PPL: 1.2975, Acc: 0.4010\n",
      "lr = 0.25\n",
      "Epoch: 34, Batch: 100, loss: 3.3220 , Train PPL: 1.0051, Train Acc: 0.4436\n",
      "Epoch: 34, Batch: 200, loss: 3.1319 , Train PPL: 1.0048, Train Acc: 0.5168\n",
      "Epoch: 34, Batch: 300, loss: 3.4141 , Train PPL: 1.0052, Train Acc: 0.4634\n",
      "Epoch: 34, Batch: 400, loss: 3.0719 , Train PPL: 1.0047, Train Acc: 0.5716\n",
      "Epoch: 34, Batch: 500, loss: 3.1059 , Train PPL: 1.0047, Train Acc: 0.5869\n",
      "Epoch: 34, Batch: 600, loss: 3.0053 , Train PPL: 1.0046, Train Acc: 0.5396\n",
      "Epoch: 34, Batch: 700, loss: 3.2970 , Train PPL: 1.0050, Train Acc: 0.4863\n",
      "Validation --- Epoch: 34, total loss: 317.6173 , PPL: 1.2986, Acc: 0.4032\n",
      "lr = 0.25\n",
      "Epoch: 35, Batch: 100, loss: 3.0669 , Train PPL: 1.0047, Train Acc: 0.5549\n",
      "Epoch: 35, Batch: 200, loss: 3.2053 , Train PPL: 1.0049, Train Acc: 0.4924\n",
      "Epoch: 35, Batch: 300, loss: 2.9736 , Train PPL: 1.0045, Train Acc: 0.4680\n",
      "Epoch: 35, Batch: 400, loss: 3.2080 , Train PPL: 1.0049, Train Acc: 0.4893\n",
      "Epoch: 35, Batch: 500, loss: 3.1022 , Train PPL: 1.0047, Train Acc: 0.5015\n",
      "Epoch: 35, Batch: 600, loss: 3.2355 , Train PPL: 1.0049, Train Acc: 0.4771\n",
      "Epoch: 35, Batch: 700, loss: 3.2042 , Train PPL: 1.0049, Train Acc: 0.4970\n",
      "Validation --- Epoch: 35, total loss: 317.7955 , PPL: 1.2988, Acc: 0.4040\n",
      "lr = 0.25\n",
      "Epoch: 36, Batch: 100, loss: 3.5383 , Train PPL: 1.0054, Train Acc: 0.4101\n",
      "Epoch: 36, Batch: 200, loss: 3.3454 , Train PPL: 1.0051, Train Acc: 0.4863\n",
      "Epoch: 36, Batch: 300, loss: 2.9202 , Train PPL: 1.0045, Train Acc: 0.5808\n",
      "Epoch: 36, Batch: 400, loss: 3.2244 , Train PPL: 1.0049, Train Acc: 0.4787\n",
      "Epoch: 36, Batch: 500, loss: 3.1420 , Train PPL: 1.0048, Train Acc: 0.4726\n",
      "Epoch: 36, Batch: 600, loss: 2.7984 , Train PPL: 1.0043, Train Acc: 0.5549\n",
      "Epoch: 36, Batch: 700, loss: 3.2024 , Train PPL: 1.0049, Train Acc: 0.4939\n",
      "Validation --- Epoch: 36, total loss: 317.8966 , PPL: 1.2989, Acc: 0.4019\n",
      "lr = 0.25\n",
      "Epoch: 37, Batch: 100, loss: 3.4967 , Train PPL: 1.0053, Train Acc: 0.4451\n",
      "Epoch: 37, Batch: 200, loss: 3.3139 , Train PPL: 1.0051, Train Acc: 0.4024\n",
      "Epoch: 37, Batch: 300, loss: 2.8635 , Train PPL: 1.0044, Train Acc: 0.5564\n",
      "Epoch: 37, Batch: 400, loss: 3.2333 , Train PPL: 1.0049, Train Acc: 0.4573\n",
      "Epoch: 37, Batch: 500, loss: 3.2767 , Train PPL: 1.0050, Train Acc: 0.4588\n",
      "Epoch: 37, Batch: 600, loss: 3.1050 , Train PPL: 1.0047, Train Acc: 0.4771\n",
      "Epoch: 37, Batch: 700, loss: 3.3291 , Train PPL: 1.0051, Train Acc: 0.4268\n",
      "Validation --- Epoch: 37, total loss: 318.3648 , PPL: 1.2994, Acc: 0.4022\n",
      "lr = 0.25\n",
      "Epoch: 38, Batch: 100, loss: 3.2501 , Train PPL: 1.0050, Train Acc: 0.4802\n",
      "Epoch: 38, Batch: 200, loss: 3.0724 , Train PPL: 1.0047, Train Acc: 0.5305\n",
      "Epoch: 38, Batch: 300, loss: 2.9745 , Train PPL: 1.0045, Train Acc: 0.5335\n",
      "Epoch: 38, Batch: 400, loss: 3.0068 , Train PPL: 1.0046, Train Acc: 0.5503\n",
      "Epoch: 38, Batch: 500, loss: 3.1965 , Train PPL: 1.0049, Train Acc: 0.4482\n",
      "Epoch: 38, Batch: 600, loss: 3.1768 , Train PPL: 1.0049, Train Acc: 0.4924\n",
      "Epoch: 38, Batch: 700, loss: 3.4565 , Train PPL: 1.0053, Train Acc: 0.4070\n",
      "Validation --- Epoch: 38, total loss: 318.7881 , PPL: 1.2999, Acc: 0.4017\n",
      "lr = 0.25\n",
      "Epoch: 39, Batch: 100, loss: 3.3198 , Train PPL: 1.0051, Train Acc: 0.4771\n",
      "Epoch: 39, Batch: 200, loss: 2.9054 , Train PPL: 1.0044, Train Acc: 0.5457\n",
      "Epoch: 39, Batch: 300, loss: 3.5219 , Train PPL: 1.0054, Train Acc: 0.4405\n",
      "Epoch: 39, Batch: 400, loss: 3.1875 , Train PPL: 1.0049, Train Acc: 0.4665\n",
      "Epoch: 39, Batch: 500, loss: 3.0786 , Train PPL: 1.0047, Train Acc: 0.5229\n",
      "Epoch: 39, Batch: 600, loss: 2.9842 , Train PPL: 1.0046, Train Acc: 0.4924\n",
      "Epoch: 39, Batch: 700, loss: 3.1842 , Train PPL: 1.0049, Train Acc: 0.4970\n",
      "Validation --- Epoch: 39, total loss: 319.0248 , PPL: 1.3001, Acc: 0.4019\n",
      "lr = 0.125\n",
      "Epoch: 40, Batch: 100, loss: 3.1635 , Train PPL: 1.0048, Train Acc: 0.4939\n",
      "Epoch: 40, Batch: 200, loss: 3.1719 , Train PPL: 1.0048, Train Acc: 0.5152\n",
      "Epoch: 40, Batch: 300, loss: 2.7683 , Train PPL: 1.0042, Train Acc: 0.5823\n",
      "Epoch: 40, Batch: 400, loss: 2.7755 , Train PPL: 1.0042, Train Acc: 0.6128\n",
      "Epoch: 40, Batch: 500, loss: 3.0160 , Train PPL: 1.0046, Train Acc: 0.5473\n",
      "Epoch: 40, Batch: 600, loss: 3.2518 , Train PPL: 1.0050, Train Acc: 0.5015\n",
      "Epoch: 40, Batch: 700, loss: 3.1836 , Train PPL: 1.0049, Train Acc: 0.4893\n",
      "Validation --- Epoch: 40, total loss: 319.1451 , PPL: 1.3002, Acc: 0.4002\n",
      "lr = 0.125\n",
      "Epoch: 41, Batch: 100, loss: 3.3012 , Train PPL: 1.0050, Train Acc: 0.4634\n",
      "Epoch: 41, Batch: 200, loss: 3.1422 , Train PPL: 1.0048, Train Acc: 0.4939\n",
      "Epoch: 41, Batch: 300, loss: 2.6504 , Train PPL: 1.0040, Train Acc: 0.6159\n",
      "Epoch: 41, Batch: 400, loss: 3.4290 , Train PPL: 1.0052, Train Acc: 0.4162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41, Batch: 500, loss: 2.9259 , Train PPL: 1.0045, Train Acc: 0.5625\n",
      "Epoch: 41, Batch: 600, loss: 3.3131 , Train PPL: 1.0051, Train Acc: 0.4329\n",
      "Epoch: 41, Batch: 700, loss: 3.2811 , Train PPL: 1.0050, Train Acc: 0.4497\n",
      "Validation --- Epoch: 41, total loss: 319.3345 , PPL: 1.3004, Acc: 0.4017\n",
      "lr = 0.125\n",
      "Epoch: 42, Batch: 100, loss: 2.9616 , Train PPL: 1.0045, Train Acc: 0.5335\n",
      "Epoch: 42, Batch: 200, loss: 2.7721 , Train PPL: 1.0042, Train Acc: 0.5732\n",
      "Epoch: 42, Batch: 300, loss: 2.9854 , Train PPL: 1.0046, Train Acc: 0.5412\n",
      "Epoch: 42, Batch: 400, loss: 3.2397 , Train PPL: 1.0050, Train Acc: 0.4604\n",
      "Epoch: 42, Batch: 500, loss: 3.2985 , Train PPL: 1.0050, Train Acc: 0.4497\n",
      "Epoch: 42, Batch: 600, loss: 3.2070 , Train PPL: 1.0049, Train Acc: 0.5046\n",
      "Epoch: 42, Batch: 700, loss: 3.0748 , Train PPL: 1.0047, Train Acc: 0.5137\n",
      "Validation --- Epoch: 42, total loss: 319.4344 , PPL: 1.3005, Acc: 0.4013\n",
      "lr = 0.125\n",
      "Epoch: 43, Batch: 100, loss: 3.1521 , Train PPL: 1.0048, Train Acc: 0.4878\n",
      "Epoch: 43, Batch: 200, loss: 2.9181 , Train PPL: 1.0045, Train Acc: 0.5869\n",
      "Epoch: 43, Batch: 300, loss: 3.4027 , Train PPL: 1.0052, Train Acc: 0.4238\n",
      "Epoch: 43, Batch: 400, loss: 2.8947 , Train PPL: 1.0044, Train Acc: 0.5457\n",
      "Epoch: 43, Batch: 500, loss: 3.1068 , Train PPL: 1.0047, Train Acc: 0.5137\n",
      "Epoch: 43, Batch: 600, loss: 3.2619 , Train PPL: 1.0050, Train Acc: 0.4131\n",
      "Epoch: 43, Batch: 700, loss: 2.8840 , Train PPL: 1.0044, Train Acc: 0.5625\n",
      "Validation --- Epoch: 43, total loss: 319.6848 , PPL: 1.3009, Acc: 0.4012\n",
      "lr = 0.125\n",
      "Epoch: 44, Batch: 100, loss: 3.0511 , Train PPL: 1.0047, Train Acc: 0.5259\n",
      "Epoch: 44, Batch: 200, loss: 3.0542 , Train PPL: 1.0047, Train Acc: 0.4726\n",
      "Epoch: 44, Batch: 300, loss: 2.8265 , Train PPL: 1.0043, Train Acc: 0.5244\n",
      "Epoch: 44, Batch: 400, loss: 3.1482 , Train PPL: 1.0048, Train Acc: 0.4771\n",
      "Epoch: 44, Batch: 500, loss: 3.2168 , Train PPL: 1.0049, Train Acc: 0.4771\n",
      "Epoch: 44, Batch: 600, loss: 3.2596 , Train PPL: 1.0050, Train Acc: 0.4634\n",
      "Epoch: 44, Batch: 700, loss: 3.2457 , Train PPL: 1.0050, Train Acc: 0.4192\n",
      "Validation --- Epoch: 44, total loss: 319.7666 , PPL: 1.3010, Acc: 0.4010\n",
      "lr = 0.125\n",
      "Epoch: 45, Batch: 100, loss: 3.1428 , Train PPL: 1.0048, Train Acc: 0.4787\n",
      "Epoch: 45, Batch: 200, loss: 2.9580 , Train PPL: 1.0045, Train Acc: 0.5549\n",
      "Epoch: 45, Batch: 300, loss: 3.4330 , Train PPL: 1.0052, Train Acc: 0.4116\n",
      "Epoch: 45, Batch: 400, loss: 3.0255 , Train PPL: 1.0046, Train Acc: 0.4985\n",
      "Epoch: 45, Batch: 500, loss: 3.1842 , Train PPL: 1.0049, Train Acc: 0.4909\n",
      "Epoch: 45, Batch: 600, loss: 2.8925 , Train PPL: 1.0044, Train Acc: 0.5518\n",
      "Epoch: 45, Batch: 700, loss: 3.3447 , Train PPL: 1.0051, Train Acc: 0.4207\n",
      "Validation --- Epoch: 45, total loss: 320.0543 , PPL: 1.3013, Acc: 0.4008\n",
      "lr = 0.0625\n",
      "Epoch: 46, Batch: 100, loss: 2.9913 , Train PPL: 1.0046, Train Acc: 0.5854\n",
      "Epoch: 46, Batch: 200, loss: 3.2268 , Train PPL: 1.0049, Train Acc: 0.4878\n",
      "Epoch: 46, Batch: 300, loss: 3.1801 , Train PPL: 1.0049, Train Acc: 0.4832\n",
      "Epoch: 46, Batch: 400, loss: 3.3277 , Train PPL: 1.0051, Train Acc: 0.4360\n",
      "Epoch: 46, Batch: 500, loss: 3.0901 , Train PPL: 1.0047, Train Acc: 0.4970\n",
      "Epoch: 46, Batch: 600, loss: 2.7654 , Train PPL: 1.0042, Train Acc: 0.5579\n",
      "Epoch: 46, Batch: 700, loss: 2.9985 , Train PPL: 1.0046, Train Acc: 0.4634\n",
      "Validation --- Epoch: 46, total loss: 320.1598 , PPL: 1.3014, Acc: 0.4017\n",
      "lr = 0.0625\n",
      "Epoch: 47, Batch: 100, loss: 3.1341 , Train PPL: 1.0048, Train Acc: 0.5183\n",
      "Epoch: 47, Batch: 200, loss: 3.1284 , Train PPL: 1.0048, Train Acc: 0.4939\n",
      "Epoch: 47, Batch: 300, loss: 2.7573 , Train PPL: 1.0042, Train Acc: 0.6235\n",
      "Epoch: 47, Batch: 400, loss: 2.9393 , Train PPL: 1.0045, Train Acc: 0.5457\n",
      "Epoch: 47, Batch: 500, loss: 3.0485 , Train PPL: 1.0047, Train Acc: 0.5213\n",
      "Epoch: 47, Batch: 600, loss: 3.1940 , Train PPL: 1.0049, Train Acc: 0.5122\n",
      "Epoch: 47, Batch: 700, loss: 3.1077 , Train PPL: 1.0047, Train Acc: 0.4604\n",
      "Validation --- Epoch: 47, total loss: 320.2437 , PPL: 1.3015, Acc: 0.4014\n",
      "lr = 0.0625\n",
      "Epoch: 48, Batch: 100, loss: 3.0826 , Train PPL: 1.0047, Train Acc: 0.4680\n",
      "Epoch: 48, Batch: 200, loss: 3.1673 , Train PPL: 1.0048, Train Acc: 0.4604\n",
      "Epoch: 48, Batch: 300, loss: 3.3035 , Train PPL: 1.0050, Train Acc: 0.4588\n",
      "Epoch: 48, Batch: 400, loss: 2.8370 , Train PPL: 1.0043, Train Acc: 0.5442\n",
      "Epoch: 48, Batch: 500, loss: 3.1264 , Train PPL: 1.0048, Train Acc: 0.4756\n",
      "Epoch: 48, Batch: 600, loss: 3.3330 , Train PPL: 1.0051, Train Acc: 0.4421\n",
      "Epoch: 48, Batch: 700, loss: 3.0826 , Train PPL: 1.0047, Train Acc: 0.4802\n",
      "Validation --- Epoch: 48, total loss: 320.1877 , PPL: 1.3014, Acc: 0.4008\n",
      "lr = 0.0625\n",
      "Epoch: 49, Batch: 100, loss: 2.7223 , Train PPL: 1.0042, Train Acc: 0.6357\n",
      "Epoch: 49, Batch: 200, loss: 3.2037 , Train PPL: 1.0049, Train Acc: 0.5107\n",
      "Epoch: 49, Batch: 300, loss: 3.0471 , Train PPL: 1.0047, Train Acc: 0.4924\n",
      "Epoch: 49, Batch: 400, loss: 2.9887 , Train PPL: 1.0046, Train Acc: 0.5229\n",
      "Epoch: 49, Batch: 500, loss: 3.3296 , Train PPL: 1.0051, Train Acc: 0.4375\n",
      "Epoch: 49, Batch: 600, loss: 2.9482 , Train PPL: 1.0045, Train Acc: 0.5015\n",
      "Epoch: 49, Batch: 700, loss: 3.0858 , Train PPL: 1.0047, Train Acc: 0.4756\n",
      "Validation --- Epoch: 49, total loss: 320.4267 , PPL: 1.3017, Acc: 0.4013\n",
      "lr = 0.0625\n",
      "Epoch: 50, Batch: 100, loss: 2.8704 , Train PPL: 1.0044, Train Acc: 0.5808\n",
      "Epoch: 50, Batch: 200, loss: 3.1004 , Train PPL: 1.0047, Train Acc: 0.5030\n",
      "Epoch: 50, Batch: 300, loss: 3.0704 , Train PPL: 1.0047, Train Acc: 0.4985\n",
      "Epoch: 50, Batch: 400, loss: 2.9109 , Train PPL: 1.0044, Train Acc: 0.5366\n",
      "Epoch: 50, Batch: 500, loss: 3.0141 , Train PPL: 1.0046, Train Acc: 0.4954\n",
      "Epoch: 50, Batch: 600, loss: 2.8882 , Train PPL: 1.0044, Train Acc: 0.5213\n",
      "Epoch: 50, Batch: 700, loss: 3.0736 , Train PPL: 1.0047, Train Acc: 0.5152\n",
      "Validation --- Epoch: 50, total loss: 320.5437 , PPL: 1.3018, Acc: 0.4016\n",
      "lr = 0.0625\n",
      "Epoch: 51, Batch: 100, loss: 3.3415 , Train PPL: 1.0051, Train Acc: 0.3979\n",
      "Epoch: 51, Batch: 200, loss: 2.9981 , Train PPL: 1.0046, Train Acc: 0.5381\n",
      "Epoch: 51, Batch: 300, loss: 3.1258 , Train PPL: 1.0048, Train Acc: 0.4726\n",
      "Epoch: 51, Batch: 400, loss: 3.2240 , Train PPL: 1.0049, Train Acc: 0.4909\n",
      "Epoch: 51, Batch: 500, loss: 3.0886 , Train PPL: 1.0047, Train Acc: 0.5198\n",
      "Epoch: 51, Batch: 600, loss: 2.5967 , Train PPL: 1.0040, Train Acc: 0.6936\n",
      "Epoch: 51, Batch: 700, loss: 2.9983 , Train PPL: 1.0046, Train Acc: 0.5107\n",
      "Validation --- Epoch: 51, total loss: 320.4845 , PPL: 1.3017, Acc: 0.4012\n",
      "lr = 0.03125\n",
      "Epoch: 52, Batch: 100, loss: 3.2123 , Train PPL: 1.0049, Train Acc: 0.4802\n",
      "Epoch: 52, Batch: 200, loss: 3.1385 , Train PPL: 1.0048, Train Acc: 0.5122\n",
      "Epoch: 52, Batch: 300, loss: 2.7893 , Train PPL: 1.0043, Train Acc: 0.5976\n",
      "Epoch: 52, Batch: 400, loss: 3.1725 , Train PPL: 1.0048, Train Acc: 0.4863\n",
      "Epoch: 52, Batch: 500, loss: 3.1226 , Train PPL: 1.0048, Train Acc: 0.4863\n",
      "Epoch: 52, Batch: 600, loss: 3.0463 , Train PPL: 1.0047, Train Acc: 0.5137\n",
      "Epoch: 52, Batch: 700, loss: 3.0625 , Train PPL: 1.0047, Train Acc: 0.5198\n",
      "Validation --- Epoch: 52, total loss: 320.5258 , PPL: 1.3018, Acc: 0.4013\n",
      "lr = 0.03125\n",
      "Epoch: 53, Batch: 100, loss: 3.2968 , Train PPL: 1.0050, Train Acc: 0.4329\n",
      "Epoch: 53, Batch: 200, loss: 3.0843 , Train PPL: 1.0047, Train Acc: 0.4817\n",
      "Epoch: 53, Batch: 300, loss: 3.0287 , Train PPL: 1.0046, Train Acc: 0.4954\n",
      "Epoch: 53, Batch: 400, loss: 2.9763 , Train PPL: 1.0045, Train Acc: 0.5076\n",
      "Epoch: 53, Batch: 500, loss: 2.9566 , Train PPL: 1.0045, Train Acc: 0.4985\n",
      "Epoch: 53, Batch: 600, loss: 3.0389 , Train PPL: 1.0046, Train Acc: 0.5290\n",
      "Epoch: 53, Batch: 700, loss: 3.2820 , Train PPL: 1.0050, Train Acc: 0.4238\n",
      "Validation --- Epoch: 53, total loss: 320.5827 , PPL: 1.3018, Acc: 0.4013\n",
      "lr = 0.03125\n",
      "Epoch: 54, Batch: 100, loss: 2.7403 , Train PPL: 1.0042, Train Acc: 0.5366\n",
      "Epoch: 54, Batch: 200, loss: 3.0303 , Train PPL: 1.0046, Train Acc: 0.4954\n",
      "Epoch: 54, Batch: 300, loss: 2.8069 , Train PPL: 1.0043, Train Acc: 0.5534\n",
      "Epoch: 54, Batch: 400, loss: 3.0787 , Train PPL: 1.0047, Train Acc: 0.4924\n",
      "Epoch: 54, Batch: 500, loss: 3.0125 , Train PPL: 1.0046, Train Acc: 0.5244\n",
      "Epoch: 54, Batch: 600, loss: 2.9460 , Train PPL: 1.0045, Train Acc: 0.5381\n",
      "Epoch: 54, Batch: 700, loss: 2.9525 , Train PPL: 1.0045, Train Acc: 0.5442\n",
      "Validation --- Epoch: 54, total loss: 320.5640 , PPL: 1.3018, Acc: 0.4014\n",
      "lr = 0.03125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55, Batch: 100, loss: 3.0350 , Train PPL: 1.0046, Train Acc: 0.4970\n",
      "Epoch: 55, Batch: 200, loss: 2.9461 , Train PPL: 1.0045, Train Acc: 0.5320\n",
      "Epoch: 55, Batch: 300, loss: 3.0697 , Train PPL: 1.0047, Train Acc: 0.4924\n",
      "Epoch: 55, Batch: 400, loss: 3.0480 , Train PPL: 1.0047, Train Acc: 0.5305\n",
      "Epoch: 55, Batch: 500, loss: 3.1348 , Train PPL: 1.0048, Train Acc: 0.4527\n",
      "Epoch: 55, Batch: 600, loss: 2.9566 , Train PPL: 1.0045, Train Acc: 0.5274\n",
      "Epoch: 55, Batch: 700, loss: 2.9976 , Train PPL: 1.0046, Train Acc: 0.5015\n",
      "Validation --- Epoch: 55, total loss: 320.7146 , PPL: 1.3020, Acc: 0.4012\n",
      "lr = 0.03125\n",
      "Epoch: 56, Batch: 100, loss: 3.1057 , Train PPL: 1.0047, Train Acc: 0.4939\n",
      "Epoch: 56, Batch: 200, loss: 3.2583 , Train PPL: 1.0050, Train Acc: 0.5183\n",
      "Epoch: 56, Batch: 300, loss: 3.0577 , Train PPL: 1.0047, Train Acc: 0.5442\n",
      "Epoch: 56, Batch: 400, loss: 2.8961 , Train PPL: 1.0044, Train Acc: 0.5244\n",
      "Epoch: 56, Batch: 500, loss: 3.1420 , Train PPL: 1.0048, Train Acc: 0.5229\n",
      "Epoch: 56, Batch: 600, loss: 3.1762 , Train PPL: 1.0049, Train Acc: 0.5122\n",
      "Epoch: 56, Batch: 700, loss: 2.9870 , Train PPL: 1.0046, Train Acc: 0.4970\n",
      "Validation --- Epoch: 56, total loss: 320.7018 , PPL: 1.3020, Acc: 0.4013\n",
      "lr = 0.03125\n",
      "Epoch: 57, Batch: 100, loss: 3.3574 , Train PPL: 1.0051, Train Acc: 0.4466\n",
      "Epoch: 57, Batch: 200, loss: 3.2429 , Train PPL: 1.0050, Train Acc: 0.4329\n",
      "Epoch: 57, Batch: 300, loss: 3.5135 , Train PPL: 1.0054, Train Acc: 0.4405\n",
      "Epoch: 57, Batch: 400, loss: 3.3772 , Train PPL: 1.0052, Train Acc: 0.4924\n",
      "Epoch: 57, Batch: 500, loss: 3.0147 , Train PPL: 1.0046, Train Acc: 0.5107\n",
      "Epoch: 57, Batch: 600, loss: 2.6088 , Train PPL: 1.0040, Train Acc: 0.6448\n",
      "Epoch: 57, Batch: 700, loss: 3.1871 , Train PPL: 1.0049, Train Acc: 0.5137\n",
      "Validation --- Epoch: 57, total loss: 320.7117 , PPL: 1.3020, Acc: 0.4007\n",
      "lr = 0.015625\n",
      "Epoch: 58, Batch: 100, loss: 3.2666 , Train PPL: 1.0050, Train Acc: 0.4314\n",
      "Epoch: 58, Batch: 200, loss: 3.1048 , Train PPL: 1.0047, Train Acc: 0.4527\n",
      "Epoch: 58, Batch: 300, loss: 3.2220 , Train PPL: 1.0049, Train Acc: 0.4680\n",
      "Epoch: 58, Batch: 400, loss: 3.1332 , Train PPL: 1.0048, Train Acc: 0.4832\n",
      "Epoch: 58, Batch: 500, loss: 2.7433 , Train PPL: 1.0042, Train Acc: 0.5381\n",
      "Epoch: 58, Batch: 600, loss: 2.7381 , Train PPL: 1.0042, Train Acc: 0.6433\n",
      "Epoch: 58, Batch: 700, loss: 3.1267 , Train PPL: 1.0048, Train Acc: 0.4771\n",
      "Validation --- Epoch: 58, total loss: 320.7544 , PPL: 1.3020, Acc: 0.4010\n",
      "lr = 0.015625\n",
      "Epoch: 59, Batch: 100, loss: 3.1380 , Train PPL: 1.0048, Train Acc: 0.4848\n",
      "Epoch: 59, Batch: 200, loss: 2.9998 , Train PPL: 1.0046, Train Acc: 0.5671\n",
      "Epoch: 59, Batch: 300, loss: 3.0575 , Train PPL: 1.0047, Train Acc: 0.5000\n",
      "Epoch: 59, Batch: 400, loss: 2.8287 , Train PPL: 1.0043, Train Acc: 0.5777\n",
      "Epoch: 59, Batch: 500, loss: 3.2234 , Train PPL: 1.0049, Train Acc: 0.5107\n",
      "Epoch: 59, Batch: 600, loss: 2.8243 , Train PPL: 1.0043, Train Acc: 0.5610\n",
      "Epoch: 59, Batch: 700, loss: 2.8310 , Train PPL: 1.0043, Train Acc: 0.5122\n",
      "Validation --- Epoch: 59, total loss: 320.8006 , PPL: 1.3021, Acc: 0.4008\n",
      "lr = 0.015625\n",
      "Epoch: 60, Batch: 100, loss: 2.7616 , Train PPL: 1.0042, Train Acc: 0.6082\n",
      "Epoch: 60, Batch: 200, loss: 3.1650 , Train PPL: 1.0048, Train Acc: 0.5046\n",
      "Epoch: 60, Batch: 300, loss: 2.9944 , Train PPL: 1.0046, Train Acc: 0.5229\n",
      "Epoch: 60, Batch: 400, loss: 3.1749 , Train PPL: 1.0049, Train Acc: 0.4405\n",
      "Epoch: 60, Batch: 500, loss: 3.2538 , Train PPL: 1.0050, Train Acc: 0.4695\n",
      "Epoch: 60, Batch: 600, loss: 3.0116 , Train PPL: 1.0046, Train Acc: 0.5579\n",
      "Epoch: 60, Batch: 700, loss: 3.0162 , Train PPL: 1.0046, Train Acc: 0.4710\n",
      "Validation --- Epoch: 60, total loss: 320.8425 , PPL: 1.3021, Acc: 0.4008\n",
      "lr = 0.015625\n",
      "Epoch: 61, Batch: 100, loss: 3.0088 , Train PPL: 1.0046, Train Acc: 0.5213\n",
      "Epoch: 61, Batch: 200, loss: 3.1575 , Train PPL: 1.0048, Train Acc: 0.4848\n",
      "Epoch: 61, Batch: 300, loss: 2.7308 , Train PPL: 1.0042, Train Acc: 0.5915\n",
      "Epoch: 61, Batch: 400, loss: 3.2142 , Train PPL: 1.0049, Train Acc: 0.4527\n",
      "Epoch: 61, Batch: 500, loss: 3.1498 , Train PPL: 1.0048, Train Acc: 0.4512\n",
      "Epoch: 61, Batch: 600, loss: 2.9894 , Train PPL: 1.0046, Train Acc: 0.5091\n",
      "Epoch: 61, Batch: 700, loss: 3.0507 , Train PPL: 1.0047, Train Acc: 0.5000\n",
      "Validation --- Epoch: 61, total loss: 320.8579 , PPL: 1.3021, Acc: 0.4010\n",
      "lr = 0.015625\n",
      "Epoch: 62, Batch: 100, loss: 2.6617 , Train PPL: 1.0041, Train Acc: 0.5777\n",
      "Epoch: 62, Batch: 200, loss: 2.6569 , Train PPL: 1.0041, Train Acc: 0.6723\n",
      "Epoch: 62, Batch: 300, loss: 2.9857 , Train PPL: 1.0046, Train Acc: 0.4893\n",
      "Epoch: 62, Batch: 400, loss: 3.2448 , Train PPL: 1.0050, Train Acc: 0.4451\n",
      "Epoch: 62, Batch: 500, loss: 3.3166 , Train PPL: 1.0051, Train Acc: 0.4558\n",
      "Epoch: 62, Batch: 600, loss: 3.1444 , Train PPL: 1.0048, Train Acc: 0.4771\n",
      "Epoch: 62, Batch: 700, loss: 3.1124 , Train PPL: 1.0048, Train Acc: 0.5320\n",
      "Validation --- Epoch: 62, total loss: 320.8336 , PPL: 1.3021, Acc: 0.4008\n",
      "lr = 0.015625\n",
      "Epoch: 63, Batch: 100, loss: 2.9230 , Train PPL: 1.0045, Train Acc: 0.5122\n",
      "Epoch: 63, Batch: 200, loss: 3.1317 , Train PPL: 1.0048, Train Acc: 0.4863\n",
      "Epoch: 63, Batch: 300, loss: 2.8056 , Train PPL: 1.0043, Train Acc: 0.6265\n",
      "Epoch: 63, Batch: 400, loss: 3.1596 , Train PPL: 1.0048, Train Acc: 0.4573\n",
      "Epoch: 63, Batch: 500, loss: 2.6253 , Train PPL: 1.0040, Train Acc: 0.5716\n",
      "Epoch: 63, Batch: 600, loss: 3.0454 , Train PPL: 1.0047, Train Acc: 0.4771\n",
      "Epoch: 63, Batch: 700, loss: 2.8381 , Train PPL: 1.0043, Train Acc: 0.5518\n",
      "Validation --- Epoch: 63, total loss: 320.8780 , PPL: 1.3022, Acc: 0.4010\n",
      "lr = 0.0078125\n",
      "Epoch: 64, Batch: 100, loss: 2.9963 , Train PPL: 1.0046, Train Acc: 0.5595\n",
      "Epoch: 64, Batch: 200, loss: 3.0508 , Train PPL: 1.0047, Train Acc: 0.4939\n",
      "Epoch: 64, Batch: 300, loss: 2.9514 , Train PPL: 1.0045, Train Acc: 0.5335\n",
      "Epoch: 64, Batch: 400, loss: 3.0687 , Train PPL: 1.0047, Train Acc: 0.5351\n",
      "Epoch: 64, Batch: 500, loss: 3.0347 , Train PPL: 1.0046, Train Acc: 0.5061\n",
      "Epoch: 64, Batch: 600, loss: 3.1548 , Train PPL: 1.0048, Train Acc: 0.4680\n",
      "Epoch: 64, Batch: 700, loss: 2.9652 , Train PPL: 1.0045, Train Acc: 0.5412\n",
      "Validation --- Epoch: 64, total loss: 320.9188 , PPL: 1.3022, Acc: 0.4009\n",
      "lr = 0.0078125\n",
      "Epoch: 65, Batch: 100, loss: 3.0384 , Train PPL: 1.0046, Train Acc: 0.4970\n",
      "Epoch: 65, Batch: 200, loss: 3.1276 , Train PPL: 1.0048, Train Acc: 0.4649\n",
      "Epoch: 65, Batch: 300, loss: 2.9310 , Train PPL: 1.0045, Train Acc: 0.5518\n",
      "Epoch: 65, Batch: 400, loss: 3.1683 , Train PPL: 1.0048, Train Acc: 0.4909\n",
      "Epoch: 65, Batch: 500, loss: 2.8242 , Train PPL: 1.0043, Train Acc: 0.5381\n",
      "Epoch: 65, Batch: 600, loss: 3.4050 , Train PPL: 1.0052, Train Acc: 0.4284\n",
      "Epoch: 65, Batch: 700, loss: 3.0863 , Train PPL: 1.0047, Train Acc: 0.4543\n",
      "Validation --- Epoch: 65, total loss: 320.9409 , PPL: 1.3022, Acc: 0.4011\n",
      "lr = 0.0078125\n",
      "Epoch: 66, Batch: 100, loss: 3.3155 , Train PPL: 1.0051, Train Acc: 0.4451\n",
      "Epoch: 66, Batch: 200, loss: 2.7421 , Train PPL: 1.0042, Train Acc: 0.5930\n",
      "Epoch: 66, Batch: 300, loss: 2.9958 , Train PPL: 1.0046, Train Acc: 0.4787\n",
      "Epoch: 66, Batch: 400, loss: 3.1203 , Train PPL: 1.0048, Train Acc: 0.4893\n",
      "Epoch: 66, Batch: 500, loss: 3.3718 , Train PPL: 1.0052, Train Acc: 0.4665\n",
      "Epoch: 66, Batch: 600, loss: 2.9457 , Train PPL: 1.0045, Train Acc: 0.5229\n",
      "Epoch: 66, Batch: 700, loss: 3.2795 , Train PPL: 1.0050, Train Acc: 0.4695\n",
      "Validation --- Epoch: 66, total loss: 320.9267 , PPL: 1.3022, Acc: 0.4011\n",
      "lr = 0.0078125\n",
      "Epoch: 67, Batch: 100, loss: 3.0268 , Train PPL: 1.0046, Train Acc: 0.5396\n",
      "Epoch: 67, Batch: 200, loss: 3.1925 , Train PPL: 1.0049, Train Acc: 0.4543\n",
      "Epoch: 67, Batch: 300, loss: 3.1950 , Train PPL: 1.0049, Train Acc: 0.4451\n",
      "Epoch: 67, Batch: 400, loss: 3.1154 , Train PPL: 1.0048, Train Acc: 0.4756\n",
      "Epoch: 67, Batch: 500, loss: 2.5448 , Train PPL: 1.0039, Train Acc: 0.6418\n",
      "Epoch: 67, Batch: 600, loss: 2.8832 , Train PPL: 1.0044, Train Acc: 0.5777\n",
      "Epoch: 67, Batch: 700, loss: 3.2737 , Train PPL: 1.0050, Train Acc: 0.4619\n",
      "Validation --- Epoch: 67, total loss: 320.9329 , PPL: 1.3022, Acc: 0.4009\n",
      "lr = 0.0078125\n",
      "Epoch: 68, Batch: 100, loss: 2.8413 , Train PPL: 1.0043, Train Acc: 0.6052\n",
      "Epoch: 68, Batch: 200, loss: 3.3325 , Train PPL: 1.0051, Train Acc: 0.4207\n",
      "Epoch: 68, Batch: 300, loss: 2.8471 , Train PPL: 1.0043, Train Acc: 0.5610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68, Batch: 400, loss: 3.0297 , Train PPL: 1.0046, Train Acc: 0.4878\n",
      "Epoch: 68, Batch: 500, loss: 3.2832 , Train PPL: 1.0050, Train Acc: 0.4390\n",
      "Epoch: 68, Batch: 600, loss: 2.6709 , Train PPL: 1.0041, Train Acc: 0.6128\n",
      "Epoch: 68, Batch: 700, loss: 3.1151 , Train PPL: 1.0048, Train Acc: 0.5091\n",
      "Validation --- Epoch: 68, total loss: 320.9619 , PPL: 1.3023, Acc: 0.4009\n",
      "lr = 0.0078125\n",
      "Epoch: 69, Batch: 100, loss: 3.1044 , Train PPL: 1.0047, Train Acc: 0.5122\n",
      "Epoch: 69, Batch: 200, loss: 2.9397 , Train PPL: 1.0045, Train Acc: 0.5381\n",
      "Epoch: 69, Batch: 300, loss: 3.3394 , Train PPL: 1.0051, Train Acc: 0.4101\n",
      "Epoch: 69, Batch: 400, loss: 3.1484 , Train PPL: 1.0048, Train Acc: 0.5320\n",
      "Epoch: 69, Batch: 500, loss: 2.8130 , Train PPL: 1.0043, Train Acc: 0.5457\n",
      "Epoch: 69, Batch: 600, loss: 3.0787 , Train PPL: 1.0047, Train Acc: 0.4588\n",
      "Epoch: 69, Batch: 700, loss: 3.1702 , Train PPL: 1.0048, Train Acc: 0.4680\n",
      "Validation --- Epoch: 69, total loss: 320.9609 , PPL: 1.3023, Acc: 0.4006\n",
      "lr = 0.00390625\n",
      "Epoch: 70, Batch: 100, loss: 3.2498 , Train PPL: 1.0050, Train Acc: 0.4497\n",
      "Epoch: 70, Batch: 200, loss: 3.2465 , Train PPL: 1.0050, Train Acc: 0.4253\n",
      "Epoch: 70, Batch: 300, loss: 2.9942 , Train PPL: 1.0046, Train Acc: 0.5503\n",
      "Epoch: 70, Batch: 400, loss: 3.4527 , Train PPL: 1.0053, Train Acc: 0.4040\n",
      "Epoch: 70, Batch: 500, loss: 3.2959 , Train PPL: 1.0050, Train Acc: 0.4101\n",
      "Epoch: 70, Batch: 600, loss: 2.8736 , Train PPL: 1.0044, Train Acc: 0.5229\n",
      "Epoch: 70, Batch: 700, loss: 2.9579 , Train PPL: 1.0045, Train Acc: 0.5564\n",
      "Validation --- Epoch: 70, total loss: 320.9483 , PPL: 1.3022, Acc: 0.4009\n",
      "lr = 0.00390625\n",
      "Epoch: 71, Batch: 100, loss: 3.0961 , Train PPL: 1.0047, Train Acc: 0.4924\n",
      "Epoch: 71, Batch: 200, loss: 3.1435 , Train PPL: 1.0048, Train Acc: 0.5168\n",
      "Epoch: 71, Batch: 300, loss: 3.0869 , Train PPL: 1.0047, Train Acc: 0.5320\n",
      "Epoch: 71, Batch: 400, loss: 3.2574 , Train PPL: 1.0050, Train Acc: 0.4543\n",
      "Epoch: 71, Batch: 500, loss: 3.2600 , Train PPL: 1.0050, Train Acc: 0.4893\n",
      "Epoch: 71, Batch: 600, loss: 3.0602 , Train PPL: 1.0047, Train Acc: 0.5107\n",
      "Epoch: 71, Batch: 700, loss: 2.9950 , Train PPL: 1.0046, Train Acc: 0.5899\n",
      "Validation --- Epoch: 71, total loss: 320.9640 , PPL: 1.3023, Acc: 0.4008\n",
      "lr = 0.00390625\n",
      "Epoch: 72, Batch: 100, loss: 3.0492 , Train PPL: 1.0047, Train Acc: 0.4604\n",
      "Epoch: 72, Batch: 200, loss: 3.1893 , Train PPL: 1.0049, Train Acc: 0.4680\n",
      "Epoch: 72, Batch: 300, loss: 2.8172 , Train PPL: 1.0043, Train Acc: 0.5396\n",
      "Epoch: 72, Batch: 400, loss: 3.0229 , Train PPL: 1.0046, Train Acc: 0.4939\n",
      "Epoch: 72, Batch: 500, loss: 3.0709 , Train PPL: 1.0047, Train Acc: 0.4939\n",
      "Epoch: 72, Batch: 600, loss: 2.9924 , Train PPL: 1.0046, Train Acc: 0.4893\n",
      "Epoch: 72, Batch: 700, loss: 3.1318 , Train PPL: 1.0048, Train Acc: 0.4649\n",
      "Validation --- Epoch: 72, total loss: 320.9790 , PPL: 1.3023, Acc: 0.4007\n",
      "lr = 0.00390625\n",
      "Epoch: 73, Batch: 100, loss: 3.1067 , Train PPL: 1.0047, Train Acc: 0.4939\n",
      "Epoch: 73, Batch: 200, loss: 3.0502 , Train PPL: 1.0047, Train Acc: 0.4970\n",
      "Epoch: 73, Batch: 300, loss: 2.7902 , Train PPL: 1.0043, Train Acc: 0.5686\n",
      "Epoch: 73, Batch: 400, loss: 3.0072 , Train PPL: 1.0046, Train Acc: 0.4878\n",
      "Epoch: 73, Batch: 500, loss: 3.4168 , Train PPL: 1.0052, Train Acc: 0.4329\n",
      "Epoch: 73, Batch: 600, loss: 3.1190 , Train PPL: 1.0048, Train Acc: 0.4863\n",
      "Epoch: 73, Batch: 700, loss: 2.7598 , Train PPL: 1.0042, Train Acc: 0.5838\n",
      "Validation --- Epoch: 73, total loss: 320.9859 , PPL: 1.3023, Acc: 0.4008\n",
      "lr = 0.00390625\n",
      "Epoch: 74, Batch: 100, loss: 3.1651 , Train PPL: 1.0048, Train Acc: 0.5290\n",
      "Epoch: 74, Batch: 200, loss: 2.8935 , Train PPL: 1.0044, Train Acc: 0.5229\n",
      "Epoch: 74, Batch: 300, loss: 3.0309 , Train PPL: 1.0046, Train Acc: 0.4710\n",
      "Epoch: 74, Batch: 400, loss: 2.9458 , Train PPL: 1.0045, Train Acc: 0.5290\n",
      "Epoch: 74, Batch: 500, loss: 2.7838 , Train PPL: 1.0043, Train Acc: 0.5991\n",
      "Epoch: 74, Batch: 600, loss: 3.0921 , Train PPL: 1.0047, Train Acc: 0.5046\n",
      "Epoch: 74, Batch: 700, loss: 3.0555 , Train PPL: 1.0047, Train Acc: 0.5305\n",
      "Validation --- Epoch: 74, total loss: 320.9779 , PPL: 1.3023, Acc: 0.4007\n",
      "lr = 0.00390625\n",
      "Epoch: 75, Batch: 100, loss: 3.1724 , Train PPL: 1.0048, Train Acc: 0.5015\n",
      "Epoch: 75, Batch: 200, loss: 3.0979 , Train PPL: 1.0047, Train Acc: 0.4726\n",
      "Epoch: 75, Batch: 300, loss: 3.1788 , Train PPL: 1.0049, Train Acc: 0.4634\n",
      "Epoch: 75, Batch: 400, loss: 2.9355 , Train PPL: 1.0045, Train Acc: 0.5198\n",
      "Epoch: 75, Batch: 500, loss: 2.9882 , Train PPL: 1.0046, Train Acc: 0.5061\n",
      "Epoch: 75, Batch: 600, loss: 3.0641 , Train PPL: 1.0047, Train Acc: 0.5274\n",
      "Epoch: 75, Batch: 700, loss: 3.0957 , Train PPL: 1.0047, Train Acc: 0.5015\n",
      "Validation --- Epoch: 75, total loss: 320.9898 , PPL: 1.3023, Acc: 0.4008\n",
      "lr = 0.001953125\n",
      "Epoch: 76, Batch: 100, loss: 2.9080 , Train PPL: 1.0044, Train Acc: 0.5640\n",
      "Epoch: 76, Batch: 200, loss: 3.0668 , Train PPL: 1.0047, Train Acc: 0.4726\n",
      "Epoch: 76, Batch: 300, loss: 3.0342 , Train PPL: 1.0046, Train Acc: 0.5320\n",
      "Epoch: 76, Batch: 400, loss: 2.9626 , Train PPL: 1.0045, Train Acc: 0.5000\n",
      "Epoch: 76, Batch: 500, loss: 3.0485 , Train PPL: 1.0047, Train Acc: 0.5168\n",
      "Epoch: 76, Batch: 600, loss: 3.3068 , Train PPL: 1.0051, Train Acc: 0.4512\n",
      "Epoch: 76, Batch: 700, loss: 3.2481 , Train PPL: 1.0050, Train Acc: 0.4375\n",
      "Validation --- Epoch: 76, total loss: 320.9809 , PPL: 1.3023, Acc: 0.4008\n",
      "lr = 0.001953125\n",
      "Epoch: 77, Batch: 100, loss: 3.2046 , Train PPL: 1.0049, Train Acc: 0.5046\n",
      "Epoch: 77, Batch: 200, loss: 2.9894 , Train PPL: 1.0046, Train Acc: 0.5076\n",
      "Epoch: 77, Batch: 300, loss: 3.1964 , Train PPL: 1.0049, Train Acc: 0.4314\n",
      "Epoch: 77, Batch: 400, loss: 2.6459 , Train PPL: 1.0040, Train Acc: 0.6646\n",
      "Epoch: 77, Batch: 500, loss: 3.0402 , Train PPL: 1.0046, Train Acc: 0.4482\n",
      "Epoch: 77, Batch: 600, loss: 3.2022 , Train PPL: 1.0049, Train Acc: 0.5061\n",
      "Epoch: 77, Batch: 700, loss: 2.9671 , Train PPL: 1.0045, Train Acc: 0.5518\n",
      "Validation --- Epoch: 77, total loss: 320.9866 , PPL: 1.3023, Acc: 0.4009\n",
      "lr = 0.001953125\n",
      "Epoch: 78, Batch: 100, loss: 3.2182 , Train PPL: 1.0049, Train Acc: 0.4802\n",
      "Epoch: 78, Batch: 200, loss: 2.9475 , Train PPL: 1.0045, Train Acc: 0.5290\n",
      "Epoch: 78, Batch: 300, loss: 2.6091 , Train PPL: 1.0040, Train Acc: 0.6662\n",
      "Epoch: 78, Batch: 400, loss: 2.9581 , Train PPL: 1.0045, Train Acc: 0.6098\n",
      "Epoch: 78, Batch: 500, loss: 3.0521 , Train PPL: 1.0047, Train Acc: 0.4985\n",
      "Epoch: 78, Batch: 600, loss: 3.5763 , Train PPL: 1.0055, Train Acc: 0.3811\n",
      "Epoch: 78, Batch: 700, loss: 3.1237 , Train PPL: 1.0048, Train Acc: 0.4970\n",
      "Validation --- Epoch: 78, total loss: 320.9860 , PPL: 1.3023, Acc: 0.4009\n",
      "lr = 0.001953125\n",
      "Epoch: 79, Batch: 100, loss: 2.9948 , Train PPL: 1.0046, Train Acc: 0.5229\n",
      "Epoch: 79, Batch: 200, loss: 3.2432 , Train PPL: 1.0050, Train Acc: 0.4497\n",
      "Epoch: 79, Batch: 300, loss: 3.3099 , Train PPL: 1.0051, Train Acc: 0.4787\n",
      "Epoch: 79, Batch: 400, loss: 3.1967 , Train PPL: 1.0049, Train Acc: 0.4634\n",
      "Epoch: 79, Batch: 500, loss: 2.8440 , Train PPL: 1.0043, Train Acc: 0.5488\n",
      "Epoch: 79, Batch: 600, loss: 2.8052 , Train PPL: 1.0043, Train Acc: 0.5290\n",
      "Epoch: 79, Batch: 700, loss: 3.1234 , Train PPL: 1.0048, Train Acc: 0.4893\n",
      "Validation --- Epoch: 79, total loss: 320.9876 , PPL: 1.3023, Acc: 0.4007\n",
      "lr = 0.001953125\n",
      "Epoch: 80, Batch: 100, loss: 3.1956 , Train PPL: 1.0049, Train Acc: 0.4466\n",
      "Epoch: 80, Batch: 200, loss: 3.0080 , Train PPL: 1.0046, Train Acc: 0.5595\n",
      "Epoch: 80, Batch: 300, loss: 3.0378 , Train PPL: 1.0046, Train Acc: 0.5351\n",
      "Epoch: 80, Batch: 400, loss: 3.2480 , Train PPL: 1.0050, Train Acc: 0.4314\n",
      "Epoch: 80, Batch: 500, loss: 2.9111 , Train PPL: 1.0044, Train Acc: 0.5244\n",
      "Epoch: 80, Batch: 600, loss: 3.1165 , Train PPL: 1.0048, Train Acc: 0.4665\n",
      "Epoch: 80, Batch: 700, loss: 3.3212 , Train PPL: 1.0051, Train Acc: 0.4375\n",
      "Validation --- Epoch: 80, total loss: 320.9862 , PPL: 1.3023, Acc: 0.4008\n",
      "lr = 0.001953125\n",
      "Epoch: 81, Batch: 100, loss: 3.1114 , Train PPL: 1.0048, Train Acc: 0.5046\n",
      "Epoch: 81, Batch: 200, loss: 2.9865 , Train PPL: 1.0046, Train Acc: 0.4848\n",
      "Epoch: 81, Batch: 300, loss: 3.0166 , Train PPL: 1.0046, Train Acc: 0.4665\n",
      "Epoch: 81, Batch: 400, loss: 2.9369 , Train PPL: 1.0045, Train Acc: 0.5854\n",
      "Epoch: 81, Batch: 500, loss: 3.0082 , Train PPL: 1.0046, Train Acc: 0.4527\n",
      "Epoch: 81, Batch: 600, loss: 2.6061 , Train PPL: 1.0040, Train Acc: 0.6433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81, Batch: 700, loss: 3.2402 , Train PPL: 1.0050, Train Acc: 0.4756\n",
      "Validation --- Epoch: 81, total loss: 320.9893 , PPL: 1.3023, Acc: 0.4008\n",
      "lr = 0.0009765625\n",
      "Epoch: 82, Batch: 100, loss: 2.9307 , Train PPL: 1.0045, Train Acc: 0.5046\n",
      "Epoch: 82, Batch: 200, loss: 3.1143 , Train PPL: 1.0048, Train Acc: 0.5290\n",
      "Epoch: 82, Batch: 300, loss: 3.1432 , Train PPL: 1.0048, Train Acc: 0.4421\n",
      "Epoch: 82, Batch: 400, loss: 2.6829 , Train PPL: 1.0041, Train Acc: 0.6189\n",
      "Epoch: 82, Batch: 500, loss: 3.1873 , Train PPL: 1.0049, Train Acc: 0.4741\n",
      "Epoch: 82, Batch: 600, loss: 3.0647 , Train PPL: 1.0047, Train Acc: 0.4802\n",
      "Epoch: 82, Batch: 700, loss: 3.0204 , Train PPL: 1.0046, Train Acc: 0.5137\n",
      "Validation --- Epoch: 82, total loss: 320.9922 , PPL: 1.3023, Acc: 0.4009\n",
      "lr = 0.0009765625\n",
      "Epoch: 83, Batch: 100, loss: 2.9823 , Train PPL: 1.0046, Train Acc: 0.5091\n",
      "Epoch: 83, Batch: 200, loss: 3.0884 , Train PPL: 1.0047, Train Acc: 0.4939\n",
      "Epoch: 83, Batch: 300, loss: 3.1238 , Train PPL: 1.0048, Train Acc: 0.5091\n",
      "Epoch: 83, Batch: 400, loss: 3.2608 , Train PPL: 1.0050, Train Acc: 0.4619\n",
      "Epoch: 83, Batch: 500, loss: 3.0923 , Train PPL: 1.0047, Train Acc: 0.4207\n",
      "Epoch: 83, Batch: 600, loss: 3.1354 , Train PPL: 1.0048, Train Acc: 0.4665\n",
      "Epoch: 83, Batch: 700, loss: 2.8919 , Train PPL: 1.0044, Train Acc: 0.5412\n",
      "Validation --- Epoch: 83, total loss: 320.9954 , PPL: 1.3023, Acc: 0.4009\n",
      "lr = 0.0009765625\n",
      "Epoch: 84, Batch: 100, loss: 2.9832 , Train PPL: 1.0046, Train Acc: 0.5518\n",
      "Epoch: 84, Batch: 200, loss: 3.0177 , Train PPL: 1.0046, Train Acc: 0.5534\n",
      "Epoch: 84, Batch: 300, loss: 2.9629 , Train PPL: 1.0045, Train Acc: 0.5762\n",
      "Epoch: 84, Batch: 400, loss: 2.7086 , Train PPL: 1.0041, Train Acc: 0.6189\n",
      "Epoch: 84, Batch: 500, loss: 2.9383 , Train PPL: 1.0045, Train Acc: 0.5046\n",
      "Epoch: 84, Batch: 600, loss: 3.0720 , Train PPL: 1.0047, Train Acc: 0.4893\n",
      "Epoch: 84, Batch: 700, loss: 3.2844 , Train PPL: 1.0050, Train Acc: 0.4345\n",
      "Validation --- Epoch: 84, total loss: 321.0006 , PPL: 1.3023, Acc: 0.4009\n",
      "lr = 0.0009765625\n",
      "Epoch: 85, Batch: 100, loss: 2.9980 , Train PPL: 1.0046, Train Acc: 0.5137\n",
      "Epoch: 85, Batch: 200, loss: 3.0412 , Train PPL: 1.0046, Train Acc: 0.4832\n",
      "Epoch: 85, Batch: 300, loss: 3.0605 , Train PPL: 1.0047, Train Acc: 0.5076\n",
      "Epoch: 85, Batch: 400, loss: 2.8152 , Train PPL: 1.0043, Train Acc: 0.5808\n",
      "Epoch: 85, Batch: 500, loss: 3.1507 , Train PPL: 1.0048, Train Acc: 0.5152\n",
      "Epoch: 85, Batch: 600, loss: 3.1673 , Train PPL: 1.0048, Train Acc: 0.4909\n",
      "Epoch: 85, Batch: 700, loss: 3.1025 , Train PPL: 1.0047, Train Acc: 0.4497\n",
      "Validation --- Epoch: 85, total loss: 321.0033 , PPL: 1.3023, Acc: 0.4009\n",
      "lr = 0.0009765625\n",
      "Epoch: 86, Batch: 100, loss: 3.2729 , Train PPL: 1.0050, Train Acc: 0.4649\n",
      "Epoch: 86, Batch: 200, loss: 3.0675 , Train PPL: 1.0047, Train Acc: 0.4863\n",
      "Epoch: 86, Batch: 300, loss: 3.2257 , Train PPL: 1.0049, Train Acc: 0.5000\n",
      "Epoch: 86, Batch: 400, loss: 3.3537 , Train PPL: 1.0051, Train Acc: 0.4192\n",
      "Epoch: 86, Batch: 500, loss: 3.2031 , Train PPL: 1.0049, Train Acc: 0.4482\n",
      "Epoch: 86, Batch: 600, loss: 3.2690 , Train PPL: 1.0050, Train Acc: 0.4543\n",
      "Epoch: 86, Batch: 700, loss: 3.2629 , Train PPL: 1.0050, Train Acc: 0.4040\n",
      "Validation --- Epoch: 86, total loss: 321.0060 , PPL: 1.3023, Acc: 0.4010\n",
      "lr = 0.0009765625\n",
      "Epoch: 87, Batch: 100, loss: 3.0811 , Train PPL: 1.0047, Train Acc: 0.5107\n",
      "Epoch: 87, Batch: 200, loss: 3.1319 , Train PPL: 1.0048, Train Acc: 0.4832\n",
      "Epoch: 87, Batch: 300, loss: 2.7872 , Train PPL: 1.0043, Train Acc: 0.6006\n",
      "Epoch: 87, Batch: 400, loss: 2.9655 , Train PPL: 1.0045, Train Acc: 0.5091\n",
      "Epoch: 87, Batch: 500, loss: 3.3512 , Train PPL: 1.0051, Train Acc: 0.4146\n",
      "Epoch: 87, Batch: 600, loss: 2.9315 , Train PPL: 1.0045, Train Acc: 0.4970\n",
      "Epoch: 87, Batch: 700, loss: 3.1632 , Train PPL: 1.0048, Train Acc: 0.4741\n",
      "Validation --- Epoch: 87, total loss: 321.0063 , PPL: 1.3023, Acc: 0.4009\n",
      "lr = 0.00048828125\n",
      "Epoch: 88, Batch: 100, loss: 2.8276 , Train PPL: 1.0043, Train Acc: 0.5610\n",
      "Epoch: 88, Batch: 200, loss: 3.2532 , Train PPL: 1.0050, Train Acc: 0.4848\n",
      "Epoch: 88, Batch: 300, loss: 3.0488 , Train PPL: 1.0047, Train Acc: 0.4405\n",
      "Epoch: 88, Batch: 400, loss: 3.1469 , Train PPL: 1.0048, Train Acc: 0.4909\n",
      "Epoch: 88, Batch: 500, loss: 2.8477 , Train PPL: 1.0044, Train Acc: 0.5534\n",
      "Epoch: 88, Batch: 600, loss: 2.7855 , Train PPL: 1.0043, Train Acc: 0.5838\n",
      "Epoch: 88, Batch: 700, loss: 3.0867 , Train PPL: 1.0047, Train Acc: 0.4726\n",
      "Validation --- Epoch: 88, total loss: 321.0043 , PPL: 1.3023, Acc: 0.4010\n",
      "lr = 0.00048828125\n",
      "Epoch: 89, Batch: 100, loss: 3.1147 , Train PPL: 1.0048, Train Acc: 0.4924\n",
      "Epoch: 89, Batch: 200, loss: 3.0956 , Train PPL: 1.0047, Train Acc: 0.4832\n",
      "Epoch: 89, Batch: 300, loss: 3.2630 , Train PPL: 1.0050, Train Acc: 0.4497\n",
      "Epoch: 89, Batch: 400, loss: 2.9743 , Train PPL: 1.0045, Train Acc: 0.5945\n",
      "Epoch: 89, Batch: 500, loss: 2.8547 , Train PPL: 1.0044, Train Acc: 0.5412\n",
      "Epoch: 89, Batch: 600, loss: 2.9161 , Train PPL: 1.0045, Train Acc: 0.5503\n",
      "Epoch: 89, Batch: 700, loss: 2.8795 , Train PPL: 1.0044, Train Acc: 0.5290\n",
      "Validation --- Epoch: 89, total loss: 321.0069 , PPL: 1.3023, Acc: 0.4010\n",
      "lr = 0.00048828125\n",
      "Epoch: 90, Batch: 100, loss: 3.1605 , Train PPL: 1.0048, Train Acc: 0.4909\n",
      "Epoch: 90, Batch: 200, loss: 3.2047 , Train PPL: 1.0049, Train Acc: 0.4848\n",
      "Epoch: 90, Batch: 300, loss: 3.2065 , Train PPL: 1.0049, Train Acc: 0.4375\n",
      "Epoch: 90, Batch: 400, loss: 3.0313 , Train PPL: 1.0046, Train Acc: 0.5091\n",
      "Epoch: 90, Batch: 500, loss: 2.9816 , Train PPL: 1.0046, Train Acc: 0.5015\n",
      "Epoch: 90, Batch: 600, loss: 3.0700 , Train PPL: 1.0047, Train Acc: 0.4649\n",
      "Epoch: 90, Batch: 700, loss: 2.8591 , Train PPL: 1.0044, Train Acc: 0.5274\n",
      "Validation --- Epoch: 90, total loss: 321.0071 , PPL: 1.3023, Acc: 0.4010\n",
      "lr = 0.00048828125\n",
      "Epoch: 91, Batch: 100, loss: 3.1612 , Train PPL: 1.0048, Train Acc: 0.5091\n",
      "Epoch: 91, Batch: 200, loss: 3.0090 , Train PPL: 1.0046, Train Acc: 0.5457\n",
      "Epoch: 91, Batch: 300, loss: 3.0087 , Train PPL: 1.0046, Train Acc: 0.4848\n",
      "Epoch: 91, Batch: 400, loss: 2.3037 , Train PPL: 1.0035, Train Acc: 0.6982\n",
      "Epoch: 91, Batch: 500, loss: 3.2176 , Train PPL: 1.0049, Train Acc: 0.4527\n",
      "Epoch: 91, Batch: 600, loss: 3.3008 , Train PPL: 1.0050, Train Acc: 0.4604\n",
      "Epoch: 91, Batch: 700, loss: 3.3569 , Train PPL: 1.0051, Train Acc: 0.3963\n",
      "Validation --- Epoch: 91, total loss: 321.0077 , PPL: 1.3023, Acc: 0.4010\n",
      "lr = 0.00048828125\n",
      "Epoch: 92, Batch: 100, loss: 3.2663 , Train PPL: 1.0050, Train Acc: 0.4543\n",
      "Epoch: 92, Batch: 200, loss: 3.0943 , Train PPL: 1.0047, Train Acc: 0.4375\n",
      "Epoch: 92, Batch: 300, loss: 3.0238 , Train PPL: 1.0046, Train Acc: 0.5473\n",
      "Epoch: 92, Batch: 400, loss: 3.3820 , Train PPL: 1.0052, Train Acc: 0.4284\n",
      "Epoch: 92, Batch: 500, loss: 2.9235 , Train PPL: 1.0045, Train Acc: 0.5076\n",
      "Epoch: 92, Batch: 600, loss: 3.0413 , Train PPL: 1.0046, Train Acc: 0.5015\n",
      "Epoch: 92, Batch: 700, loss: 3.3008 , Train PPL: 1.0050, Train Acc: 0.4726\n",
      "Validation --- Epoch: 92, total loss: 321.0082 , PPL: 1.3023, Acc: 0.4010\n",
      "lr = 0.00048828125\n",
      "Epoch: 93, Batch: 100, loss: 3.0612 , Train PPL: 1.0047, Train Acc: 0.4954\n",
      "Epoch: 93, Batch: 200, loss: 3.2448 , Train PPL: 1.0050, Train Acc: 0.5000\n",
      "Epoch: 93, Batch: 300, loss: 3.0140 , Train PPL: 1.0046, Train Acc: 0.5381\n",
      "Epoch: 93, Batch: 400, loss: 2.9694 , Train PPL: 1.0045, Train Acc: 0.5290\n",
      "Epoch: 93, Batch: 500, loss: 2.9564 , Train PPL: 1.0045, Train Acc: 0.5381\n",
      "Epoch: 93, Batch: 600, loss: 3.1311 , Train PPL: 1.0048, Train Acc: 0.4512\n",
      "Epoch: 93, Batch: 700, loss: 2.6651 , Train PPL: 1.0041, Train Acc: 0.5869\n",
      "Validation --- Epoch: 93, total loss: 321.0079 , PPL: 1.3023, Acc: 0.4009\n",
      "lr = 0.000244140625\n",
      "Epoch: 94, Batch: 100, loss: 3.0157 , Train PPL: 1.0046, Train Acc: 0.4726\n",
      "Epoch: 94, Batch: 200, loss: 2.7379 , Train PPL: 1.0042, Train Acc: 0.5747\n",
      "Epoch: 94, Batch: 300, loss: 3.2023 , Train PPL: 1.0049, Train Acc: 0.4665\n",
      "Epoch: 94, Batch: 400, loss: 3.0931 , Train PPL: 1.0047, Train Acc: 0.4802\n",
      "Epoch: 94, Batch: 500, loss: 3.1253 , Train PPL: 1.0048, Train Acc: 0.4802\n",
      "Epoch: 94, Batch: 600, loss: 3.5134 , Train PPL: 1.0054, Train Acc: 0.4146\n",
      "Epoch: 94, Batch: 700, loss: 3.1685 , Train PPL: 1.0048, Train Acc: 0.5076\n",
      "Validation --- Epoch: 94, total loss: 321.0081 , PPL: 1.3023, Acc: 0.4009\n",
      "lr = 0.000244140625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95, Batch: 100, loss: 3.1320 , Train PPL: 1.0048, Train Acc: 0.4863\n",
      "Epoch: 95, Batch: 200, loss: 2.5634 , Train PPL: 1.0039, Train Acc: 0.6860\n",
      "Epoch: 95, Batch: 300, loss: 2.9254 , Train PPL: 1.0045, Train Acc: 0.5396\n",
      "Epoch: 95, Batch: 400, loss: 3.1410 , Train PPL: 1.0048, Train Acc: 0.5122\n",
      "Epoch: 95, Batch: 500, loss: 2.5596 , Train PPL: 1.0039, Train Acc: 0.6707\n",
      "Epoch: 95, Batch: 600, loss: 3.2438 , Train PPL: 1.0050, Train Acc: 0.4893\n",
      "Epoch: 95, Batch: 700, loss: 3.0958 , Train PPL: 1.0047, Train Acc: 0.4558\n",
      "Validation --- Epoch: 95, total loss: 321.0082 , PPL: 1.3023, Acc: 0.4009\n",
      "lr = 0.000244140625\n",
      "Epoch: 96, Batch: 100, loss: 3.2197 , Train PPL: 1.0049, Train Acc: 0.4375\n",
      "Epoch: 96, Batch: 200, loss: 3.1035 , Train PPL: 1.0047, Train Acc: 0.5244\n",
      "Epoch: 96, Batch: 300, loss: 2.8640 , Train PPL: 1.0044, Train Acc: 0.5244\n",
      "Epoch: 96, Batch: 400, loss: 2.7068 , Train PPL: 1.0041, Train Acc: 0.6296\n",
      "Epoch: 96, Batch: 500, loss: 3.1603 , Train PPL: 1.0048, Train Acc: 0.4893\n",
      "Epoch: 96, Batch: 600, loss: 2.6859 , Train PPL: 1.0041, Train Acc: 0.5945\n",
      "Epoch: 96, Batch: 700, loss: 3.1094 , Train PPL: 1.0048, Train Acc: 0.4832\n",
      "Validation --- Epoch: 96, total loss: 321.0079 , PPL: 1.3023, Acc: 0.4009\n",
      "lr = 0.000244140625\n",
      "Epoch: 97, Batch: 100, loss: 3.2140 , Train PPL: 1.0049, Train Acc: 0.4695\n",
      "Epoch: 97, Batch: 200, loss: 3.0764 , Train PPL: 1.0047, Train Acc: 0.4680\n",
      "Epoch: 97, Batch: 300, loss: 3.2664 , Train PPL: 1.0050, Train Acc: 0.4710\n",
      "Epoch: 97, Batch: 400, loss: 3.1014 , Train PPL: 1.0047, Train Acc: 0.4970\n",
      "Epoch: 97, Batch: 500, loss: 3.2316 , Train PPL: 1.0049, Train Acc: 0.4360\n",
      "Epoch: 97, Batch: 600, loss: 3.0970 , Train PPL: 1.0047, Train Acc: 0.4863\n",
      "Epoch: 97, Batch: 700, loss: 3.0627 , Train PPL: 1.0047, Train Acc: 0.4710\n",
      "Validation --- Epoch: 97, total loss: 321.0080 , PPL: 1.3023, Acc: 0.4009\n",
      "lr = 0.000244140625\n",
      "Epoch: 98, Batch: 100, loss: 2.7332 , Train PPL: 1.0042, Train Acc: 0.5777\n",
      "Epoch: 98, Batch: 200, loss: 3.2489 , Train PPL: 1.0050, Train Acc: 0.4558\n",
      "Epoch: 98, Batch: 300, loss: 3.1149 , Train PPL: 1.0048, Train Acc: 0.5351\n",
      "Epoch: 98, Batch: 400, loss: 3.3628 , Train PPL: 1.0051, Train Acc: 0.4390\n",
      "Epoch: 98, Batch: 500, loss: 3.1352 , Train PPL: 1.0048, Train Acc: 0.4878\n",
      "Epoch: 98, Batch: 600, loss: 2.9351 , Train PPL: 1.0045, Train Acc: 0.5381\n",
      "Epoch: 98, Batch: 700, loss: 2.6682 , Train PPL: 1.0041, Train Acc: 0.5732\n",
      "Validation --- Epoch: 98, total loss: 321.0078 , PPL: 1.3023, Acc: 0.4008\n",
      "lr = 0.000244140625\n",
      "Epoch: 99, Batch: 100, loss: 3.2060 , Train PPL: 1.0049, Train Acc: 0.4451\n",
      "Epoch: 99, Batch: 200, loss: 3.1603 , Train PPL: 1.0048, Train Acc: 0.4695\n",
      "Epoch: 99, Batch: 300, loss: 3.2932 , Train PPL: 1.0050, Train Acc: 0.4527\n",
      "Epoch: 99, Batch: 400, loss: 2.9642 , Train PPL: 1.0045, Train Acc: 0.5335\n",
      "Epoch: 99, Batch: 500, loss: 3.2176 , Train PPL: 1.0049, Train Acc: 0.4619\n",
      "Epoch: 99, Batch: 600, loss: 3.1969 , Train PPL: 1.0049, Train Acc: 0.4634\n",
      "Epoch: 99, Batch: 700, loss: 2.7500 , Train PPL: 1.0042, Train Acc: 0.5625\n",
      "Validation --- Epoch: 99, total loss: 321.0073 , PPL: 1.3023, Acc: 0.4008\n",
      "lr = 0.0001220703125\n"
     ]
    }
   ],
   "source": [
    "model = TCN(3, [2400,2400], kernel=3, dropout=0.45, embedding_size = 2400, n_words = n_words,tied=True)\n",
    "model.cuda()\n",
    "model_parameters = filter(lambda x: x.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print('Network with {} parameters'.format(params))\n",
    "print('Receptive field of network is {}'.format(model.receptive_field))\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode=\"min\",patience=5,min_lr=1e-6,factor=0.5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "best_vloss = 1e8\n",
    "for e in range(100):\n",
    "    train_loop(e)\n",
    "    validation_loss = validation_loop(e)\n",
    "    scheduler.step(validation_loss)\n",
    "    print('lr = {}'.format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "    if validation_loss < best_vloss:\n",
    "                with open(\"model_3_layers_k3_2400_filters_2.pt\", 'wb') as f:\n",
    "                    print('wrote model')\n",
    "                    torch.save(model, f)\n",
    "                best_vloss = validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network with 18263601 parameters\n",
      "Receptive field of network is 64\n",
      "Epoch: 0, Batch: 100, loss: 4419.4844 , Train PPL: 843.0439, Train Acc: 0.1220\n",
      "Epoch: 0, Batch: 200, loss: 4249.6191 , Train PPL: 650.7190, Train Acc: 0.1204\n",
      "Epoch: 0, Batch: 300, loss: 4265.4570 , Train PPL: 666.6206, Train Acc: 0.1418\n",
      "Epoch: 0, Batch: 400, loss: 4035.4622 , Train PPL: 469.4768, Train Acc: 0.2226\n",
      "Epoch: 0, Batch: 500, loss: 3761.5793 , Train PPL: 309.2391, Train Acc: 0.1997\n",
      "Epoch: 0, Batch: 600, loss: 4059.3159 , Train PPL: 486.8624, Train Acc: 0.1829\n",
      "Epoch: 0, Batch: 700, loss: 3935.5732 , Train PPL: 403.1665, Train Acc: 0.2134\n",
      "Validation --- Epoch: 0, total loss: 219868.5312 , PPL: 364.3120, Acc: 0.2145\n",
      "lr = 4\n",
      "wrote model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type TCN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type TC_block. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Chomp1d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 100, loss: 3790.4998 , Train PPL: 323.1772, Train Acc: 0.2332\n",
      "Epoch: 1, Batch: 200, loss: 3739.6423 , Train PPL: 299.0689, Train Acc: 0.2424\n",
      "Epoch: 1, Batch: 300, loss: 3611.4888 , Train PPL: 245.9967, Train Acc: 0.2973\n",
      "Epoch: 1, Batch: 400, loss: 3780.1719 , Train PPL: 318.1290, Train Acc: 0.2561\n",
      "Epoch: 1, Batch: 500, loss: 3595.8835 , Train PPL: 240.2139, Train Acc: 0.3064\n",
      "Epoch: 1, Batch: 600, loss: 3591.4871 , Train PPL: 238.6094, Train Acc: 0.2896\n",
      "Epoch: 1, Batch: 700, loss: 3566.2009 , Train PPL: 229.5869, Train Acc: 0.3095\n",
      "Validation --- Epoch: 1, total loss: 207214.1875 , PPL: 259.6427, Acc: 0.3159\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 2, Batch: 100, loss: 3694.0825 , Train PPL: 279.0032, Train Acc: 0.2942\n",
      "Epoch: 2, Batch: 200, loss: 3721.0845 , Train PPL: 290.7270, Train Acc: 0.2759\n",
      "Epoch: 2, Batch: 300, loss: 3708.8442 , Train PPL: 285.3528, Train Acc: 0.3338\n",
      "Epoch: 2, Batch: 400, loss: 3678.4421 , Train PPL: 272.4299, Train Acc: 0.2942\n",
      "Epoch: 2, Batch: 500, loss: 3628.8494 , Train PPL: 252.5938, Train Acc: 0.3155\n",
      "Epoch: 2, Batch: 600, loss: 3494.2617 , Train PPL: 205.7411, Train Acc: 0.2835\n",
      "Epoch: 2, Batch: 700, loss: 3522.9736 , Train PPL: 214.9460, Train Acc: 0.3125\n",
      "Validation --- Epoch: 2, total loss: 204828.0156 , PPL: 244.5955, Acc: 0.2745\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 3, Batch: 100, loss: 3678.6907 , Train PPL: 272.5331, Train Acc: 0.2698\n",
      "Epoch: 3, Batch: 200, loss: 3479.9600 , Train PPL: 201.3042, Train Acc: 0.3018\n",
      "Epoch: 3, Batch: 300, loss: 3677.3123 , Train PPL: 271.9611, Train Acc: 0.2607\n",
      "Epoch: 3, Batch: 400, loss: 3560.4724 , Train PPL: 227.5908, Train Acc: 0.3293\n",
      "Epoch: 3, Batch: 500, loss: 3633.0696 , Train PPL: 254.2240, Train Acc: 0.3201\n",
      "Epoch: 3, Batch: 600, loss: 3561.4673 , Train PPL: 227.9363, Train Acc: 0.2835\n",
      "Epoch: 3, Batch: 700, loss: 3434.9478 , Train PPL: 187.9547, Train Acc: 0.3186\n",
      "Validation --- Epoch: 3, total loss: 199612.5156 , PPL: 212.5326, Acc: 0.3300\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 4, Batch: 100, loss: 3321.6353 , Train PPL: 158.1381, Train Acc: 0.3399\n",
      "Epoch: 4, Batch: 200, loss: 3613.1616 , Train PPL: 246.6248, Train Acc: 0.3308\n",
      "Epoch: 4, Batch: 300, loss: 3622.0764 , Train PPL: 249.9993, Train Acc: 0.2591\n",
      "Epoch: 4, Batch: 400, loss: 3511.9661 , Train PPL: 211.3693, Train Acc: 0.3079\n",
      "Epoch: 4, Batch: 500, loss: 3217.4841 , Train PPL: 134.9226, Train Acc: 0.3918\n",
      "Epoch: 4, Batch: 600, loss: 3492.1714 , Train PPL: 205.0865, Train Acc: 0.2729\n",
      "Epoch: 4, Batch: 700, loss: 3489.4258 , Train PPL: 204.2300, Train Acc: 0.3140\n",
      "Validation --- Epoch: 4, total loss: 196843.2500 , PPL: 197.7985, Acc: 0.3464\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 5, Batch: 100, loss: 3298.6924 , Train PPL: 152.7029, Train Acc: 0.3201\n",
      "Epoch: 5, Batch: 200, loss: 3423.9180 , Train PPL: 184.8209, Train Acc: 0.2851\n",
      "Epoch: 5, Batch: 300, loss: 3346.1494 , Train PPL: 164.1594, Train Acc: 0.3186\n",
      "Epoch: 5, Batch: 400, loss: 3534.6926 , Train PPL: 218.8203, Train Acc: 0.2759\n",
      "Epoch: 5, Batch: 500, loss: 3435.2068 , Train PPL: 188.0289, Train Acc: 0.3369\n",
      "Epoch: 5, Batch: 600, loss: 3531.2827 , Train PPL: 217.6858, Train Acc: 0.3125\n",
      "Epoch: 5, Batch: 700, loss: 3300.1826 , Train PPL: 153.0502, Train Acc: 0.3430\n",
      "Validation --- Epoch: 5, total loss: 196444.0312 , PPL: 195.9776, Acc: 0.3506\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 6, Batch: 100, loss: 3344.9331 , Train PPL: 163.8552, Train Acc: 0.3689\n",
      "Epoch: 6, Batch: 200, loss: 3341.7185 , Train PPL: 163.0543, Train Acc: 0.3216\n",
      "Epoch: 6, Batch: 300, loss: 3510.7749 , Train PPL: 210.9858, Train Acc: 0.2790\n",
      "Epoch: 6, Batch: 400, loss: 3274.1768 , Train PPL: 147.1016, Train Acc: 0.3476\n",
      "Epoch: 6, Batch: 500, loss: 3361.2568 , Train PPL: 167.9837, Train Acc: 0.3384\n",
      "Epoch: 6, Batch: 600, loss: 3453.3743 , Train PPL: 193.3090, Train Acc: 0.3125\n",
      "Epoch: 6, Batch: 700, loss: 3389.3071 , Train PPL: 175.3225, Train Acc: 0.3155\n",
      "Validation --- Epoch: 6, total loss: 193536.0312 , PPL: 181.4183, Acc: 0.3656\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 7, Batch: 100, loss: 3270.6870 , Train PPL: 146.3211, Train Acc: 0.3582\n",
      "Epoch: 7, Batch: 200, loss: 3354.0544 , Train PPL: 166.1495, Train Acc: 0.3430\n",
      "Epoch: 7, Batch: 300, loss: 3271.4836 , Train PPL: 146.4989, Train Acc: 0.3826\n",
      "Epoch: 7, Batch: 400, loss: 3124.3250 , Train PPL: 117.0605, Train Acc: 0.3659\n",
      "Epoch: 7, Batch: 500, loss: 3225.6582 , Train PPL: 136.6143, Train Acc: 0.3216\n",
      "Epoch: 7, Batch: 600, loss: 3083.8774 , Train PPL: 110.0608, Train Acc: 0.3948\n",
      "Epoch: 7, Batch: 700, loss: 3155.8252 , Train PPL: 122.8187, Train Acc: 0.3735\n",
      "Validation --- Epoch: 7, total loss: 193244.9688 , PPL: 180.1624, Acc: 0.3358\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 8, Batch: 100, loss: 3019.7224 , Train PPL: 99.8067, Train Acc: 0.4070\n",
      "Epoch: 8, Batch: 200, loss: 3347.0737 , Train PPL: 164.3908, Train Acc: 0.3216\n",
      "Epoch: 8, Batch: 300, loss: 3202.5811 , Train PPL: 131.8920, Train Acc: 0.4101\n",
      "Epoch: 8, Batch: 400, loss: 3381.7678 , Train PPL: 173.3190, Train Acc: 0.3201\n",
      "Epoch: 8, Batch: 500, loss: 3168.3687 , Train PPL: 125.1897, Train Acc: 0.3628\n",
      "Epoch: 8, Batch: 600, loss: 3227.9417 , Train PPL: 137.0907, Train Acc: 0.3598\n",
      "Epoch: 8, Batch: 700, loss: 3200.8140 , Train PPL: 131.5372, Train Acc: 0.3826\n",
      "Validation --- Epoch: 8, total loss: 191334.0000 , PPL: 171.4528, Acc: 0.3716\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 9, Batch: 100, loss: 3284.1787 , Train PPL: 149.3616, Train Acc: 0.3598\n",
      "Epoch: 9, Batch: 200, loss: 3272.2600 , Train PPL: 146.6723, Train Acc: 0.3643\n",
      "Epoch: 9, Batch: 300, loss: 3206.9434 , Train PPL: 132.7720, Train Acc: 0.4146\n",
      "Epoch: 9, Batch: 400, loss: 3309.3606 , Train PPL: 155.2066, Train Acc: 0.3369\n",
      "Epoch: 9, Batch: 500, loss: 3406.4062 , Train PPL: 179.9524, Train Acc: 0.3506\n",
      "Epoch: 9, Batch: 600, loss: 3071.6587 , Train PPL: 108.0297, Train Acc: 0.3796\n",
      "Epoch: 9, Batch: 700, loss: 3141.6296 , Train PPL: 120.1895, Train Acc: 0.3872\n",
      "Validation --- Epoch: 9, total loss: 190416.9688 , PPL: 167.2300, Acc: 0.3796\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 10, Batch: 100, loss: 3134.2766 , Train PPL: 118.8498, Train Acc: 0.3079\n",
      "Epoch: 10, Batch: 200, loss: 2954.5786 , Train PPL: 90.3717, Train Acc: 0.3613\n",
      "Epoch: 10, Batch: 300, loss: 3125.1238 , Train PPL: 117.2031, Train Acc: 0.3887\n",
      "Epoch: 10, Batch: 400, loss: 3184.1826 , Train PPL: 128.2443, Train Acc: 0.3582\n",
      "Epoch: 10, Batch: 500, loss: 3169.2964 , Train PPL: 125.3669, Train Acc: 0.3582\n",
      "Epoch: 10, Batch: 600, loss: 3159.2090 , Train PPL: 123.4538, Train Acc: 0.4101\n",
      "Epoch: 10, Batch: 700, loss: 3064.2771 , Train PPL: 106.8209, Train Acc: 0.3582\n",
      "Validation --- Epoch: 10, total loss: 191341.0156 , PPL: 171.6524, Acc: 0.3597\n",
      "lr = 4\n",
      "Epoch: 11, Batch: 100, loss: 3272.5330 , Train PPL: 146.7334, Train Acc: 0.3613\n",
      "Epoch: 11, Batch: 200, loss: 3029.9526 , Train PPL: 101.3754, Train Acc: 0.4314\n",
      "Epoch: 11, Batch: 300, loss: 3229.4473 , Train PPL: 137.4057, Train Acc: 0.3643\n",
      "Epoch: 11, Batch: 400, loss: 3171.2361 , Train PPL: 125.7381, Train Acc: 0.3415\n",
      "Epoch: 11, Batch: 500, loss: 2917.1829 , Train PPL: 85.3641, Train Acc: 0.4146\n",
      "Epoch: 11, Batch: 600, loss: 3166.9868 , Train PPL: 124.9263, Train Acc: 0.3140\n",
      "Epoch: 11, Batch: 700, loss: 3238.8806 , Train PPL: 139.3959, Train Acc: 0.3506\n",
      "Validation --- Epoch: 11, total loss: 189601.2969 , PPL: 164.0394, Acc: 0.3729\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 12, Batch: 100, loss: 2962.7302 , Train PPL: 91.5016, Train Acc: 0.4238\n",
      "Epoch: 12, Batch: 200, loss: 3221.2236 , Train PPL: 135.6939, Train Acc: 0.3430\n",
      "Epoch: 12, Batch: 300, loss: 3219.9341 , Train PPL: 135.4275, Train Acc: 0.3171\n",
      "Epoch: 12, Batch: 400, loss: 3293.9087 , Train PPL: 151.5934, Train Acc: 0.3293\n",
      "Epoch: 12, Batch: 500, loss: 2943.0212 , Train PPL: 88.7934, Train Acc: 0.4223\n",
      "Epoch: 12, Batch: 600, loss: 3213.6184 , Train PPL: 134.1299, Train Acc: 0.3277\n",
      "Epoch: 12, Batch: 700, loss: 3361.3232 , Train PPL: 168.0007, Train Acc: 0.3338\n",
      "Validation --- Epoch: 12, total loss: 189316.5312 , PPL: 162.7422, Acc: 0.3749\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 13, Batch: 100, loss: 3022.6074 , Train PPL: 100.2466, Train Acc: 0.3872\n",
      "Epoch: 13, Batch: 200, loss: 3276.9397 , Train PPL: 147.7224, Train Acc: 0.3521\n",
      "Epoch: 13, Batch: 300, loss: 3050.8523 , Train PPL: 104.6571, Train Acc: 0.3826\n",
      "Epoch: 13, Batch: 400, loss: 2977.2043 , Train PPL: 93.5430, Train Acc: 0.3780\n",
      "Epoch: 13, Batch: 500, loss: 2986.6826 , Train PPL: 94.9044, Train Acc: 0.4405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Batch: 600, loss: 2995.9189 , Train PPL: 96.2501, Train Acc: 0.4024\n",
      "Epoch: 13, Batch: 700, loss: 3053.4644 , Train PPL: 105.0747, Train Acc: 0.3933\n",
      "Validation --- Epoch: 13, total loss: 188537.0469 , PPL: 159.6843, Acc: 0.3852\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 14, Batch: 100, loss: 2988.4265 , Train PPL: 95.1570, Train Acc: 0.4131\n",
      "Epoch: 14, Batch: 200, loss: 3045.3308 , Train PPL: 103.7799, Train Acc: 0.3750\n",
      "Epoch: 14, Batch: 300, loss: 3252.9695 , Train PPL: 142.4221, Train Acc: 0.3354\n",
      "Epoch: 14, Batch: 400, loss: 3052.5176 , Train PPL: 104.9231, Train Acc: 0.3735\n",
      "Epoch: 14, Batch: 500, loss: 3059.5903 , Train PPL: 106.0605, Train Acc: 0.3552\n",
      "Epoch: 14, Batch: 600, loss: 2695.0078 , Train PPL: 60.8398, Train Acc: 0.5168\n",
      "Epoch: 14, Batch: 700, loss: 3157.0166 , Train PPL: 123.0419, Train Acc: 0.3720\n",
      "Validation --- Epoch: 14, total loss: 188597.7656 , PPL: 159.7622, Acc: 0.3828\n",
      "lr = 4\n",
      "Epoch: 15, Batch: 100, loss: 3081.4651 , Train PPL: 109.6568, Train Acc: 0.3689\n",
      "Epoch: 15, Batch: 200, loss: 3172.5447 , Train PPL: 125.9892, Train Acc: 0.3491\n",
      "Epoch: 15, Batch: 300, loss: 3125.1130 , Train PPL: 117.2011, Train Acc: 0.3354\n",
      "Epoch: 15, Batch: 400, loss: 3349.9919 , Train PPL: 165.1237, Train Acc: 0.2896\n",
      "Epoch: 15, Batch: 500, loss: 3031.5667 , Train PPL: 101.6251, Train Acc: 0.3765\n",
      "Epoch: 15, Batch: 600, loss: 3222.2651 , Train PPL: 135.9096, Train Acc: 0.3415\n",
      "Epoch: 15, Batch: 700, loss: 3148.9294 , Train PPL: 121.5344, Train Acc: 0.3811\n",
      "Validation --- Epoch: 15, total loss: 188351.6094 , PPL: 158.9521, Acc: 0.3826\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 16, Batch: 100, loss: 2733.2971 , Train PPL: 64.4965, Train Acc: 0.4009\n",
      "Epoch: 16, Batch: 200, loss: 2992.9587 , Train PPL: 95.8167, Train Acc: 0.3643\n",
      "Epoch: 16, Batch: 300, loss: 3020.0776 , Train PPL: 99.8608, Train Acc: 0.3857\n",
      "Epoch: 16, Batch: 400, loss: 2823.8843 , Train PPL: 74.0471, Train Acc: 0.4466\n",
      "Epoch: 16, Batch: 500, loss: 3049.1116 , Train PPL: 104.3798, Train Acc: 0.3841\n",
      "Epoch: 16, Batch: 600, loss: 2851.1768 , Train PPL: 77.1928, Train Acc: 0.4543\n",
      "Epoch: 16, Batch: 700, loss: 2932.7258 , Train PPL: 87.4108, Train Acc: 0.4024\n",
      "Validation --- Epoch: 16, total loss: 187829.0000 , PPL: 156.8888, Acc: 0.3845\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 17, Batch: 100, loss: 3058.9568 , Train PPL: 105.9581, Train Acc: 0.3765\n",
      "Epoch: 17, Batch: 200, loss: 2888.6499 , Train PPL: 81.7307, Train Acc: 0.4238\n",
      "Epoch: 17, Batch: 300, loss: 2989.8447 , Train PPL: 95.3630, Train Acc: 0.3872\n",
      "Epoch: 17, Batch: 400, loss: 3176.6790 , Train PPL: 126.7857, Train Acc: 0.3369\n",
      "Epoch: 17, Batch: 500, loss: 3070.3784 , Train PPL: 107.8191, Train Acc: 0.3933\n",
      "Epoch: 17, Batch: 600, loss: 3220.8735 , Train PPL: 135.6215, Train Acc: 0.3338\n",
      "Epoch: 17, Batch: 700, loss: 3117.5835 , Train PPL: 115.8636, Train Acc: 0.3506\n",
      "Validation --- Epoch: 17, total loss: 187724.2500 , PPL: 156.4568, Acc: 0.3844\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 18, Batch: 100, loss: 2938.7029 , Train PPL: 88.2109, Train Acc: 0.3796\n",
      "Epoch: 18, Batch: 200, loss: 3048.5459 , Train PPL: 104.2898, Train Acc: 0.3780\n",
      "Epoch: 18, Batch: 300, loss: 2813.1738 , Train PPL: 72.8480, Train Acc: 0.3780\n",
      "Epoch: 18, Batch: 400, loss: 2954.2495 , Train PPL: 90.3263, Train Acc: 0.4085\n",
      "Epoch: 18, Batch: 500, loss: 3026.6646 , Train PPL: 100.8685, Train Acc: 0.3780\n",
      "Epoch: 18, Batch: 600, loss: 3079.7266 , Train PPL: 109.3665, Train Acc: 0.3689\n",
      "Epoch: 18, Batch: 700, loss: 3101.0754 , Train PPL: 112.9843, Train Acc: 0.3826\n",
      "Validation --- Epoch: 18, total loss: 187785.7031 , PPL: 156.8718, Acc: 0.3929\n",
      "lr = 4\n",
      "Epoch: 19, Batch: 100, loss: 2748.0371 , Train PPL: 65.9622, Train Acc: 0.4680\n",
      "Epoch: 19, Batch: 200, loss: 2974.5554 , Train PPL: 93.1660, Train Acc: 0.3643\n",
      "Epoch: 19, Batch: 300, loss: 2866.1309 , Train PPL: 78.9727, Train Acc: 0.3796\n",
      "Epoch: 19, Batch: 400, loss: 2910.6318 , Train PPL: 84.5158, Train Acc: 0.3872\n",
      "Epoch: 19, Batch: 500, loss: 3172.2888 , Train PPL: 125.9400, Train Acc: 0.3247\n",
      "Epoch: 19, Batch: 600, loss: 2998.8228 , Train PPL: 96.6771, Train Acc: 0.3338\n",
      "Epoch: 19, Batch: 700, loss: 3117.9080 , Train PPL: 115.9210, Train Acc: 0.3521\n",
      "Validation --- Epoch: 19, total loss: 187606.4375 , PPL: 156.3221, Acc: 0.3941\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 20, Batch: 100, loss: 2780.5598 , Train PPL: 69.3148, Train Acc: 0.4405\n",
      "Epoch: 20, Batch: 200, loss: 2755.5171 , Train PPL: 66.7186, Train Acc: 0.4573\n",
      "Epoch: 20, Batch: 300, loss: 3072.4690 , Train PPL: 108.1633, Train Acc: 0.4009\n",
      "Epoch: 20, Batch: 400, loss: 2954.7527 , Train PPL: 90.3957, Train Acc: 0.4101\n",
      "Epoch: 20, Batch: 500, loss: 2886.4229 , Train PPL: 81.4537, Train Acc: 0.3704\n",
      "Epoch: 20, Batch: 600, loss: 3018.1929 , Train PPL: 99.5743, Train Acc: 0.3628\n",
      "Epoch: 20, Batch: 700, loss: 3075.9028 , Train PPL: 108.7309, Train Acc: 0.3689\n",
      "Validation --- Epoch: 20, total loss: 187319.6406 , PPL: 155.1208, Acc: 0.3924\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 21, Batch: 100, loss: 3048.1130 , Train PPL: 104.2210, Train Acc: 0.3598\n",
      "Epoch: 21, Batch: 200, loss: 3015.4514 , Train PPL: 99.1590, Train Acc: 0.3582\n",
      "Epoch: 21, Batch: 300, loss: 3020.8640 , Train PPL: 99.9806, Train Acc: 0.3811\n",
      "Epoch: 21, Batch: 400, loss: 3060.2515 , Train PPL: 106.1675, Train Acc: 0.3049\n",
      "Epoch: 21, Batch: 500, loss: 2986.6868 , Train PPL: 94.9050, Train Acc: 0.3613\n",
      "Epoch: 21, Batch: 600, loss: 3127.1777 , Train PPL: 117.5706, Train Acc: 0.3384\n",
      "Epoch: 21, Batch: 700, loss: 3122.3140 , Train PPL: 116.7021, Train Acc: 0.3476\n",
      "Validation --- Epoch: 21, total loss: 187579.1406 , PPL: 156.3791, Acc: 0.3948\n",
      "lr = 4\n",
      "Epoch: 22, Batch: 100, loss: 2868.4048 , Train PPL: 79.2469, Train Acc: 0.3674\n",
      "Epoch: 22, Batch: 200, loss: 2601.4219 , Train PPL: 52.7510, Train Acc: 0.4405\n",
      "Epoch: 22, Batch: 300, loss: 3108.7354 , Train PPL: 114.3114, Train Acc: 0.3399\n",
      "Epoch: 22, Batch: 400, loss: 2913.3564 , Train PPL: 84.8676, Train Acc: 0.4146\n",
      "Epoch: 22, Batch: 500, loss: 2986.2139 , Train PPL: 94.8366, Train Acc: 0.3613\n",
      "Epoch: 22, Batch: 600, loss: 2970.4722 , Train PPL: 92.5879, Train Acc: 0.3750\n",
      "Epoch: 22, Batch: 700, loss: 3088.1121 , Train PPL: 110.7736, Train Acc: 0.3125\n",
      "Validation --- Epoch: 22, total loss: 187419.1719 , PPL: 155.6788, Acc: 0.3963\n",
      "lr = 4\n",
      "Epoch: 23, Batch: 100, loss: 2749.1077 , Train PPL: 66.0699, Train Acc: 0.4070\n",
      "Epoch: 23, Batch: 200, loss: 2830.2927 , Train PPL: 74.7740, Train Acc: 0.3857\n",
      "Epoch: 23, Batch: 300, loss: 2729.0859 , Train PPL: 64.0838, Train Acc: 0.4451\n",
      "Epoch: 23, Batch: 400, loss: 2564.9751 , Train PPL: 49.9001, Train Acc: 0.4649\n",
      "Epoch: 23, Batch: 500, loss: 2765.7090 , Train PPL: 67.7632, Train Acc: 0.4329\n",
      "Epoch: 23, Batch: 600, loss: 2846.4629 , Train PPL: 76.6401, Train Acc: 0.3887\n",
      "Epoch: 23, Batch: 700, loss: 2941.0769 , Train PPL: 88.5307, Train Acc: 0.3689\n",
      "Validation --- Epoch: 23, total loss: 188198.4531 , PPL: 158.8827, Acc: 0.3924\n",
      "lr = 4\n",
      "Epoch: 24, Batch: 100, loss: 2897.2913 , Train PPL: 82.8145, Train Acc: 0.3979\n",
      "Epoch: 24, Batch: 200, loss: 2938.4600 , Train PPL: 88.1782, Train Acc: 0.3674\n",
      "Epoch: 24, Batch: 300, loss: 2803.9558 , Train PPL: 71.8315, Train Acc: 0.4360\n",
      "Epoch: 24, Batch: 400, loss: 2886.5317 , Train PPL: 81.4672, Train Acc: 0.3659\n",
      "Epoch: 24, Batch: 500, loss: 2824.9897 , Train PPL: 74.1720, Train Acc: 0.3887\n",
      "Epoch: 24, Batch: 600, loss: 2936.1267 , Train PPL: 87.8651, Train Acc: 0.3841\n",
      "Epoch: 24, Batch: 700, loss: 2989.6768 , Train PPL: 95.3385, Train Acc: 0.3369\n",
      "Validation --- Epoch: 24, total loss: 187302.0938 , PPL: 155.4393, Acc: 0.3968\n",
      "lr = 2.0\n",
      "wrote model\n",
      "Epoch: 25, Batch: 100, loss: 2646.2788 , Train PPL: 56.4842, Train Acc: 0.4543\n",
      "Epoch: 25, Batch: 200, loss: 2764.0349 , Train PPL: 67.5905, Train Acc: 0.4116\n",
      "Epoch: 25, Batch: 300, loss: 2756.0537 , Train PPL: 66.7732, Train Acc: 0.4268\n",
      "Epoch: 25, Batch: 400, loss: 2724.4683 , Train PPL: 63.6343, Train Acc: 0.4619\n",
      "Epoch: 25, Batch: 500, loss: 2769.6438 , Train PPL: 68.1709, Train Acc: 0.4055\n",
      "Epoch: 25, Batch: 600, loss: 2668.9756 , Train PPL: 58.4727, Train Acc: 0.4741\n",
      "Epoch: 25, Batch: 700, loss: 2718.1953 , Train PPL: 63.0287, Train Acc: 0.4451\n",
      "Validation --- Epoch: 25, total loss: 187648.5469 , PPL: 157.1175, Acc: 0.4094\n",
      "lr = 2.0\n",
      "Epoch: 26, Batch: 100, loss: 2889.9316 , Train PPL: 81.8905, Train Acc: 0.3826\n",
      "Epoch: 26, Batch: 200, loss: 2497.5928 , Train PPL: 45.0290, Train Acc: 0.5015\n",
      "Epoch: 26, Batch: 300, loss: 2625.0667 , Train PPL: 54.6870, Train Acc: 0.4817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Batch: 400, loss: 2724.0710 , Train PPL: 63.5958, Train Acc: 0.4985\n",
      "Epoch: 26, Batch: 500, loss: 2641.0051 , Train PPL: 56.0320, Train Acc: 0.4192\n",
      "Epoch: 26, Batch: 600, loss: 2830.8938 , Train PPL: 74.8426, Train Acc: 0.3750\n",
      "Epoch: 26, Batch: 700, loss: 2869.4814 , Train PPL: 79.3771, Train Acc: 0.3841\n",
      "Validation --- Epoch: 26, total loss: 187616.0156 , PPL: 156.9619, Acc: 0.4060\n",
      "lr = 2.0\n",
      "Epoch: 27, Batch: 100, loss: 2531.3167 , Train PPL: 47.4044, Train Acc: 0.4360\n",
      "Epoch: 27, Batch: 200, loss: 2560.3345 , Train PPL: 49.5483, Train Acc: 0.4604\n",
      "Epoch: 27, Batch: 300, loss: 2843.5845 , Train PPL: 76.3046, Train Acc: 0.3735\n",
      "Epoch: 27, Batch: 400, loss: 2529.7615 , Train PPL: 47.2921, Train Acc: 0.4604\n",
      "Epoch: 27, Batch: 500, loss: 2727.6643 , Train PPL: 63.9451, Train Acc: 0.4131\n",
      "Epoch: 27, Batch: 600, loss: 2568.9380 , Train PPL: 50.2025, Train Acc: 0.4588\n",
      "Epoch: 27, Batch: 700, loss: 2863.6379 , Train PPL: 78.6731, Train Acc: 0.3872\n",
      "Validation --- Epoch: 27, total loss: 187986.1094 , PPL: 158.7903, Acc: 0.4074\n",
      "lr = 2.0\n",
      "Epoch: 28, Batch: 100, loss: 2743.4888 , Train PPL: 65.5064, Train Acc: 0.3735\n",
      "Epoch: 28, Batch: 200, loss: 2704.7014 , Train PPL: 61.7455, Train Acc: 0.4146\n",
      "Epoch: 28, Batch: 300, loss: 2852.7817 , Train PPL: 77.3819, Train Acc: 0.3598\n",
      "Epoch: 28, Batch: 400, loss: 2848.4998 , Train PPL: 76.8784, Train Acc: 0.3735\n",
      "Epoch: 28, Batch: 500, loss: 2705.7361 , Train PPL: 61.8429, Train Acc: 0.4223\n",
      "Epoch: 28, Batch: 600, loss: 2407.2173 , Train PPL: 39.2338, Train Acc: 0.5152\n",
      "Epoch: 28, Batch: 700, loss: 2592.4763 , Train PPL: 52.0365, Train Acc: 0.4527\n",
      "Validation --- Epoch: 28, total loss: 188329.6719 , PPL: 160.0404, Acc: 0.4071\n",
      "lr = 1.0\n",
      "Epoch: 29, Batch: 100, loss: 2759.6143 , Train PPL: 67.1366, Train Acc: 0.3826\n",
      "Epoch: 29, Batch: 200, loss: 2762.4646 , Train PPL: 67.4289, Train Acc: 0.4009\n",
      "Epoch: 29, Batch: 300, loss: 2540.8440 , Train PPL: 48.0979, Train Acc: 0.4512\n",
      "Epoch: 29, Batch: 400, loss: 2766.8010 , Train PPL: 67.8761, Train Acc: 0.4055\n",
      "Epoch: 29, Batch: 500, loss: 2725.8728 , Train PPL: 63.7707, Train Acc: 0.4070\n",
      "Epoch: 29, Batch: 600, loss: 2795.5630 , Train PPL: 70.9183, Train Acc: 0.4238\n",
      "Epoch: 29, Batch: 700, loss: 2688.6311 , Train PPL: 60.2512, Train Acc: 0.3704\n",
      "Validation --- Epoch: 29, total loss: 188417.9844 , PPL: 160.7159, Acc: 0.4122\n",
      "lr = 1.0\n",
      "Epoch: 30, Batch: 100, loss: 2894.6841 , Train PPL: 82.4860, Train Acc: 0.3750\n",
      "Epoch: 30, Batch: 200, loss: 2726.5752 , Train PPL: 63.8390, Train Acc: 0.3857\n",
      "Epoch: 30, Batch: 300, loss: 2618.7612 , Train PPL: 54.1639, Train Acc: 0.4299\n",
      "Epoch: 30, Batch: 400, loss: 3032.6392 , Train PPL: 101.7914, Train Acc: 0.3247\n",
      "Epoch: 30, Batch: 500, loss: 2811.9336 , Train PPL: 72.7104, Train Acc: 0.3948\n",
      "Epoch: 30, Batch: 600, loss: 2656.7827 , Train PPL: 57.3959, Train Acc: 0.4817\n",
      "Epoch: 30, Batch: 700, loss: 2854.7437 , Train PPL: 77.6136, Train Acc: 0.3857\n",
      "Validation --- Epoch: 30, total loss: 188537.1719 , PPL: 161.2943, Acc: 0.4116\n",
      "lr = 1.0\n",
      "Epoch: 31, Batch: 100, loss: 2516.9058 , Train PPL: 46.3744, Train Acc: 0.4390\n",
      "Epoch: 31, Batch: 200, loss: 2894.6826 , Train PPL: 82.4858, Train Acc: 0.3643\n",
      "Epoch: 31, Batch: 300, loss: 2815.0349 , Train PPL: 73.0549, Train Acc: 0.4055\n",
      "Epoch: 31, Batch: 400, loss: 2671.2825 , Train PPL: 58.6787, Train Acc: 0.4223\n",
      "Epoch: 31, Batch: 500, loss: 2757.4326 , Train PPL: 66.9137, Train Acc: 0.3979\n",
      "Epoch: 31, Batch: 600, loss: 2810.7041 , Train PPL: 72.5742, Train Acc: 0.3506\n",
      "Epoch: 31, Batch: 700, loss: 2308.6355 , Train PPL: 33.7595, Train Acc: 0.5518\n",
      "Validation --- Epoch: 31, total loss: 188835.0156 , PPL: 162.6803, Acc: 0.4143\n",
      "lr = 1.0\n",
      "Epoch: 32, Batch: 100, loss: 2692.9875 , Train PPL: 60.6527, Train Acc: 0.4101\n",
      "Epoch: 32, Batch: 200, loss: 2525.9917 , Train PPL: 47.0211, Train Acc: 0.4207\n",
      "Epoch: 32, Batch: 300, loss: 2668.2554 , Train PPL: 58.4086, Train Acc: 0.3979\n",
      "Epoch: 32, Batch: 400, loss: 2512.4275 , Train PPL: 46.0589, Train Acc: 0.4375\n",
      "Epoch: 32, Batch: 500, loss: 2751.6287 , Train PPL: 66.3243, Train Acc: 0.4009\n",
      "Epoch: 32, Batch: 600, loss: 2660.2942 , Train PPL: 57.7040, Train Acc: 0.3963\n",
      "Epoch: 32, Batch: 700, loss: 2263.1143 , Train PPL: 31.4963, Train Acc: 0.5610\n",
      "Validation --- Epoch: 32, total loss: 189084.4688 , PPL: 163.6925, Acc: 0.4127\n",
      "lr = 0.5\n",
      "Epoch: 33, Batch: 100, loss: 2589.6030 , Train PPL: 51.8091, Train Acc: 0.4299\n",
      "Epoch: 33, Batch: 200, loss: 2536.1929 , Train PPL: 47.7581, Train Acc: 0.4527\n",
      "Epoch: 33, Batch: 300, loss: 2776.8369 , Train PPL: 68.9225, Train Acc: 0.3857\n",
      "Epoch: 33, Batch: 400, loss: 2662.5476 , Train PPL: 57.9026, Train Acc: 0.3948\n",
      "Epoch: 33, Batch: 500, loss: 2615.0239 , Train PPL: 53.8562, Train Acc: 0.4177\n",
      "Epoch: 33, Batch: 600, loss: 2672.6123 , Train PPL: 58.7978, Train Acc: 0.4177\n",
      "Epoch: 33, Batch: 700, loss: 2580.2224 , Train PPL: 51.0735, Train Acc: 0.4345\n",
      "Validation --- Epoch: 33, total loss: 189118.2031 , PPL: 163.8883, Acc: 0.4120\n",
      "lr = 0.5\n",
      "Epoch: 34, Batch: 100, loss: 2408.1760 , Train PPL: 39.2912, Train Acc: 0.5320\n",
      "Epoch: 34, Batch: 200, loss: 2760.6724 , Train PPL: 67.2450, Train Acc: 0.3811\n",
      "Epoch: 34, Batch: 300, loss: 2551.0862 , Train PPL: 48.8547, Train Acc: 0.4802\n",
      "Epoch: 34, Batch: 400, loss: 2625.9751 , Train PPL: 54.7628, Train Acc: 0.4116\n",
      "Epoch: 34, Batch: 500, loss: 2724.5452 , Train PPL: 63.6418, Train Acc: 0.3704\n",
      "Epoch: 34, Batch: 600, loss: 2668.3508 , Train PPL: 58.4171, Train Acc: 0.4192\n",
      "Epoch: 34, Batch: 700, loss: 2713.6462 , Train PPL: 62.5931, Train Acc: 0.3780\n",
      "Validation --- Epoch: 34, total loss: 189164.6562 , PPL: 164.1904, Acc: 0.4146\n",
      "lr = 0.5\n",
      "Epoch: 35, Batch: 100, loss: 2488.8027 , Train PPL: 44.4296, Train Acc: 0.5030\n",
      "Epoch: 35, Batch: 200, loss: 2567.1428 , Train PPL: 50.0653, Train Acc: 0.4375\n",
      "Epoch: 35, Batch: 300, loss: 2668.7856 , Train PPL: 58.4558, Train Acc: 0.3918\n",
      "Epoch: 35, Batch: 400, loss: 2396.6270 , Train PPL: 38.6055, Train Acc: 0.5305\n",
      "Epoch: 35, Batch: 500, loss: 2748.1025 , Train PPL: 65.9687, Train Acc: 0.3735\n",
      "Epoch: 35, Batch: 600, loss: 2584.1177 , Train PPL: 51.3777, Train Acc: 0.4695\n",
      "Epoch: 35, Batch: 700, loss: 2745.7957 , Train PPL: 65.7371, Train Acc: 0.3689\n",
      "Validation --- Epoch: 35, total loss: 189569.0781 , PPL: 166.0802, Acc: 0.4141\n",
      "lr = 0.5\n",
      "Epoch: 36, Batch: 100, loss: 2736.0188 , Train PPL: 64.7647, Train Acc: 0.3979\n",
      "Epoch: 36, Batch: 200, loss: 2648.6006 , Train PPL: 56.6845, Train Acc: 0.4162\n",
      "Epoch: 36, Batch: 300, loss: 2542.3066 , Train PPL: 48.2052, Train Acc: 0.4207\n",
      "Epoch: 36, Batch: 400, loss: 2738.2600 , Train PPL: 64.9863, Train Acc: 0.3963\n",
      "Epoch: 36, Batch: 500, loss: 2714.8789 , Train PPL: 62.7109, Train Acc: 0.4207\n",
      "Epoch: 36, Batch: 600, loss: 2519.8831 , Train PPL: 46.5853, Train Acc: 0.4497\n",
      "Epoch: 36, Batch: 700, loss: 2599.1760 , Train PPL: 52.5707, Train Acc: 0.4345\n",
      "Validation --- Epoch: 36, total loss: 189473.6875 , PPL: 165.6271, Acc: 0.4139\n",
      "lr = 0.25\n",
      "Epoch: 37, Batch: 100, loss: 2385.5205 , Train PPL: 37.9574, Train Acc: 0.4695\n",
      "Epoch: 37, Batch: 200, loss: 2520.1638 , Train PPL: 46.6053, Train Acc: 0.4223\n",
      "Epoch: 37, Batch: 300, loss: 2692.3547 , Train PPL: 60.5942, Train Acc: 0.3460\n",
      "Epoch: 37, Batch: 400, loss: 2436.4575 , Train PPL: 41.0221, Train Acc: 0.4787\n",
      "Epoch: 37, Batch: 500, loss: 2713.5684 , Train PPL: 62.5857, Train Acc: 0.3994\n",
      "Epoch: 37, Batch: 600, loss: 2573.4099 , Train PPL: 50.5459, Train Acc: 0.4588\n",
      "Epoch: 37, Batch: 700, loss: 2539.8020 , Train PPL: 48.0215, Train Acc: 0.4284\n",
      "Validation --- Epoch: 37, total loss: 189699.2500 , PPL: 166.6684, Acc: 0.4157\n",
      "lr = 0.25\n",
      "Epoch: 38, Batch: 100, loss: 2719.4524 , Train PPL: 63.1496, Train Acc: 0.3994\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-237-acae58e62174>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mbest_vloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mvalidation_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-234-209193d385d4>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.35\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = TCN(5, [800,800,800,800], kernel=2, dropout=0.45, embedding_size = 800, n_words = n_words,tied=True)\n",
    "model.cuda()\n",
    "model_parameters = filter(lambda x: x.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print('Network with {} parameters'.format(params))\n",
    "print('Receptive field of network is {}'.format(model.receptive_field))\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode=\"min\",patience=3,min_lr=1e-6,factor=0.5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "best_vloss = 1e8\n",
    "for e in range(100):\n",
    "    train_loop(e)\n",
    "    validation_loss = validation_loop(e)\n",
    "    scheduler.step(validation_loss)\n",
    "    print('lr = {}'.format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "    if validation_loss < best_vloss:\n",
    "                with open(\"model_5_layers_k2_800_filters_1.pt\", 'wb') as f:\n",
    "                    print('wrote model')\n",
    "                    torch.save(model, f)\n",
    "                best_vloss = validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network with 4633901 parameters\n",
      "Receptive field of network is 62\n",
      "Epoch: 0, Batch: 100, loss: 4461.0923 , Train PPL: 898.2474, Train Acc: 0.1067\n",
      "Epoch: 0, Batch: 200, loss: 4086.4915 , Train PPL: 507.4549, Train Acc: 0.1570\n",
      "Epoch: 0, Batch: 300, loss: 4237.2412 , Train PPL: 638.5557, Train Acc: 0.1479\n",
      "Epoch: 0, Batch: 400, loss: 4115.2642 , Train PPL: 530.2076, Train Acc: 0.1723\n",
      "Epoch: 0, Batch: 500, loss: 4034.4351 , Train PPL: 468.7425, Train Acc: 0.1814\n",
      "Epoch: 0, Batch: 600, loss: 4090.6443 , Train PPL: 510.6775, Train Acc: 0.2165\n",
      "Epoch: 0, Batch: 700, loss: 3893.9153 , Train PPL: 378.3601, Train Acc: 0.1966\n",
      "Validation --- Epoch: 0, total loss: 224179.4688 , PPL: 408.1991, Acc: 0.2064\n",
      "lr = 4\n",
      "wrote model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type TCN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type TC_block. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Chomp1d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 100, loss: 3836.0598 , Train PPL: 346.4200, Train Acc: 0.2348\n",
      "Epoch: 1, Batch: 200, loss: 3854.4668 , Train PPL: 356.2780, Train Acc: 0.2530\n",
      "Epoch: 1, Batch: 300, loss: 3693.9321 , Train PPL: 278.9393, Train Acc: 0.2500\n",
      "Epoch: 1, Batch: 400, loss: 3780.3418 , Train PPL: 318.2115, Train Acc: 0.2348\n",
      "Epoch: 1, Batch: 500, loss: 3718.5388 , Train PPL: 289.6010, Train Acc: 0.2820\n",
      "Epoch: 1, Batch: 600, loss: 3890.6375 , Train PPL: 376.4742, Train Acc: 0.2058\n",
      "Epoch: 1, Batch: 700, loss: 3811.1089 , Train PPL: 333.4914, Train Acc: 0.2713\n",
      "Validation --- Epoch: 1, total loss: 211806.6562 , PPL: 293.5097, Acc: 0.2591\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 2, Batch: 100, loss: 3879.3967 , Train PPL: 370.0782, Train Acc: 0.2271\n",
      "Epoch: 2, Batch: 200, loss: 3864.2261 , Train PPL: 361.6181, Train Acc: 0.2683\n",
      "Epoch: 2, Batch: 300, loss: 3796.9519 , Train PPL: 326.3715, Train Acc: 0.2759\n",
      "Epoch: 2, Batch: 400, loss: 3658.0876 , Train PPL: 264.1067, Train Acc: 0.2652\n",
      "Epoch: 2, Batch: 500, loss: 3688.3926 , Train PPL: 276.5937, Train Acc: 0.2820\n",
      "Epoch: 2, Batch: 600, loss: 3637.2004 , Train PPL: 255.8299, Train Acc: 0.2759\n",
      "Epoch: 2, Batch: 700, loss: 3696.4033 , Train PPL: 279.9921, Train Acc: 0.2896\n",
      "Validation --- Epoch: 2, total loss: 206137.5469 , PPL: 252.9835, Acc: 0.3063\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 3, Batch: 100, loss: 3720.7034 , Train PPL: 290.5582, Train Acc: 0.3171\n",
      "Epoch: 3, Batch: 200, loss: 3759.2144 , Train PPL: 308.1263, Train Acc: 0.2393\n",
      "Epoch: 3, Batch: 300, loss: 3649.1316 , Train PPL: 260.5255, Train Acc: 0.2835\n",
      "Epoch: 3, Batch: 400, loss: 3848.2585 , Train PPL: 352.9222, Train Acc: 0.2576\n",
      "Epoch: 3, Batch: 500, loss: 3660.2024 , Train PPL: 264.9595, Train Acc: 0.3034\n",
      "Epoch: 3, Batch: 600, loss: 3645.3384 , Train PPL: 259.0233, Train Acc: 0.2881\n",
      "Epoch: 3, Batch: 700, loss: 3507.5762 , Train PPL: 209.9595, Train Acc: 0.2713\n",
      "Validation --- Epoch: 3, total loss: 203962.8750 , PPL: 238.8597, Acc: 0.3151\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 4, Batch: 100, loss: 3727.8164 , Train PPL: 293.7258, Train Acc: 0.2881\n",
      "Epoch: 4, Batch: 200, loss: 3641.9414 , Train PPL: 257.6856, Train Acc: 0.3110\n",
      "Epoch: 4, Batch: 300, loss: 3642.8789 , Train PPL: 258.0541, Train Acc: 0.2805\n",
      "Epoch: 4, Batch: 400, loss: 3625.8635 , Train PPL: 251.4467, Train Acc: 0.2805\n",
      "Epoch: 4, Batch: 500, loss: 3493.8733 , Train PPL: 205.6193, Train Acc: 0.3323\n",
      "Epoch: 4, Batch: 600, loss: 3663.9985 , Train PPL: 266.4971, Train Acc: 0.2973\n",
      "Epoch: 4, Batch: 700, loss: 3631.4910 , Train PPL: 253.6130, Train Acc: 0.2912\n",
      "Validation --- Epoch: 4, total loss: 200452.0781 , PPL: 217.4904, Acc: 0.3444\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 5, Batch: 100, loss: 3449.8003 , Train PPL: 192.2587, Train Acc: 0.3095\n",
      "Epoch: 5, Batch: 200, loss: 3848.6755 , Train PPL: 353.1466, Train Acc: 0.2790\n",
      "Epoch: 5, Batch: 300, loss: 3737.8884 , Train PPL: 298.2704, Train Acc: 0.2622\n",
      "Epoch: 5, Batch: 400, loss: 3573.6719 , Train PPL: 232.2166, Train Acc: 0.2576\n",
      "Epoch: 5, Batch: 500, loss: 3651.8450 , Train PPL: 261.6053, Train Acc: 0.2973\n",
      "Epoch: 5, Batch: 600, loss: 3692.4707 , Train PPL: 278.3186, Train Acc: 0.3155\n",
      "Epoch: 5, Batch: 700, loss: 3507.1421 , Train PPL: 209.8207, Train Acc: 0.3049\n",
      "Validation --- Epoch: 5, total loss: 199760.8125 , PPL: 213.9872, Acc: 0.3323\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 6, Batch: 100, loss: 3657.9351 , Train PPL: 264.0452, Train Acc: 0.3034\n",
      "Epoch: 6, Batch: 200, loss: 3518.2930 , Train PPL: 213.4178, Train Acc: 0.3232\n",
      "Epoch: 6, Batch: 300, loss: 3673.4607 , Train PPL: 270.3690, Train Acc: 0.2759\n",
      "Epoch: 6, Batch: 400, loss: 3466.9363 , Train PPL: 197.3470, Train Acc: 0.3643\n",
      "Epoch: 6, Batch: 500, loss: 3455.7209 , Train PPL: 194.0017, Train Acc: 0.3232\n",
      "Epoch: 6, Batch: 600, loss: 3434.9868 , Train PPL: 187.9659, Train Acc: 0.3415\n",
      "Epoch: 6, Batch: 700, loss: 3241.3152 , Train PPL: 139.9142, Train Acc: 0.3110\n",
      "Validation --- Epoch: 6, total loss: 197835.9688 , PPL: 202.9587, Acc: 0.3512\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 7, Batch: 100, loss: 3471.8914 , Train PPL: 198.8433, Train Acc: 0.2973\n",
      "Epoch: 7, Batch: 200, loss: 3718.9976 , Train PPL: 289.8037, Train Acc: 0.2652\n",
      "Epoch: 7, Batch: 300, loss: 3459.4153 , Train PPL: 195.0974, Train Acc: 0.3171\n",
      "Epoch: 7, Batch: 400, loss: 3344.7998 , Train PPL: 163.8219, Train Acc: 0.2835\n",
      "Epoch: 7, Batch: 500, loss: 3586.0044 , Train PPL: 236.6235, Train Acc: 0.2805\n",
      "Epoch: 7, Batch: 600, loss: 3449.9287 , Train PPL: 192.2963, Train Acc: 0.2881\n",
      "Epoch: 7, Batch: 700, loss: 3549.8301 , Train PPL: 223.9284, Train Acc: 0.2881\n",
      "Validation --- Epoch: 7, total loss: 196527.0312 , PPL: 196.0419, Acc: 0.3476\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 8, Batch: 100, loss: 3214.4624 , Train PPL: 134.3025, Train Acc: 0.3338\n",
      "Epoch: 8, Batch: 200, loss: 3470.8481 , Train PPL: 198.5274, Train Acc: 0.2668\n",
      "Epoch: 8, Batch: 300, loss: 3621.4727 , Train PPL: 249.7692, Train Acc: 0.2668\n",
      "Epoch: 8, Batch: 400, loss: 3358.8381 , Train PPL: 167.3655, Train Acc: 0.3399\n",
      "Epoch: 8, Batch: 500, loss: 3505.1123 , Train PPL: 209.1725, Train Acc: 0.2774\n",
      "Epoch: 8, Batch: 600, loss: 3479.3118 , Train PPL: 201.1053, Train Acc: 0.3095\n",
      "Epoch: 8, Batch: 700, loss: 3423.9961 , Train PPL: 184.8429, Train Acc: 0.3125\n",
      "Validation --- Epoch: 8, total loss: 195858.5781 , PPL: 193.0154, Acc: 0.3494\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 9, Batch: 100, loss: 3287.2590 , Train PPL: 150.0645, Train Acc: 0.3277\n",
      "Epoch: 9, Batch: 200, loss: 3392.9905 , Train PPL: 176.3096, Train Acc: 0.3506\n",
      "Epoch: 9, Batch: 300, loss: 3509.2612 , Train PPL: 210.4995, Train Acc: 0.2896\n",
      "Epoch: 9, Batch: 400, loss: 3611.9048 , Train PPL: 246.1528, Train Acc: 0.2866\n",
      "Epoch: 9, Batch: 500, loss: 3451.9961 , Train PPL: 192.9034, Train Acc: 0.3003\n",
      "Epoch: 9, Batch: 600, loss: 3424.3445 , Train PPL: 184.9411, Train Acc: 0.2790\n",
      "Epoch: 9, Batch: 700, loss: 3544.3240 , Train PPL: 222.0568, Train Acc: 0.3399\n",
      "Validation --- Epoch: 9, total loss: 195368.8750 , PPL: 190.4547, Acc: 0.3487\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 10, Batch: 100, loss: 3269.6150 , Train PPL: 146.0821, Train Acc: 0.3567\n",
      "Epoch: 10, Batch: 200, loss: 3466.9072 , Train PPL: 197.3383, Train Acc: 0.2591\n",
      "Epoch: 10, Batch: 300, loss: 3511.5435 , Train PPL: 211.2332, Train Acc: 0.2973\n",
      "Epoch: 10, Batch: 400, loss: 3449.7861 , Train PPL: 192.2546, Train Acc: 0.3338\n",
      "Epoch: 10, Batch: 500, loss: 3256.4094 , Train PPL: 143.1709, Train Acc: 0.3155\n",
      "Epoch: 10, Batch: 600, loss: 3109.1746 , Train PPL: 114.3879, Train Acc: 0.3567\n",
      "Epoch: 10, Batch: 700, loss: 3411.5676 , Train PPL: 181.3739, Train Acc: 0.3323\n",
      "Validation --- Epoch: 10, total loss: 193839.9844 , PPL: 182.6848, Acc: 0.3602\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 11, Batch: 100, loss: 3405.1406 , Train PPL: 179.6055, Train Acc: 0.3079\n",
      "Epoch: 11, Batch: 200, loss: 3455.3652 , Train PPL: 193.8966, Train Acc: 0.3399\n",
      "Epoch: 11, Batch: 300, loss: 3394.4314 , Train PPL: 176.6973, Train Acc: 0.3491\n",
      "Epoch: 11, Batch: 400, loss: 3560.3086 , Train PPL: 227.5340, Train Acc: 0.3415\n",
      "Epoch: 11, Batch: 500, loss: 3589.6001 , Train PPL: 237.9240, Train Acc: 0.3155\n",
      "Epoch: 11, Batch: 600, loss: 3360.5811 , Train PPL: 167.8108, Train Acc: 0.2942\n",
      "Epoch: 11, Batch: 700, loss: 3464.2749 , Train PPL: 196.5480, Train Acc: 0.3155\n",
      "Validation --- Epoch: 11, total loss: 193485.9531 , PPL: 181.0954, Acc: 0.3571\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 12, Batch: 100, loss: 3514.1997 , Train PPL: 212.0902, Train Acc: 0.2927\n",
      "Epoch: 12, Batch: 200, loss: 3516.2739 , Train PPL: 212.7619, Train Acc: 0.3064\n",
      "Epoch: 12, Batch: 300, loss: 3273.6794 , Train PPL: 146.9901, Train Acc: 0.3659\n",
      "Epoch: 12, Batch: 400, loss: 3401.8604 , Train PPL: 178.7097, Train Acc: 0.3323\n",
      "Epoch: 12, Batch: 500, loss: 3235.8804 , Train PPL: 138.7598, Train Acc: 0.3567\n",
      "Epoch: 12, Batch: 600, loss: 3273.0669 , Train PPL: 146.8529, Train Acc: 0.3354\n",
      "Epoch: 12, Batch: 700, loss: 3165.3323 , Train PPL: 124.6116, Train Acc: 0.3811\n",
      "Validation --- Epoch: 12, total loss: 193034.2031 , PPL: 178.9118, Acc: 0.3573\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 13, Batch: 100, loss: 3385.9871 , Train PPL: 174.4374, Train Acc: 0.3171\n",
      "Epoch: 13, Batch: 200, loss: 3345.6008 , Train PPL: 164.0221, Train Acc: 0.2896\n",
      "Epoch: 13, Batch: 300, loss: 3135.3008 , Train PPL: 119.0355, Train Acc: 0.3720\n",
      "Epoch: 13, Batch: 400, loss: 3321.0476 , Train PPL: 157.9964, Train Acc: 0.3521\n",
      "Epoch: 13, Batch: 500, loss: 3356.8435 , Train PPL: 166.8574, Train Acc: 0.3338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Batch: 600, loss: 3310.7744 , Train PPL: 155.5414, Train Acc: 0.3186\n",
      "Epoch: 13, Batch: 700, loss: 3248.7510 , Train PPL: 141.5092, Train Acc: 0.3613\n",
      "Validation --- Epoch: 13, total loss: 193105.9844 , PPL: 179.1382, Acc: 0.3564\n",
      "lr = 4\n",
      "Epoch: 14, Batch: 100, loss: 3365.7588 , Train PPL: 169.1405, Train Acc: 0.3582\n",
      "Epoch: 14, Batch: 200, loss: 3381.1064 , Train PPL: 173.1444, Train Acc: 0.3338\n",
      "Epoch: 14, Batch: 300, loss: 3557.1633 , Train PPL: 226.4456, Train Acc: 0.2881\n",
      "Epoch: 14, Batch: 400, loss: 3347.9709 , Train PPL: 164.6158, Train Acc: 0.3598\n",
      "Epoch: 14, Batch: 500, loss: 3200.5295 , Train PPL: 131.4802, Train Acc: 0.4009\n",
      "Epoch: 14, Batch: 600, loss: 3451.4070 , Train PPL: 192.7301, Train Acc: 0.2851\n",
      "Epoch: 14, Batch: 700, loss: 3377.6763 , Train PPL: 172.2413, Train Acc: 0.3445\n",
      "Validation --- Epoch: 14, total loss: 191680.3750 , PPL: 172.7063, Acc: 0.3673\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 15, Batch: 100, loss: 3401.0010 , Train PPL: 178.4757, Train Acc: 0.3384\n",
      "Epoch: 15, Batch: 200, loss: 3432.6606 , Train PPL: 187.3005, Train Acc: 0.3476\n",
      "Epoch: 15, Batch: 300, loss: 3451.6062 , Train PPL: 192.7887, Train Acc: 0.2744\n",
      "Epoch: 15, Batch: 400, loss: 3161.3245 , Train PPL: 123.8526, Train Acc: 0.3918\n",
      "Epoch: 15, Batch: 500, loss: 3346.0762 , Train PPL: 164.1410, Train Acc: 0.3506\n",
      "Epoch: 15, Batch: 600, loss: 3299.2832 , Train PPL: 152.8405, Train Acc: 0.3201\n",
      "Epoch: 15, Batch: 700, loss: 3303.5774 , Train PPL: 153.8443, Train Acc: 0.3247\n",
      "Validation --- Epoch: 15, total loss: 191300.7656 , PPL: 170.8746, Acc: 0.3691\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 16, Batch: 100, loss: 3314.9438 , Train PPL: 156.5332, Train Acc: 0.3567\n",
      "Epoch: 16, Batch: 200, loss: 3328.9185 , Train PPL: 159.9035, Train Acc: 0.3247\n",
      "Epoch: 16, Batch: 300, loss: 3604.5483 , Train PPL: 243.4078, Train Acc: 0.2713\n",
      "Epoch: 16, Batch: 400, loss: 3239.3457 , Train PPL: 139.4947, Train Acc: 0.3567\n",
      "Epoch: 16, Batch: 500, loss: 3147.3193 , Train PPL: 121.2365, Train Acc: 0.3796\n",
      "Epoch: 16, Batch: 600, loss: 3201.4519 , Train PPL: 131.6651, Train Acc: 0.3415\n",
      "Epoch: 16, Batch: 700, loss: 3297.2400 , Train PPL: 152.3652, Train Acc: 0.3887\n",
      "Validation --- Epoch: 16, total loss: 191269.4844 , PPL: 171.0207, Acc: 0.3642\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 17, Batch: 100, loss: 3308.2673 , Train PPL: 154.9481, Train Acc: 0.3384\n",
      "Epoch: 17, Batch: 200, loss: 3191.4785 , Train PPL: 129.6786, Train Acc: 0.3293\n",
      "Epoch: 17, Batch: 300, loss: 3142.9116 , Train PPL: 120.4246, Train Acc: 0.3796\n",
      "Epoch: 17, Batch: 400, loss: 3296.4683 , Train PPL: 152.1861, Train Acc: 0.2820\n",
      "Epoch: 17, Batch: 500, loss: 3147.8474 , Train PPL: 121.3341, Train Acc: 0.3735\n",
      "Epoch: 17, Batch: 600, loss: 3266.0581 , Train PPL: 145.2922, Train Acc: 0.3201\n",
      "Epoch: 17, Batch: 700, loss: 3243.7249 , Train PPL: 140.4291, Train Acc: 0.3750\n",
      "Validation --- Epoch: 17, total loss: 190842.2188 , PPL: 169.0337, Acc: 0.3677\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 18, Batch: 100, loss: 3197.0271 , Train PPL: 130.7801, Train Acc: 0.3613\n",
      "Epoch: 18, Batch: 200, loss: 3345.4414 , Train PPL: 163.9822, Train Acc: 0.3232\n",
      "Epoch: 18, Batch: 300, loss: 3258.3625 , Train PPL: 143.5978, Train Acc: 0.3384\n",
      "Epoch: 18, Batch: 400, loss: 3070.1360 , Train PPL: 107.7793, Train Acc: 0.3948\n",
      "Epoch: 18, Batch: 500, loss: 3293.1631 , Train PPL: 151.4213, Train Acc: 0.3125\n",
      "Epoch: 18, Batch: 600, loss: 3196.4419 , Train PPL: 130.6634, Train Acc: 0.4055\n",
      "Epoch: 18, Batch: 700, loss: 3367.3809 , Train PPL: 169.5593, Train Acc: 0.3110\n",
      "Validation --- Epoch: 18, total loss: 191468.4062 , PPL: 171.9675, Acc: 0.3617\n",
      "lr = 4\n",
      "Epoch: 19, Batch: 100, loss: 3258.9893 , Train PPL: 143.7350, Train Acc: 0.3491\n",
      "Epoch: 19, Batch: 200, loss: 3275.1057 , Train PPL: 147.3100, Train Acc: 0.4040\n",
      "Epoch: 19, Batch: 300, loss: 3174.5762 , Train PPL: 126.3800, Train Acc: 0.3979\n",
      "Epoch: 19, Batch: 400, loss: 3211.1318 , Train PPL: 133.6224, Train Acc: 0.3445\n",
      "Epoch: 19, Batch: 500, loss: 3319.2437 , Train PPL: 157.5626, Train Acc: 0.2774\n",
      "Epoch: 19, Batch: 600, loss: 3410.0461 , Train PPL: 180.9537, Train Acc: 0.2988\n",
      "Epoch: 19, Batch: 700, loss: 3232.3252 , Train PPL: 138.0098, Train Acc: 0.3521\n",
      "Validation --- Epoch: 19, total loss: 190111.0312 , PPL: 165.7983, Acc: 0.3694\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 20, Batch: 100, loss: 3031.4062 , Train PPL: 101.6003, Train Acc: 0.3841\n",
      "Epoch: 20, Batch: 200, loss: 3217.6116 , Train PPL: 134.9489, Train Acc: 0.3399\n",
      "Epoch: 20, Batch: 300, loss: 3415.4890 , Train PPL: 182.4613, Train Acc: 0.3125\n",
      "Epoch: 20, Batch: 400, loss: 3136.8718 , Train PPL: 119.3210, Train Acc: 0.3491\n",
      "Epoch: 20, Batch: 500, loss: 3126.3687 , Train PPL: 117.4257, Train Acc: 0.3948\n",
      "Epoch: 20, Batch: 600, loss: 3303.8040 , Train PPL: 153.8974, Train Acc: 0.2896\n",
      "Epoch: 20, Batch: 700, loss: 3151.0269 , Train PPL: 121.9236, Train Acc: 0.3780\n",
      "Validation --- Epoch: 20, total loss: 190301.5781 , PPL: 166.7775, Acc: 0.3710\n",
      "lr = 4\n",
      "Epoch: 21, Batch: 100, loss: 3355.6174 , Train PPL: 166.5458, Train Acc: 0.2851\n",
      "Epoch: 21, Batch: 200, loss: 3262.6951 , Train PPL: 144.5493, Train Acc: 0.3521\n",
      "Epoch: 21, Batch: 300, loss: 3170.7322 , Train PPL: 125.6416, Train Acc: 0.4085\n",
      "Epoch: 21, Batch: 400, loss: 3084.8494 , Train PPL: 110.2239, Train Acc: 0.3521\n",
      "Epoch: 21, Batch: 500, loss: 3339.0366 , Train PPL: 162.3890, Train Acc: 0.3415\n",
      "Epoch: 21, Batch: 600, loss: 3244.3096 , Train PPL: 140.5543, Train Acc: 0.3262\n",
      "Epoch: 21, Batch: 700, loss: 3133.1750 , Train PPL: 118.6504, Train Acc: 0.3765\n",
      "Validation --- Epoch: 21, total loss: 189567.8750 , PPL: 163.3923, Acc: 0.3712\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 22, Batch: 100, loss: 3246.8857 , Train PPL: 141.1074, Train Acc: 0.3445\n",
      "Epoch: 22, Batch: 200, loss: 3228.9365 , Train PPL: 137.2988, Train Acc: 0.3110\n",
      "Epoch: 22, Batch: 300, loss: 3196.3152 , Train PPL: 130.6382, Train Acc: 0.3399\n",
      "Epoch: 22, Batch: 400, loss: 3060.2346 , Train PPL: 106.1647, Train Acc: 0.3780\n",
      "Epoch: 22, Batch: 500, loss: 3343.2158 , Train PPL: 163.4268, Train Acc: 0.3460\n",
      "Epoch: 22, Batch: 600, loss: 3211.0413 , Train PPL: 133.6039, Train Acc: 0.3750\n",
      "Epoch: 22, Batch: 700, loss: 3077.6755 , Train PPL: 109.0251, Train Acc: 0.3979\n",
      "Validation --- Epoch: 22, total loss: 189637.6562 , PPL: 163.5992, Acc: 0.3728\n",
      "lr = 4\n",
      "Epoch: 23, Batch: 100, loss: 3404.7305 , Train PPL: 179.4933, Train Acc: 0.2866\n",
      "Epoch: 23, Batch: 200, loss: 2922.3315 , Train PPL: 86.0367, Train Acc: 0.4085\n",
      "Epoch: 23, Batch: 300, loss: 3175.4241 , Train PPL: 126.5434, Train Acc: 0.3704\n",
      "Epoch: 23, Batch: 400, loss: 3292.0654 , Train PPL: 151.1681, Train Acc: 0.3430\n",
      "Epoch: 23, Batch: 500, loss: 3300.9863 , Train PPL: 153.2379, Train Acc: 0.3277\n",
      "Epoch: 23, Batch: 600, loss: 3089.3481 , Train PPL: 110.9824, Train Acc: 0.3857\n",
      "Epoch: 23, Batch: 700, loss: 3326.7981 , Train PPL: 159.3875, Train Acc: 0.3049\n",
      "Validation --- Epoch: 23, total loss: 189983.6875 , PPL: 165.2638, Acc: 0.3686\n",
      "lr = 4\n",
      "Epoch: 24, Batch: 100, loss: 3245.4436 , Train PPL: 140.7975, Train Acc: 0.3125\n",
      "Epoch: 24, Batch: 200, loss: 3168.4465 , Train PPL: 125.2046, Train Acc: 0.3704\n",
      "Epoch: 24, Batch: 300, loss: 3169.9719 , Train PPL: 125.4961, Train Acc: 0.4009\n",
      "Epoch: 24, Batch: 400, loss: 3064.1836 , Train PPL: 106.8057, Train Acc: 0.3460\n",
      "Epoch: 24, Batch: 500, loss: 3272.3730 , Train PPL: 146.6977, Train Acc: 0.3537\n",
      "Epoch: 24, Batch: 600, loss: 3103.0752 , Train PPL: 113.3293, Train Acc: 0.3918\n",
      "Epoch: 24, Batch: 700, loss: 3109.6707 , Train PPL: 114.4744, Train Acc: 0.3887\n",
      "Validation --- Epoch: 24, total loss: 189067.3438 , PPL: 161.2828, Acc: 0.3761\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 25, Batch: 100, loss: 2910.2583 , Train PPL: 84.4677, Train Acc: 0.4009\n",
      "Epoch: 25, Batch: 200, loss: 3100.7729 , Train PPL: 112.9322, Train Acc: 0.3567\n",
      "Epoch: 25, Batch: 300, loss: 3252.1938 , Train PPL: 142.2538, Train Acc: 0.3338\n",
      "Epoch: 25, Batch: 400, loss: 3301.0029 , Train PPL: 153.2417, Train Acc: 0.3735\n",
      "Epoch: 25, Batch: 500, loss: 3392.1536 , Train PPL: 176.0848, Train Acc: 0.3049\n",
      "Epoch: 25, Batch: 600, loss: 3214.3296 , Train PPL: 134.2754, Train Acc: 0.3491\n",
      "Epoch: 25, Batch: 700, loss: 3205.1326 , Train PPL: 132.4060, Train Acc: 0.3445\n",
      "Validation --- Epoch: 25, total loss: 189553.1562 , PPL: 163.6076, Acc: 0.3651\n",
      "lr = 4\n",
      "Epoch: 26, Batch: 100, loss: 3146.3169 , Train PPL: 121.0513, Train Acc: 0.3598\n",
      "Epoch: 26, Batch: 200, loss: 3213.5081 , Train PPL: 134.1073, Train Acc: 0.3491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Batch: 300, loss: 3048.4851 , Train PPL: 104.2801, Train Acc: 0.3720\n",
      "Epoch: 26, Batch: 400, loss: 3177.4333 , Train PPL: 126.9316, Train Acc: 0.3765\n",
      "Epoch: 26, Batch: 500, loss: 3290.4629 , Train PPL: 150.7993, Train Acc: 0.3323\n",
      "Epoch: 26, Batch: 600, loss: 3225.8179 , Train PPL: 136.6476, Train Acc: 0.3430\n",
      "Epoch: 26, Batch: 700, loss: 3048.5591 , Train PPL: 104.2919, Train Acc: 0.3628\n",
      "Validation --- Epoch: 26, total loss: 188585.1406 , PPL: 159.2173, Acc: 0.3762\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 27, Batch: 100, loss: 3094.0969 , Train PPL: 111.7888, Train Acc: 0.3857\n",
      "Epoch: 27, Batch: 200, loss: 3209.3210 , Train PPL: 133.2541, Train Acc: 0.3506\n",
      "Epoch: 27, Batch: 300, loss: 3324.9995 , Train PPL: 158.9511, Train Acc: 0.3247\n",
      "Epoch: 27, Batch: 400, loss: 3125.7617 , Train PPL: 117.3171, Train Acc: 0.3293\n",
      "Epoch: 27, Batch: 500, loss: 3335.7366 , Train PPL: 161.5742, Train Acc: 0.3171\n",
      "Epoch: 27, Batch: 600, loss: 3280.9246 , Train PPL: 148.6225, Train Acc: 0.3537\n",
      "Epoch: 27, Batch: 700, loss: 3420.3792 , Train PPL: 183.8265, Train Acc: 0.2866\n",
      "Validation --- Epoch: 27, total loss: 188316.1250 , PPL: 158.2306, Acc: 0.3752\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 28, Batch: 100, loss: 3187.0894 , Train PPL: 128.8138, Train Acc: 0.2942\n",
      "Epoch: 28, Batch: 200, loss: 3149.9968 , Train PPL: 121.7323, Train Acc: 0.3689\n",
      "Epoch: 28, Batch: 300, loss: 3356.0820 , Train PPL: 166.6638, Train Acc: 0.2774\n",
      "Epoch: 28, Batch: 400, loss: 3302.6011 , Train PPL: 153.6155, Train Acc: 0.3323\n",
      "Epoch: 28, Batch: 500, loss: 3354.9165 , Train PPL: 166.3680, Train Acc: 0.3537\n",
      "Epoch: 28, Batch: 600, loss: 3230.2275 , Train PPL: 137.5693, Train Acc: 0.3841\n",
      "Epoch: 28, Batch: 700, loss: 3288.6787 , Train PPL: 150.3897, Train Acc: 0.3506\n",
      "Validation --- Epoch: 28, total loss: 189226.6562 , PPL: 162.3876, Acc: 0.3636\n",
      "lr = 4\n",
      "Epoch: 29, Batch: 100, loss: 2938.3213 , Train PPL: 88.1596, Train Acc: 0.3659\n",
      "Epoch: 29, Batch: 200, loss: 2807.3257 , Train PPL: 72.2014, Train Acc: 0.4741\n",
      "Epoch: 29, Batch: 300, loss: 3313.2522 , Train PPL: 156.1300, Train Acc: 0.2881\n",
      "Epoch: 29, Batch: 400, loss: 2978.7625 , Train PPL: 93.7654, Train Acc: 0.3857\n",
      "Epoch: 29, Batch: 500, loss: 3003.1646 , Train PPL: 97.3191, Train Acc: 0.3445\n",
      "Epoch: 29, Batch: 600, loss: 3126.6118 , Train PPL: 117.4693, Train Acc: 0.3476\n",
      "Epoch: 29, Batch: 700, loss: 3290.3723 , Train PPL: 150.7784, Train Acc: 0.3460\n",
      "Validation --- Epoch: 29, total loss: 188163.9531 , PPL: 157.6152, Acc: 0.3773\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 30, Batch: 100, loss: 3065.8689 , Train PPL: 107.0805, Train Acc: 0.3994\n",
      "Epoch: 30, Batch: 200, loss: 3269.7751 , Train PPL: 146.1178, Train Acc: 0.3186\n",
      "Epoch: 30, Batch: 300, loss: 3118.0510 , Train PPL: 115.9462, Train Acc: 0.3567\n",
      "Epoch: 30, Batch: 400, loss: 3174.1602 , Train PPL: 126.2998, Train Acc: 0.3415\n",
      "Epoch: 30, Batch: 500, loss: 3319.8435 , Train PPL: 157.7067, Train Acc: 0.3369\n",
      "Epoch: 30, Batch: 600, loss: 3120.7981 , Train PPL: 116.4328, Train Acc: 0.3552\n",
      "Epoch: 30, Batch: 700, loss: 3144.9270 , Train PPL: 120.7951, Train Acc: 0.3460\n",
      "Validation --- Epoch: 30, total loss: 188163.8750 , PPL: 157.6043, Acc: 0.3771\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 31, Batch: 100, loss: 3135.6472 , Train PPL: 119.0984, Train Acc: 0.3171\n",
      "Epoch: 31, Batch: 200, loss: 3133.9370 , Train PPL: 118.7883, Train Acc: 0.3826\n",
      "Epoch: 31, Batch: 300, loss: 3052.3660 , Train PPL: 104.8989, Train Acc: 0.4177\n",
      "Epoch: 31, Batch: 400, loss: 3187.0522 , Train PPL: 128.8065, Train Acc: 0.3963\n",
      "Epoch: 31, Batch: 500, loss: 3067.0139 , Train PPL: 107.2675, Train Acc: 0.3506\n",
      "Epoch: 31, Batch: 600, loss: 3120.1196 , Train PPL: 116.3124, Train Acc: 0.3095\n",
      "Epoch: 31, Batch: 700, loss: 3084.0659 , Train PPL: 110.0924, Train Acc: 0.3659\n",
      "Validation --- Epoch: 31, total loss: 187884.4219 , PPL: 156.5715, Acc: 0.3794\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 32, Batch: 100, loss: 3135.8289 , Train PPL: 119.1314, Train Acc: 0.3095\n",
      "Epoch: 32, Batch: 200, loss: 2868.5959 , Train PPL: 79.2700, Train Acc: 0.4253\n",
      "Epoch: 32, Batch: 300, loss: 3126.4011 , Train PPL: 117.4315, Train Acc: 0.3659\n",
      "Epoch: 32, Batch: 400, loss: 3309.9353 , Train PPL: 155.3427, Train Acc: 0.3064\n",
      "Epoch: 32, Batch: 500, loss: 3117.1912 , Train PPL: 115.7943, Train Acc: 0.3491\n",
      "Epoch: 32, Batch: 600, loss: 3070.8416 , Train PPL: 107.8953, Train Acc: 0.4055\n",
      "Epoch: 32, Batch: 700, loss: 3204.3909 , Train PPL: 132.2564, Train Acc: 0.3979\n",
      "Validation --- Epoch: 32, total loss: 187977.5938 , PPL: 156.8448, Acc: 0.3790\n",
      "lr = 4\n",
      "Epoch: 33, Batch: 100, loss: 2884.0544 , Train PPL: 81.1601, Train Acc: 0.3796\n",
      "Epoch: 33, Batch: 200, loss: 3028.1038 , Train PPL: 101.0900, Train Acc: 0.3796\n",
      "Epoch: 33, Batch: 300, loss: 3105.0010 , Train PPL: 113.6624, Train Acc: 0.3308\n",
      "Epoch: 33, Batch: 400, loss: 3142.8770 , Train PPL: 120.4182, Train Acc: 0.3430\n",
      "Epoch: 33, Batch: 500, loss: 3074.5220 , Train PPL: 108.5023, Train Acc: 0.3765\n",
      "Epoch: 33, Batch: 600, loss: 3120.0439 , Train PPL: 116.2990, Train Acc: 0.3415\n",
      "Epoch: 33, Batch: 700, loss: 3119.9900 , Train PPL: 116.2895, Train Acc: 0.3613\n",
      "Validation --- Epoch: 33, total loss: 188177.4219 , PPL: 157.8146, Acc: 0.3741\n",
      "lr = 4\n",
      "Epoch: 34, Batch: 100, loss: 2997.0576 , Train PPL: 96.4173, Train Acc: 0.4101\n",
      "Epoch: 34, Batch: 200, loss: 3195.9661 , Train PPL: 130.5687, Train Acc: 0.3171\n",
      "Epoch: 34, Batch: 300, loss: 3232.2085 , Train PPL: 137.9853, Train Acc: 0.3521\n",
      "Epoch: 34, Batch: 400, loss: 3195.0745 , Train PPL: 130.3913, Train Acc: 0.3049\n",
      "Epoch: 34, Batch: 500, loss: 2972.0762 , Train PPL: 92.8146, Train Acc: 0.3918\n",
      "Epoch: 34, Batch: 600, loss: 3123.7766 , Train PPL: 116.9626, Train Acc: 0.3963\n",
      "Epoch: 34, Batch: 700, loss: 3250.8660 , Train PPL: 141.9661, Train Acc: 0.3369\n",
      "Validation --- Epoch: 34, total loss: 187667.5000 , PPL: 155.7648, Acc: 0.3756\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 35, Batch: 100, loss: 2982.8911 , Train PPL: 94.3574, Train Acc: 0.3735\n",
      "Epoch: 35, Batch: 200, loss: 3067.8914 , Train PPL: 107.4111, Train Acc: 0.3720\n",
      "Epoch: 35, Batch: 300, loss: 3174.0637 , Train PPL: 126.2813, Train Acc: 0.3232\n",
      "Epoch: 35, Batch: 400, loss: 3070.0212 , Train PPL: 107.7604, Train Acc: 0.4268\n",
      "Epoch: 35, Batch: 500, loss: 3248.5354 , Train PPL: 141.4627, Train Acc: 0.3369\n",
      "Epoch: 35, Batch: 600, loss: 3050.3606 , Train PPL: 104.5787, Train Acc: 0.3689\n",
      "Epoch: 35, Batch: 700, loss: 3167.6929 , Train PPL: 125.0608, Train Acc: 0.3780\n",
      "Validation --- Epoch: 35, total loss: 187674.2500 , PPL: 155.7451, Acc: 0.3805\n",
      "lr = 4\n",
      "Epoch: 36, Batch: 100, loss: 3197.6270 , Train PPL: 130.8997, Train Acc: 0.3780\n",
      "Epoch: 36, Batch: 200, loss: 3010.4836 , Train PPL: 98.4109, Train Acc: 0.3735\n",
      "Epoch: 36, Batch: 300, loss: 3171.6448 , Train PPL: 125.8165, Train Acc: 0.3140\n",
      "Epoch: 36, Batch: 400, loss: 3145.2141 , Train PPL: 120.8480, Train Acc: 0.3582\n",
      "Epoch: 36, Batch: 500, loss: 3148.4749 , Train PPL: 121.4502, Train Acc: 0.3460\n",
      "Epoch: 36, Batch: 600, loss: 3191.2961 , Train PPL: 129.6425, Train Acc: 0.3369\n",
      "Epoch: 36, Batch: 700, loss: 3195.3113 , Train PPL: 130.4384, Train Acc: 0.3399\n",
      "Validation --- Epoch: 36, total loss: 187649.9375 , PPL: 155.7306, Acc: 0.3816\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 37, Batch: 100, loss: 3207.9160 , Train PPL: 132.9690, Train Acc: 0.3521\n",
      "Epoch: 37, Batch: 200, loss: 3091.6396 , Train PPL: 111.3708, Train Acc: 0.3826\n",
      "Epoch: 37, Batch: 300, loss: 3021.7229 , Train PPL: 100.1115, Train Acc: 0.3552\n",
      "Epoch: 37, Batch: 400, loss: 3032.5542 , Train PPL: 101.7782, Train Acc: 0.3735\n",
      "Epoch: 37, Batch: 500, loss: 3035.5247 , Train PPL: 102.2401, Train Acc: 0.3765\n",
      "Epoch: 37, Batch: 600, loss: 3294.4180 , Train PPL: 151.7112, Train Acc: 0.3704\n",
      "Epoch: 37, Batch: 700, loss: 3040.6995 , Train PPL: 103.0498, Train Acc: 0.3811\n",
      "Validation --- Epoch: 37, total loss: 187806.1719 , PPL: 156.4279, Acc: 0.3770\n",
      "lr = 4\n",
      "Epoch: 38, Batch: 100, loss: 3046.4182 , Train PPL: 103.9521, Train Acc: 0.3293\n",
      "Epoch: 38, Batch: 200, loss: 3069.4087 , Train PPL: 107.6598, Train Acc: 0.3338\n",
      "Epoch: 38, Batch: 300, loss: 3092.7117 , Train PPL: 111.5530, Train Acc: 0.3674\n",
      "Epoch: 38, Batch: 400, loss: 3117.1072 , Train PPL: 115.7796, Train Acc: 0.3780\n",
      "Epoch: 38, Batch: 500, loss: 3156.9456 , Train PPL: 123.0286, Train Acc: 0.3537\n",
      "Epoch: 38, Batch: 600, loss: 3051.7266 , Train PPL: 104.7967, Train Acc: 0.3704\n",
      "Epoch: 38, Batch: 700, loss: 3163.6582 , Train PPL: 124.2940, Train Acc: 0.3674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --- Epoch: 38, total loss: 187938.6875 , PPL: 156.9186, Acc: 0.3768\n",
      "lr = 4\n",
      "Epoch: 39, Batch: 100, loss: 2965.6387 , Train PPL: 91.9082, Train Acc: 0.4238\n",
      "Epoch: 39, Batch: 200, loss: 2951.9639 , Train PPL: 90.0122, Train Acc: 0.3537\n",
      "Epoch: 39, Batch: 300, loss: 3151.8035 , Train PPL: 122.0680, Train Acc: 0.3308\n",
      "Epoch: 39, Batch: 400, loss: 2840.8645 , Train PPL: 75.9888, Train Acc: 0.4238\n",
      "Epoch: 39, Batch: 500, loss: 3066.6145 , Train PPL: 107.2023, Train Acc: 0.3613\n",
      "Epoch: 39, Batch: 600, loss: 3164.7925 , Train PPL: 124.5091, Train Acc: 0.3125\n",
      "Epoch: 39, Batch: 700, loss: 3039.6597 , Train PPL: 102.8866, Train Acc: 0.4070\n",
      "Validation --- Epoch: 39, total loss: 187941.4844 , PPL: 157.1770, Acc: 0.3770\n",
      "lr = 4\n",
      "Epoch: 40, Batch: 100, loss: 3220.4998 , Train PPL: 135.5443, Train Acc: 0.3186\n",
      "Epoch: 40, Batch: 200, loss: 3231.2261 , Train PPL: 137.7788, Train Acc: 0.2896\n",
      "Epoch: 40, Batch: 300, loss: 3156.7534 , Train PPL: 122.9926, Train Acc: 0.3201\n",
      "Epoch: 40, Batch: 400, loss: 2933.7810 , Train PPL: 87.5515, Train Acc: 0.3567\n",
      "Epoch: 40, Batch: 500, loss: 2878.5042 , Train PPL: 80.4764, Train Acc: 0.4284\n",
      "Epoch: 40, Batch: 600, loss: 3077.6028 , Train PPL: 109.0131, Train Acc: 0.4177\n",
      "Epoch: 40, Batch: 700, loss: 3053.9983 , Train PPL: 105.1602, Train Acc: 0.4040\n",
      "Validation --- Epoch: 40, total loss: 187998.9375 , PPL: 157.0167, Acc: 0.3739\n",
      "lr = 2.0\n",
      "Epoch: 41, Batch: 100, loss: 3018.4485 , Train PPL: 99.6131, Train Acc: 0.3857\n",
      "Epoch: 41, Batch: 200, loss: 3186.2688 , Train PPL: 128.6528, Train Acc: 0.3293\n",
      "Epoch: 41, Batch: 300, loss: 3223.6631 , Train PPL: 136.1995, Train Acc: 0.3598\n",
      "Epoch: 41, Batch: 400, loss: 3077.9612 , Train PPL: 109.0727, Train Acc: 0.3567\n",
      "Epoch: 41, Batch: 500, loss: 2985.3862 , Train PPL: 94.7170, Train Acc: 0.4009\n",
      "Epoch: 41, Batch: 600, loss: 2994.9143 , Train PPL: 96.1028, Train Acc: 0.3902\n",
      "Epoch: 41, Batch: 700, loss: 3029.9351 , Train PPL: 101.3727, Train Acc: 0.4116\n",
      "Validation --- Epoch: 41, total loss: 186315.1094 , PPL: 150.3878, Acc: 0.3888\n",
      "lr = 2.0\n",
      "wrote model\n",
      "Epoch: 42, Batch: 100, loss: 2912.8152 , Train PPL: 84.7976, Train Acc: 0.3750\n",
      "Epoch: 42, Batch: 200, loss: 2906.5310 , Train PPL: 83.9891, Train Acc: 0.3918\n",
      "Epoch: 42, Batch: 300, loss: 3023.9587 , Train PPL: 100.4533, Train Acc: 0.3735\n",
      "Epoch: 42, Batch: 400, loss: 3073.1494 , Train PPL: 108.2755, Train Acc: 0.3567\n",
      "Epoch: 42, Batch: 500, loss: 2942.5227 , Train PPL: 88.7260, Train Acc: 0.3948\n",
      "Epoch: 42, Batch: 600, loss: 2992.7021 , Train PPL: 95.7793, Train Acc: 0.3765\n",
      "Epoch: 42, Batch: 700, loss: 3054.1975 , Train PPL: 105.1922, Train Acc: 0.3628\n",
      "Validation --- Epoch: 42, total loss: 186191.8438 , PPL: 150.0227, Acc: 0.3928\n",
      "lr = 2.0\n",
      "wrote model\n",
      "Epoch: 43, Batch: 100, loss: 2810.5063 , Train PPL: 72.5524, Train Acc: 0.4329\n",
      "Epoch: 43, Batch: 200, loss: 3200.9758 , Train PPL: 131.5696, Train Acc: 0.3628\n",
      "Epoch: 43, Batch: 300, loss: 3077.0056 , Train PPL: 108.9139, Train Acc: 0.3933\n",
      "Epoch: 43, Batch: 400, loss: 3099.2690 , Train PPL: 112.6736, Train Acc: 0.3537\n",
      "Epoch: 43, Batch: 500, loss: 3017.7825 , Train PPL: 99.5120, Train Acc: 0.3521\n",
      "Epoch: 43, Batch: 600, loss: 3170.7991 , Train PPL: 125.6544, Train Acc: 0.3460\n",
      "Epoch: 43, Batch: 700, loss: 2983.7642 , Train PPL: 94.4831, Train Acc: 0.3659\n",
      "Validation --- Epoch: 43, total loss: 186624.7656 , PPL: 151.7471, Acc: 0.3858\n",
      "lr = 2.0\n",
      "Epoch: 44, Batch: 100, loss: 2816.3665 , Train PPL: 73.2034, Train Acc: 0.4101\n",
      "Epoch: 44, Batch: 200, loss: 2921.5696 , Train PPL: 85.9368, Train Acc: 0.3948\n",
      "Epoch: 44, Batch: 300, loss: 2784.1904 , Train PPL: 69.6995, Train Acc: 0.4482\n",
      "Epoch: 44, Batch: 400, loss: 3152.3438 , Train PPL: 122.1686, Train Acc: 0.3430\n",
      "Epoch: 44, Batch: 500, loss: 3205.9160 , Train PPL: 132.5642, Train Acc: 0.3537\n",
      "Epoch: 44, Batch: 600, loss: 3091.7083 , Train PPL: 111.3825, Train Acc: 0.3399\n",
      "Epoch: 44, Batch: 700, loss: 2963.8735 , Train PPL: 91.6613, Train Acc: 0.4207\n",
      "Validation --- Epoch: 44, total loss: 186441.0312 , PPL: 151.0700, Acc: 0.3879\n",
      "lr = 2.0\n",
      "Epoch: 45, Batch: 100, loss: 3079.9656 , Train PPL: 109.4064, Train Acc: 0.4085\n",
      "Epoch: 45, Batch: 200, loss: 2981.2290 , Train PPL: 94.1187, Train Acc: 0.3796\n",
      "Epoch: 45, Batch: 300, loss: 3084.4624 , Train PPL: 110.1590, Train Acc: 0.3384\n",
      "Epoch: 45, Batch: 400, loss: 3037.0867 , Train PPL: 102.4839, Train Acc: 0.3857\n",
      "Epoch: 45, Batch: 500, loss: 2801.3206 , Train PPL: 71.5435, Train Acc: 0.4284\n",
      "Epoch: 45, Batch: 600, loss: 3104.5376 , Train PPL: 113.5822, Train Acc: 0.3628\n",
      "Epoch: 45, Batch: 700, loss: 3203.9036 , Train PPL: 132.1582, Train Acc: 0.3186\n",
      "Validation --- Epoch: 45, total loss: 186099.4062 , PPL: 149.7190, Acc: 0.3912\n",
      "lr = 2.0\n",
      "wrote model\n",
      "Epoch: 46, Batch: 100, loss: 3003.2761 , Train PPL: 97.3356, Train Acc: 0.3628\n",
      "Epoch: 46, Batch: 200, loss: 2968.3042 , Train PPL: 92.2825, Train Acc: 0.3476\n",
      "Epoch: 46, Batch: 300, loss: 2992.9219 , Train PPL: 95.8113, Train Acc: 0.3674\n",
      "Epoch: 46, Batch: 400, loss: 2999.1726 , Train PPL: 96.7286, Train Acc: 0.3552\n",
      "Epoch: 46, Batch: 500, loss: 3097.9224 , Train PPL: 112.4426, Train Acc: 0.3643\n",
      "Epoch: 46, Batch: 600, loss: 3000.0447 , Train PPL: 96.8573, Train Acc: 0.4436\n",
      "Epoch: 46, Batch: 700, loss: 2906.9409 , Train PPL: 84.0417, Train Acc: 0.4162\n",
      "Validation --- Epoch: 46, total loss: 186157.2188 , PPL: 149.9368, Acc: 0.3926\n",
      "lr = 2.0\n",
      "Epoch: 47, Batch: 100, loss: 3147.9873 , Train PPL: 121.3600, Train Acc: 0.3674\n",
      "Epoch: 47, Batch: 200, loss: 3014.6016 , Train PPL: 99.0307, Train Acc: 0.4040\n",
      "Epoch: 47, Batch: 300, loss: 2922.9202 , Train PPL: 86.1139, Train Acc: 0.4207\n",
      "Epoch: 47, Batch: 400, loss: 3151.6147 , Train PPL: 122.0329, Train Acc: 0.3643\n",
      "Epoch: 47, Batch: 500, loss: 3066.7302 , Train PPL: 107.2212, Train Acc: 0.4146\n",
      "Epoch: 47, Batch: 600, loss: 2825.8740 , Train PPL: 74.2721, Train Acc: 0.4131\n",
      "Epoch: 47, Batch: 700, loss: 2994.6995 , Train PPL: 96.0713, Train Acc: 0.3476\n",
      "Validation --- Epoch: 47, total loss: 186088.4531 , PPL: 149.6961, Acc: 0.3920\n",
      "lr = 2.0\n",
      "wrote model\n",
      "Epoch: 48, Batch: 100, loss: 2841.1221 , Train PPL: 76.0187, Train Acc: 0.4101\n",
      "Epoch: 48, Batch: 200, loss: 3026.6008 , Train PPL: 100.8587, Train Acc: 0.3521\n",
      "Epoch: 48, Batch: 300, loss: 3363.6003 , Train PPL: 168.5849, Train Acc: 0.3018\n",
      "Epoch: 48, Batch: 400, loss: 3045.8062 , Train PPL: 103.8552, Train Acc: 0.3582\n",
      "Epoch: 48, Batch: 500, loss: 2952.5466 , Train PPL: 90.0922, Train Acc: 0.3918\n",
      "Epoch: 48, Batch: 600, loss: 3052.0217 , Train PPL: 104.8438, Train Acc: 0.4040\n",
      "Epoch: 48, Batch: 700, loss: 2993.6123 , Train PPL: 95.9122, Train Acc: 0.3659\n",
      "Validation --- Epoch: 48, total loss: 186106.8125 , PPL: 149.7517, Acc: 0.3917\n",
      "lr = 2.0\n",
      "Epoch: 49, Batch: 100, loss: 2991.9045 , Train PPL: 95.6629, Train Acc: 0.3277\n",
      "Epoch: 49, Batch: 200, loss: 2960.2886 , Train PPL: 91.1617, Train Acc: 0.3521\n",
      "Epoch: 49, Batch: 300, loss: 2908.8713 , Train PPL: 84.2893, Train Acc: 0.4177\n",
      "Epoch: 49, Batch: 400, loss: 2905.6401 , Train PPL: 83.8752, Train Acc: 0.4146\n",
      "Epoch: 49, Batch: 500, loss: 3064.9785 , Train PPL: 106.9353, Train Acc: 0.3277\n",
      "Epoch: 49, Batch: 600, loss: 2958.5620 , Train PPL: 90.9221, Train Acc: 0.4421\n",
      "Epoch: 49, Batch: 700, loss: 3113.9949 , Train PPL: 115.2315, Train Acc: 0.3186\n",
      "Validation --- Epoch: 49, total loss: 186024.8906 , PPL: 149.3876, Acc: 0.3924\n",
      "lr = 2.0\n",
      "wrote model\n",
      "Epoch: 50, Batch: 100, loss: 3019.7468 , Train PPL: 99.8104, Train Acc: 0.3948\n",
      "Epoch: 50, Batch: 200, loss: 3065.5974 , Train PPL: 107.0362, Train Acc: 0.3338\n",
      "Epoch: 50, Batch: 300, loss: 2804.5901 , Train PPL: 71.9010, Train Acc: 0.4299\n",
      "Epoch: 50, Batch: 400, loss: 2880.1272 , Train PPL: 80.6757, Train Acc: 0.4116\n",
      "Epoch: 50, Batch: 500, loss: 2957.6216 , Train PPL: 90.7919, Train Acc: 0.3918\n",
      "Epoch: 50, Batch: 600, loss: 2944.1729 , Train PPL: 88.9495, Train Acc: 0.3826\n",
      "Epoch: 50, Batch: 700, loss: 3112.1392 , Train PPL: 114.9060, Train Acc: 0.3750\n",
      "Validation --- Epoch: 50, total loss: 186145.6406 , PPL: 150.0505, Acc: 0.3920\n",
      "lr = 2.0\n",
      "Epoch: 51, Batch: 100, loss: 2947.9253 , Train PPL: 89.4597, Train Acc: 0.3841\n",
      "Epoch: 51, Batch: 200, loss: 2956.5325 , Train PPL: 90.6412, Train Acc: 0.3659\n",
      "Epoch: 51, Batch: 300, loss: 3043.2410 , Train PPL: 103.4498, Train Acc: 0.3643\n",
      "Epoch: 51, Batch: 400, loss: 2997.6482 , Train PPL: 96.5041, Train Acc: 0.3415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51, Batch: 500, loss: 3116.3850 , Train PPL: 115.6522, Train Acc: 0.3354\n",
      "Epoch: 51, Batch: 600, loss: 2902.3120 , Train PPL: 83.4507, Train Acc: 0.4116\n",
      "Epoch: 51, Batch: 700, loss: 3118.7886 , Train PPL: 116.0767, Train Acc: 0.3720\n",
      "Validation --- Epoch: 51, total loss: 185891.4062 , PPL: 148.9198, Acc: 0.3928\n",
      "lr = 2.0\n",
      "wrote model\n",
      "Epoch: 52, Batch: 100, loss: 2862.0383 , Train PPL: 78.4815, Train Acc: 0.4238\n",
      "Epoch: 52, Batch: 200, loss: 3088.3704 , Train PPL: 110.8171, Train Acc: 0.3841\n",
      "Epoch: 52, Batch: 300, loss: 2855.5371 , Train PPL: 77.7076, Train Acc: 0.4390\n",
      "Epoch: 52, Batch: 400, loss: 2963.5693 , Train PPL: 91.6188, Train Acc: 0.3841\n",
      "Epoch: 52, Batch: 500, loss: 3086.4370 , Train PPL: 110.4911, Train Acc: 0.3415\n",
      "Epoch: 52, Batch: 600, loss: 2761.7742 , Train PPL: 67.3580, Train Acc: 0.3979\n",
      "Epoch: 52, Batch: 700, loss: 3069.9504 , Train PPL: 107.7488, Train Acc: 0.3537\n",
      "Validation --- Epoch: 52, total loss: 185871.2188 , PPL: 148.7552, Acc: 0.3920\n",
      "lr = 2.0\n",
      "wrote model\n",
      "Epoch: 53, Batch: 100, loss: 2700.5144 , Train PPL: 61.3526, Train Acc: 0.4482\n",
      "Epoch: 53, Batch: 200, loss: 2976.6536 , Train PPL: 93.4645, Train Acc: 0.3826\n",
      "Epoch: 53, Batch: 300, loss: 3037.3792 , Train PPL: 102.5296, Train Acc: 0.3521\n",
      "Epoch: 53, Batch: 400, loss: 3035.9810 , Train PPL: 102.3113, Train Acc: 0.3506\n",
      "Epoch: 53, Batch: 500, loss: 2971.4602 , Train PPL: 92.7275, Train Acc: 0.4055\n",
      "Epoch: 53, Batch: 600, loss: 3073.0942 , Train PPL: 108.2664, Train Acc: 0.3537\n",
      "Epoch: 53, Batch: 700, loss: 3179.8289 , Train PPL: 127.3960, Train Acc: 0.3811\n",
      "Validation --- Epoch: 53, total loss: 186026.2188 , PPL: 149.4659, Acc: 0.3921\n",
      "lr = 2.0\n",
      "Epoch: 54, Batch: 100, loss: 2873.2815 , Train PPL: 79.8382, Train Acc: 0.3689\n",
      "Epoch: 54, Batch: 200, loss: 2956.3206 , Train PPL: 90.6119, Train Acc: 0.3963\n",
      "Epoch: 54, Batch: 300, loss: 2877.5483 , Train PPL: 80.3592, Train Acc: 0.4040\n",
      "Epoch: 54, Batch: 400, loss: 3153.4111 , Train PPL: 122.3676, Train Acc: 0.3155\n",
      "Epoch: 54, Batch: 500, loss: 2841.0159 , Train PPL: 76.0063, Train Acc: 0.3902\n",
      "Epoch: 54, Batch: 600, loss: 2955.3689 , Train PPL: 90.4806, Train Acc: 0.4070\n",
      "Epoch: 54, Batch: 700, loss: 2813.5544 , Train PPL: 72.8902, Train Acc: 0.4421\n",
      "Validation --- Epoch: 54, total loss: 185872.8281 , PPL: 148.7978, Acc: 0.3920\n",
      "lr = 2.0\n",
      "Epoch: 55, Batch: 100, loss: 2848.9370 , Train PPL: 76.9297, Train Acc: 0.4466\n",
      "Epoch: 55, Batch: 200, loss: 2980.1326 , Train PPL: 93.9615, Train Acc: 0.3841\n",
      "Epoch: 55, Batch: 300, loss: 3144.1794 , Train PPL: 120.6575, Train Acc: 0.3384\n",
      "Epoch: 55, Batch: 400, loss: 3157.5906 , Train PPL: 123.1497, Train Acc: 0.3796\n",
      "Epoch: 55, Batch: 500, loss: 2826.1677 , Train PPL: 74.3053, Train Acc: 0.4375\n",
      "Epoch: 55, Batch: 600, loss: 2947.5383 , Train PPL: 89.4070, Train Acc: 0.3963\n",
      "Epoch: 55, Batch: 700, loss: 3097.9980 , Train PPL: 112.4556, Train Acc: 0.3445\n",
      "Validation --- Epoch: 55, total loss: 186290.0625 , PPL: 150.6666, Acc: 0.3906\n",
      "lr = 2.0\n",
      "Epoch: 56, Batch: 100, loss: 3014.5051 , Train PPL: 99.0161, Train Acc: 0.3399\n",
      "Epoch: 56, Batch: 200, loss: 2840.8665 , Train PPL: 75.9891, Train Acc: 0.4329\n",
      "Epoch: 56, Batch: 300, loss: 3005.0474 , Train PPL: 97.5988, Train Acc: 0.3902\n",
      "Epoch: 56, Batch: 400, loss: 2873.4851 , Train PPL: 79.8630, Train Acc: 0.3750\n",
      "Epoch: 56, Batch: 500, loss: 3030.6455 , Train PPL: 101.4825, Train Acc: 0.3582\n",
      "Epoch: 56, Batch: 600, loss: 3028.9795 , Train PPL: 101.2251, Train Acc: 0.3704\n",
      "Epoch: 56, Batch: 700, loss: 3117.8684 , Train PPL: 115.9140, Train Acc: 0.3643\n",
      "Validation --- Epoch: 56, total loss: 185981.2656 , PPL: 149.3380, Acc: 0.3924\n",
      "lr = 2.0\n",
      "Epoch: 57, Batch: 100, loss: 2911.7109 , Train PPL: 84.6550, Train Acc: 0.4634\n",
      "Epoch: 57, Batch: 200, loss: 2945.6895 , Train PPL: 89.1553, Train Acc: 0.3841\n",
      "Epoch: 57, Batch: 300, loss: 3022.0610 , Train PPL: 100.1632, Train Acc: 0.3552\n",
      "Epoch: 57, Batch: 400, loss: 3027.4656 , Train PPL: 100.9918, Train Acc: 0.3521\n",
      "Epoch: 57, Batch: 500, loss: 2952.8279 , Train PPL: 90.1308, Train Acc: 0.3613\n",
      "Epoch: 57, Batch: 600, loss: 3019.7502 , Train PPL: 99.8109, Train Acc: 0.3125\n",
      "Epoch: 57, Batch: 700, loss: 2946.0847 , Train PPL: 89.2091, Train Acc: 0.3963\n",
      "Validation --- Epoch: 57, total loss: 185784.5625 , PPL: 148.5749, Acc: 0.3927\n",
      "lr = 2.0\n",
      "wrote model\n",
      "Epoch: 58, Batch: 100, loss: 2894.1018 , Train PPL: 82.4128, Train Acc: 0.4085\n",
      "Epoch: 58, Batch: 200, loss: 2838.0171 , Train PPL: 75.6597, Train Acc: 0.4360\n",
      "Epoch: 58, Batch: 300, loss: 3085.3069 , Train PPL: 110.3009, Train Acc: 0.3780\n",
      "Epoch: 58, Batch: 400, loss: 3064.8848 , Train PPL: 106.9200, Train Acc: 0.3582\n",
      "Epoch: 58, Batch: 500, loss: 3034.4211 , Train PPL: 102.0683, Train Acc: 0.3506\n",
      "Epoch: 58, Batch: 600, loss: 2655.9028 , Train PPL: 57.3190, Train Acc: 0.4848\n",
      "Epoch: 58, Batch: 700, loss: 2884.6328 , Train PPL: 81.2317, Train Acc: 0.4573\n",
      "Validation --- Epoch: 58, total loss: 186328.9375 , PPL: 150.7836, Acc: 0.3908\n",
      "lr = 2.0\n",
      "Epoch: 59, Batch: 100, loss: 2854.7017 , Train PPL: 77.6087, Train Acc: 0.3872\n",
      "Epoch: 59, Batch: 200, loss: 3103.0598 , Train PPL: 113.3266, Train Acc: 0.3323\n",
      "Epoch: 59, Batch: 300, loss: 2910.6733 , Train PPL: 84.5212, Train Acc: 0.3948\n",
      "Epoch: 59, Batch: 400, loss: 3088.1479 , Train PPL: 110.7796, Train Acc: 0.3308\n",
      "Epoch: 59, Batch: 500, loss: 2967.3713 , Train PPL: 92.1513, Train Acc: 0.4009\n",
      "Epoch: 59, Batch: 600, loss: 2825.8247 , Train PPL: 74.2665, Train Acc: 0.4299\n",
      "Epoch: 59, Batch: 700, loss: 2964.5979 , Train PPL: 91.7625, Train Acc: 0.3811\n",
      "Validation --- Epoch: 59, total loss: 186059.7812 , PPL: 149.7103, Acc: 0.3931\n",
      "lr = 2.0\n",
      "Epoch: 60, Batch: 100, loss: 2939.5679 , Train PPL: 88.3272, Train Acc: 0.3994\n",
      "Epoch: 60, Batch: 200, loss: 2855.8059 , Train PPL: 77.7394, Train Acc: 0.4284\n",
      "Epoch: 60, Batch: 300, loss: 2698.4272 , Train PPL: 61.1577, Train Acc: 0.4024\n",
      "Epoch: 60, Batch: 400, loss: 3010.9336 , Train PPL: 98.4785, Train Acc: 0.3628\n",
      "Epoch: 60, Batch: 500, loss: 2851.3364 , Train PPL: 77.2116, Train Acc: 0.4101\n",
      "Epoch: 60, Batch: 600, loss: 3031.2280 , Train PPL: 101.5726, Train Acc: 0.3887\n",
      "Epoch: 60, Batch: 700, loss: 2814.0952 , Train PPL: 72.9504, Train Acc: 0.4070\n",
      "Validation --- Epoch: 60, total loss: 185976.4688 , PPL: 149.3338, Acc: 0.3921\n",
      "lr = 2.0\n",
      "Epoch: 61, Batch: 100, loss: 2959.7688 , Train PPL: 91.0895, Train Acc: 0.3765\n",
      "Epoch: 61, Batch: 200, loss: 2915.7715 , Train PPL: 85.1806, Train Acc: 0.3796\n",
      "Epoch: 61, Batch: 300, loss: 3010.2910 , Train PPL: 98.3820, Train Acc: 0.4070\n",
      "Epoch: 61, Batch: 400, loss: 2890.6643 , Train PPL: 81.9820, Train Acc: 0.3948\n",
      "Epoch: 61, Batch: 500, loss: 2987.1833 , Train PPL: 94.9769, Train Acc: 0.3521\n",
      "Epoch: 61, Batch: 600, loss: 3076.0239 , Train PPL: 108.7510, Train Acc: 0.3628\n",
      "Epoch: 61, Batch: 700, loss: 3062.2922 , Train PPL: 106.4982, Train Acc: 0.3491\n",
      "Validation --- Epoch: 61, total loss: 186359.5156 , PPL: 151.0314, Acc: 0.3831\n",
      "lr = 2.0\n",
      "Epoch: 62, Batch: 100, loss: 2965.6584 , Train PPL: 91.9110, Train Acc: 0.3841\n",
      "Epoch: 62, Batch: 200, loss: 3098.9143 , Train PPL: 112.6127, Train Acc: 0.3384\n",
      "Epoch: 62, Batch: 300, loss: 2985.2925 , Train PPL: 94.7035, Train Acc: 0.3262\n",
      "Epoch: 62, Batch: 400, loss: 3061.4214 , Train PPL: 106.3570, Train Acc: 0.3735\n",
      "Epoch: 62, Batch: 500, loss: 2905.6023 , Train PPL: 83.8703, Train Acc: 0.3689\n",
      "Epoch: 62, Batch: 600, loss: 2866.0481 , Train PPL: 78.9627, Train Acc: 0.4268\n",
      "Epoch: 62, Batch: 700, loss: 2923.5857 , Train PPL: 86.2013, Train Acc: 0.3963\n",
      "Validation --- Epoch: 62, total loss: 185806.4688 , PPL: 148.7484, Acc: 0.3909\n",
      "lr = 2.0\n",
      "Epoch: 63, Batch: 100, loss: 2883.4141 , Train PPL: 81.0810, Train Acc: 0.4070\n",
      "Epoch: 63, Batch: 200, loss: 2943.1306 , Train PPL: 88.8083, Train Acc: 0.4040\n",
      "Epoch: 63, Batch: 300, loss: 3055.4797 , Train PPL: 105.3980, Train Acc: 0.3720\n",
      "Epoch: 63, Batch: 400, loss: 2819.2544 , Train PPL: 73.5263, Train Acc: 0.3826\n",
      "Epoch: 63, Batch: 500, loss: 3064.6467 , Train PPL: 106.8812, Train Acc: 0.3567\n",
      "Epoch: 63, Batch: 600, loss: 2887.6626 , Train PPL: 81.6078, Train Acc: 0.3460\n",
      "Epoch: 63, Batch: 700, loss: 2911.4414 , Train PPL: 84.6202, Train Acc: 0.3872\n",
      "Validation --- Epoch: 63, total loss: 185955.9219 , PPL: 149.3214, Acc: 0.3925\n",
      "lr = 1.0\n",
      "Epoch: 64, Batch: 100, loss: 2795.7371 , Train PPL: 70.9371, Train Acc: 0.4253\n",
      "Epoch: 64, Batch: 200, loss: 2830.6289 , Train PPL: 74.8123, Train Acc: 0.4345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 64, Batch: 300, loss: 2915.1150 , Train PPL: 85.0954, Train Acc: 0.3735\n",
      "Epoch: 64, Batch: 400, loss: 2858.2717 , Train PPL: 78.0322, Train Acc: 0.3918\n",
      "Epoch: 64, Batch: 500, loss: 3089.4714 , Train PPL: 111.0033, Train Acc: 0.3582\n",
      "Epoch: 64, Batch: 600, loss: 2876.5271 , Train PPL: 80.2342, Train Acc: 0.4421\n",
      "Epoch: 64, Batch: 700, loss: 2741.1584 , Train PPL: 65.2741, Train Acc: 0.4848\n",
      "Validation --- Epoch: 64, total loss: 185719.1875 , PPL: 148.4956, Acc: 0.3958\n",
      "lr = 1.0\n",
      "wrote model\n",
      "Epoch: 65, Batch: 100, loss: 2759.4048 , Train PPL: 67.1152, Train Acc: 0.4101\n",
      "Epoch: 65, Batch: 200, loss: 3020.0657 , Train PPL: 99.8590, Train Acc: 0.3780\n",
      "Epoch: 65, Batch: 300, loss: 2956.3022 , Train PPL: 90.6094, Train Acc: 0.3933\n",
      "Epoch: 65, Batch: 400, loss: 3050.2869 , Train PPL: 104.5669, Train Acc: 0.3415\n",
      "Epoch: 65, Batch: 500, loss: 2871.0701 , Train PPL: 79.5695, Train Acc: 0.3918\n",
      "Epoch: 65, Batch: 600, loss: 3005.1355 , Train PPL: 97.6119, Train Acc: 0.3567\n",
      "Epoch: 65, Batch: 700, loss: 3037.7847 , Train PPL: 102.5930, Train Acc: 0.3598\n",
      "Validation --- Epoch: 65, total loss: 185772.1094 , PPL: 148.7340, Acc: 0.3967\n",
      "lr = 1.0\n",
      "Epoch: 66, Batch: 100, loss: 2948.8252 , Train PPL: 89.5826, Train Acc: 0.4055\n",
      "Epoch: 66, Batch: 200, loss: 2920.0190 , Train PPL: 85.7339, Train Acc: 0.3674\n",
      "Epoch: 66, Batch: 300, loss: 2850.1099 , Train PPL: 77.0674, Train Acc: 0.4177\n",
      "Epoch: 66, Batch: 400, loss: 3013.7822 , Train PPL: 98.9070, Train Acc: 0.3735\n",
      "Epoch: 66, Batch: 500, loss: 2869.8000 , Train PPL: 79.4156, Train Acc: 0.4329\n",
      "Epoch: 66, Batch: 600, loss: 3087.2085 , Train PPL: 110.6211, Train Acc: 0.3582\n",
      "Epoch: 66, Batch: 700, loss: 2909.4893 , Train PPL: 84.3687, Train Acc: 0.3841\n",
      "Validation --- Epoch: 66, total loss: 185882.6562 , PPL: 149.1696, Acc: 0.3966\n",
      "lr = 1.0\n",
      "Epoch: 67, Batch: 100, loss: 2718.1157 , Train PPL: 63.0211, Train Acc: 0.4543\n",
      "Epoch: 67, Batch: 200, loss: 2803.0110 , Train PPL: 71.7281, Train Acc: 0.4405\n",
      "Epoch: 67, Batch: 300, loss: 2739.3398 , Train PPL: 65.0934, Train Acc: 0.4619\n",
      "Epoch: 67, Batch: 400, loss: 3080.2883 , Train PPL: 109.4602, Train Acc: 0.3841\n",
      "Epoch: 67, Batch: 500, loss: 2914.2505 , Train PPL: 84.9833, Train Acc: 0.3537\n",
      "Epoch: 67, Batch: 600, loss: 3093.5540 , Train PPL: 111.6963, Train Acc: 0.3674\n",
      "Epoch: 67, Batch: 700, loss: 2997.7224 , Train PPL: 96.5150, Train Acc: 0.3659\n",
      "Validation --- Epoch: 67, total loss: 185691.2656 , PPL: 148.3448, Acc: 0.3978\n",
      "lr = 1.0\n",
      "wrote model\n",
      "Epoch: 68, Batch: 100, loss: 2896.6238 , Train PPL: 82.7302, Train Acc: 0.3811\n",
      "Epoch: 68, Batch: 200, loss: 2979.2908 , Train PPL: 93.8410, Train Acc: 0.3445\n",
      "Epoch: 68, Batch: 300, loss: 2959.0068 , Train PPL: 90.9838, Train Acc: 0.4451\n",
      "Epoch: 68, Batch: 400, loss: 2741.5325 , Train PPL: 65.3113, Train Acc: 0.4604\n",
      "Epoch: 68, Batch: 500, loss: 2918.5090 , Train PPL: 85.5368, Train Acc: 0.4314\n",
      "Epoch: 68, Batch: 600, loss: 3002.1814 , Train PPL: 97.1733, Train Acc: 0.3674\n",
      "Epoch: 68, Batch: 700, loss: 2728.7693 , Train PPL: 64.0529, Train Acc: 0.3918\n",
      "Validation --- Epoch: 68, total loss: 185783.0312 , PPL: 148.8143, Acc: 0.3974\n",
      "lr = 1.0\n",
      "Epoch: 69, Batch: 100, loss: 2968.2236 , Train PPL: 92.2711, Train Acc: 0.4619\n",
      "Epoch: 69, Batch: 200, loss: 2927.1028 , Train PPL: 86.6647, Train Acc: 0.3918\n",
      "Epoch: 69, Batch: 300, loss: 2841.1646 , Train PPL: 76.0236, Train Acc: 0.3735\n",
      "Epoch: 69, Batch: 400, loss: 2955.1958 , Train PPL: 90.4567, Train Acc: 0.3918\n",
      "Epoch: 69, Batch: 500, loss: 2842.6208 , Train PPL: 76.1925, Train Acc: 0.3963\n",
      "Epoch: 69, Batch: 600, loss: 2800.0581 , Train PPL: 71.4059, Train Acc: 0.4390\n",
      "Epoch: 69, Batch: 700, loss: 2715.8647 , Train PPL: 62.8052, Train Acc: 0.4314\n",
      "Validation --- Epoch: 69, total loss: 185704.1250 , PPL: 148.4657, Acc: 0.3975\n",
      "lr = 1.0\n",
      "Epoch: 70, Batch: 100, loss: 2883.3406 , Train PPL: 81.0719, Train Acc: 0.3979\n",
      "Epoch: 70, Batch: 200, loss: 2765.5889 , Train PPL: 67.7508, Train Acc: 0.4253\n",
      "Epoch: 70, Batch: 300, loss: 2827.7278 , Train PPL: 74.4822, Train Acc: 0.4284\n",
      "Epoch: 70, Batch: 400, loss: 2712.3730 , Train PPL: 62.4718, Train Acc: 0.4101\n",
      "Epoch: 70, Batch: 500, loss: 2802.0850 , Train PPL: 71.6269, Train Acc: 0.4085\n",
      "Epoch: 70, Batch: 600, loss: 2892.6951 , Train PPL: 82.2362, Train Acc: 0.4146\n",
      "Epoch: 70, Batch: 700, loss: 2996.5051 , Train PPL: 96.3361, Train Acc: 0.3445\n",
      "Validation --- Epoch: 70, total loss: 185660.5938 , PPL: 148.2597, Acc: 0.3979\n",
      "lr = 1.0\n",
      "wrote model\n",
      "Epoch: 71, Batch: 100, loss: 2810.4692 , Train PPL: 72.5482, Train Acc: 0.4314\n",
      "Epoch: 71, Batch: 200, loss: 2843.0679 , Train PPL: 76.2445, Train Acc: 0.4299\n",
      "Epoch: 71, Batch: 300, loss: 2937.2788 , Train PPL: 88.0196, Train Acc: 0.3521\n",
      "Epoch: 71, Batch: 400, loss: 2843.8679 , Train PPL: 76.3375, Train Acc: 0.4070\n",
      "Epoch: 71, Batch: 500, loss: 2898.5273 , Train PPL: 82.9706, Train Acc: 0.3826\n",
      "Epoch: 71, Batch: 600, loss: 2820.5344 , Train PPL: 73.6700, Train Acc: 0.4055\n",
      "Epoch: 71, Batch: 700, loss: 2729.3696 , Train PPL: 64.1115, Train Acc: 0.4649\n",
      "Validation --- Epoch: 71, total loss: 185854.8281 , PPL: 149.1473, Acc: 0.3976\n",
      "lr = 1.0\n",
      "Epoch: 72, Batch: 100, loss: 2901.6611 , Train PPL: 83.3680, Train Acc: 0.3704\n",
      "Epoch: 72, Batch: 200, loss: 3068.8279 , Train PPL: 107.5645, Train Acc: 0.3720\n",
      "Epoch: 72, Batch: 300, loss: 2771.0447 , Train PPL: 68.3167, Train Acc: 0.4512\n",
      "Epoch: 72, Batch: 400, loss: 2894.7312 , Train PPL: 82.4919, Train Acc: 0.4345\n",
      "Epoch: 72, Batch: 500, loss: 2937.2974 , Train PPL: 88.0220, Train Acc: 0.3963\n",
      "Epoch: 72, Batch: 600, loss: 3028.0195 , Train PPL: 101.0771, Train Acc: 0.3399\n",
      "Epoch: 72, Batch: 700, loss: 2859.7646 , Train PPL: 78.2100, Train Acc: 0.3704\n",
      "Validation --- Epoch: 72, total loss: 185849.9062 , PPL: 149.0582, Acc: 0.3969\n",
      "lr = 1.0\n",
      "Epoch: 73, Batch: 100, loss: 2810.1826 , Train PPL: 72.5166, Train Acc: 0.4573\n",
      "Epoch: 73, Batch: 200, loss: 2747.9346 , Train PPL: 65.9518, Train Acc: 0.4085\n",
      "Epoch: 73, Batch: 300, loss: 3137.3799 , Train PPL: 119.4134, Train Acc: 0.3674\n",
      "Epoch: 73, Batch: 400, loss: 2815.3276 , Train PPL: 73.0875, Train Acc: 0.4040\n",
      "Epoch: 73, Batch: 500, loss: 3002.3704 , Train PPL: 97.2013, Train Acc: 0.3857\n",
      "Epoch: 73, Batch: 600, loss: 2890.8918 , Train PPL: 82.0105, Train Acc: 0.3750\n",
      "Epoch: 73, Batch: 700, loss: 2903.7944 , Train PPL: 83.6395, Train Acc: 0.3750\n",
      "Validation --- Epoch: 73, total loss: 185793.7969 , PPL: 148.8841, Acc: 0.3960\n",
      "lr = 1.0\n",
      "Epoch: 74, Batch: 100, loss: 2754.3191 , Train PPL: 66.5968, Train Acc: 0.4329\n",
      "Epoch: 74, Batch: 200, loss: 2808.1812 , Train PPL: 72.2957, Train Acc: 0.4360\n",
      "Epoch: 74, Batch: 300, loss: 2872.7532 , Train PPL: 79.7740, Train Acc: 0.4162\n",
      "Epoch: 74, Batch: 400, loss: 3044.1440 , Train PPL: 103.5923, Train Acc: 0.3552\n",
      "Epoch: 74, Batch: 500, loss: 3009.4321 , Train PPL: 98.2533, Train Acc: 0.3735\n",
      "Epoch: 74, Batch: 600, loss: 2969.3845 , Train PPL: 92.4345, Train Acc: 0.4223\n",
      "Epoch: 74, Batch: 700, loss: 2915.0095 , Train PPL: 85.0817, Train Acc: 0.4421\n",
      "Validation --- Epoch: 74, total loss: 185779.1094 , PPL: 148.7924, Acc: 0.3962\n",
      "lr = 1.0\n",
      "Epoch: 75, Batch: 100, loss: 2730.5212 , Train PPL: 64.2242, Train Acc: 0.4375\n",
      "Epoch: 75, Batch: 200, loss: 2885.2493 , Train PPL: 81.3081, Train Acc: 0.3902\n",
      "Epoch: 75, Batch: 300, loss: 3004.1433 , Train PPL: 97.4644, Train Acc: 0.3826\n",
      "Epoch: 75, Batch: 400, loss: 2881.5598 , Train PPL: 80.8521, Train Acc: 0.3841\n",
      "Epoch: 75, Batch: 500, loss: 3006.5845 , Train PPL: 97.8277, Train Acc: 0.3887\n",
      "Epoch: 75, Batch: 600, loss: 2911.6699 , Train PPL: 84.6497, Train Acc: 0.3902\n",
      "Epoch: 75, Batch: 700, loss: 3013.5383 , Train PPL: 98.8703, Train Acc: 0.3521\n",
      "Validation --- Epoch: 75, total loss: 185801.3438 , PPL: 148.8848, Acc: 0.3972\n",
      "lr = 1.0\n",
      "Epoch: 76, Batch: 100, loss: 2674.2212 , Train PPL: 58.9422, Train Acc: 0.4360\n",
      "Epoch: 76, Batch: 200, loss: 2767.3108 , Train PPL: 67.9289, Train Acc: 0.4223\n",
      "Epoch: 76, Batch: 300, loss: 2993.5657 , Train PPL: 95.9054, Train Acc: 0.3780\n",
      "Epoch: 76, Batch: 400, loss: 2978.2705 , Train PPL: 93.6952, Train Acc: 0.3704\n",
      "Epoch: 76, Batch: 500, loss: 2986.1455 , Train PPL: 94.8267, Train Acc: 0.3887\n",
      "Epoch: 76, Batch: 600, loss: 3052.6108 , Train PPL: 104.9381, Train Acc: 0.3582\n",
      "Epoch: 76, Batch: 700, loss: 2915.3491 , Train PPL: 85.1258, Train Acc: 0.3338\n",
      "Validation --- Epoch: 76, total loss: 185829.3438 , PPL: 149.1080, Acc: 0.3973\n",
      "lr = 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77, Batch: 100, loss: 2987.9290 , Train PPL: 95.0848, Train Acc: 0.3399\n",
      "Epoch: 77, Batch: 200, loss: 2858.0259 , Train PPL: 78.0030, Train Acc: 0.3704\n",
      "Epoch: 77, Batch: 300, loss: 2713.0852 , Train PPL: 62.5397, Train Acc: 0.4116\n",
      "Epoch: 77, Batch: 400, loss: 2881.4680 , Train PPL: 80.8408, Train Acc: 0.3857\n",
      "Epoch: 77, Batch: 500, loss: 2924.7361 , Train PPL: 86.3526, Train Acc: 0.3841\n",
      "Epoch: 77, Batch: 600, loss: 2885.3259 , Train PPL: 81.3176, Train Acc: 0.3857\n",
      "Epoch: 77, Batch: 700, loss: 3004.3293 , Train PPL: 97.4920, Train Acc: 0.3201\n",
      "Validation --- Epoch: 77, total loss: 185720.8281 , PPL: 148.6611, Acc: 0.3984\n",
      "lr = 0.5\n",
      "Epoch: 78, Batch: 100, loss: 2919.7539 , Train PPL: 85.6993, Train Acc: 0.3780\n",
      "Epoch: 78, Batch: 200, loss: 2702.3442 , Train PPL: 61.5240, Train Acc: 0.4726\n",
      "Epoch: 78, Batch: 300, loss: 2907.2820 , Train PPL: 84.0854, Train Acc: 0.3902\n",
      "Epoch: 78, Batch: 400, loss: 2775.6787 , Train PPL: 68.8009, Train Acc: 0.4512\n",
      "Epoch: 78, Batch: 500, loss: 2835.5330 , Train PPL: 75.3737, Train Acc: 0.4207\n",
      "Epoch: 78, Batch: 600, loss: 2742.4285 , Train PPL: 65.4006, Train Acc: 0.4360\n",
      "Epoch: 78, Batch: 700, loss: 2965.9739 , Train PPL: 91.9552, Train Acc: 0.3750\n",
      "Validation --- Epoch: 78, total loss: 185730.2500 , PPL: 148.7265, Acc: 0.3973\n",
      "lr = 0.5\n",
      "Epoch: 79, Batch: 100, loss: 2759.6421 , Train PPL: 67.1394, Train Acc: 0.4238\n",
      "Epoch: 79, Batch: 200, loss: 2775.4009 , Train PPL: 68.7718, Train Acc: 0.4421\n",
      "Epoch: 79, Batch: 300, loss: 2842.4512 , Train PPL: 76.1728, Train Acc: 0.4040\n",
      "Epoch: 79, Batch: 400, loss: 2879.7527 , Train PPL: 80.6297, Train Acc: 0.4070\n",
      "Epoch: 79, Batch: 500, loss: 2986.8770 , Train PPL: 94.9325, Train Acc: 0.3872\n",
      "Epoch: 79, Batch: 600, loss: 2883.5098 , Train PPL: 81.0928, Train Acc: 0.4070\n",
      "Epoch: 79, Batch: 700, loss: 2886.9590 , Train PPL: 81.5203, Train Acc: 0.4116\n",
      "Validation --- Epoch: 79, total loss: 185746.9062 , PPL: 148.7330, Acc: 0.3996\n",
      "lr = 0.5\n",
      "Epoch: 80, Batch: 100, loss: 2795.1594 , Train PPL: 70.8747, Train Acc: 0.4314\n",
      "Epoch: 80, Batch: 200, loss: 2779.8923 , Train PPL: 69.2443, Train Acc: 0.4573\n",
      "Epoch: 80, Batch: 300, loss: 2865.8079 , Train PPL: 78.9338, Train Acc: 0.4527\n",
      "Epoch: 80, Batch: 400, loss: 2945.3223 , Train PPL: 89.1054, Train Acc: 0.3582\n",
      "Epoch: 80, Batch: 500, loss: 3119.8113 , Train PPL: 116.2578, Train Acc: 0.3628\n",
      "Epoch: 80, Batch: 600, loss: 2820.6980 , Train PPL: 73.6884, Train Acc: 0.4375\n",
      "Epoch: 80, Batch: 700, loss: 2918.9717 , Train PPL: 85.5971, Train Acc: 0.3979\n",
      "Validation --- Epoch: 80, total loss: 185745.1875 , PPL: 148.7185, Acc: 0.3983\n",
      "lr = 0.5\n",
      "Epoch: 81, Batch: 100, loss: 2968.0886 , Train PPL: 92.2521, Train Acc: 0.3628\n",
      "Epoch: 81, Batch: 200, loss: 2899.2288 , Train PPL: 83.0594, Train Acc: 0.3979\n",
      "Epoch: 81, Batch: 300, loss: 2697.9053 , Train PPL: 61.1091, Train Acc: 0.4360\n",
      "Epoch: 81, Batch: 400, loss: 2867.4995 , Train PPL: 79.1376, Train Acc: 0.3887\n",
      "Epoch: 81, Batch: 500, loss: 2788.9282 , Train PPL: 70.2047, Train Acc: 0.4192\n",
      "Epoch: 81, Batch: 600, loss: 2884.8257 , Train PPL: 81.2556, Train Acc: 0.3780\n",
      "Epoch: 81, Batch: 700, loss: 2792.7185 , Train PPL: 70.6115, Train Acc: 0.3826\n",
      "Validation --- Epoch: 81, total loss: 185767.7656 , PPL: 148.8144, Acc: 0.3995\n",
      "lr = 0.5\n",
      "Epoch: 82, Batch: 100, loss: 3091.6504 , Train PPL: 111.3726, Train Acc: 0.3430\n",
      "Epoch: 82, Batch: 200, loss: 2890.1912 , Train PPL: 81.9230, Train Acc: 0.4101\n",
      "Epoch: 82, Batch: 300, loss: 2844.4673 , Train PPL: 76.4073, Train Acc: 0.4223\n",
      "Epoch: 82, Batch: 400, loss: 2634.4731 , Train PPL: 55.4768, Train Acc: 0.4588\n",
      "Epoch: 82, Batch: 500, loss: 2688.8074 , Train PPL: 60.2674, Train Acc: 0.4588\n",
      "Epoch: 82, Batch: 600, loss: 2861.5857 , Train PPL: 78.4274, Train Acc: 0.3765\n",
      "Epoch: 82, Batch: 700, loss: 3081.7607 , Train PPL: 109.7062, Train Acc: 0.3323\n",
      "Validation --- Epoch: 82, total loss: 185721.5156 , PPL: 148.6204, Acc: 0.3997\n",
      "lr = 0.25\n",
      "Epoch: 83, Batch: 100, loss: 2890.5154 , Train PPL: 81.9634, Train Acc: 0.3582\n",
      "Epoch: 83, Batch: 200, loss: 3012.6238 , Train PPL: 98.7325, Train Acc: 0.3750\n",
      "Epoch: 83, Batch: 300, loss: 2761.6838 , Train PPL: 67.3487, Train Acc: 0.4223\n",
      "Epoch: 83, Batch: 400, loss: 2806.3801 , Train PPL: 72.0974, Train Acc: 0.4116\n",
      "Epoch: 83, Batch: 500, loss: 2667.7271 , Train PPL: 58.3615, Train Acc: 0.4619\n",
      "Epoch: 83, Batch: 600, loss: 2847.1555 , Train PPL: 76.7210, Train Acc: 0.4299\n",
      "Epoch: 83, Batch: 700, loss: 2909.5674 , Train PPL: 84.3788, Train Acc: 0.3963\n",
      "Validation --- Epoch: 83, total loss: 185767.7812 , PPL: 148.8594, Acc: 0.4008\n",
      "lr = 0.25\n",
      "Epoch: 84, Batch: 100, loss: 2954.1938 , Train PPL: 90.3187, Train Acc: 0.3476\n",
      "Epoch: 84, Batch: 200, loss: 2925.6016 , Train PPL: 86.4666, Train Acc: 0.4040\n",
      "Epoch: 84, Batch: 300, loss: 2884.6201 , Train PPL: 81.2302, Train Acc: 0.4055\n",
      "Epoch: 84, Batch: 400, loss: 2878.6924 , Train PPL: 80.4995, Train Acc: 0.3704\n",
      "Epoch: 84, Batch: 500, loss: 2882.2380 , Train PPL: 80.9358, Train Acc: 0.4299\n",
      "Epoch: 84, Batch: 600, loss: 2737.7034 , Train PPL: 64.9312, Train Acc: 0.4223\n",
      "Epoch: 84, Batch: 700, loss: 3070.8181 , Train PPL: 107.8914, Train Acc: 0.3506\n",
      "Validation --- Epoch: 84, total loss: 185784.3594 , PPL: 148.9420, Acc: 0.3996\n",
      "lr = 0.25\n",
      "Epoch: 85, Batch: 100, loss: 2906.9458 , Train PPL: 84.0423, Train Acc: 0.3552\n",
      "Epoch: 85, Batch: 200, loss: 2916.1951 , Train PPL: 85.2356, Train Acc: 0.3765\n",
      "Epoch: 85, Batch: 300, loss: 2992.2146 , Train PPL: 95.7081, Train Acc: 0.3613\n",
      "Epoch: 85, Batch: 400, loss: 2946.9375 , Train PPL: 89.3251, Train Acc: 0.4055\n",
      "Epoch: 85, Batch: 500, loss: 2674.2009 , Train PPL: 58.9403, Train Acc: 0.4360\n",
      "Epoch: 85, Batch: 600, loss: 2867.0911 , Train PPL: 79.0883, Train Acc: 0.4085\n",
      "Epoch: 85, Batch: 700, loss: 2747.6641 , Train PPL: 65.9246, Train Acc: 0.4055\n",
      "Validation --- Epoch: 85, total loss: 185812.6875 , PPL: 149.0311, Acc: 0.3987\n",
      "lr = 0.25\n",
      "Epoch: 86, Batch: 100, loss: 3041.6353 , Train PPL: 103.1969, Train Acc: 0.3552\n",
      "Epoch: 86, Batch: 200, loss: 2586.4570 , Train PPL: 51.5612, Train Acc: 0.4756\n",
      "Epoch: 86, Batch: 300, loss: 2792.9622 , Train PPL: 70.6377, Train Acc: 0.3918\n",
      "Epoch: 86, Batch: 400, loss: 2855.8079 , Train PPL: 77.7396, Train Acc: 0.3963\n",
      "Epoch: 86, Batch: 500, loss: 3040.8884 , Train PPL: 103.0795, Train Acc: 0.3430\n",
      "Epoch: 86, Batch: 600, loss: 2726.4519 , Train PPL: 63.8270, Train Acc: 0.4497\n",
      "Epoch: 86, Batch: 700, loss: 3091.9844 , Train PPL: 111.4294, Train Acc: 0.3476\n",
      "Validation --- Epoch: 86, total loss: 185807.4844 , PPL: 149.0671, Acc: 0.4004\n",
      "lr = 0.25\n",
      "Epoch: 87, Batch: 100, loss: 2857.8660 , Train PPL: 77.9839, Train Acc: 0.3369\n",
      "Epoch: 87, Batch: 200, loss: 2782.7969 , Train PPL: 69.5516, Train Acc: 0.4207\n",
      "Epoch: 87, Batch: 300, loss: 2976.1587 , Train PPL: 93.3940, Train Acc: 0.3521\n",
      "Epoch: 87, Batch: 400, loss: 2822.9109 , Train PPL: 73.9373, Train Acc: 0.4253\n",
      "Epoch: 87, Batch: 500, loss: 2937.1108 , Train PPL: 87.9970, Train Acc: 0.3399\n",
      "Epoch: 87, Batch: 600, loss: 2932.2612 , Train PPL: 87.3489, Train Acc: 0.3872\n",
      "Epoch: 87, Batch: 700, loss: 2982.9885 , Train PPL: 94.3715, Train Acc: 0.3445\n",
      "Validation --- Epoch: 87, total loss: 185729.8438 , PPL: 148.7075, Acc: 0.4003\n",
      "lr = 0.25\n",
      "Epoch: 88, Batch: 100, loss: 2821.7419 , Train PPL: 73.8057, Train Acc: 0.4116\n",
      "Epoch: 88, Batch: 200, loss: 2752.4495 , Train PPL: 66.4073, Train Acc: 0.4055\n",
      "Epoch: 88, Batch: 300, loss: 2934.1501 , Train PPL: 87.6008, Train Acc: 0.3933\n",
      "Epoch: 88, Batch: 400, loss: 2908.2986 , Train PPL: 84.2158, Train Acc: 0.3963\n",
      "Epoch: 88, Batch: 500, loss: 2941.8459 , Train PPL: 88.6345, Train Acc: 0.3780\n",
      "Epoch: 88, Batch: 600, loss: 2813.3047 , Train PPL: 72.8625, Train Acc: 0.4207\n",
      "Epoch: 88, Batch: 700, loss: 2908.6443 , Train PPL: 84.2601, Train Acc: 0.3628\n",
      "Validation --- Epoch: 88, total loss: 185719.8750 , PPL: 148.6704, Acc: 0.4002\n",
      "lr = 0.125\n",
      "Epoch: 89, Batch: 100, loss: 2816.1289 , Train PPL: 73.1769, Train Acc: 0.4345\n",
      "Epoch: 89, Batch: 200, loss: 2865.5090 , Train PPL: 78.8979, Train Acc: 0.4497\n",
      "Epoch: 89, Batch: 300, loss: 2812.8132 , Train PPL: 72.8079, Train Acc: 0.4162\n",
      "Epoch: 89, Batch: 400, loss: 2716.7424 , Train PPL: 62.8893, Train Acc: 0.4162\n",
      "Epoch: 89, Batch: 500, loss: 2967.9541 , Train PPL: 92.2332, Train Acc: 0.3537\n",
      "Epoch: 89, Batch: 600, loss: 2895.7383 , Train PPL: 82.6186, Train Acc: 0.4253\n",
      "Epoch: 89, Batch: 700, loss: 2991.6738 , Train PPL: 95.6292, Train Acc: 0.3430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --- Epoch: 89, total loss: 185774.0625 , PPL: 148.9189, Acc: 0.4003\n",
      "lr = 0.125\n",
      "Epoch: 90, Batch: 100, loss: 2845.3801 , Train PPL: 76.5137, Train Acc: 0.3918\n",
      "Epoch: 90, Batch: 200, loss: 2750.4065 , Train PPL: 66.2008, Train Acc: 0.4695\n",
      "Epoch: 90, Batch: 300, loss: 2859.6965 , Train PPL: 78.2019, Train Acc: 0.3659\n",
      "Epoch: 90, Batch: 400, loss: 2556.7190 , Train PPL: 49.2760, Train Acc: 0.4817\n",
      "Epoch: 90, Batch: 500, loss: 2881.4463 , Train PPL: 80.8381, Train Acc: 0.3933\n",
      "Epoch: 90, Batch: 600, loss: 2837.2773 , Train PPL: 75.5744, Train Acc: 0.3841\n",
      "Epoch: 90, Batch: 700, loss: 2873.9700 , Train PPL: 79.9221, Train Acc: 0.3857\n",
      "Validation --- Epoch: 90, total loss: 185741.6562 , PPL: 148.7882, Acc: 0.4006\n",
      "lr = 0.125\n",
      "Epoch: 91, Batch: 100, loss: 2595.6006 , Train PPL: 52.2849, Train Acc: 0.4832\n",
      "Epoch: 91, Batch: 200, loss: 2795.4751 , Train PPL: 70.9088, Train Acc: 0.4040\n",
      "Epoch: 91, Batch: 300, loss: 2899.3589 , Train PPL: 83.0759, Train Acc: 0.4024\n",
      "Epoch: 91, Batch: 400, loss: 2830.2698 , Train PPL: 74.7714, Train Acc: 0.3841\n",
      "Epoch: 91, Batch: 500, loss: 2798.6606 , Train PPL: 71.2540, Train Acc: 0.3841\n",
      "Epoch: 91, Batch: 600, loss: 2860.6099 , Train PPL: 78.3108, Train Acc: 0.4802\n",
      "Epoch: 91, Batch: 700, loss: 2971.5798 , Train PPL: 92.7444, Train Acc: 0.3796\n",
      "Validation --- Epoch: 91, total loss: 185750.9688 , PPL: 148.8105, Acc: 0.4009\n",
      "lr = 0.125\n",
      "Epoch: 92, Batch: 100, loss: 2690.6199 , Train PPL: 60.4342, Train Acc: 0.4634\n",
      "Epoch: 92, Batch: 200, loss: 2885.4209 , Train PPL: 81.3294, Train Acc: 0.3171\n",
      "Epoch: 92, Batch: 300, loss: 2820.5408 , Train PPL: 73.6707, Train Acc: 0.4177\n",
      "Epoch: 92, Batch: 400, loss: 2732.1169 , Train PPL: 64.3806, Train Acc: 0.4009\n",
      "Epoch: 92, Batch: 500, loss: 2609.8926 , Train PPL: 53.4365, Train Acc: 0.4863\n",
      "Epoch: 92, Batch: 600, loss: 2790.8926 , Train PPL: 70.4152, Train Acc: 0.3994\n",
      "Epoch: 92, Batch: 700, loss: 3052.4124 , Train PPL: 104.9063, Train Acc: 0.3399\n",
      "Validation --- Epoch: 92, total loss: 185738.2656 , PPL: 148.7738, Acc: 0.4004\n",
      "lr = 0.125\n",
      "Epoch: 93, Batch: 100, loss: 2879.2812 , Train PPL: 80.5718, Train Acc: 0.4238\n",
      "Epoch: 93, Batch: 200, loss: 2686.5713 , Train PPL: 60.0623, Train Acc: 0.4604\n",
      "Epoch: 93, Batch: 300, loss: 2779.5039 , Train PPL: 69.2033, Train Acc: 0.3902\n",
      "Epoch: 93, Batch: 400, loss: 3091.1165 , Train PPL: 111.2820, Train Acc: 0.3323\n",
      "Epoch: 93, Batch: 500, loss: 2844.0337 , Train PPL: 76.3568, Train Acc: 0.3857\n",
      "Epoch: 93, Batch: 600, loss: 2807.6899 , Train PPL: 72.2415, Train Acc: 0.4223\n",
      "Epoch: 93, Batch: 700, loss: 3001.4795 , Train PPL: 97.0694, Train Acc: 0.3506\n",
      "Validation --- Epoch: 93, total loss: 185801.8750 , PPL: 149.0423, Acc: 0.4013\n",
      "lr = 0.125\n",
      "Epoch: 94, Batch: 100, loss: 2743.0352 , Train PPL: 65.4611, Train Acc: 0.4284\n",
      "Epoch: 94, Batch: 200, loss: 3074.8818 , Train PPL: 108.5618, Train Acc: 0.3369\n",
      "Epoch: 94, Batch: 300, loss: 2913.1597 , Train PPL: 84.8421, Train Acc: 0.3796\n",
      "Epoch: 94, Batch: 400, loss: 2825.4294 , Train PPL: 74.2217, Train Acc: 0.4726\n",
      "Epoch: 94, Batch: 500, loss: 2717.2800 , Train PPL: 62.9408, Train Acc: 0.4405\n",
      "Epoch: 94, Batch: 600, loss: 2733.7253 , Train PPL: 64.5387, Train Acc: 0.4512\n",
      "Epoch: 94, Batch: 700, loss: 3126.9656 , Train PPL: 117.5326, Train Acc: 0.3552\n",
      "Validation --- Epoch: 94, total loss: 185782.9375 , PPL: 148.9652, Acc: 0.4013\n",
      "lr = 0.0625\n",
      "Epoch: 95, Batch: 100, loss: 2994.7695 , Train PPL: 96.0816, Train Acc: 0.3491\n",
      "Epoch: 95, Batch: 200, loss: 2930.3074 , Train PPL: 87.0891, Train Acc: 0.3384\n",
      "Epoch: 95, Batch: 300, loss: 2735.1528 , Train PPL: 64.6792, Train Acc: 0.4055\n",
      "Epoch: 95, Batch: 400, loss: 2863.3867 , Train PPL: 78.6430, Train Acc: 0.3704\n",
      "Epoch: 95, Batch: 500, loss: 2989.6519 , Train PPL: 95.3349, Train Acc: 0.3521\n",
      "Epoch: 95, Batch: 600, loss: 2817.4680 , Train PPL: 73.3264, Train Acc: 0.4131\n",
      "Epoch: 95, Batch: 700, loss: 2883.4175 , Train PPL: 81.0814, Train Acc: 0.3994\n",
      "Validation --- Epoch: 95, total loss: 185773.7812 , PPL: 148.9320, Acc: 0.4013\n",
      "lr = 0.0625\n",
      "Epoch: 96, Batch: 100, loss: 2847.9934 , Train PPL: 76.8191, Train Acc: 0.4055\n",
      "Epoch: 96, Batch: 200, loss: 2943.9912 , Train PPL: 88.9248, Train Acc: 0.3979\n",
      "Epoch: 96, Batch: 300, loss: 3002.9873 , Train PPL: 97.2927, Train Acc: 0.3780\n",
      "Epoch: 96, Batch: 400, loss: 2933.0889 , Train PPL: 87.4592, Train Acc: 0.3750\n",
      "Epoch: 96, Batch: 500, loss: 2861.0537 , Train PPL: 78.3638, Train Acc: 0.4314\n",
      "Epoch: 96, Batch: 600, loss: 2669.7605 , Train PPL: 58.5427, Train Acc: 0.4131\n",
      "Epoch: 96, Batch: 700, loss: 2768.1418 , Train PPL: 68.0150, Train Acc: 0.4192\n",
      "Validation --- Epoch: 96, total loss: 185804.4688 , PPL: 149.0510, Acc: 0.4010\n",
      "lr = 0.0625\n",
      "Epoch: 97, Batch: 100, loss: 2808.3328 , Train PPL: 72.3123, Train Acc: 0.4040\n",
      "Epoch: 97, Batch: 200, loss: 2927.5049 , Train PPL: 86.7179, Train Acc: 0.3216\n",
      "Epoch: 97, Batch: 300, loss: 2770.8706 , Train PPL: 68.2985, Train Acc: 0.4131\n",
      "Epoch: 97, Batch: 400, loss: 2761.7668 , Train PPL: 67.3572, Train Acc: 0.4710\n",
      "Epoch: 97, Batch: 500, loss: 2917.6865 , Train PPL: 85.4296, Train Acc: 0.3841\n",
      "Epoch: 97, Batch: 600, loss: 2791.2842 , Train PPL: 70.4573, Train Acc: 0.4192\n",
      "Epoch: 97, Batch: 700, loss: 3149.4368 , Train PPL: 121.6284, Train Acc: 0.3201\n",
      "Validation --- Epoch: 97, total loss: 185794.1250 , PPL: 149.0052, Acc: 0.4009\n",
      "lr = 0.0625\n",
      "Epoch: 98, Batch: 100, loss: 2678.8162 , Train PPL: 59.3565, Train Acc: 0.4466\n",
      "Epoch: 98, Batch: 200, loss: 2932.9775 , Train PPL: 87.4444, Train Acc: 0.3582\n",
      "Epoch: 98, Batch: 300, loss: 2986.2842 , Train PPL: 94.8467, Train Acc: 0.3476\n",
      "Epoch: 98, Batch: 400, loss: 2866.6360 , Train PPL: 79.0335, Train Acc: 0.3628\n",
      "Epoch: 98, Batch: 500, loss: 2900.2061 , Train PPL: 83.1832, Train Acc: 0.3704\n",
      "Epoch: 98, Batch: 600, loss: 2721.7102 , Train PPL: 63.3673, Train Acc: 0.4512\n",
      "Epoch: 98, Batch: 700, loss: 2622.9348 , Train PPL: 54.5096, Train Acc: 0.4329\n",
      "Validation --- Epoch: 98, total loss: 185800.8281 , PPL: 149.0336, Acc: 0.4010\n",
      "lr = 0.0625\n",
      "Epoch: 99, Batch: 100, loss: 2979.3682 , Train PPL: 93.8521, Train Acc: 0.4055\n",
      "Epoch: 99, Batch: 200, loss: 3003.8452 , Train PPL: 97.4201, Train Acc: 0.3430\n",
      "Epoch: 99, Batch: 300, loss: 2579.7583 , Train PPL: 51.0374, Train Acc: 0.4909\n",
      "Epoch: 99, Batch: 400, loss: 3049.1414 , Train PPL: 104.3845, Train Acc: 0.3628\n",
      "Epoch: 99, Batch: 500, loss: 2870.8774 , Train PPL: 79.5462, Train Acc: 0.3765\n",
      "Epoch: 99, Batch: 600, loss: 2856.2688 , Train PPL: 77.7943, Train Acc: 0.4177\n",
      "Epoch: 99, Batch: 700, loss: 2894.4565 , Train PPL: 82.4574, Train Acc: 0.4207\n",
      "Validation --- Epoch: 99, total loss: 185812.5156 , PPL: 149.0897, Acc: 0.4018\n",
      "lr = 0.0625\n"
     ]
    }
   ],
   "source": [
    "model = TCN(4, [300,300,300], kernel=3, dropout=0.5, embedding_size = 300, n_words = n_words,tied=True,embedding=word2vec)\n",
    "model.cuda()\n",
    "model_parameters = filter(lambda x: x.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print('Network with {} parameters'.format(params))\n",
    "print('Receptive field of network is {}'.format(model.receptive_field))\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode=\"min\",patience=5,min_lr=1e-6,factor=0.5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "best_vloss = 1e8\n",
    "for e in range(100):\n",
    "    train_loop(e)\n",
    "    validation_loss = validation_loop(e)\n",
    "    scheduler.step(validation_loss)\n",
    "    print('lr = {}'.format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "    if validation_loss < best_vloss:\n",
    "                with open(\"model_4_layers_k3_word2vec2.pt\", 'wb') as f:\n",
    "                    print('wrote model')\n",
    "                    torch.save(model, f)\n",
    "                best_vloss = validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network with 93151601 parameters\n",
      "Receptive field of network is 30\n",
      "Epoch: 0, Batch: 100, loss: 4604.0986 , Train PPL: 1117.0461, Train Acc: 0.0991\n",
      "Epoch: 0, Batch: 200, loss: 4198.4863 , Train PPL: 601.9243, Train Acc: 0.1174\n",
      "Epoch: 0, Batch: 300, loss: 3981.2520 , Train PPL: 432.2403, Train Acc: 0.1707\n",
      "Epoch: 0, Batch: 400, loss: 3997.0073 , Train PPL: 442.7472, Train Acc: 0.2210\n",
      "Epoch: 0, Batch: 500, loss: 4164.5488 , Train PPL: 571.5762, Train Acc: 0.1616\n",
      "Epoch: 0, Batch: 600, loss: 3640.7839 , Train PPL: 257.2312, Train Acc: 0.2134\n",
      "Epoch: 0, Batch: 700, loss: 4065.6482 , Train PPL: 491.5847, Train Acc: 0.2271\n",
      "Validation --- Epoch: 0, total loss: 218778.5000 , PPL: 353.6627, Acc: 0.2563\n",
      "lr = 4\n",
      "wrote model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type TCN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type TC_block. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Chomp1d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 100, loss: 3923.4895 , Train PPL: 395.8079, Train Acc: 0.1570\n",
      "Epoch: 1, Batch: 200, loss: 3828.5520 , Train PPL: 342.4780, Train Acc: 0.2378\n",
      "Epoch: 1, Batch: 300, loss: 3724.7554 , Train PPL: 292.3585, Train Acc: 0.2988\n",
      "Epoch: 1, Batch: 400, loss: 3885.8931 , Train PPL: 373.7613, Train Acc: 0.2561\n",
      "Epoch: 1, Batch: 500, loss: 3825.0310 , Train PPL: 340.6447, Train Acc: 0.2561\n",
      "Epoch: 1, Batch: 600, loss: 3814.1641 , Train PPL: 335.0482, Train Acc: 0.2713\n",
      "Epoch: 1, Batch: 700, loss: 3827.1775 , Train PPL: 341.7612, Train Acc: 0.3049\n",
      "Validation --- Epoch: 1, total loss: 208751.8906 , PPL: 271.0483, Acc: 0.2888\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 2, Batch: 100, loss: 3656.9932 , Train PPL: 263.6664, Train Acc: 0.2927\n",
      "Epoch: 2, Batch: 200, loss: 3628.8262 , Train PPL: 252.5849, Train Acc: 0.2790\n",
      "Epoch: 2, Batch: 300, loss: 3458.2314 , Train PPL: 194.7456, Train Acc: 0.3476\n",
      "Epoch: 2, Batch: 400, loss: 3678.3176 , Train PPL: 272.3782, Train Acc: 0.3323\n",
      "Epoch: 2, Batch: 500, loss: 3649.2668 , Train PPL: 260.5792, Train Acc: 0.2988\n",
      "Epoch: 2, Batch: 600, loss: 3573.4399 , Train PPL: 232.1345, Train Acc: 0.2881\n",
      "Epoch: 2, Batch: 700, loss: 3643.2131 , Train PPL: 258.1855, Train Acc: 0.2790\n",
      "Validation --- Epoch: 2, total loss: 202982.7031 , PPL: 232.8300, Acc: 0.3193\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 3, Batch: 100, loss: 3512.5686 , Train PPL: 211.5635, Train Acc: 0.3232\n",
      "Epoch: 3, Batch: 200, loss: 3376.0283 , Train PPL: 171.8092, Train Acc: 0.3689\n",
      "Epoch: 3, Batch: 300, loss: 3454.6528 , Train PPL: 193.6862, Train Acc: 0.3277\n",
      "Epoch: 3, Batch: 400, loss: 3664.0996 , Train PPL: 266.5383, Train Acc: 0.2820\n",
      "Epoch: 3, Batch: 500, loss: 3560.3000 , Train PPL: 227.5310, Train Acc: 0.3186\n",
      "Epoch: 3, Batch: 600, loss: 3535.7939 , Train PPL: 219.1880, Train Acc: 0.3186\n",
      "Epoch: 3, Batch: 700, loss: 3364.8110 , Train PPL: 168.8964, Train Acc: 0.3415\n",
      "Validation --- Epoch: 3, total loss: 199622.1250 , PPL: 213.2477, Acc: 0.3437\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 4, Batch: 100, loss: 3413.6416 , Train PPL: 181.9482, Train Acc: 0.3491\n",
      "Epoch: 4, Batch: 200, loss: 3434.0823 , Train PPL: 187.7068, Train Acc: 0.2942\n",
      "Epoch: 4, Batch: 300, loss: 3741.7334 , Train PPL: 300.0238, Train Acc: 0.2988\n",
      "Epoch: 4, Batch: 400, loss: 3417.8606 , Train PPL: 183.1222, Train Acc: 0.3857\n",
      "Epoch: 4, Batch: 500, loss: 3611.6829 , Train PPL: 246.0696, Train Acc: 0.3049\n",
      "Epoch: 4, Batch: 600, loss: 3320.0122 , Train PPL: 157.7473, Train Acc: 0.3872\n",
      "Epoch: 4, Batch: 700, loss: 3407.7131 , Train PPL: 180.3112, Train Acc: 0.3293\n",
      "Validation --- Epoch: 4, total loss: 196764.0469 , PPL: 197.7140, Acc: 0.3509\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 5, Batch: 100, loss: 3113.7305 , Train PPL: 115.1851, Train Acc: 0.3841\n",
      "Epoch: 5, Batch: 200, loss: 3311.2603 , Train PPL: 155.6567, Train Acc: 0.3704\n",
      "Epoch: 5, Batch: 300, loss: 3331.2068 , Train PPL: 160.4623, Train Acc: 0.3171\n",
      "Epoch: 5, Batch: 400, loss: 3474.4597 , Train PPL: 199.6234, Train Acc: 0.3216\n",
      "Epoch: 5, Batch: 500, loss: 3330.3655 , Train PPL: 160.2566, Train Acc: 0.3765\n",
      "Epoch: 5, Batch: 600, loss: 3469.5051 , Train PPL: 198.1214, Train Acc: 0.3613\n",
      "Epoch: 5, Batch: 700, loss: 3482.3203 , Train PPL: 202.0297, Train Acc: 0.3293\n",
      "Validation --- Epoch: 5, total loss: 194431.5469 , PPL: 186.0771, Acc: 0.3636\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 6, Batch: 100, loss: 3379.4031 , Train PPL: 172.6954, Train Acc: 0.3491\n",
      "Epoch: 6, Batch: 200, loss: 3191.0347 , Train PPL: 129.5909, Train Acc: 0.3430\n",
      "Epoch: 6, Batch: 300, loss: 3304.5576 , Train PPL: 154.0744, Train Acc: 0.3399\n",
      "Epoch: 6, Batch: 400, loss: 3336.5544 , Train PPL: 161.7758, Train Acc: 0.3582\n",
      "Epoch: 6, Batch: 500, loss: 3405.8108 , Train PPL: 179.7892, Train Acc: 0.2851\n",
      "Epoch: 6, Batch: 600, loss: 3487.3250 , Train PPL: 203.5770, Train Acc: 0.3155\n",
      "Epoch: 6, Batch: 700, loss: 3248.2048 , Train PPL: 141.3914, Train Acc: 0.3216\n",
      "Validation --- Epoch: 6, total loss: 193897.6562 , PPL: 183.8088, Acc: 0.3481\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 7, Batch: 100, loss: 3320.8486 , Train PPL: 157.9485, Train Acc: 0.3537\n",
      "Epoch: 7, Batch: 200, loss: 3236.5237 , Train PPL: 138.8960, Train Acc: 0.3277\n",
      "Epoch: 7, Batch: 300, loss: 3107.2480 , Train PPL: 114.0525, Train Acc: 0.4284\n",
      "Epoch: 7, Batch: 400, loss: 3098.0876 , Train PPL: 112.4709, Train Acc: 0.3674\n",
      "Epoch: 7, Batch: 500, loss: 3327.8965 , Train PPL: 159.6546, Train Acc: 0.3552\n",
      "Epoch: 7, Batch: 600, loss: 3268.6572 , Train PPL: 145.8691, Train Acc: 0.3537\n",
      "Epoch: 7, Batch: 700, loss: 3012.6006 , Train PPL: 98.7290, Train Acc: 0.3613\n",
      "Validation --- Epoch: 7, total loss: 192872.4531 , PPL: 178.6352, Acc: 0.3607\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 8, Batch: 100, loss: 3274.1726 , Train PPL: 147.1006, Train Acc: 0.3384\n",
      "Epoch: 8, Batch: 200, loss: 3106.1379 , Train PPL: 113.8596, Train Acc: 0.3750\n",
      "Epoch: 8, Batch: 300, loss: 3375.8125 , Train PPL: 171.7527, Train Acc: 0.3247\n",
      "Epoch: 8, Batch: 400, loss: 3165.8298 , Train PPL: 124.7061, Train Acc: 0.3598\n",
      "Epoch: 8, Batch: 500, loss: 3263.2805 , Train PPL: 144.6784, Train Acc: 0.3415\n",
      "Epoch: 8, Batch: 600, loss: 3044.9729 , Train PPL: 103.7233, Train Acc: 0.4314\n",
      "Epoch: 8, Batch: 700, loss: 3194.8789 , Train PPL: 130.3525, Train Acc: 0.3369\n",
      "Validation --- Epoch: 8, total loss: 191196.2344 , PPL: 171.0101, Acc: 0.3736\n",
      "lr = 4\n",
      "wrote model\n",
      "Epoch: 9, Batch: 100, loss: 3197.2219 , Train PPL: 130.8189, Train Acc: 0.3476\n",
      "Epoch: 9, Batch: 200, loss: 2971.2283 , Train PPL: 92.6947, Train Acc: 0.4207\n",
      "Epoch: 9, Batch: 300, loss: 3098.7729 , Train PPL: 112.5884, Train Acc: 0.3796\n",
      "Epoch: 9, Batch: 400, loss: 3217.7783 , Train PPL: 134.9832, Train Acc: 0.3841\n"
     ]
    }
   ],
   "source": [
    "model = TCN(3, [2400,2400], kernel=3, dropout=0.5, embedding_size = 2400, n_words = n_words,tied=True)\n",
    "model.cuda()\n",
    "model_parameters = filter(lambda x: x.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print('Network with {} parameters'.format(params))\n",
    "print('Receptive field of network is {}'.format(model.receptive_field))\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode=\"min\",patience=5,min_lr=1e-6,factor=0.5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "best_vloss = 1e8\n",
    "for e in range(100):\n",
    "    train_loop(e)\n",
    "    validation_loss = validation_loop(e)\n",
    "    scheduler.step(validation_loss)\n",
    "    print('lr = {}'.format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "    if validation_loss < best_vloss:\n",
    "                with open(\"model_3_layers_k3_2400.pt\", 'wb') as f:\n",
    "                    print('wrote model')\n",
    "                    torch.save(model, f)\n",
    "                best_vloss = validation_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ensemble(models,e=0):\n",
    "    [m.eval() for m in models]\n",
    "    \n",
    "    batch_idx = 0\n",
    "    ppl = []\n",
    "    acc = 0\n",
    "    total_loss = 0\n",
    "    for batch in val_iter:\n",
    "        X,y = get_batch(batch)\n",
    "        prob = [m(X) for m in models]\n",
    "        prob = torch.mean(torch.stack(prob,dim=3),dim=3)\n",
    "        # skip some chars for loss\n",
    "        skip = int(X.shape[1]/2)\n",
    "        target = y[:, skip:].contiguous()\n",
    "        output = prob[:, skip:,:].contiguous().transpose(1,2)\n",
    "        loss = criterion(output, target).cpu().detach()\n",
    "        total_loss += loss\n",
    "        batch_idx +=1\n",
    "        batch_size = X.shape[0]\n",
    "        ppl.append(np.exp(loss / (batch_size * (seqlen-skip)))) # update\n",
    "        acc += torch.sum(torch.argmax(prob.cpu().detach(),dim=2) == y.cpu().detach()).float() / torch.FloatTensor([batch_size*(seqlen-skip)])\n",
    "    print('Validation --- Epoch: %d, total loss: %.4f , PPL: %.4f, Acc: %.4f' % (e, total_loss.cpu().detach(), np.mean(ppl), acc/batch_idx))\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_store = ['model_4_layers_k3_word2vec2.pt','model_4_layers_k3_600_tied_3.pt','model_5_layers_k2_600_filters_1.pt']\n",
    "models = []\n",
    "for path in models_store:\n",
    "    models.append(torch.load(path))\n",
    "[m.cuda() for m in models]\n",
    "test_ensemble(models,0)\n",
    "\n",
    "make_predictions_for_kaggle_from_ensemble(models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
