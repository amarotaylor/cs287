{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 287, Homework 3: Neural Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "torch.__version__\n",
    "from common import *\n",
    "## Setup\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#!pip install --upgrade pip\n",
    "#!pip install -q numpy\n",
    "\n",
    "#!pip install -q torch torchtext spacy opt_einsum\n",
    "#!pip install -qU git+https://github.com/harvardnlp/namedtensor\n",
    "#!python -m spacy download en\n",
    "#!python -m spacy download de\n",
    "\n",
    "# Torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "# Text text processing library and methods for pretrained word embeddings\n",
    "from torchtext import data, datasets\n",
    "# Named Tensor wrappers\n",
    "from namedtensor import ntorch, NamedTensor\n",
    "from namedtensor.text import NamedField\n",
    "import numpy as np\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': <namedtensor.text.torch_text.NamedField object at 0x7fecc05cb438>, 'trg': <namedtensor.text.torch_text.NamedField object at 0x7fecb98336a0>}\n",
      "119076\n",
      "{'src': ['David', 'Gallo', ':', 'Das', 'ist', 'Bill', 'Lange', '.', 'Ich', 'bin', 'Dave', 'Gallo', '.'], 'trg': ['David', 'Gallo', ':', 'This', 'is', 'Bill', 'Lange', '.', 'I', \"'m\", 'Dave', 'Gallo', '.']}\n",
      "[('.', 113253), (',', 67237), ('ist', 24189), ('die', 23778), ('das', 17102), ('der', 15727), ('und', 15622), ('Sie', 15085), ('es', 13197), ('ich', 12946)]\n",
      "Size of German vocab 13353\n",
      "[('.', 113433), (',', 59512), ('the', 46029), ('to', 29177), ('a', 27548), ('of', 26794), ('I', 24887), ('is', 21775), (\"'s\", 20630), ('that', 19814)]\n",
      "Size of English vocab 11560\n",
      "2 3\n",
      "1 0\n"
     ]
    }
   ],
   "source": [
    "# split raw data into tokens\n",
    "import spacy\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# add beginning-of-sentence and end-of-sentence tokens to target\n",
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "DE = NamedField(names=('srcSeqlen',), tokenize=tokenize_de)\n",
    "EN = NamedField(names=('trgSeqlen',), tokenize=tokenize_en,\n",
    "                init_token = BOS_WORD, eos_token = EOS_WORD) # only target needs BOS/EOS\n",
    "\n",
    "# download dataset of 200K pairs of sentences\n",
    "# start with MAXLEN = 20\n",
    "MAX_LEN = 20\n",
    "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(DE, EN), \n",
    "                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
    "print(train.fields)\n",
    "print(len(train))\n",
    "print(vars(train[0]))\n",
    "\n",
    "# WHAT DOES THIS DO?\n",
    "'''src = open(\"valid.src\", \"w\")\n",
    "trg = open(\"valid.trg\", \"w\")\n",
    "for example in val:\n",
    "    print(\" \".join(example.src), file=src)\n",
    "    print(\" \".join(example.trg), file=trg)\n",
    "src.close()\n",
    "trg.close()'''\n",
    "\n",
    "# build vocab, convert words to indices\n",
    "MIN_FREQ = 5\n",
    "DE.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "EN.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
    "print(DE.vocab.freqs.most_common(10))\n",
    "print(\"Size of German vocab\", len(DE.vocab))\n",
    "print(EN.vocab.freqs.most_common(10))\n",
    "print(\"Size of English vocab\", len(EN.vocab))\n",
    "print(EN.vocab.stoi[\"<s>\"], EN.vocab.stoi[\"</s>\"])\n",
    "\n",
    "print(EN.vocab.stoi[\"<pad>\"], EN.vocab.stoi[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into batches\n",
    "BATCH_SIZE = 32\n",
    "device = torch.device('cuda:0')\n",
    "train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=BATCH_SIZE, device=device,\n",
    "                                                  repeat=False, sort_key=lambda x: len(x.src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to Sequence Learning with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- English to French translation, $p \\left( y_1, \\dots, y_{T'} \\ | \\ x_1, \\dots, x_T \\right) = \\prod_{t = 1}^{T'} p \\left( y_t \\ | \\ v, y_1, \\dots, y_{t-1} \\right)$\n",
    "- Each sentence ends in '<EOS\\>', out-of-vocab words denoted '<UNK\\>'\n",
    "- Model specs: \n",
    "    * Input vocabulary of 160,000 and output vocabulary of 80,000\n",
    "    * Deep LSTM to map (encode) input sequence to fixed-len vector\n",
    "    * Another deep LSTM to translate (decode) fixed-len vector to output sequence\n",
    "    * 4 layers per LSTM, 1000 cells per layer, 1000-dimensional word embeddings, softmax over 80,000 words\n",
    "    * Reversing order of words in source (but not target) improved performance\n",
    "        * Each word in the source is far from its corresponding word in the target (large minimal time lag); reversing the source reduces the minimal time lag, thereby allowing backprop to establish communication between source and target more easily\n",
    "- Training specs:\n",
    "    * Initialize all LSTM params $\\sim Unif[-0.08,0.08]$\n",
    "    * SGD w/o momentum, lr = 0.7\n",
    "        * After 5 epochs, halve the lr every half-epoch\n",
    "        * Train for 7.5 epochs\n",
    "    * Batch size = 128; divide gradient by batch size (denoted $g$)\n",
    "    * Hard constraint gradient norm; if $s = ||g||_2 > 5$, set $s = 5$\n",
    "    * Make sure all sentences within a minibatch are roughly the same length\n",
    "- Objective: $max \\frac{1}{|S|} \\sum_{(T,S) \\in \\mathcal{S}} log \\ p(T \\ | \\ S)$, where $\\mathcal{S}$ is the training set\n",
    "- Prediction: $\\hat{T} = argmax \\ p(T \\ | \\ S)$ via beam search, where beam size $B \\in {1,2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 500\n",
    "num_layers = 2\n",
    "attn_context2trg = attn_RNNet_batched(input_size=len(EN.vocab),hidden_size=context_size,num_layers=num_layers)\n",
    "attn_context2trg = attn_context2trg.cuda()\n",
    "attn_context2trg_optimizer = torch.optim.Adam(attn_context2trg.parameters(), lr=1e-3)\n",
    "\n",
    "seq2context = SequenceModel(len(DE.vocab),context_size,num_layers=num_layers)\n",
    "seq2context_optimizer = torch.optim.Adam(seq2context.parameters(), lr=1e-3)\n",
    "seq2context = seq2context.cuda()\n",
    "\n",
    "\n",
    "\n",
    "scheduler_c2t = torch.optim.lr_scheduler.ReduceLROnPlateau(attn_context2trg_optimizer, mode=\"min\", patience=4)\n",
    "scheduler_s2c = torch.optim.lr_scheduler.ReduceLROnPlateau(seq2context_optimizer, mode=\"min\", patience=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss: 196.80746459960938\n",
      "Epoch: 0, Batch: 500, Loss: 57.9072151184082\n",
      "Epoch: 0, Batch: 1000, Loss: 59.319637298583984\n",
      "Epoch: 0, Batch: 1500, Loss: 46.97551345825195\n",
      "Epoch: 0, Batch: 2000, Loss: 52.368019104003906\n",
      "Epoch: 0, Batch: 2500, Loss: 42.604759216308594\n",
      "Epoch: 0, Batch: 3000, Loss: 46.63115692138672\n",
      "Epoch: 0, Batch: 3500, Loss: 36.59870147705078\n",
      "Epoch: 0, Validation PPL: 7.792147636413574\n",
      "Wrote model!\n",
      "Epoch: 1, Batch: 0, Loss: 37.57067108154297\n",
      "Epoch: 1, Batch: 500, Loss: 41.15217971801758\n",
      "Epoch: 1, Batch: 1000, Loss: 38.78003692626953\n",
      "Epoch: 1, Batch: 1500, Loss: 37.293521881103516\n",
      "Epoch: 1, Batch: 2000, Loss: 37.09842300415039\n",
      "Epoch: 1, Batch: 2500, Loss: 40.06744384765625\n",
      "Epoch: 1, Batch: 3000, Loss: 40.234317779541016\n",
      "Epoch: 1, Batch: 3500, Loss: 41.856075286865234\n",
      "Epoch: 1, Validation PPL: 5.6893157958984375\n",
      "Wrote model!\n",
      "Epoch: 2, Batch: 0, Loss: 37.55695343017578\n",
      "Epoch: 2, Batch: 500, Loss: 37.20197677612305\n",
      "Epoch: 2, Batch: 1000, Loss: 38.441993713378906\n",
      "Epoch: 2, Batch: 1500, Loss: 30.511547088623047\n",
      "Epoch: 2, Batch: 2000, Loss: 31.77158546447754\n",
      "Epoch: 2, Batch: 2500, Loss: 34.8766975402832\n",
      "Epoch: 2, Batch: 3000, Loss: 31.02699089050293\n",
      "Epoch: 2, Batch: 3500, Loss: 32.78199768066406\n",
      "Epoch: 2, Validation PPL: 5.0230488777160645\n",
      "Wrote model!\n",
      "Epoch: 3, Batch: 0, Loss: 29.95269012451172\n",
      "Epoch: 3, Batch: 500, Loss: 30.613174438476562\n",
      "Epoch: 3, Batch: 1000, Loss: 33.06015396118164\n",
      "Epoch: 3, Batch: 1500, Loss: 32.293853759765625\n"
     ]
    }
   ],
   "source": [
    "best_ppl = 1e8\n",
    "for e in range(0,300):\n",
    "    attn_training_loop(e,train_iter,seq2context,attn_context2trg,seq2context_optimizer,attn_context2trg_optimizer)\n",
    "    ppl = attn_validation_loop(e,val_iter,seq2context,attn_context2trg,scheduler_c2t,scheduler_s2c,BATCH_SIZE=32,context_size=500)\n",
    "    if ppl < best_ppl:\n",
    "        torch.save(seq2context.state_dict(),'best_seq2seq_withattn_seq2context.pt')\n",
    "        torch.save(attn_context2trg.state_dict(),'best_seq2seq_withattn_context2trg.pt')\n",
    "        best_ppl = ppl\n",
    "        print('Wrote model!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65, Validation PPL: 4.2662811279296875\n"
     ]
    }
   ],
   "source": [
    "ppl = attn_validation_loop(e,val_iter,seq2context,attn_context2trg,scheduler_c2t,scheduler_s2c,BATCH_SIZE=32,context_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss: 40.098819732666016\n",
      "Epoch: 0, Batch: 500, Loss: 44.26935577392578\n",
      "Epoch: 0, Batch: 1000, Loss: 36.21977233886719\n",
      "Epoch: 0, Batch: 1500, Loss: 54.8524169921875\n",
      "Epoch: 0, Batch: 2000, Loss: 44.83747482299805\n",
      "Epoch: 0, Batch: 2500, Loss: 47.17257308959961\n",
      "Epoch: 0, Batch: 3000, Loss: 30.97538948059082\n",
      "Epoch: 0, Batch: 3500, Loss: 54.668338775634766\n",
      "Epoch: 0, Validation PPL: 1.0001386404037476\n",
      "Wrote model!\n",
      "Epoch: 0, Batch: 0, Loss: 34.754852294921875\n",
      "Epoch: 0, Batch: 500, Loss: 44.461830139160156\n",
      "Epoch: 0, Batch: 1000, Loss: 52.99005126953125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3b49d9d08bb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#best_ppl = 1e8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mattn_training_split_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq2context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattn_context2trg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq2context_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattn_context2trg_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_validation_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq2context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattn_context2trg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler_c2t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler_s2c\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontext_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mppl\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_ppl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cs287/hw3/common.py\u001b[0m in \u001b[0;36mattn_training_split_loop\u001b[0;34m(e, train_iter, seq2context, attn_context2trg, seq2context_optimizer, attn_context2trg_optimizer, BATCH_SIZE, context_size, EN)\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m                 \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeamsearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq2context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_context2trg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m                 \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cs287/hw3/common.py\u001b[0m in \u001b[0;36mbeamsearch\u001b[0;34m(seq2context, context2trg, context_size, src, beam_width, max_len, output_width, alpha, BATCH_SIZE, padding, EN)\u001b[0m\n\u001b[1;32m    363\u001b[0m                             \u001b[0mcurr_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_b_prob\u001b[0m \u001b[0;31m# set top prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m                             \u001b[0mcurr_string\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb_string\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# set sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m                             \u001b[0mcurr_string\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoppreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# set top word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# if preds are from subsequent beams, compare to existing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#best_ppl = 1e8\n",
    "for e in range(0,300):\n",
    "    attn_training_split_loop(0,train_iter,seq2context,attn_context2trg,seq2context_optimizer,attn_context2trg_optimizer,EN=EN)\n",
    "    ppl = attn_validation_loop(e,val_iter,seq2context,attn_context2trg,scheduler_c2t,scheduler_s2c,BATCH_SIZE=32,context_size=500)\n",
    "    if ppl < best_ppl:\n",
    "        torch.save(seq2context.state_dict(),'best_seq2seq_withattn_seq2context_splittrain2.pt')\n",
    "        torch.save(attn_context2trg.state_dict(),'best_seq2seq_withattn_context2trg_splittrain2.pt')\n",
    "        best_ppl = ppl\n",
    "        print('Wrote model!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Validation PPL: 15.696435928344727\n"
     ]
    }
   ],
   "source": [
    "ppl = attn_validation_loop(0,val_iter,seq2context,attn_context2trg,scheduler_c2t,scheduler_s2c,BATCH_SIZE=32,context_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2context.train()\n",
    "attn_context2trg.train()\n",
    "for ix,batch in enumerate(train_iter):\n",
    "        src = batch.src.values.transpose(0,1)\n",
    "        src = reverse_sequence(src)\n",
    "        trg = batch.trg.values.transpose(0,1)\n",
    "        break\n",
    "        if trg.shape[0] == BATCH_SIZE:\n",
    "        \n",
    "            seq2context_optimizer.zero_grad()\n",
    "            attn_context2trg_optimizer.zero_grad()\n",
    "        \n",
    "            encoder_outputs, encoder_hidden = seq2context(src)\n",
    "            loss = 0\n",
    "            decoder_context = torch.zeros(BATCH_SIZE, context_size, device='cuda') # 32 x 500\n",
    "            decoder_hidden = encoder_hidden\n",
    "            sentence = []\n",
    "            for j in range(trg.shape[1] - 1):\n",
    "                word_input = trg[:,j]\n",
    "                decoder_output, decoder_context, decoder_hidden, decoder_attention = attn_context2trg(word_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "                #print(decoder_output.shape, trg[i,j+1].view(-1).shape)\n",
    "                loss += criterion_train(decoder_output, trg[:,j+1])\n",
    "                \n",
    "                if np.mod(ix,100) == 0:\n",
    "                    sentence.extend([torch.argmax(decoder_output[0,:],dim=0)])\n",
    "                \n",
    "            loss.backward()\n",
    "            seq2context_optimizer.step()\n",
    "            attn_context2trg_optimizer.step()\n",
    "        \n",
    "            if np.mod(ix,500) == 0:\n",
    "                print('Epoch: {}, Batch: {}, Loss: {}'.format(e, ix, loss.cpu().detach()/BATCH_SIZE))\n",
    "                #print([EN.vocab.itos[i] for i in sentence])\n",
    "                #print([EN.vocab.itos[i] for i in trg[0,:]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "context_size = 500\n",
    "num_layers = 2\n",
    "max_len = 3\n",
    "attn_seq2context = SequenceModel(len(DE.vocab),context_size,num_layers=num_layers)\n",
    "state_dict = torch.load('best_seq2seq_withattn_seq2context.pt')\n",
    "attn_seq2context.load_state_dict(state_dict)\n",
    "attn_seq2context = attn_seq2context.cuda()\n",
    "\n",
    "attn_context2trg = attn_RNNet_batched(input_size=len(EN.vocab),hidden_size=context_size,num_layers=num_layers)\n",
    "state_dict = torch.load('best_seq2seq_withattn_context2trg.pt')\n",
    "attn_context2trg.load_state_dict(state_dict)\n",
    "attn_context2trg = attn_context2trg.cuda()\n",
    "\n",
    "def beam_search(seq2context, attn_context2trg, BEAM_WIDTH = 2, BATCH_SIZE=32, max_len=3):\n",
    "    top_p = {}\n",
    "    top_s = {}\n",
    "    stopped = torch.zeros(BATCH_SIZE,BEAM_WIDTH)==1\n",
    "    items = []\n",
    "    for i in range(BATCH_SIZE):\n",
    "        top_p[i] = []\n",
    "        top_s[i] = []\n",
    "        items.append(i)\n",
    "    \n",
    "    encoder_outputs, encoder_hidden = seq2context(src)\n",
    "    decoder_context = torch.zeros(BATCH_SIZE, context_size, device='cuda') # 32 x 500\n",
    "    decoder_hidden = encoder_hidden\n",
    "    word_input = trg[:,0]\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = attn_context2trg(word_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "\n",
    "    next_words = torch.argsort(lsm2(decoder_output),dim=1, descending=True)[:,0:BEAM_WIDTH]\n",
    "    p_words_init = torch.stack([torch.index_select(decoder_output[i,:],-1,next_words[i,:]) for i in range(BATCH_SIZE)])\n",
    "    p_words_running = torch.stack([p_words_init[:,b].repeat(1,2) for b in range(BEAM_WIDTH)]).view(BEAM_WIDTH**2,BATCH_SIZE).transpose(0,1)\n",
    "    next_words = next_words.transpose(0,1).flatten()\n",
    "    \n",
    "    top_p.update(dict(zip(items, p_words_init)))\n",
    "    top_s.update(dict(zip(items, next_words)))\n",
    "    \n",
    "    encoder_outputs = encoder_outputs.repeat(BEAM_WIDTH,1,1)\n",
    "    decoder_hidden = tuple([h.repeat(1,BEAM_WIDTH,1) for h in decoder_hidden])\n",
    "    decoder_context = decoder_context.repeat(2,1)\n",
    "    for j in range(max_len):\n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = attn_context2trg(next_words, decoder_context, decoder_hidden, encoder_outputs)\n",
    "            args = torch.argsort(lsm2(decoder_output),dim=1, descending=True)[:,0:BEAM_WIDTH]\n",
    "            next_words = torch.cat([args[BATCH_SIZE*(b):BATCH_SIZE*(b+1),:] for b in range(BEAM_WIDTH)],dim=1)\n",
    "            p_words = torch.stack([torch.index_select(decoder_output[i,:],-1,next_words[i,:]) for i in range(BATCH_SIZE)])\n",
    "            p_words_running += p_words\n",
    "            p_words_norm = p_words_running/(j+1) \n",
    "            word_selector = torch.argsort(p_words_norm,dim=1,descending=True)[:,:BEAM_WIDTH]\n",
    "            next_words = torch.stack([torch.index_select(next_words[s,:],0,word_selector[s,:]) for s in range(BATCH_SIZE)]).transpose(0,1).flatten()\n",
    "            break\n",
    "            torch.index_select(top_s.values, beam_indicator)\n",
    "            top_s.update(dict(zip(items, )))\n",
    "            \n",
    "            beam_indicator = word_selector>=2\n",
    "            indexs = torch.zeros(BATCH_SIZE,2,device='cuda')\n",
    "            for i in range(BATCH_SIZE):\n",
    "                indexs[i,:] += i+(BATCH_SIZE*beam_indicator[i,:].float())\n",
    "            indexs = indexs.long()\n",
    "            indexs = indexs.transpose(0,1).flatten()\n",
    "            decoder_hidden = tuple([torch.index_select(h,1,indexs) for h in decoder_hidden])\n",
    "            decoder_context = torch.index_select(decoder_context,0,indexs)\n",
    "            \n",
    "            \n",
    "            top_p.update(dict(zip(items, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_words.shape\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_p = {}\n",
    "top_s = {}\n",
    "stopped = torch.zeros(BATCH_SIZE,BEAM_WIDTH)==1\n",
    "items = []\n",
    "for i in range(BATCH_SIZE):\n",
    "    top_p[i] = []\n",
    "    top_s[i] = []\n",
    "    items.append(i)\n",
    "\n",
    "encoder_outputs, encoder_hidden = seq2context(src)\n",
    "decoder_context = torch.zeros(BATCH_SIZE, context_size, device='cuda') # 32 x 500\n",
    "decoder_hidden = encoder_hidden\n",
    "word_input = trg[:,0]\n",
    "decoder_output, decoder_context, decoder_hidden, decoder_attention = attn_context2trg(word_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "\n",
    "next_words = torch.argsort(lsm2(decoder_output),dim=1, descending=True)[:,0:BEAM_WIDTH]\n",
    "p_words_init = torch.stack([torch.index_select(decoder_output[i,:],-1,next_words[i,:]) for i in range(BATCH_SIZE)])\n",
    "p_words_running = torch.stack([p_words_init[:,b].repeat(1,2) for b in range(BEAM_WIDTH)]).view(BEAM_WIDTH**2,BATCH_SIZE).transpose(0,1)\n",
    "\n",
    "\n",
    "update = []\n",
    "for ix,p in enumerate(next_words):\n",
    "    update.append([torch.stack([trg[0,0]]+([next_words[ix,b]])) for b in range(BEAM_WIDTH)])\n",
    "\n",
    "top_p.update(dict(zip(items, p_words_init)))\n",
    "top_s.update(dict(zip(items, update)))\n",
    "next_words = next_words.transpose(0,1).flatten()\n",
    "\n",
    "encoder_outputs = encoder_outputs.repeat(BEAM_WIDTH,1,1)\n",
    "decoder_hidden = tuple([h.repeat(1,BEAM_WIDTH,1) for h in decoder_hidden])\n",
    "decoder_context = decoder_context.repeat(2,1)\n",
    "for j in range(max_len):\n",
    "        break\n",
    "        decoder_output, decoder_context, decoder_hidden, decoder_attention = attn_context2trg(next_words, decoder_context, decoder_hidden, encoder_outputs)\n",
    "        args = torch.argsort(lsm2(decoder_output),dim=1, descending=True)[:,0:BEAM_WIDTH]\n",
    "        next_words = torch.cat([args[BATCH_SIZE*(b):BATCH_SIZE*(b+1),:] for b in range(BEAM_WIDTH)],dim=1)\n",
    "        p_words = torch.stack([torch.index_select(decoder_output[i,:],-1,next_words[i,:]) for i in range(BATCH_SIZE)])\n",
    "        p_words_running += p_words\n",
    "        p_words_norm = p_words_running/(j+1) \n",
    "        word_selector = torch.argsort(p_words_norm,dim=1,descending=True)[:,:BEAM_WIDTH]\n",
    "        prev_words = torch.stack(list(top_s.values()))\n",
    "        words=([torch.index_select(prev_words[s,:],0, beam_indicator[s,:].long()) for s in range(BATCH_SIZE)])\n",
    "\n",
    "        update = []\n",
    "        for ix,p in enumerate(words):\n",
    "            update.append([torch.stack([p[b]]+([next_words[ix,b]])) for b in range(BEAM_WIDTH)])\n",
    "        top_s.update(dict(zip(items, update)))\n",
    "        next_words = torch.stack([torch.index_select(next_words[s,:],0,word_selector[s,:]) for s in range(BATCH_SIZE)]).transpose(0,1).flatten()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "update = []\n",
    "for ix,p in enumerate(words):\n",
    "    update.append([torch.stack([p[b]]+([next_words[ix,b]])) for b in range(BEAM_WIDTH)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([2, 0], device='cuda:0'), tensor([ 2, 14], device='cuda:0')],\n",
       " [tensor([2, 0], device='cuda:0'), tensor([ 2, 24], device='cuda:0')],\n",
       " [tensor([2, 0], device='cuda:0'), tensor([ 2, 27], device='cuda:0')],\n",
       " [tensor([2, 0], device='cuda:0'), tensor([ 2, 14], device='cuda:0')],\n",
       " [tensor([2, 0], device='cuda:0'), tensor([ 2, 14], device='cuda:0')],\n",
       " [tensor([2, 0], device='cuda:0'), tensor([ 2, 27], device='cuda:0')],\n",
       " [tensor([2, 0], device='cuda:0'), tensor([ 2, 42], device='cuda:0')],\n",
       " [tensor([ 2, 27], device='cuda:0'), tensor([ 2, 24], device='cuda:0')],\n",
       " [tensor([2, 0], device='cuda:0'), tensor([ 2, 24], device='cuda:0')],\n",
       " [tensor([2, 0], device='cuda:0'), tensor([ 2, 24], device='cuda:0')],\n",
       " [tensor([2, 0], device='cuda:0'), tensor([ 2, 52], device='cuda:0')],\n",
       " [tensor([ 2, 14], device='cuda:0'), tensor([ 2, 27], device='cuda:0')],\n",
       " [tensor([2, 0], device='cuda:0'), tensor([ 2, 27], device='cuda:0')],\n",
       " [tensor([2, 0], device='cuda:0'), tensor([ 2, 24], device='cuda:0')],\n",
       " [tensor([2, 0], device='cuda:0'), tensor([ 2, 97], device='cuda:0')],\n",
       " [tensor([2, 0], device='cuda:0'), tensor([ 2, 14], device='cuda:0')],\n",
       " [tensor([ 2, 27], device='cuda:0'), tensor([ 2, 14], device='cuda:0')],\n",
       " [tensor([2, 0], device='cuda:0'), tensor([ 2, 14], device='cuda:0')],\n",
       " [tensor([2, 0], device='cuda:0'), tensor([ 2, 24], device='cuda:0')],\n",
       " [tensor([2, 0], device='cuda:0'), tensor([ 2, 27], device='cuda:0')],\n",
       " [tensor([2, 0], device='cuda:0'), tensor([ 2, 24], device='cuda:0')],\n",
       " [tensor([2, 0], device='cuda:0'), tensor([ 2, 14], device='cuda:0')],\n",
       " [tensor([2, 0], device='cuda:0'), tensor([ 2, 14], device='cuda:0')],\n",
       " [tensor([2, 0], device='cuda:0'), tensor([ 2, 24], device='cuda:0')],\n",
       " [tensor([ 2, 27], device='cuda:0'), tensor([ 2, 14], device='cuda:0')],\n",
       " [tensor([2, 0], device='cuda:0'), tensor([ 2, 14], device='cuda:0')],\n",
       " [tensor([2, 0], device='cuda:0'), tensor([ 2, 14], device='cuda:0')],\n",
       " [tensor([2, 0], device='cuda:0'), tensor([ 2, 14], device='cuda:0')],\n",
       " [tensor([2, 0], device='cuda:0'), tensor([ 2, 27], device='cuda:0')],\n",
       " [tensor([2, 0], device='cuda:0'), tensor([ 2, 14], device='cuda:0')],\n",
       " [tensor([ 2, 14], device='cuda:0'), tensor([ 2, 27], device='cuda:0')],\n",
       " [tensor([ 2, 14], device='cuda:0'), tensor([ 2, 24], device='cuda:0')]]"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 5], device='cuda:0')"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_indicator = torch.argsort(p_words_running,dim=1,descending=True)[:,:BEAM_WIDTH]>=2\n",
    "indexs = torch.zeros(BATCH_SIZE,2,device='cuda')\n",
    "for i in range(BATCH_SIZE):\n",
    "    indexs[i,:] += i+(BATCH_SIZE*beam_indicator[i,:].float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexs = indexs.long()\n",
    "indexs = indexs.transpose(0,1).flatten()\n",
    "decoder_hidden = tuple([torch.index_select(h,1,indexs) for h in decoder_hidden])\n",
    "decoder_context = torch.index_select(decoder_context,0,indexs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 500])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_words = torch.cat([args[BATCH_SIZE*(b):BATCH_SIZE*(b+1),:] for b in range(BEAM_WIDTH)],dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_words = torch.stack([torch.index_select(decoder_output[i,:],-1,next_words[i,:]) for i in range(BATCH_SIZE)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.12301008936547375,\n",
       " 1: 0.1743470876801082,\n",
       " 2: 0.024813375393648696,\n",
       " 3: 0.4313473508338772,\n",
       " 4: 0.7169799087697334,\n",
       " 5: 0.2156059142994965,\n",
       " 6: 0.07298992879668831,\n",
       " 7: 0.8894616281344004,\n",
       " 8: 0.3090598195280607,\n",
       " 9: 0.6069338103608437}"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = []\n",
    "for i in range(10):\n",
    "    items.append(i)\n",
    "    dc[i] = []\n",
    "x = np.random.rand(10)\n",
    "dc.update(dict(zip(items, x)))\n",
    "dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test set\n",
    "sentences = []\n",
    "for i, l in enumerate(open(\"source_test.txt\"), 1):\n",
    "    sentences.append(re.split(' ', l))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
