{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 287, Homework 3: Neural Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "torch.__version__\n",
    "from common import *\n",
    "## Setup\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#!pip install --upgrade pip\n",
    "#!pip install -q numpy\n",
    "\n",
    "#!pip install -q torch torchtext spacy opt_einsum\n",
    "#!pip install -qU git+https://github.com/harvardnlp/namedtensor\n",
    "#!python -m spacy download en\n",
    "#!python -m spacy download de\n",
    "\n",
    "# Torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "# Text text processing library and methods for pretrained word embeddings\n",
    "from torchtext import data, datasets\n",
    "# Named Tensor wrappers\n",
    "from namedtensor import ntorch, NamedTensor\n",
    "from namedtensor.text import NamedField\n",
    "import numpy as np\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': <namedtensor.text.torch_text.NamedField object at 0x7f4d721a20b8>, 'trg': <namedtensor.text.torch_text.NamedField object at 0x7f4d6a7b3ac8>}\n",
      "119076\n",
      "{'src': ['David', 'Gallo', ':', 'Das', 'ist', 'Bill', 'Lange', '.', 'Ich', 'bin', 'Dave', 'Gallo', '.'], 'trg': ['David', 'Gallo', ':', 'This', 'is', 'Bill', 'Lange', '.', 'I', \"'m\", 'Dave', 'Gallo', '.']}\n",
      "[('.', 113253), (',', 67237), ('ist', 24189), ('die', 23778), ('das', 17102), ('der', 15727), ('und', 15622), ('Sie', 15085), ('es', 13197), ('ich', 12946)]\n",
      "Size of German vocab 13353\n",
      "[('.', 113433), (',', 59512), ('the', 46029), ('to', 29177), ('a', 27548), ('of', 26794), ('I', 24887), ('is', 21775), (\"'s\", 20630), ('that', 19814)]\n",
      "Size of English vocab 11560\n",
      "2 3\n",
      "1 0\n"
     ]
    }
   ],
   "source": [
    "# split raw data into tokens\n",
    "import spacy\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# add beginning-of-sentence and end-of-sentence tokens to target\n",
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "DE = NamedField(names=('srcSeqlen',), tokenize=tokenize_de)\n",
    "EN = NamedField(names=('trgSeqlen',), tokenize=tokenize_en,\n",
    "                init_token = BOS_WORD, eos_token = EOS_WORD) # only target needs BOS/EOS\n",
    "\n",
    "# download dataset of 200K pairs of sentences\n",
    "# start with MAXLEN = 20\n",
    "MAX_LEN = 20\n",
    "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(DE, EN), \n",
    "                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
    "print(train.fields)\n",
    "print(len(train))\n",
    "print(vars(train[0]))\n",
    "\n",
    "# WHAT DOES THIS DO?\n",
    "'''src = open(\"valid.src\", \"w\")\n",
    "trg = open(\"valid.trg\", \"w\")\n",
    "for example in val:\n",
    "    print(\" \".join(example.src), file=src)\n",
    "    print(\" \".join(example.trg), file=trg)\n",
    "src.close()\n",
    "trg.close()'''\n",
    "\n",
    "# build vocab, convert words to indices\n",
    "MIN_FREQ = 5\n",
    "DE.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "EN.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
    "print(DE.vocab.freqs.most_common(10))\n",
    "print(\"Size of German vocab\", len(DE.vocab))\n",
    "print(EN.vocab.freqs.most_common(10))\n",
    "print(\"Size of English vocab\", len(EN.vocab))\n",
    "print(EN.vocab.stoi[\"<s>\"], EN.vocab.stoi[\"</s>\"])\n",
    "\n",
    "print(EN.vocab.stoi[\"<pad>\"], EN.vocab.stoi[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into batches\n",
    "BATCH_SIZE = 32\n",
    "device = torch.device('cuda:0')\n",
    "train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=BATCH_SIZE, device=device,\n",
    "                                                  repeat=False, sort_key=lambda x: len(x.src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to Sequence Learning with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- English to French translation, $p \\left( y_1, \\dots, y_{T'} \\ | \\ x_1, \\dots, x_T \\right) = \\prod_{t = 1}^{T'} p \\left( y_t \\ | \\ v, y_1, \\dots, y_{t-1} \\right)$\n",
    "- Each sentence ends in '<EOS\\>', out-of-vocab words denoted '<UNK\\>'\n",
    "- Model specs: \n",
    "    * Input vocabulary of 160,000 and output vocabulary of 80,000\n",
    "    * Deep LSTM to map (encode) input sequence to fixed-len vector\n",
    "    * Another deep LSTM to translate (decode) fixed-len vector to output sequence\n",
    "    * 4 layers per LSTM, 1000 cells per layer, 1000-dimensional word embeddings, softmax over 80,000 words\n",
    "    * Reversing order of words in source (but not target) improved performance\n",
    "        * Each word in the source is far from its corresponding word in the target (large minimal time lag); reversing the source reduces the minimal time lag, thereby allowing backprop to establish communication between source and target more easily\n",
    "- Training specs:\n",
    "    * Initialize all LSTM params $\\sim Unif[-0.08,0.08]$\n",
    "    * SGD w/o momentum, lr = 0.7\n",
    "        * After 5 epochs, halve the lr every half-epoch\n",
    "        * Train for 7.5 epochs\n",
    "    * Batch size = 128; divide gradient by batch size (denoted $g$)\n",
    "    * Hard constraint gradient norm; if $s = ||g||_2 > 5$, set $s = 5$\n",
    "    * Make sure all sentences within a minibatch are roughly the same length\n",
    "- Objective: $max \\frac{1}{|S|} \\sum_{(T,S) \\in \\mathcal{S}} log \\ p(T \\ | \\ S)$, where $\\mathcal{S}$ is the training set\n",
    "- Prediction: $\\hat{T} = argmax \\ p(T \\ | \\ S)$ via beam search, where beam size $B \\in {1,2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 500\n",
    "num_layers = 2\n",
    "seq2context = SequenceModel(len(DE.vocab),context_size,num_layers=2)\n",
    "context2trg = RNNet(input_size=len(EN.vocab),hidden_size=context_size,num_layers=2,weight_tie=True)\n",
    "\n",
    "seq2context,context2trg = seq2context.cuda(),context2trg.cuda()\n",
    "seq2context_optimizer = torch.optim.SGD(seq2context.parameters(), lr=1)\n",
    "context2trg_optimizer = torch.optim.SGD(context2trg.parameters(), lr=1)\n",
    "\n",
    "lr_lambda = lambda t: 1 / (1.2**max(t-6,0))\n",
    "scheduler_c2t = torch.optim.lr_scheduler.LambdaLR(context2trg_optimizer, lr_lambda, last_epoch=-1)\n",
    "scheduler_s2c = torch.optim.lr_scheduler.LambdaLR(seq2context_optimizer, lr_lambda, last_epoch=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2context.train()\n",
    "context2trg.train()\n",
    "context_size = seq2context.context_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix,batch in enumerate(train_iter):\n",
    "    seq2context_optimizer.zero_grad()\n",
    "    context2trg_optimizer.zero_grad()\n",
    "        \n",
    "    src = batch.src.values.transpose(0,1)\n",
    "    src = reverse_sequence(src)\n",
    "    trg = batch.trg.values.transpose(0,1)\n",
    "    context, hidden_s2c = seq2context(src)\n",
    "    output, hidden_lm = context2trg(trg[:,:-1],hidden_s2c)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss: 30.344459533691406, Variance: 166542.078125\n",
      "Epoch: 0, Batch: 100, Loss: 38.58367919921875, Variance: 152495.078125\n",
      "Epoch: 0, Batch: 200, Loss: 32.62057876586914, Variance: 70155.3125\n",
      "Epoch: 0, Batch: 300, Loss: 33.22350311279297, Variance: 77269.9453125\n",
      "Epoch: 0, Batch: 400, Loss: 30.61174964904785, Variance: 58268.31640625\n",
      "Epoch: 0, Batch: 500, Loss: 30.48539161682129, Variance: 179236.0625\n",
      "Epoch: 0, Batch: 600, Loss: 30.524948120117188, Variance: 20881.13671875\n",
      "Epoch: 0, Batch: 700, Loss: 36.902435302734375, Variance: 36496.84765625\n",
      "Epoch: 0, Batch: 800, Loss: 31.251785278320312, Variance: 22397.3046875\n",
      "Epoch: 0, Batch: 900, Loss: 36.22175598144531, Variance: 49594.46484375\n",
      "Epoch: 0, Batch: 1000, Loss: 38.293609619140625, Variance: 32203.990234375\n",
      "Epoch: 0, Batch: 1100, Loss: 32.65779113769531, Variance: 28263.1875\n",
      "Epoch: 0, Batch: 1200, Loss: 30.370241165161133, Variance: 76748.1875\n",
      "Epoch: 0, Batch: 1300, Loss: 28.27395248413086, Variance: 140897.828125\n",
      "Epoch: 0, Batch: 1400, Loss: 30.277626037597656, Variance: 47471.63671875\n",
      "Epoch: 0, Batch: 1500, Loss: 31.97120475769043, Variance: 75248.21875\n",
      "Epoch: 0, Batch: 1600, Loss: 31.493701934814453, Variance: 59847.20703125\n",
      "Epoch: 0, Batch: 1700, Loss: 27.366355895996094, Variance: 189057.828125\n",
      "Epoch: 0, Batch: 1800, Loss: 34.587894439697266, Variance: 40314.6171875\n",
      "Epoch: 0, Batch: 1900, Loss: 35.344520568847656, Variance: 57529.421875\n",
      "Epoch: 0, Batch: 2000, Loss: 33.41920471191406, Variance: 14528.5712890625\n",
      "Epoch: 0, Batch: 2100, Loss: 39.866233825683594, Variance: 38726.2265625\n",
      "Epoch: 0, Batch: 2200, Loss: 33.72633743286133, Variance: 135808.46875\n",
      "Epoch: 0, Batch: 2300, Loss: 32.83989334106445, Variance: 67417.328125\n",
      "Epoch: 0, Batch: 2400, Loss: 38.574546813964844, Variance: 31862.845703125\n",
      "Epoch: 0, Batch: 2500, Loss: 34.33720397949219, Variance: 63712.7109375\n",
      "Epoch: 0, Batch: 2600, Loss: 33.9091796875, Variance: 59127.10546875\n",
      "Epoch: 0, Batch: 2700, Loss: 33.485618591308594, Variance: 42718.78515625\n",
      "Epoch: 0, Batch: 2800, Loss: 36.73780822753906, Variance: 75325.4765625\n",
      "Epoch: 0, Batch: 2900, Loss: 40.533668518066406, Variance: 56563.9375\n",
      "Epoch: 0, Batch: 3000, Loss: 33.85834503173828, Variance: 29357.87890625\n",
      "Epoch: 0, Batch: 3100, Loss: 30.06552505493164, Variance: 34040.78515625\n",
      "Epoch: 0, Batch: 3200, Loss: 31.538589477539062, Variance: 29487.80078125\n",
      "Epoch: 0, Batch: 3300, Loss: 33.753265380859375, Variance: 90475.7265625\n",
      "Epoch: 0, Batch: 3400, Loss: 29.194602966308594, Variance: 49586.10546875\n",
      "Epoch: 0, Batch: 3500, Loss: 31.86270523071289, Variance: 43843.484375\n",
      "Epoch: 0, Batch: 3600, Loss: 33.808040618896484, Variance: 55676.4453125\n",
      "Epoch: 0, Batch: 3700, Loss: 32.25387191772461, Variance: 96359.421875\n",
      "Epoch: 0, Validation loss: 31.123291015625, Validation ppl: 13.918624877929688\n",
      "Epoch: 1, Batch: 0, Loss: 33.14558792114258, Variance: 31638.0625\n",
      "Epoch: 1, Batch: 100, Loss: 31.308151245117188, Variance: 52943.6640625\n",
      "Epoch: 1, Batch: 200, Loss: 31.338790893554688, Variance: 122132.3046875\n",
      "Epoch: 1, Batch: 300, Loss: 35.57720184326172, Variance: 96598.3125\n",
      "Epoch: 1, Batch: 400, Loss: 32.86964797973633, Variance: 111306.8125\n",
      "Epoch: 1, Batch: 500, Loss: 32.643394470214844, Variance: 91927.0\n",
      "Epoch: 1, Batch: 600, Loss: 31.646900177001953, Variance: 52465.71484375\n",
      "Epoch: 1, Batch: 700, Loss: 37.033416748046875, Variance: 85748.4765625\n",
      "Epoch: 1, Batch: 800, Loss: 32.215484619140625, Variance: 73507.8125\n",
      "Epoch: 1, Batch: 900, Loss: 34.2385368347168, Variance: 52367.8984375\n",
      "Epoch: 1, Batch: 1000, Loss: 32.53631591796875, Variance: 88429.453125\n",
      "Epoch: 1, Batch: 1100, Loss: 26.976003646850586, Variance: 138923.640625\n",
      "Epoch: 1, Batch: 1200, Loss: 37.83195114135742, Variance: 59014.87109375\n",
      "Epoch: 1, Batch: 1300, Loss: 34.38013458251953, Variance: 128108.15625\n",
      "Epoch: 1, Batch: 1400, Loss: 33.787864685058594, Variance: 39702.98828125\n",
      "Epoch: 1, Batch: 1500, Loss: 34.7271614074707, Variance: 81316.6796875\n",
      "Epoch: 1, Batch: 1600, Loss: 34.79294967651367, Variance: 37356.6640625\n",
      "Epoch: 1, Batch: 1700, Loss: 36.978633880615234, Variance: 111041.875\n",
      "Epoch: 1, Batch: 1800, Loss: 33.27288055419922, Variance: 86933.96875\n",
      "Epoch: 1, Batch: 1900, Loss: 33.84308624267578, Variance: 194413.53125\n",
      "Epoch: 1, Batch: 2000, Loss: 30.928775787353516, Variance: 86886.8359375\n",
      "Epoch: 1, Batch: 2100, Loss: 42.73088455200195, Variance: 170397.65625\n",
      "Epoch: 1, Batch: 2200, Loss: 30.95254898071289, Variance: 39954.79296875\n",
      "Epoch: 1, Batch: 2300, Loss: 36.785057067871094, Variance: 25550.84375\n",
      "Epoch: 1, Batch: 2400, Loss: 30.969240188598633, Variance: 57390.3671875\n",
      "Epoch: 1, Batch: 2500, Loss: 34.587032318115234, Variance: 33829.015625\n",
      "Epoch: 1, Batch: 2600, Loss: 35.44499206542969, Variance: 28442.017578125\n",
      "Epoch: 1, Batch: 2700, Loss: 36.533912658691406, Variance: 33198.37109375\n",
      "Epoch: 1, Batch: 2800, Loss: 34.21054458618164, Variance: 54247.65625\n",
      "Epoch: 1, Batch: 2900, Loss: 34.74455642700195, Variance: 84948.4375\n",
      "Epoch: 1, Batch: 3000, Loss: 33.950016021728516, Variance: 76599.5625\n",
      "Epoch: 1, Batch: 3100, Loss: 34.1436653137207, Variance: 52402.91015625\n",
      "Epoch: 1, Batch: 3200, Loss: 35.24757385253906, Variance: 54516.71875\n",
      "Epoch: 1, Batch: 3300, Loss: 31.08953094482422, Variance: 83817.3125\n",
      "Epoch: 1, Batch: 3400, Loss: 33.645843505859375, Variance: 50461.22265625\n",
      "Epoch: 1, Batch: 3500, Loss: 36.6168212890625, Variance: 97222.984375\n",
      "Epoch: 1, Batch: 3600, Loss: 33.19778060913086, Variance: 158032.5\n",
      "Epoch: 1, Batch: 3700, Loss: 31.160964965820312, Variance: 95065.109375\n",
      "Epoch: 1, Validation loss: 31.129947662353516, Validation ppl: 13.926465034484863\n",
      "Epoch: 2, Batch: 0, Loss: 29.256702423095703, Variance: 75038.9609375\n",
      "Epoch: 2, Batch: 100, Loss: 35.01935577392578, Variance: 52274.88671875\n",
      "Epoch: 2, Batch: 200, Loss: 29.368377685546875, Variance: 59959.72265625\n",
      "Epoch: 2, Batch: 300, Loss: 33.954833984375, Variance: 139344.53125\n",
      "Epoch: 2, Batch: 400, Loss: 27.16849136352539, Variance: 90449.09375\n",
      "Epoch: 2, Batch: 500, Loss: 29.332014083862305, Variance: 50898.5546875\n",
      "Epoch: 2, Batch: 600, Loss: 27.72403907775879, Variance: 82981.5\n",
      "Epoch: 2, Batch: 700, Loss: 36.38492965698242, Variance: 54348.9765625\n",
      "Epoch: 2, Batch: 800, Loss: 37.69612121582031, Variance: 125497.09375\n",
      "Epoch: 2, Batch: 900, Loss: 32.38453674316406, Variance: 64380.81640625\n",
      "Epoch: 2, Batch: 1000, Loss: 36.45228576660156, Variance: 148598.796875\n",
      "Epoch: 2, Batch: 1100, Loss: 31.048574447631836, Variance: 76554.5859375\n",
      "Epoch: 2, Batch: 1200, Loss: 30.66250228881836, Variance: 49767.6015625\n",
      "Epoch: 2, Batch: 1300, Loss: 34.291099548339844, Variance: 84012.1796875\n",
      "Epoch: 2, Batch: 1400, Loss: 28.13417625427246, Variance: 175030.03125\n",
      "Epoch: 2, Batch: 1500, Loss: 28.29662322998047, Variance: 100459.9921875\n",
      "Epoch: 2, Batch: 1600, Loss: 30.829179763793945, Variance: 34158.0\n",
      "Epoch: 2, Batch: 1700, Loss: 33.0491943359375, Variance: 39599.0234375\n",
      "Epoch: 2, Batch: 1800, Loss: 33.461551666259766, Variance: 67685.21875\n",
      "Epoch: 2, Batch: 1900, Loss: 34.14624786376953, Variance: 92052.609375\n",
      "Epoch: 2, Batch: 2000, Loss: 34.870147705078125, Variance: 36283.2421875\n",
      "Epoch: 2, Batch: 2100, Loss: 29.924766540527344, Variance: 36381.02734375\n",
      "Epoch: 2, Batch: 2200, Loss: 39.57575225830078, Variance: 60165.20703125\n",
      "Epoch: 2, Batch: 2300, Loss: 36.53828430175781, Variance: 144369.8125\n",
      "Epoch: 2, Batch: 2400, Loss: 30.186416625976562, Variance: 49821.52734375\n",
      "Epoch: 2, Batch: 2500, Loss: 30.37752342224121, Variance: 71783.9296875\n",
      "Epoch: 2, Batch: 2600, Loss: 30.467670440673828, Variance: 40758.20703125\n",
      "Epoch: 2, Batch: 2700, Loss: 31.43929672241211, Variance: 28066.42578125\n",
      "Epoch: 2, Batch: 2800, Loss: 36.947269439697266, Variance: 147646.734375\n",
      "Epoch: 2, Batch: 2900, Loss: 23.32921600341797, Variance: 37204.703125\n",
      "Epoch: 2, Batch: 3000, Loss: 29.772815704345703, Variance: 25698.7109375\n",
      "Epoch: 2, Batch: 3100, Loss: 32.863887786865234, Variance: 46087.40625\n",
      "Epoch: 2, Batch: 3200, Loss: 31.60537338256836, Variance: 115858.8828125\n",
      "Epoch: 2, Batch: 3300, Loss: 33.07027053833008, Variance: 110680.5703125\n",
      "Epoch: 2, Batch: 3400, Loss: 30.439720153808594, Variance: 115186.71875\n",
      "Epoch: 2, Batch: 3500, Loss: 35.739933013916016, Variance: 140677.953125\n",
      "Epoch: 2, Batch: 3600, Loss: 31.157398223876953, Variance: 62635.40234375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Batch: 3700, Loss: 30.963459014892578, Variance: 155103.625\n",
      "Epoch: 2, Validation loss: 30.977779388427734, Validation ppl: 13.748322486877441\n",
      "Epoch: 3, Batch: 0, Loss: 29.794776916503906, Variance: 126414.0390625\n",
      "Epoch: 3, Batch: 100, Loss: 34.436279296875, Variance: 56234.109375\n",
      "Epoch: 3, Batch: 200, Loss: 28.13518714904785, Variance: 163698.875\n",
      "Epoch: 3, Batch: 300, Loss: 30.396564483642578, Variance: 38462.11328125\n",
      "Epoch: 3, Batch: 400, Loss: 33.30223083496094, Variance: 92151.859375\n",
      "Epoch: 3, Batch: 500, Loss: 35.09160614013672, Variance: 58381.34375\n",
      "Epoch: 3, Batch: 600, Loss: 39.76861572265625, Variance: 91850.5\n",
      "Epoch: 3, Batch: 700, Loss: 32.66163635253906, Variance: 18709.67578125\n",
      "Epoch: 3, Batch: 800, Loss: 32.45072937011719, Variance: 53173.19140625\n",
      "Epoch: 3, Batch: 900, Loss: 37.27729415893555, Variance: 47641.8828125\n",
      "Epoch: 3, Batch: 1000, Loss: 29.915870666503906, Variance: 50089.875\n",
      "Epoch: 3, Batch: 1100, Loss: 30.188358306884766, Variance: 141873.59375\n",
      "Epoch: 3, Batch: 1200, Loss: 34.714111328125, Variance: 34609.67578125\n",
      "Epoch: 3, Batch: 1300, Loss: 32.90971374511719, Variance: 39049.015625\n",
      "Epoch: 3, Batch: 1400, Loss: 32.28287124633789, Variance: 96100.2578125\n",
      "Epoch: 3, Batch: 1500, Loss: 37.61097717285156, Variance: 39376.3046875\n",
      "Epoch: 3, Batch: 1600, Loss: 30.08230209350586, Variance: 51354.09765625\n",
      "Epoch: 3, Batch: 1700, Loss: 31.253986358642578, Variance: 120568.2734375\n",
      "Epoch: 3, Batch: 1800, Loss: 34.3541259765625, Variance: 61229.25390625\n",
      "Epoch: 3, Batch: 1900, Loss: 30.139135360717773, Variance: 220479.6875\n",
      "Epoch: 3, Batch: 2000, Loss: 30.458650588989258, Variance: 64269.4296875\n",
      "Epoch: 3, Batch: 2100, Loss: 32.2907829284668, Variance: 76629.234375\n",
      "Epoch: 3, Batch: 2200, Loss: 26.851093292236328, Variance: 58314.3515625\n",
      "Epoch: 3, Batch: 2300, Loss: 38.00432586669922, Variance: 72852.1796875\n",
      "Epoch: 3, Batch: 2400, Loss: 37.74999237060547, Variance: 160935.875\n",
      "Epoch: 3, Batch: 2500, Loss: 32.61554718017578, Variance: 151790.828125\n",
      "Epoch: 3, Batch: 2600, Loss: 36.403656005859375, Variance: 85082.953125\n",
      "Epoch: 3, Batch: 2700, Loss: 34.95270538330078, Variance: 54595.73828125\n",
      "Epoch: 3, Batch: 2800, Loss: 32.93659591674805, Variance: 147816.65625\n",
      "Epoch: 3, Batch: 2900, Loss: 33.27803039550781, Variance: 38796.55859375\n",
      "Epoch: 3, Batch: 3000, Loss: 32.01642990112305, Variance: 201510.390625\n",
      "Epoch: 3, Batch: 3100, Loss: 30.467689514160156, Variance: 165672.625\n",
      "Epoch: 3, Batch: 3200, Loss: 31.537353515625, Variance: 40481.5703125\n",
      "Epoch: 3, Batch: 3300, Loss: 28.820199966430664, Variance: 125547.953125\n",
      "Epoch: 3, Batch: 3400, Loss: 33.77405548095703, Variance: 24454.8984375\n",
      "Epoch: 3, Batch: 3500, Loss: 37.80613708496094, Variance: 101680.1640625\n",
      "Epoch: 3, Batch: 3600, Loss: 33.48314666748047, Variance: 57565.68359375\n",
      "Epoch: 3, Batch: 3700, Loss: 31.025096893310547, Variance: 38296.8046875\n",
      "Epoch: 3, Validation loss: 30.93628692626953, Validation ppl: 13.700140953063965\n",
      "Epoch: 4, Batch: 0, Loss: 33.39966583251953, Variance: 93085.734375\n",
      "Epoch: 4, Batch: 100, Loss: 38.14143753051758, Variance: 149289.640625\n",
      "Epoch: 4, Batch: 200, Loss: 36.07477951049805, Variance: 34967.59375\n",
      "Epoch: 4, Batch: 300, Loss: 31.742427825927734, Variance: 64993.609375\n",
      "Epoch: 4, Batch: 400, Loss: 28.374082565307617, Variance: 63057.578125\n",
      "Epoch: 4, Batch: 500, Loss: 35.745208740234375, Variance: 25570.87890625\n",
      "Epoch: 4, Batch: 600, Loss: 30.237167358398438, Variance: 209087.046875\n",
      "Epoch: 4, Batch: 700, Loss: 32.34170913696289, Variance: 67991.15625\n",
      "Epoch: 4, Batch: 800, Loss: 32.79475402832031, Variance: 142779.171875\n",
      "Epoch: 4, Batch: 900, Loss: 31.559974670410156, Variance: 29588.076171875\n",
      "Epoch: 4, Batch: 1000, Loss: 35.23783874511719, Variance: 145169.5\n",
      "Epoch: 4, Batch: 1100, Loss: 35.85528564453125, Variance: 281040.9375\n",
      "Epoch: 4, Batch: 1200, Loss: 32.776824951171875, Variance: 48363.5\n",
      "Epoch: 4, Batch: 1300, Loss: 30.308860778808594, Variance: 30667.99609375\n",
      "Epoch: 4, Batch: 1400, Loss: 31.391571044921875, Variance: 36340.05859375\n",
      "Epoch: 4, Batch: 1500, Loss: 31.806299209594727, Variance: 41229.73046875\n",
      "Epoch: 4, Batch: 1600, Loss: 30.43379020690918, Variance: 177341.96875\n",
      "Epoch: 4, Batch: 1700, Loss: 37.64384078979492, Variance: 51095.66796875\n",
      "Epoch: 4, Batch: 1800, Loss: 29.810409545898438, Variance: 98936.484375\n",
      "Epoch: 4, Batch: 1900, Loss: 34.73408889770508, Variance: 57482.80078125\n",
      "Epoch: 4, Batch: 2000, Loss: 35.99097442626953, Variance: 114650.0078125\n",
      "Epoch: 4, Batch: 2100, Loss: 30.087635040283203, Variance: 113019.8203125\n",
      "Epoch: 4, Batch: 2200, Loss: 24.534446716308594, Variance: 230972.0\n",
      "Epoch: 4, Batch: 2300, Loss: 32.29668045043945, Variance: 93983.15625\n",
      "Epoch: 4, Batch: 2400, Loss: 34.30413818359375, Variance: 104579.734375\n",
      "Epoch: 4, Batch: 2500, Loss: 34.95366668701172, Variance: 60814.9921875\n",
      "Epoch: 4, Batch: 2600, Loss: 33.152488708496094, Variance: 77758.4765625\n",
      "Epoch: 4, Batch: 2700, Loss: 36.63154602050781, Variance: 201107.03125\n",
      "Epoch: 4, Batch: 2800, Loss: 29.39848518371582, Variance: 43548.76171875\n",
      "Epoch: 4, Batch: 2900, Loss: 31.004915237426758, Variance: 38772.78125\n",
      "Epoch: 4, Batch: 3000, Loss: 35.371681213378906, Variance: 53468.0078125\n",
      "Epoch: 4, Batch: 3100, Loss: 36.30638122558594, Variance: 99126.3984375\n",
      "Epoch: 4, Batch: 3200, Loss: 31.58080291748047, Variance: 36256.9140625\n",
      "Epoch: 4, Batch: 3300, Loss: 30.199628829956055, Variance: 109900.6328125\n",
      "Epoch: 4, Batch: 3400, Loss: 36.63349151611328, Variance: 91241.0078125\n",
      "Epoch: 4, Batch: 3500, Loss: 31.988346099853516, Variance: 68751.28125\n",
      "Epoch: 4, Batch: 3600, Loss: 33.718597412109375, Variance: 84707.5078125\n",
      "Epoch: 4, Batch: 3700, Loss: 30.988996505737305, Variance: 135972.59375\n",
      "Epoch: 4, Validation loss: 30.90847396850586, Validation ppl: 13.667943000793457\n"
     ]
    }
   ],
   "source": [
    "for e in range(5):\n",
    "    training_loop(e,train_iter,seq2context,context2trg,seq2context_optimizer,context2trg_optimizer,BATCH_SIZE)\n",
    "    validation_loop(e,val_iter,seq2context,context2trg,scheduler_s2c,scheduler_c2t,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transfer', 'effective', 'formula', 'collide', 'open', 'breathes', 'demanding', 'Also', 'vanished', 'party', 'condom', 'Things', 'daf-2', 'formula', 'party', 'James', 'party', 'outsiders', 'Bonnet', 'party', 'averse']\n",
      "['<s>', 'We', 'also', 'use', '[', 'an', ']', 'electronic', 'medical', 'record', 'system', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "for ix,batch in enumerate(train_iter):\n",
    "    src = batch.src.values.transpose(0,1)\n",
    "    src = reverse_sequence(src)\n",
    "    trg = batch.trg.values.transpose(0,1)\n",
    "    break\n",
    "\n",
    "h0=None\n",
    "context, hidden_s2c = seq2context(src,h0)\n",
    "output, hidden_lm = context2trg(trg[:,:-1],hidden_s2c)\n",
    "\n",
    "\n",
    "print([EN.vocab.itos[i] for i in torch.argmax(lsm(output),2)[3,:]])\n",
    "print([EN.vocab.itos[i] for i in trg[3,:]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_input, last_context, last_hidden, encoder_outputs\n",
    "\n",
    "encoder_outputs, encoder_hidden = seq2context(src)\n",
    "encoder_outputs = encoder_outputs[0,:,:]\n",
    "word_input = torch.tensor([DE.vocab.stoi['<s>']], device='cuda')\n",
    "last_context = torch.zeros([1, context_size], device='cuda') # 1 x 500\n",
    "last_hidden = encoder_hidden\n",
    "last_hidden = tuple([last_hidden[0][:,0,:].view(2,1,500).contiguous(),last_hidden[1][:,0,:].view(2,1,500).contiguous()])\n",
    "word_embedded = context2trg.emb(word_input).view(1, 1, -1) # 1 x 1 x 500\n",
    "rnn_input = torch.cat((word_embedded, last_context.unsqueeze(0)), 2) # 1 x 1 x 1000\n",
    "rnn_output, hidden = attn_context2trg.rnn(rnn_input, last_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_dot(rnn_output,encoder_outputs):\n",
    "    return F.softmax(torch.matmul(rnn_output.squeeze(0),encoder_outputs.transpose(0,1)).squeeze(),dim=0).unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights = attn_dot(rnn_output,encoder_outputs)\n",
    "context = attn_weights.bmm(encoder_outputs.unsqueeze(1).transpose(0, 1))\n",
    "rnn_output = rnn_output.squeeze(0)\n",
    "context = context.squeeze(1)\n",
    "output = F.log_softmax(attn_context2trg.lnr(torch.cat((rnn_output, context), 1)),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs, encoder_hidden = seq2context(src)\n",
    "#encoder_outputs = encoder_outputs[0,:,:]\n",
    "word_input = torch.tensor([DE.vocab.stoi['<s>']], device='cuda')\n",
    "last_context = torch.zeros(1, context_size, device='cuda') # 1 x 500\n",
    "last_hidden = encoder_hidden\n",
    "#last_hidden = tuple([last_hidden[0][:,0,:].view(2,1,500).contiguous(),last_hidden[1][:,0,:].view(2,1,500).contiguous()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output, decoder_context, decoder_hidden, decoder_attention = attn_context2trg(word_input, last_context, last_hidden, encoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 500])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " decoder_context.shape #decoder_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1000])\n"
     ]
    }
   ],
   "source": [
    "word_input = trg[1,1]\n",
    "decoder_output, decoder_context, decoder_hidden, decoder_attention = attn_context2trg(word_input, decoder_context, decoder_hidden, encoder_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1000])\n"
     ]
    }
   ],
   "source": [
    "def compare_sentence(trg,ix=32):\n",
    "    outputs = []\n",
    "    for j in range(trg.shape[1] - 1):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Batch: 0, Loss: 161.0377197265625\n",
      "['Solar', 'the', 'we', 'we', 'slow', 'for', 'body', 'like', 'we', 'for', '<pad>', 'get', 'structures', 'the', 'incredible', '<pad>', '<pad>', 'times', '<pad>', 'the', 'off']\n",
      "['<s>', 'It', 'looks', 'like', 'it', \"'s\", 'kind', 'of', 'been', 'there', ',', 'and', 'then', 'crashed', 'all', 'these', 'simpler', 'forms', 'into', 'it', '.', '</s>']\n",
      "Epoch: 2, Batch: 100, Loss: 195.60531616210938\n",
      "['To', 'I', 'be', 'comes', '?', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'Just', 'does', \"n't\", 'make', 'sense', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Epoch: 2, Batch: 200, Loss: 258.91851806640625\n",
      "['It', 'car', 'will', 'things', 'of', '<pad>', 'about', '<pad>', 'of', '<pad>', '?', 'book', '.', 'U', '.', 'enhance', '<pad>', '<pad>', '.', '<pad>', '.']\n",
      "['<s>', 'So', 'the', 'People', \"'s\", 'Building', ',', 'as', 'we', 'called', 'it', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Epoch: 2, Batch: 300, Loss: 234.93382263183594\n",
      "['Finally', 'HG', 'context', 'pools', 'nuclear', 'War', '<pad>', ',', 'body', 'excerpt', '<pad>', 'roll', 'everyone', 'cardboard', 'pesticides', ',', '1986', '<pad>', 'Stanley', '<pad>', 'disgusting']\n",
      "['<s>', 'They', 'are', 'expected', 'to', 'bear', 'the', 'burden', 'of', 'costs', 'in', 'married', 'life', ',', 'but', 'they', 'ca', \"n't\", 'find', 'jobs', '.', '</s>']\n",
      "Epoch: 2, Batch: 400, Loss: 233.083251953125\n",
      "['rhetoric', 'challenged', '70', '<pad>', 'solution', '<pad>', '<pad>', 'be', '<pad>', '<pad>', '</s>', '<pad>', '<pad>', 'okay', '<pad>', 'ages', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'I', 'said', ',', '\"', 'Look', ',', 'it', \"'s\", ',', 'after', 'all', ',', 'two', 'dollars', '.', 'Take', 'it', 'home', '.', '\"', '</s>']\n",
      "Epoch: 2, Batch: 500, Loss: 324.41217041015625\n",
      "['They', 'I', 'ears', 'opportunity', 'for', 'read', '<pad>', 'equal', '<pad>', '<pad>', '<pad>', 'complex', '<pad>', '<pad>', '<pad>', '<pad>', 'certainly', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'These', 'are', 'the', 'models', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Epoch: 2, Batch: 600, Loss: 624.012451171875\n",
      "['But', 'Pope', 'in', 'at', 'as', '<pad>', '-', 'die', 'World', 'Mr.', 'fantasy', 'month', 'decided', 'and', 'die', '<pad>', 'but', 'measured', '<pad>', 'die', '<pad>']\n",
      "['<s>', 'There', \"'s\", 'something', 'very', 'peculiar', 'about', 'Venice', ',', 'that', 'its', 'administration', 'has', 'been', 'very', ',', 'very', 'bureaucratic', '.', '</s>', '<pad>', '<pad>']\n",
      "Epoch: 2, Batch: 700, Loss: 350.88470458984375\n",
      "['The', '3D', 'sooner', 'the', 'I', '\"', 'the', 'first', 'film', 'done', 'this', 'the', 'used', 'him', 'prices', 'children', 'devices', 'problems', 'despite', 'prices', 'I']\n",
      "['<s>', 'So', 'I', 'was', 'at', 'my', 'minimum', 'altitude', 'in', 'that', 'vector', '--', 'fast', '--', 'so', 'I', 'pulled', 'that', '.', '</s>', '<pad>', '<pad>']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-8a006e589129>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mattn_training_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-154-497d3a69be54>\u001b[0m in \u001b[0;36mattn_training_loop\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mword_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_context2trg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0;31m#print(decoder_output.shape, trg[i,j+1].view(-1).shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-133-259b96fc509a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, word_input, last_context, last_hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mword_embedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mrnn_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_embedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_context\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# batch x 1 x hiddenx2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mrnn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_context2trg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# batch x src_seqlen x 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 179\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 500\n",
    "num_layers = 2\n",
    "attn_context2trg = attn_RNNet_batched(input_size=len(EN.vocab),hidden_size=context_size,num_layers=num_layers)\n",
    "attn_context2trg = attn_context2trg.cuda()\n",
    "attn_context2trg_optimizer = torch.optim.Adam(attn_context2trg.parameters(), lr=1e-3)\n",
    "\n",
    "seq2context = SequenceModel(len(DE.vocab),context_size,num_layers=num_layers)\n",
    "seq2context_optimizer = torch.optim.Adam(seq2context.parameters(), lr=1e-3)\n",
    "seq2context = seq2context.cuda()\n",
    "\n",
    "\n",
    "\n",
    "scheduler_c2t = torch.optim.lr_scheduler.ReduceLROnPlateau(attn_context2trg_optimizer, mode=\"min\", patience=4)\n",
    "scheduler_s2c = torch.optim.lr_scheduler.ReduceLROnPlateau(seq2context_optimizer, mode=\"min\", patience=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Validation PPL: 7016.48095703125\n",
      "Wrote model!\n"
     ]
    }
   ],
   "source": [
    "best_ppl = 1e8\n",
    "for e in range(0,300):\n",
    "    attn_training_loop(e,train_iter,seq2context,attn_context2trg,seq2context_optimizer,attn_context2trg_optimizer)\n",
    "    ppl = attn_validation_loop(e,val_iter,seq2context,attn_context2trg,scheduler_c2t,scheduler_s2c,BATCH_SIZE=32,context_size=500)\n",
    "    if ppl < best_ppl:\n",
    "        torch.save(seq2context.state_dict(),'best_seq2seq_withattn_seq2context.pt')\n",
    "        torch.save(attn_context2trg.state_dict(),'best_seq2seq_withattn_context2trg.pt')\n",
    "        best_ppl = ppl\n",
    "        print('Wrote model!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.6539e-02,  4.2958e-02, -3.3869e-02,  ...,  3.6437e-02,\n",
       "          1.6895e-02, -6.5814e-03]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(torch.matmul(rnn_output.squeeze(0),encoder_outputs.transpose(0,1)).squeeze(),dim=0).unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "    \n",
    "    # Prepare input and output variables\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))\n",
    "    decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "\n",
    "    # Choose whether to use teacher forcing\n",
    "    use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "    if use_teacher_forcing:\n",
    "        \n",
    "        # Teacher forcing: Use the ground-truth target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            decoder_input = target_variable[di]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test set\n",
    "sentences = []\n",
    "for i, l in enumerate(open(\"source_test.txt\"), 1):\n",
    "  sentences.append(re.split(' ', l))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
