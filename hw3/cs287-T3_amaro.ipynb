{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 287, Homework 3: Neural Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "torch.__version__\n",
    "from common import *\n",
    "## Setup\n",
    "\n",
    "#!pip install --upgrade pip\n",
    "#!pip install -q numpy\n",
    "\n",
    "#!pip install -q torch torchtext spacy opt_einsum\n",
    "#!pip install -qU git+https://github.com/harvardnlp/namedtensor\n",
    "#!python -m spacy download en\n",
    "#!python -m spacy download de\n",
    "\n",
    "# Torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "# Text text processing library and methods for pretrained word embeddings\n",
    "from torchtext import data, datasets\n",
    "# Named Tensor wrappers\n",
    "from namedtensor import ntorch, NamedTensor\n",
    "from namedtensor.text import NamedField\n",
    "import numpy as np\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': <namedtensor.text.torch_text.NamedField object at 0x7f7656851b70>, 'trg': <namedtensor.text.torch_text.NamedField object at 0x7f764f700710>}\n",
      "119076\n",
      "{'src': ['David', 'Gallo', ':', 'Das', 'ist', 'Bill', 'Lange', '.', 'Ich', 'bin', 'Dave', 'Gallo', '.'], 'trg': ['David', 'Gallo', ':', 'This', 'is', 'Bill', 'Lange', '.', 'I', \"'m\", 'Dave', 'Gallo', '.']}\n",
      "[('.', 113253), (',', 67237), ('ist', 24189), ('die', 23778), ('das', 17102), ('der', 15727), ('und', 15622), ('Sie', 15085), ('es', 13197), ('ich', 12946)]\n",
      "Size of German vocab 13353\n",
      "[('.', 113433), (',', 59512), ('the', 46029), ('to', 29177), ('a', 27548), ('of', 26794), ('I', 24887), ('is', 21775), (\"'s\", 20630), ('that', 19814)]\n",
      "Size of English vocab 11560\n",
      "2 3\n",
      "1 0\n"
     ]
    }
   ],
   "source": [
    "# split raw data into tokens\n",
    "import spacy\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# add beginning-of-sentence and end-of-sentence tokens to target\n",
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "DE = NamedField(names=('srcSeqlen',), tokenize=tokenize_de)\n",
    "EN = NamedField(names=('trgSeqlen',), tokenize=tokenize_en,\n",
    "                init_token = BOS_WORD, eos_token = EOS_WORD) # only target needs BOS/EOS\n",
    "\n",
    "# download dataset of 200K pairs of sentences\n",
    "# start with MAXLEN = 20\n",
    "MAX_LEN = 20\n",
    "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(DE, EN), \n",
    "                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
    "print(train.fields)\n",
    "print(len(train))\n",
    "print(vars(train[0]))\n",
    "\n",
    "# WHAT DOES THIS DO?\n",
    "'''src = open(\"valid.src\", \"w\")\n",
    "trg = open(\"valid.trg\", \"w\")\n",
    "for example in val:\n",
    "    print(\" \".join(example.src), file=src)\n",
    "    print(\" \".join(example.trg), file=trg)\n",
    "src.close()\n",
    "trg.close()'''\n",
    "\n",
    "# build vocab, convert words to indices\n",
    "MIN_FREQ = 5\n",
    "DE.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "EN.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
    "print(DE.vocab.freqs.most_common(10))\n",
    "print(\"Size of German vocab\", len(DE.vocab))\n",
    "print(EN.vocab.freqs.most_common(10))\n",
    "print(\"Size of English vocab\", len(EN.vocab))\n",
    "print(EN.vocab.stoi[\"<s>\"], EN.vocab.stoi[\"</s>\"])\n",
    "\n",
    "print(EN.vocab.stoi[\"<pad>\"], EN.vocab.stoi[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into batches\n",
    "BATCH_SIZE = 32\n",
    "device = torch.device('cuda:0')\n",
    "train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=BATCH_SIZE, device=device,\n",
    "                                                  repeat=False, sort_key=lambda x: len(x.src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to Sequence Learning with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- English to French translation, $p \\left( y_1, \\dots, y_{T'} \\ | \\ x_1, \\dots, x_T \\right) = \\prod_{t = 1}^{T'} p \\left( y_t \\ | \\ v, y_1, \\dots, y_{t-1} \\right)$\n",
    "- Each sentence ends in '<EOS\\>', out-of-vocab words denoted '<UNK\\>'\n",
    "- Model specs: \n",
    "    * Input vocabulary of 160,000 and output vocabulary of 80,000\n",
    "    * Deep LSTM to map (encode) input sequence to fixed-len vector\n",
    "    * Another deep LSTM to translate (decode) fixed-len vector to output sequence\n",
    "    * 4 layers per LSTM, 1000 cells per layer, 1000-dimensional word embeddings, softmax over 80,000 words\n",
    "    * Reversing order of words in source (but not target) improved performance\n",
    "        * Each word in the source is far from its corresponding word in the target (large minimal time lag); reversing the source reduces the minimal time lag, thereby allowing backprop to establish communication between source and target more easily\n",
    "- Training specs:\n",
    "    * Initialize all LSTM params $\\sim Unif[-0.08,0.08]$\n",
    "    * SGD w/o momentum, lr = 0.7\n",
    "        * After 5 epochs, halve the lr every half-epoch\n",
    "        * Train for 7.5 epochs\n",
    "    * Batch size = 128; divide gradient by batch size (denoted $g$)\n",
    "    * Hard constraint gradient norm; if $s = ||g||_2 > 5$, set $s = 5$\n",
    "    * Make sure all sentences within a minibatch are roughly the same length\n",
    "- Objective: $max \\frac{1}{|S|} \\sum_{(T,S) \\in \\mathcal{S}} log \\ p(T \\ | \\ S)$, where $\\mathcal{S}$ is the training set\n",
    "- Prediction: $\\hat{T} = argmax \\ p(T \\ | \\ S)$ via beam search, where beam size $B \\in {1,2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 500\n",
    "seq2context = SequenceModel(len(DE.vocab),context_size,num_layers=2)\n",
    "context2trg = RNNet(input_size=len(EN.vocab),hidden_size=context_size,num_layers=2,weight_tie=True)\n",
    "\n",
    "seq2context,context2trg = seq2context.cuda(),context2trg.cuda()\n",
    "seq2context_optimizer = torch.optim.SGD(seq2context.parameters(), lr=1)\n",
    "context2trg_optimizer = torch.optim.SGD(context2trg.parameters(), lr=1)\n",
    "\n",
    "lr_lambda = lambda t: learning_rate / (1.2**max(t-6,0))\n",
    "scheduler_c2t = torch.optim.lr_scheduler.LambdaLR(context2trg_optimizer, lr_lambda, last_epoch=-1)\n",
    "scheduler_s2c = torch.optim.lr_scheduler.LambdaLR(seq2context_optimizer, lr_lambda, last_epoch=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2context.train()\n",
    "context2trg.train()\n",
    "context_size = seq2context.context_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix,batch in enumerate(train_iter):\n",
    "    seq2context_optimizer.zero_grad()\n",
    "    context2trg_optimizer.zero_grad()\n",
    "        \n",
    "    src = batch.src.values.transpose(0,1)\n",
    "    src = reverse_sequence(src)\n",
    "    trg = batch.trg.values.transpose(0,1)\n",
    "    context, hidden_s2c = seq2context(src)\n",
    "    output, hidden_lm = context2trg(trg[:,:-1],hidden_s2c)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss: 52.18206024169922, Variance: 1337.692626953125\n",
      "Epoch: 0, Batch: 100, Loss: 54.820556640625, Variance: 1533.808349609375\n",
      "Epoch: 0, Batch: 200, Loss: 45.864437103271484, Variance: 1175.11474609375\n",
      "Epoch: 0, Batch: 300, Loss: 58.96554183959961, Variance: 4183.87158203125\n",
      "Epoch: 0, Batch: 400, Loss: 56.798648834228516, Variance: 1819.83203125\n",
      "Epoch: 0, Batch: 500, Loss: 54.7036018371582, Variance: 1071.6414794921875\n",
      "Epoch: 0, Batch: 600, Loss: 61.153053283691406, Variance: 3251.94580078125\n",
      "Epoch: 0, Batch: 700, Loss: 54.923439025878906, Variance: 1505.384521484375\n",
      "Epoch: 0, Batch: 800, Loss: 46.069190979003906, Variance: 4835.55810546875\n",
      "Epoch: 0, Batch: 900, Loss: 59.25823974609375, Variance: 2074.23779296875\n",
      "Epoch: 0, Batch: 1000, Loss: 45.67032241821289, Variance: 2824.869873046875\n",
      "Epoch: 0, Batch: 1100, Loss: 57.12811279296875, Variance: 4684.85546875\n",
      "Epoch: 0, Batch: 1200, Loss: 49.58352279663086, Variance: 915.0695190429688\n",
      "Epoch: 0, Batch: 1300, Loss: 52.87333679199219, Variance: 1621.8192138671875\n",
      "Epoch: 0, Batch: 1400, Loss: 57.219444274902344, Variance: 1836.1029052734375\n",
      "Epoch: 0, Batch: 1500, Loss: 56.31717300415039, Variance: 853.0186157226562\n",
      "Epoch: 0, Batch: 1600, Loss: 44.4184455871582, Variance: 5445.017578125\n",
      "Epoch: 0, Batch: 1700, Loss: 60.67025375366211, Variance: 2729.3134765625\n",
      "Epoch: 0, Batch: 1800, Loss: 57.35791778564453, Variance: 1693.0716552734375\n",
      "Epoch: 0, Batch: 1900, Loss: 49.20964813232422, Variance: 6145.02734375\n",
      "Epoch: 0, Batch: 2000, Loss: 53.06172180175781, Variance: 2615.97314453125\n",
      "Epoch: 0, Batch: 2100, Loss: 54.2663459777832, Variance: 2384.895751953125\n",
      "Epoch: 0, Batch: 2200, Loss: 51.23918151855469, Variance: 2118.021728515625\n",
      "Epoch: 0, Batch: 2300, Loss: 50.33542251586914, Variance: 1114.486572265625\n",
      "Epoch: 0, Batch: 2400, Loss: 59.3071403503418, Variance: 942.457275390625\n",
      "Epoch: 0, Batch: 2500, Loss: 45.333335876464844, Variance: 2072.489013671875\n",
      "Epoch: 0, Batch: 2600, Loss: 48.649688720703125, Variance: 8042.5771484375\n",
      "Epoch: 0, Batch: 2700, Loss: 55.934410095214844, Variance: 1992.370361328125\n",
      "Epoch: 0, Batch: 2800, Loss: 50.63018798828125, Variance: 3065.224365234375\n",
      "Epoch: 0, Batch: 2900, Loss: 56.67488098144531, Variance: 2111.59619140625\n",
      "Epoch: 0, Batch: 3000, Loss: 54.38906478881836, Variance: 2654.6279296875\n",
      "Epoch: 0, Batch: 3100, Loss: 50.672119140625, Variance: 28459.421875\n",
      "Epoch: 0, Batch: 3200, Loss: 45.632843017578125, Variance: 2088.938720703125\n",
      "Epoch: 0, Batch: 3300, Loss: 51.53522872924805, Variance: 2278.2861328125\n",
      "Epoch: 0, Batch: 3400, Loss: 45.741825103759766, Variance: 2001.392333984375\n",
      "Epoch: 0, Batch: 3500, Loss: 45.354270935058594, Variance: 1243.97119140625\n",
      "Epoch: 0, Batch: 3600, Loss: 51.480552673339844, Variance: 3943.684814453125\n",
      "Epoch: 0, Batch: 3700, Loss: 51.06293869018555, Variance: 2272.55517578125\n",
      "Epoch: 1, Batch: 0, Loss: 53.65303421020508, Variance: 11809.431640625\n",
      "Epoch: 1, Batch: 100, Loss: 51.772972106933594, Variance: 10171.30078125\n",
      "Epoch: 1, Batch: 200, Loss: 50.70870590209961, Variance: 3274.083984375\n",
      "Epoch: 1, Batch: 300, Loss: 53.2548828125, Variance: 7930.7451171875\n",
      "Epoch: 1, Batch: 400, Loss: 54.157413482666016, Variance: 2304.943603515625\n",
      "Epoch: 1, Batch: 500, Loss: 55.48809051513672, Variance: 2703.966552734375\n",
      "Epoch: 1, Batch: 600, Loss: 45.531471252441406, Variance: 3151.342529296875\n",
      "Epoch: 1, Batch: 700, Loss: 53.130836486816406, Variance: 4328.71533203125\n",
      "Epoch: 1, Batch: 800, Loss: 41.596839904785156, Variance: 4066.669921875\n",
      "Epoch: 1, Batch: 900, Loss: 47.57976531982422, Variance: 6078.990234375\n",
      "Epoch: 1, Batch: 1000, Loss: 45.02070236206055, Variance: 4066.58642578125\n",
      "Epoch: 1, Batch: 1100, Loss: 42.71446990966797, Variance: 21711.6328125\n",
      "Epoch: 1, Batch: 1200, Loss: 51.49696350097656, Variance: 6028.75439453125\n",
      "Epoch: 1, Batch: 1300, Loss: 42.34107208251953, Variance: 3856.768310546875\n",
      "Epoch: 1, Batch: 1400, Loss: 50.835567474365234, Variance: 3173.609375\n",
      "Epoch: 1, Batch: 1500, Loss: 50.87444305419922, Variance: 1824.63037109375\n",
      "Epoch: 1, Batch: 1600, Loss: 51.003074645996094, Variance: 2385.790283203125\n",
      "Epoch: 1, Batch: 1700, Loss: 48.72993087768555, Variance: 2783.044677734375\n",
      "Epoch: 1, Batch: 1800, Loss: 54.414573669433594, Variance: 3107.677490234375\n",
      "Epoch: 1, Batch: 1900, Loss: 53.99021911621094, Variance: 5202.2080078125\n",
      "Epoch: 1, Batch: 2000, Loss: 49.47993469238281, Variance: 39019.96484375\n",
      "Epoch: 1, Batch: 2100, Loss: 51.12324142456055, Variance: 10901.6220703125\n",
      "Epoch: 1, Batch: 2200, Loss: 51.51592254638672, Variance: 8593.400390625\n",
      "Epoch: 1, Batch: 2300, Loss: 54.62562942504883, Variance: 8902.359375\n",
      "Epoch: 1, Batch: 2400, Loss: 47.0948486328125, Variance: 11690.28515625\n",
      "Epoch: 1, Batch: 2500, Loss: 48.289947509765625, Variance: 17712.431640625\n",
      "Epoch: 1, Batch: 2600, Loss: 47.72113037109375, Variance: 14071.9453125\n",
      "Epoch: 1, Batch: 2700, Loss: 47.83607864379883, Variance: 6343.63134765625\n",
      "Epoch: 1, Batch: 2800, Loss: 44.96561050415039, Variance: 7796.3369140625\n",
      "Epoch: 1, Batch: 2900, Loss: 41.52980422973633, Variance: 9521.5166015625\n",
      "Epoch: 1, Batch: 3000, Loss: 51.1795539855957, Variance: 3034.545166015625\n",
      "Epoch: 1, Batch: 3100, Loss: 49.38688659667969, Variance: 3645.21826171875\n",
      "Epoch: 1, Batch: 3200, Loss: 43.922237396240234, Variance: 7403.9365234375\n",
      "Epoch: 1, Batch: 3300, Loss: 46.44063186645508, Variance: 30456.70703125\n",
      "Epoch: 1, Batch: 3400, Loss: 47.21965408325195, Variance: 5322.3486328125\n",
      "Epoch: 1, Batch: 3500, Loss: 42.31610870361328, Variance: 17812.263671875\n",
      "Epoch: 1, Batch: 3600, Loss: 55.646751403808594, Variance: 7694.7333984375\n",
      "Epoch: 1, Batch: 3700, Loss: 45.43902587890625, Variance: 6247.6337890625\n",
      "Epoch: 2, Batch: 0, Loss: 39.01393508911133, Variance: 9997.197265625\n",
      "Epoch: 2, Batch: 100, Loss: 46.594913482666016, Variance: 4348.43896484375\n",
      "Epoch: 2, Batch: 200, Loss: 44.7685661315918, Variance: 4480.42919921875\n",
      "Epoch: 2, Batch: 300, Loss: 53.744911193847656, Variance: 3078.622802734375\n",
      "Epoch: 2, Batch: 400, Loss: 48.30543518066406, Variance: 12136.427734375\n",
      "Epoch: 2, Batch: 500, Loss: 46.04738235473633, Variance: 5337.2451171875\n",
      "Epoch: 2, Batch: 600, Loss: 46.685577392578125, Variance: 4301.74609375\n",
      "Epoch: 2, Batch: 700, Loss: 46.22732925415039, Variance: 6284.31884765625\n",
      "Epoch: 2, Batch: 800, Loss: 52.7490119934082, Variance: 2603.954833984375\n",
      "Epoch: 2, Batch: 900, Loss: 44.29228591918945, Variance: 6574.13623046875\n",
      "Epoch: 2, Batch: 1000, Loss: 54.339962005615234, Variance: 6331.90234375\n",
      "Epoch: 2, Batch: 1100, Loss: 47.166378021240234, Variance: 8421.3701171875\n",
      "Epoch: 2, Batch: 1200, Loss: 41.74897766113281, Variance: 6539.607421875\n",
      "Epoch: 2, Batch: 1300, Loss: 44.818965911865234, Variance: 5847.34375\n",
      "Epoch: 2, Batch: 1400, Loss: 41.3622932434082, Variance: 10820.9462890625\n",
      "Epoch: 2, Batch: 1500, Loss: 48.41162872314453, Variance: 2734.375732421875\n",
      "Epoch: 2, Batch: 1600, Loss: 44.14040756225586, Variance: 4131.57373046875\n",
      "Epoch: 2, Batch: 1700, Loss: 41.83790969848633, Variance: 2652.505126953125\n",
      "Epoch: 2, Batch: 1800, Loss: 43.48773193359375, Variance: 4552.947265625\n",
      "Epoch: 2, Batch: 1900, Loss: 41.75433349609375, Variance: 7196.212890625\n",
      "Epoch: 2, Batch: 2000, Loss: 41.49979019165039, Variance: 2313.618896484375\n",
      "Epoch: 2, Batch: 2100, Loss: 50.211917877197266, Variance: 4108.31787109375\n",
      "Epoch: 2, Batch: 2200, Loss: 41.7056884765625, Variance: 3875.821044921875\n",
      "Epoch: 2, Batch: 2300, Loss: 39.71772384643555, Variance: 4099.58447265625\n",
      "Epoch: 2, Batch: 2400, Loss: 48.11983871459961, Variance: 44482.0703125\n",
      "Epoch: 2, Batch: 2500, Loss: 54.481048583984375, Variance: 8012.28564453125\n",
      "Epoch: 2, Batch: 2600, Loss: 45.329551696777344, Variance: 17953.671875\n",
      "Epoch: 2, Batch: 2700, Loss: 52.12291717529297, Variance: 75911.640625\n",
      "Epoch: 2, Batch: 2800, Loss: 48.04579544067383, Variance: 3675.84765625\n",
      "Epoch: 2, Batch: 2900, Loss: 43.58169937133789, Variance: 6586.2880859375\n",
      "Epoch: 2, Batch: 3000, Loss: 46.45214080810547, Variance: 1747.521240234375\n",
      "Epoch: 2, Batch: 3100, Loss: 46.357330322265625, Variance: 3721.686279296875\n",
      "Epoch: 2, Batch: 3200, Loss: 40.53202438354492, Variance: 24451.619140625\n",
      "Epoch: 2, Batch: 3300, Loss: 44.1508674621582, Variance: 5538.16455078125\n",
      "Epoch: 2, Batch: 3400, Loss: 43.75728988647461, Variance: 5449.33056640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Batch: 3500, Loss: 41.845130920410156, Variance: 5340.4267578125\n",
      "Epoch: 2, Batch: 3600, Loss: 51.503623962402344, Variance: 4922.91845703125\n",
      "Epoch: 2, Batch: 3700, Loss: 45.78595733642578, Variance: 6115.80224609375\n",
      "Epoch: 3, Batch: 0, Loss: 46.17428207397461, Variance: 3764.007568359375\n",
      "Epoch: 3, Batch: 100, Loss: 47.73191452026367, Variance: 14665.396484375\n",
      "Epoch: 3, Batch: 200, Loss: 43.12935256958008, Variance: 5597.5595703125\n",
      "Epoch: 3, Batch: 300, Loss: 47.79056167602539, Variance: 9760.158203125\n",
      "Epoch: 3, Batch: 400, Loss: 46.635135650634766, Variance: 21501.12109375\n",
      "Epoch: 3, Batch: 500, Loss: 47.73276138305664, Variance: 9774.1220703125\n",
      "Epoch: 3, Batch: 600, Loss: 45.85759353637695, Variance: 9467.9189453125\n",
      "Epoch: 3, Batch: 700, Loss: 46.8093376159668, Variance: 6838.7412109375\n",
      "Epoch: 3, Batch: 800, Loss: 39.19867706298828, Variance: 7082.6884765625\n",
      "Epoch: 3, Batch: 900, Loss: 39.12876892089844, Variance: 30252.23046875\n",
      "Epoch: 3, Batch: 1000, Loss: 41.857208251953125, Variance: 16248.1337890625\n",
      "Epoch: 3, Batch: 1100, Loss: 45.218910217285156, Variance: 12387.65625\n",
      "Epoch: 3, Batch: 1200, Loss: 56.3449592590332, Variance: 7118.26318359375\n",
      "Epoch: 3, Batch: 1300, Loss: 42.14653396606445, Variance: 9149.33984375\n",
      "Epoch: 3, Batch: 1400, Loss: 42.1205940246582, Variance: 13878.6640625\n",
      "Epoch: 3, Batch: 1500, Loss: 42.729915618896484, Variance: 8637.5185546875\n",
      "Epoch: 3, Batch: 1600, Loss: 41.46837615966797, Variance: 12065.2236328125\n",
      "Epoch: 3, Batch: 1700, Loss: 50.85300827026367, Variance: 6007.29541015625\n",
      "Epoch: 3, Batch: 1800, Loss: 39.4958381652832, Variance: 11596.6572265625\n",
      "Epoch: 3, Batch: 1900, Loss: 45.18581771850586, Variance: 5856.81787109375\n",
      "Epoch: 3, Batch: 2000, Loss: 36.67180252075195, Variance: 7242.15869140625\n",
      "Epoch: 3, Batch: 2100, Loss: 48.13609313964844, Variance: 5538.58056640625\n",
      "Epoch: 3, Batch: 2200, Loss: 42.82103729248047, Variance: 8675.359375\n",
      "Epoch: 3, Batch: 2300, Loss: 41.56782913208008, Variance: 5427.44677734375\n",
      "Epoch: 3, Batch: 2400, Loss: 39.87864685058594, Variance: 4758.4296875\n",
      "Epoch: 3, Batch: 2500, Loss: 42.7409553527832, Variance: 5461.21435546875\n",
      "Epoch: 3, Batch: 2600, Loss: 52.039981842041016, Variance: 11794.287109375\n",
      "Epoch: 3, Batch: 2700, Loss: 44.653831481933594, Variance: 14183.5693359375\n",
      "Epoch: 3, Batch: 2800, Loss: 42.949954986572266, Variance: 26527.646484375\n",
      "Epoch: 3, Batch: 2900, Loss: 40.21285629272461, Variance: 3678.044921875\n",
      "Epoch: 3, Batch: 3000, Loss: 45.83242416381836, Variance: 8565.6962890625\n",
      "Epoch: 3, Batch: 3100, Loss: 41.342464447021484, Variance: 15044.8017578125\n",
      "Epoch: 3, Batch: 3200, Loss: 47.599090576171875, Variance: 83050.09375\n",
      "Epoch: 3, Batch: 3300, Loss: 44.16416931152344, Variance: 7400.97705078125\n",
      "Epoch: 3, Batch: 3400, Loss: 41.47443389892578, Variance: 12661.6708984375\n",
      "Epoch: 3, Batch: 3500, Loss: 52.49482727050781, Variance: 9515.9619140625\n",
      "Epoch: 3, Batch: 3600, Loss: 45.545066833496094, Variance: 47755.2734375\n",
      "Epoch: 3, Batch: 3700, Loss: 46.564109802246094, Variance: 66717.2109375\n",
      "Epoch: 4, Batch: 0, Loss: 47.872528076171875, Variance: 4687.28173828125\n",
      "Epoch: 4, Batch: 100, Loss: 40.49217987060547, Variance: 8319.05859375\n",
      "Epoch: 4, Batch: 200, Loss: 39.45719528198242, Variance: 9569.4755859375\n",
      "Epoch: 4, Batch: 300, Loss: 43.601314544677734, Variance: 40566.734375\n",
      "Epoch: 4, Batch: 400, Loss: 41.99893569946289, Variance: 7492.1533203125\n",
      "Epoch: 4, Batch: 500, Loss: 41.84941864013672, Variance: 9579.2080078125\n",
      "Epoch: 4, Batch: 600, Loss: 40.187259674072266, Variance: 10779.134765625\n",
      "Epoch: 4, Batch: 700, Loss: 47.40610122680664, Variance: 12345.92578125\n",
      "Epoch: 4, Batch: 800, Loss: 41.6658821105957, Variance: 6477.96044921875\n",
      "Epoch: 4, Batch: 900, Loss: 46.120819091796875, Variance: 13744.54296875\n",
      "Epoch: 4, Batch: 1000, Loss: 41.342041015625, Variance: 10039.0400390625\n",
      "Epoch: 4, Batch: 1100, Loss: 40.12089538574219, Variance: 24706.419921875\n",
      "Epoch: 4, Batch: 1200, Loss: 39.419368743896484, Variance: 4748.3828125\n",
      "Epoch: 4, Batch: 1300, Loss: 38.39616012573242, Variance: 5512.90380859375\n",
      "Epoch: 4, Batch: 1400, Loss: 40.139522552490234, Variance: 10736.205078125\n",
      "Epoch: 4, Batch: 1500, Loss: 43.4052848815918, Variance: 5221.640625\n",
      "Epoch: 4, Batch: 1600, Loss: 47.38878631591797, Variance: 27626.56640625\n",
      "Epoch: 4, Batch: 1700, Loss: 41.0232048034668, Variance: 15605.7490234375\n",
      "Epoch: 4, Batch: 1800, Loss: 38.901023864746094, Variance: 10832.048828125\n",
      "Epoch: 4, Batch: 1900, Loss: 39.232688903808594, Variance: 8780.349609375\n",
      "Epoch: 4, Batch: 2000, Loss: 49.99702835083008, Variance: 6265.28271484375\n",
      "Epoch: 4, Batch: 2100, Loss: 41.41423416137695, Variance: 10603.068359375\n",
      "Epoch: 4, Batch: 2200, Loss: 40.16611099243164, Variance: 6875.72119140625\n",
      "Epoch: 4, Batch: 2300, Loss: 35.99809265136719, Variance: 30051.642578125\n",
      "Epoch: 4, Batch: 2400, Loss: 36.7415771484375, Variance: 7207.7978515625\n",
      "Epoch: 4, Batch: 2500, Loss: 43.602821350097656, Variance: 6924.5283203125\n",
      "Epoch: 4, Batch: 2600, Loss: 44.96829605102539, Variance: 5357.49560546875\n",
      "Epoch: 4, Batch: 2700, Loss: 43.523319244384766, Variance: 88840.1796875\n",
      "Epoch: 4, Batch: 2800, Loss: 46.944549560546875, Variance: 31663.525390625\n",
      "Epoch: 4, Batch: 2900, Loss: 49.34909439086914, Variance: 12002.16796875\n",
      "Epoch: 4, Batch: 3000, Loss: 44.55815505981445, Variance: 9757.8994140625\n",
      "Epoch: 4, Batch: 3100, Loss: 45.78150177001953, Variance: 9946.2802734375\n",
      "Epoch: 4, Batch: 3200, Loss: 40.14574432373047, Variance: 8489.6435546875\n",
      "Epoch: 4, Batch: 3300, Loss: 49.25155258178711, Variance: 7927.78759765625\n",
      "Epoch: 4, Batch: 3400, Loss: 34.96000671386719, Variance: 12339.1748046875\n",
      "Epoch: 4, Batch: 3500, Loss: 52.650108337402344, Variance: 10559.931640625\n",
      "Epoch: 4, Batch: 3600, Loss: 46.061580657958984, Variance: 10177.3818359375\n",
      "Epoch: 4, Batch: 3700, Loss: 46.370967864990234, Variance: 30536.798828125\n",
      "Epoch: 5, Batch: 0, Loss: 35.94706344604492, Variance: 12316.537109375\n",
      "Epoch: 5, Batch: 100, Loss: 41.90086364746094, Variance: 16803.728515625\n",
      "Epoch: 5, Batch: 200, Loss: 39.453895568847656, Variance: 8243.2001953125\n",
      "Epoch: 5, Batch: 300, Loss: 36.46812438964844, Variance: 14713.845703125\n",
      "Epoch: 5, Batch: 400, Loss: 36.92039108276367, Variance: 8915.3681640625\n",
      "Epoch: 5, Batch: 500, Loss: 42.0894660949707, Variance: 7573.8359375\n",
      "Epoch: 5, Batch: 600, Loss: 37.132041931152344, Variance: 8408.560546875\n",
      "Epoch: 5, Batch: 700, Loss: 39.930816650390625, Variance: 28696.873046875\n",
      "Epoch: 5, Batch: 800, Loss: 43.20976257324219, Variance: 15040.7705078125\n",
      "Epoch: 5, Batch: 900, Loss: 36.94548797607422, Variance: 31390.05859375\n",
      "Epoch: 5, Batch: 1000, Loss: 40.468082427978516, Variance: 10278.6103515625\n",
      "Epoch: 5, Batch: 1100, Loss: 44.66392135620117, Variance: 12969.404296875\n",
      "Epoch: 5, Batch: 1200, Loss: 44.913482666015625, Variance: 12535.4501953125\n",
      "Epoch: 5, Batch: 1300, Loss: 42.19561004638672, Variance: 63416.01171875\n",
      "Epoch: 5, Batch: 1400, Loss: 41.18883514404297, Variance: 8583.52734375\n",
      "Epoch: 5, Batch: 1500, Loss: 45.5949821472168, Variance: 8941.1953125\n",
      "Epoch: 5, Batch: 1600, Loss: 46.818092346191406, Variance: 18356.798828125\n",
      "Epoch: 5, Batch: 1700, Loss: 39.5085334777832, Variance: 10149.6650390625\n",
      "Epoch: 5, Batch: 1800, Loss: 47.315975189208984, Variance: 20622.240234375\n",
      "Epoch: 5, Batch: 1900, Loss: 40.42976760864258, Variance: 14149.2294921875\n",
      "Epoch: 5, Batch: 2000, Loss: 38.81498718261719, Variance: 10243.3046875\n",
      "Epoch: 5, Batch: 2100, Loss: 44.200294494628906, Variance: 6430.59521484375\n",
      "Epoch: 5, Batch: 2200, Loss: 41.516517639160156, Variance: 23734.328125\n",
      "Epoch: 5, Batch: 2300, Loss: 46.9246826171875, Variance: 6150.49658203125\n",
      "Epoch: 5, Batch: 2400, Loss: 39.7518310546875, Variance: 18528.025390625\n",
      "Epoch: 5, Batch: 2500, Loss: 40.9860725402832, Variance: 17906.599609375\n",
      "Epoch: 5, Batch: 2600, Loss: 38.569862365722656, Variance: 16694.326171875\n",
      "Epoch: 5, Batch: 2700, Loss: 43.79335403442383, Variance: 17126.974609375\n",
      "Epoch: 5, Batch: 2800, Loss: 39.27372360229492, Variance: 55409.109375\n",
      "Epoch: 5, Batch: 2900, Loss: 38.80244064331055, Variance: 29382.404296875\n",
      "Epoch: 5, Batch: 3000, Loss: 44.59453201293945, Variance: 4078.53515625\n",
      "Epoch: 5, Batch: 3100, Loss: 40.432281494140625, Variance: 11516.505859375\n",
      "Epoch: 5, Batch: 3200, Loss: 40.159950256347656, Variance: 7792.3994140625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Batch: 3300, Loss: 34.462039947509766, Variance: 8221.2626953125\n",
      "Epoch: 5, Batch: 3400, Loss: 40.93482971191406, Variance: 8114.609375\n",
      "Epoch: 5, Batch: 3500, Loss: 42.83749771118164, Variance: 30532.6953125\n",
      "Epoch: 5, Batch: 3600, Loss: 43.540283203125, Variance: 43135.3046875\n",
      "Epoch: 5, Batch: 3700, Loss: 46.27184295654297, Variance: 6681.1279296875\n",
      "Epoch: 6, Batch: 0, Loss: 37.7484245300293, Variance: 46228.7890625\n",
      "Epoch: 6, Batch: 100, Loss: 46.45466232299805, Variance: 44198.33984375\n",
      "Epoch: 6, Batch: 200, Loss: 41.95960235595703, Variance: 17436.609375\n",
      "Epoch: 6, Batch: 300, Loss: 43.59656524658203, Variance: 84472.2421875\n",
      "Epoch: 6, Batch: 400, Loss: 43.830257415771484, Variance: 8190.99951171875\n",
      "Epoch: 6, Batch: 500, Loss: 40.59212875366211, Variance: 31837.552734375\n",
      "Epoch: 6, Batch: 600, Loss: 40.56708908081055, Variance: 8624.8916015625\n",
      "Epoch: 6, Batch: 700, Loss: 37.19648361206055, Variance: 5548.4462890625\n",
      "Epoch: 6, Batch: 800, Loss: 39.65617370605469, Variance: 26506.185546875\n",
      "Epoch: 6, Batch: 900, Loss: 45.96199417114258, Variance: 6518.64404296875\n",
      "Epoch: 6, Batch: 1000, Loss: 34.94472885131836, Variance: 66948.671875\n",
      "Epoch: 6, Batch: 1100, Loss: 42.620155334472656, Variance: 10926.611328125\n",
      "Epoch: 6, Batch: 1200, Loss: 31.878360748291016, Variance: 12791.4130859375\n",
      "Epoch: 6, Batch: 1300, Loss: 38.149803161621094, Variance: 7380.0830078125\n",
      "Epoch: 6, Batch: 1400, Loss: 46.573158264160156, Variance: 11064.3857421875\n",
      "Epoch: 6, Batch: 1500, Loss: 38.78105926513672, Variance: 53626.1875\n",
      "Epoch: 6, Batch: 1600, Loss: 40.59678268432617, Variance: 35957.3515625\n",
      "Epoch: 6, Batch: 1700, Loss: 36.77229309082031, Variance: 7683.2177734375\n",
      "Epoch: 6, Batch: 1800, Loss: 32.41745376586914, Variance: 11346.4560546875\n",
      "Epoch: 6, Batch: 1900, Loss: 35.66453552246094, Variance: 20010.603515625\n",
      "Epoch: 6, Batch: 2000, Loss: 35.74091720581055, Variance: 21808.564453125\n",
      "Epoch: 6, Batch: 2100, Loss: 43.024574279785156, Variance: 23841.814453125\n",
      "Epoch: 6, Batch: 2200, Loss: 42.768341064453125, Variance: 12408.1630859375\n",
      "Epoch: 6, Batch: 2300, Loss: 38.50708770751953, Variance: 3762.673095703125\n",
      "Epoch: 6, Batch: 2400, Loss: 43.9169807434082, Variance: 15280.177734375\n",
      "Epoch: 6, Batch: 2500, Loss: 37.838951110839844, Variance: 16512.306640625\n",
      "Epoch: 6, Batch: 2600, Loss: 37.450923919677734, Variance: 37346.98828125\n",
      "Epoch: 6, Batch: 2700, Loss: 38.488189697265625, Variance: 23782.828125\n",
      "Epoch: 6, Batch: 2800, Loss: 41.20012664794922, Variance: 21916.482421875\n",
      "Epoch: 6, Batch: 2900, Loss: 41.325096130371094, Variance: 66027.515625\n",
      "Epoch: 6, Batch: 3000, Loss: 37.24138641357422, Variance: 22705.6640625\n",
      "Epoch: 6, Batch: 3100, Loss: 39.97968292236328, Variance: 24480.9921875\n",
      "Epoch: 6, Batch: 3200, Loss: 42.17624282836914, Variance: 25114.25390625\n",
      "Epoch: 6, Batch: 3300, Loss: 34.108158111572266, Variance: 38244.95703125\n",
      "Epoch: 6, Batch: 3400, Loss: 47.0012321472168, Variance: 11790.0\n",
      "Epoch: 6, Batch: 3500, Loss: 35.95873260498047, Variance: 26478.3828125\n",
      "Epoch: 6, Batch: 3600, Loss: 38.73578643798828, Variance: 14875.7138671875\n",
      "Epoch: 6, Batch: 3700, Loss: 45.12423324584961, Variance: 13356.7197265625\n",
      "Epoch: 7, Batch: 0, Loss: 36.02009201049805, Variance: 16452.080078125\n",
      "Epoch: 7, Batch: 100, Loss: 33.75735855102539, Variance: 16673.162109375\n",
      "Epoch: 7, Batch: 200, Loss: 42.340110778808594, Variance: 12542.8134765625\n",
      "Epoch: 7, Batch: 300, Loss: 40.26668167114258, Variance: 20839.01171875\n",
      "Epoch: 7, Batch: 400, Loss: 37.023067474365234, Variance: 27364.6015625\n",
      "Epoch: 7, Batch: 500, Loss: 43.56543731689453, Variance: 11617.7607421875\n",
      "Epoch: 7, Batch: 600, Loss: 40.40863037109375, Variance: 18844.349609375\n",
      "Epoch: 7, Batch: 700, Loss: 37.829708099365234, Variance: 9643.814453125\n",
      "Epoch: 7, Batch: 800, Loss: 43.04766082763672, Variance: 15920.90625\n",
      "Epoch: 7, Batch: 900, Loss: 43.767662048339844, Variance: 8619.0751953125\n",
      "Epoch: 7, Batch: 1000, Loss: 46.639801025390625, Variance: 161888.09375\n",
      "Epoch: 7, Batch: 1100, Loss: 46.94258499145508, Variance: 56059.39453125\n",
      "Epoch: 7, Batch: 1200, Loss: 38.24010467529297, Variance: 11788.0400390625\n",
      "Epoch: 7, Batch: 1300, Loss: 36.6117057800293, Variance: 26253.990234375\n",
      "Epoch: 7, Batch: 1400, Loss: 37.532447814941406, Variance: 10451.208984375\n",
      "Epoch: 7, Batch: 1500, Loss: 42.22293472290039, Variance: 11060.9404296875\n",
      "Epoch: 7, Batch: 1600, Loss: 41.86846160888672, Variance: 36343.76171875\n",
      "Epoch: 7, Batch: 1700, Loss: 41.1040153503418, Variance: 27656.0234375\n",
      "Epoch: 7, Batch: 1800, Loss: 34.869022369384766, Variance: 28349.896484375\n",
      "Epoch: 7, Batch: 1900, Loss: 44.43321228027344, Variance: 12394.095703125\n",
      "Epoch: 7, Batch: 2000, Loss: 44.76459503173828, Variance: 11401.47265625\n",
      "Epoch: 7, Batch: 2100, Loss: 41.67873001098633, Variance: 15569.166015625\n",
      "Epoch: 7, Batch: 2200, Loss: 40.824642181396484, Variance: 9427.5908203125\n",
      "Epoch: 7, Batch: 2300, Loss: 37.77037811279297, Variance: 18335.5546875\n",
      "Epoch: 7, Batch: 2400, Loss: 39.12716293334961, Variance: 32353.6484375\n",
      "Epoch: 7, Batch: 2500, Loss: 37.05647277832031, Variance: 16894.267578125\n",
      "Epoch: 7, Batch: 2600, Loss: 36.051429748535156, Variance: 62736.53515625\n",
      "Epoch: 7, Batch: 2700, Loss: 42.73448944091797, Variance: 35363.7265625\n",
      "Epoch: 7, Batch: 2800, Loss: 40.455284118652344, Variance: 24335.0859375\n",
      "Epoch: 7, Batch: 2900, Loss: 36.1311149597168, Variance: 14573.923828125\n",
      "Epoch: 7, Batch: 3000, Loss: 43.32574462890625, Variance: 8947.2041015625\n",
      "Epoch: 7, Batch: 3100, Loss: 44.40463638305664, Variance: 15286.1611328125\n",
      "Epoch: 7, Batch: 3200, Loss: 39.1174201965332, Variance: 105773.5625\n",
      "Epoch: 7, Batch: 3300, Loss: 36.48069763183594, Variance: 12458.908203125\n",
      "Epoch: 7, Batch: 3400, Loss: 40.66120529174805, Variance: 26564.751953125\n",
      "Epoch: 7, Batch: 3500, Loss: 32.484737396240234, Variance: 16403.708984375\n",
      "Epoch: 7, Batch: 3600, Loss: 32.021488189697266, Variance: 42009.53125\n",
      "Epoch: 7, Batch: 3700, Loss: 37.657535552978516, Variance: 18831.732421875\n",
      "Epoch: 8, Batch: 0, Loss: 43.43418884277344, Variance: 39287.81640625\n",
      "Epoch: 8, Batch: 100, Loss: 36.80004119873047, Variance: 73087.2890625\n",
      "Epoch: 8, Batch: 200, Loss: 38.86223602294922, Variance: 12122.4931640625\n",
      "Epoch: 8, Batch: 300, Loss: 40.233184814453125, Variance: 103062.109375\n",
      "Epoch: 8, Batch: 400, Loss: 36.98378372192383, Variance: 23127.44921875\n",
      "Epoch: 8, Batch: 500, Loss: 40.734989166259766, Variance: 26487.24609375\n",
      "Epoch: 8, Batch: 600, Loss: 31.487918853759766, Variance: 38070.70703125\n",
      "Epoch: 8, Batch: 700, Loss: 36.73189926147461, Variance: 6281.28759765625\n",
      "Epoch: 8, Batch: 800, Loss: 42.52078628540039, Variance: 60470.87890625\n",
      "Epoch: 8, Batch: 900, Loss: 33.01350021362305, Variance: 23768.619140625\n",
      "Epoch: 8, Batch: 1000, Loss: 43.05934524536133, Variance: 14241.1416015625\n",
      "Epoch: 8, Batch: 1100, Loss: 37.9644660949707, Variance: 57692.96875\n",
      "Epoch: 8, Batch: 1200, Loss: 34.193965911865234, Variance: 10219.5322265625\n",
      "Epoch: 8, Batch: 1300, Loss: 38.77731704711914, Variance: 108282.75\n",
      "Epoch: 8, Batch: 1400, Loss: 45.17674255371094, Variance: 12779.8447265625\n",
      "Epoch: 8, Batch: 1500, Loss: 41.75392532348633, Variance: 9621.837890625\n",
      "Epoch: 8, Batch: 1600, Loss: 39.0688591003418, Variance: 32664.7265625\n",
      "Epoch: 8, Batch: 1700, Loss: 29.713895797729492, Variance: 45431.1953125\n",
      "Epoch: 8, Batch: 1800, Loss: 42.57832717895508, Variance: 40917.72265625\n",
      "Epoch: 8, Batch: 1900, Loss: 39.429718017578125, Variance: 10205.3232421875\n",
      "Epoch: 8, Batch: 2000, Loss: 38.46492004394531, Variance: 82854.796875\n",
      "Epoch: 8, Batch: 2100, Loss: 42.45762252807617, Variance: 11838.5888671875\n",
      "Epoch: 8, Batch: 2200, Loss: 36.56264114379883, Variance: 57932.45703125\n",
      "Epoch: 8, Batch: 2300, Loss: 38.137428283691406, Variance: 11186.5009765625\n",
      "Epoch: 8, Batch: 2400, Loss: 42.137325286865234, Variance: 17155.85546875\n",
      "Epoch: 8, Batch: 2500, Loss: 37.492698669433594, Variance: 28264.7421875\n",
      "Epoch: 8, Batch: 2600, Loss: 41.490806579589844, Variance: 37110.953125\n",
      "Epoch: 8, Batch: 2700, Loss: 44.137054443359375, Variance: 15975.7548828125\n",
      "Epoch: 8, Batch: 2800, Loss: 44.32184600830078, Variance: 103709.9765625\n",
      "Epoch: 8, Batch: 2900, Loss: 39.24661636352539, Variance: 40443.0625\n",
      "Epoch: 8, Batch: 3000, Loss: 43.62856674194336, Variance: 28229.98046875\n",
      "Epoch: 8, Batch: 3100, Loss: 32.932716369628906, Variance: 17678.275390625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Batch: 3200, Loss: 36.66256332397461, Variance: 74106.6171875\n",
      "Epoch: 8, Batch: 3300, Loss: 37.974395751953125, Variance: 25507.521484375\n",
      "Epoch: 8, Batch: 3400, Loss: 40.10917663574219, Variance: 11226.9541015625\n",
      "Epoch: 8, Batch: 3500, Loss: 37.86681365966797, Variance: 61479.40234375\n",
      "Epoch: 8, Batch: 3600, Loss: 33.697078704833984, Variance: 27347.599609375\n",
      "Epoch: 8, Batch: 3700, Loss: 43.85010528564453, Variance: 43441.5234375\n",
      "Epoch: 9, Batch: 0, Loss: 42.134159088134766, Variance: 40197.92578125\n",
      "Epoch: 9, Batch: 100, Loss: 32.69289016723633, Variance: 37285.796875\n",
      "Epoch: 9, Batch: 200, Loss: 35.8642692565918, Variance: 14448.7099609375\n",
      "Epoch: 9, Batch: 300, Loss: 38.35967254638672, Variance: 110047.7109375\n",
      "Epoch: 9, Batch: 400, Loss: 37.76131820678711, Variance: 11352.5830078125\n",
      "Epoch: 9, Batch: 500, Loss: 41.570919036865234, Variance: 141680.046875\n",
      "Epoch: 9, Batch: 600, Loss: 41.1958122253418, Variance: 15427.5673828125\n",
      "Epoch: 9, Batch: 700, Loss: 38.54166030883789, Variance: 52443.22265625\n",
      "Epoch: 9, Batch: 800, Loss: 39.6798210144043, Variance: 30949.056640625\n",
      "Epoch: 9, Batch: 900, Loss: 38.285213470458984, Variance: 40037.890625\n",
      "Epoch: 9, Batch: 1000, Loss: 43.97538757324219, Variance: 59191.828125\n",
      "Epoch: 9, Batch: 1100, Loss: 38.28433609008789, Variance: 33660.65234375\n",
      "Epoch: 9, Batch: 1200, Loss: 38.9666748046875, Variance: 19759.5\n",
      "Epoch: 9, Batch: 1300, Loss: 38.60619354248047, Variance: 56612.12109375\n",
      "Epoch: 9, Batch: 1400, Loss: 33.79859161376953, Variance: 19868.734375\n",
      "Epoch: 9, Batch: 1500, Loss: 35.830814361572266, Variance: 62029.6171875\n",
      "Epoch: 9, Batch: 1600, Loss: 33.13034439086914, Variance: 21535.953125\n",
      "Epoch: 9, Batch: 1700, Loss: 39.092201232910156, Variance: 24488.7578125\n",
      "Epoch: 9, Batch: 1800, Loss: 36.0852165222168, Variance: 19202.087890625\n",
      "Epoch: 9, Batch: 1900, Loss: 38.8098258972168, Variance: 31501.666015625\n",
      "Epoch: 9, Batch: 2000, Loss: 34.973541259765625, Variance: 72366.484375\n",
      "Epoch: 9, Batch: 2100, Loss: 43.675865173339844, Variance: 26913.833984375\n",
      "Epoch: 9, Batch: 2200, Loss: 38.589683532714844, Variance: 21224.849609375\n",
      "Epoch: 9, Batch: 2300, Loss: 36.76289367675781, Variance: 22140.486328125\n",
      "Epoch: 9, Batch: 2400, Loss: 31.31151580810547, Variance: 19323.732421875\n",
      "Epoch: 9, Batch: 2500, Loss: 40.809566497802734, Variance: 22982.177734375\n",
      "Epoch: 9, Batch: 2600, Loss: 38.80349349975586, Variance: 27075.541015625\n",
      "Epoch: 9, Batch: 2700, Loss: 38.82992935180664, Variance: 22433.568359375\n",
      "Epoch: 9, Batch: 2800, Loss: 43.516239166259766, Variance: 10425.861328125\n",
      "Epoch: 9, Batch: 2900, Loss: 39.450897216796875, Variance: 10506.5625\n",
      "Epoch: 9, Batch: 3000, Loss: 39.70527648925781, Variance: 28227.380859375\n",
      "Epoch: 9, Batch: 3100, Loss: 45.616249084472656, Variance: 6080.7900390625\n",
      "Epoch: 9, Batch: 3200, Loss: 31.494394302368164, Variance: 61249.8203125\n",
      "Epoch: 9, Batch: 3300, Loss: 42.90850067138672, Variance: 13273.2939453125\n",
      "Epoch: 9, Batch: 3400, Loss: 42.24296569824219, Variance: 18811.8984375\n",
      "Epoch: 9, Batch: 3500, Loss: 35.42280960083008, Variance: 32002.7734375\n",
      "Epoch: 9, Batch: 3600, Loss: 41.746620178222656, Variance: 103824.296875\n",
      "Epoch: 9, Batch: 3700, Loss: 39.96739959716797, Variance: 40679.6015625\n"
     ]
    }
   ],
   "source": [
    "for e in range(10):\n",
    "    training_loop(e,train_iter,seq2context,context2trg,seq2context_optimizer,context2trg_optimizer,BATCH_SIZE)\n",
    "    scheduler_c2t.step()\n",
    "    scheduler_s2c.step()\n",
    "    #validation_loop(e,val_iter,seq2context,context2trg,seq2context_sch,context2trg_sch,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Well', ',', 'a', 'of', 'to', 'this', 'with', 'this', 'problem', 'is', '<unk>', '<unk>', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'Now', ',', 'one', 'way', 'of', 'dealing', 'with', 'this', 'problem', 'is', 'by', '<unk>', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "for ix,batch in enumerate(train_iter):\n",
    "    src = batch.src.values.transpose(0,1)\n",
    "    src = reverse_sequence(src)\n",
    "    trg = batch.trg.values.transpose(0,1)\n",
    "    break\n",
    "\n",
    "h0=None\n",
    "context, hidden_s2c = seq2context(src,h0)\n",
    "output, hidden_lm = context2trg(trg[:,:-1],hidden_s2c)\n",
    "\n",
    "\n",
    "print([EN.vocab.itos[i] for i in torch.argmax(lsm(output),2)[3,:]])\n",
    "print([EN.vocab.itos[i] for i in trg[3,:]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test set\n",
    "sentences = []\n",
    "for i, l in enumerate(open(\"source_test.txt\"), 1):\n",
    "  sentences.append(re.split(' ', l))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
