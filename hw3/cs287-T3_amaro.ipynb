{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 287, Homework 3: Neural Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "torch.__version__\n",
    "from common import *\n",
    "## Setup\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#!pip install --upgrade pip\n",
    "#!pip install -q numpy\n",
    "\n",
    "#!pip install -q torch torchtext spacy opt_einsum\n",
    "#!pip install -qU git+https://github.com/harvardnlp/namedtensor\n",
    "#!python -m spacy download en\n",
    "#!python -m spacy download de\n",
    "\n",
    "# Torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "# Text text processing library and methods for pretrained word embeddings\n",
    "from torchtext import data, datasets\n",
    "# Named Tensor wrappers\n",
    "from namedtensor import ntorch, NamedTensor\n",
    "from namedtensor.text import NamedField\n",
    "import numpy as np\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': <namedtensor.text.torch_text.NamedField object at 0x7f0210038b38>, 'trg': <namedtensor.text.torch_text.NamedField object at 0x7f02097ef5c0>}\n",
      "119076\n",
      "{'src': ['David', 'Gallo', ':', 'Das', 'ist', 'Bill', 'Lange', '.', 'Ich', 'bin', 'Dave', 'Gallo', '.'], 'trg': ['David', 'Gallo', ':', 'This', 'is', 'Bill', 'Lange', '.', 'I', \"'m\", 'Dave', 'Gallo', '.']}\n",
      "[('.', 113253), (',', 67237), ('ist', 24189), ('die', 23778), ('das', 17102), ('der', 15727), ('und', 15622), ('Sie', 15085), ('es', 13197), ('ich', 12946)]\n",
      "Size of German vocab 13353\n",
      "[('.', 113433), (',', 59512), ('the', 46029), ('to', 29177), ('a', 27548), ('of', 26794), ('I', 24887), ('is', 21775), (\"'s\", 20630), ('that', 19814)]\n",
      "Size of English vocab 11560\n",
      "2 3\n",
      "1 0\n"
     ]
    }
   ],
   "source": [
    "# split raw data into tokens\n",
    "import spacy\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# add beginning-of-sentence and end-of-sentence tokens to target\n",
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "DE = NamedField(names=('srcSeqlen',), tokenize=tokenize_de)\n",
    "EN = NamedField(names=('trgSeqlen',), tokenize=tokenize_en,\n",
    "                init_token = BOS_WORD, eos_token = EOS_WORD) # only target needs BOS/EOS\n",
    "\n",
    "# download dataset of 200K pairs of sentences\n",
    "# start with MAXLEN = 20\n",
    "MAX_LEN = 20\n",
    "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(DE, EN), \n",
    "                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
    "print(train.fields)\n",
    "print(len(train))\n",
    "print(vars(train[0]))\n",
    "\n",
    "# WHAT DOES THIS DO?\n",
    "'''src = open(\"valid.src\", \"w\")\n",
    "trg = open(\"valid.trg\", \"w\")\n",
    "for example in val:\n",
    "    print(\" \".join(example.src), file=src)\n",
    "    print(\" \".join(example.trg), file=trg)\n",
    "src.close()\n",
    "trg.close()'''\n",
    "\n",
    "# build vocab, convert words to indices\n",
    "MIN_FREQ = 5\n",
    "DE.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "EN.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
    "print(DE.vocab.freqs.most_common(10))\n",
    "print(\"Size of German vocab\", len(DE.vocab))\n",
    "print(EN.vocab.freqs.most_common(10))\n",
    "print(\"Size of English vocab\", len(EN.vocab))\n",
    "print(EN.vocab.stoi[\"<s>\"], EN.vocab.stoi[\"</s>\"])\n",
    "\n",
    "print(EN.vocab.stoi[\"<pad>\"], EN.vocab.stoi[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into batches\n",
    "BATCH_SIZE = 32\n",
    "device = torch.device('cuda:0')\n",
    "train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=BATCH_SIZE, device=device,\n",
    "                                                  repeat=False, sort_key=lambda x: len(x.src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to Sequence Learning with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- English to French translation, $p \\left( y_1, \\dots, y_{T'} \\ | \\ x_1, \\dots, x_T \\right) = \\prod_{t = 1}^{T'} p \\left( y_t \\ | \\ v, y_1, \\dots, y_{t-1} \\right)$\n",
    "- Each sentence ends in '<EOS\\>', out-of-vocab words denoted '<UNK\\>'\n",
    "- Model specs: \n",
    "    * Input vocabulary of 160,000 and output vocabulary of 80,000\n",
    "    * Deep LSTM to map (encode) input sequence to fixed-len vector\n",
    "    * Another deep LSTM to translate (decode) fixed-len vector to output sequence\n",
    "    * 4 layers per LSTM, 1000 cells per layer, 1000-dimensional word embeddings, softmax over 80,000 words\n",
    "    * Reversing order of words in source (but not target) improved performance\n",
    "        * Each word in the source is far from its corresponding word in the target (large minimal time lag); reversing the source reduces the minimal time lag, thereby allowing backprop to establish communication between source and target more easily\n",
    "- Training specs:\n",
    "    * Initialize all LSTM params $\\sim Unif[-0.08,0.08]$\n",
    "    * SGD w/o momentum, lr = 0.7\n",
    "        * After 5 epochs, halve the lr every half-epoch\n",
    "        * Train for 7.5 epochs\n",
    "    * Batch size = 128; divide gradient by batch size (denoted $g$)\n",
    "    * Hard constraint gradient norm; if $s = ||g||_2 > 5$, set $s = 5$\n",
    "    * Make sure all sentences within a minibatch are roughly the same length\n",
    "- Objective: $max \\frac{1}{|S|} \\sum_{(T,S) \\in \\mathcal{S}} log \\ p(T \\ | \\ S)$, where $\\mathcal{S}$ is the training set\n",
    "- Prediction: $\\hat{T} = argmax \\ p(T \\ | \\ S)$ via beam search, where beam size $B \\in {1,2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 500\n",
    "num_layers = 2\n",
    "attn_context2trg = attn_RNNet_batched(input_size=len(EN.vocab),hidden_size=context_size,num_layers=num_layers)\n",
    "attn_context2trg = attn_context2trg.cuda()\n",
    "attn_context2trg_optimizer = torch.optim.Adam(attn_context2trg.parameters(), lr=1e-3)\n",
    "\n",
    "seq2context = SequenceModel(len(DE.vocab),context_size,num_layers=num_layers)\n",
    "seq2context_optimizer = torch.optim.Adam(seq2context.parameters(), lr=1e-3)\n",
    "seq2context = seq2context.cuda()\n",
    "\n",
    "\n",
    "\n",
    "scheduler_c2t = torch.optim.lr_scheduler.ReduceLROnPlateau(attn_context2trg_optimizer, mode=\"min\", patience=4)\n",
    "scheduler_s2c = torch.optim.lr_scheduler.ReduceLROnPlateau(seq2context_optimizer, mode=\"min\", patience=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss: 196.80746459960938\n",
      "Epoch: 0, Batch: 500, Loss: 57.9072151184082\n",
      "Epoch: 0, Batch: 1000, Loss: 59.319637298583984\n",
      "Epoch: 0, Batch: 1500, Loss: 46.97551345825195\n",
      "Epoch: 0, Batch: 2000, Loss: 52.368019104003906\n",
      "Epoch: 0, Batch: 2500, Loss: 42.604759216308594\n",
      "Epoch: 0, Batch: 3000, Loss: 46.63115692138672\n",
      "Epoch: 0, Batch: 3500, Loss: 36.59870147705078\n",
      "Epoch: 0, Validation PPL: 7.792147636413574\n",
      "Wrote model!\n",
      "Epoch: 1, Batch: 0, Loss: 37.57067108154297\n",
      "Epoch: 1, Batch: 500, Loss: 41.15217971801758\n",
      "Epoch: 1, Batch: 1000, Loss: 38.78003692626953\n",
      "Epoch: 1, Batch: 1500, Loss: 37.293521881103516\n",
      "Epoch: 1, Batch: 2000, Loss: 37.09842300415039\n",
      "Epoch: 1, Batch: 2500, Loss: 40.06744384765625\n",
      "Epoch: 1, Batch: 3000, Loss: 40.234317779541016\n",
      "Epoch: 1, Batch: 3500, Loss: 41.856075286865234\n",
      "Epoch: 1, Validation PPL: 5.6893157958984375\n",
      "Wrote model!\n",
      "Epoch: 2, Batch: 0, Loss: 37.55695343017578\n",
      "Epoch: 2, Batch: 500, Loss: 37.20197677612305\n",
      "Epoch: 2, Batch: 1000, Loss: 38.441993713378906\n",
      "Epoch: 2, Batch: 1500, Loss: 30.511547088623047\n",
      "Epoch: 2, Batch: 2000, Loss: 31.77158546447754\n",
      "Epoch: 2, Batch: 2500, Loss: 34.8766975402832\n",
      "Epoch: 2, Batch: 3000, Loss: 31.02699089050293\n",
      "Epoch: 2, Batch: 3500, Loss: 32.78199768066406\n",
      "Epoch: 2, Validation PPL: 5.0230488777160645\n",
      "Wrote model!\n",
      "Epoch: 3, Batch: 0, Loss: 29.95269012451172\n",
      "Epoch: 3, Batch: 500, Loss: 30.613174438476562\n",
      "Epoch: 3, Batch: 1000, Loss: 33.06015396118164\n",
      "Epoch: 3, Batch: 1500, Loss: 32.293853759765625\n"
     ]
    }
   ],
   "source": [
    "best_ppl = 1e8\n",
    "for e in range(0,300):\n",
    "    attn_training_loop(e,train_iter,seq2context,attn_context2trg,seq2context_optimizer,attn_context2trg_optimizer)\n",
    "    ppl = attn_validation_loop(e,val_iter,seq2context,attn_context2trg,scheduler_c2t,scheduler_s2c,BATCH_SIZE=32,context_size=500)\n",
    "    if ppl < best_ppl:\n",
    "        torch.save(seq2context.state_dict(),'best_seq2seq_withattn_seq2context.pt')\n",
    "        torch.save(attn_context2trg.state_dict(),'best_seq2seq_withattn_context2trg.pt')\n",
    "        best_ppl = ppl\n",
    "        print('Wrote model!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65, Validation PPL: 4.2662811279296875\n"
     ]
    }
   ],
   "source": [
    "ppl = attn_validation_loop(e,val_iter,seq2context,attn_context2trg,scheduler_c2t,scheduler_s2c,BATCH_SIZE=32,context_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss: 196.8906707763672\n",
      "Epoch: 0, Batch: 500, Loss: 70.20809936523438\n",
      "Epoch: 0, Batch: 1000, Loss: 48.192955017089844\n",
      "Epoch: 0, Batch: 1500, Loss: 52.265480041503906\n",
      "Epoch: 0, Batch: 2000, Loss: 57.046905517578125\n",
      "Epoch: 0, Batch: 2500, Loss: 69.36911010742188\n",
      "Epoch: 0, Batch: 3000, Loss: 68.65656280517578\n",
      "Epoch: 0, Batch: 3500, Loss: 50.92197799682617\n",
      "Epoch: 0, Validation PPL: 11.415091514587402\n",
      "Wrote model!\n",
      "Epoch: 0, Batch: 0, Loss: 64.52406311035156\n",
      "Epoch: 0, Batch: 500, Loss: 57.46894836425781\n",
      "Epoch: 0, Batch: 1000, Loss: 55.048831939697266\n",
      "Epoch: 0, Batch: 1500, Loss: 43.473289489746094\n",
      "Epoch: 0, Batch: 2000, Loss: 58.344329833984375\n",
      "Epoch: 0, Batch: 2500, Loss: 43.607215881347656\n",
      "Epoch: 0, Batch: 3000, Loss: 65.991943359375\n",
      "Epoch: 0, Batch: 3500, Loss: 44.39083480834961\n",
      "Epoch: 1, Validation PPL: 8.097634315490723\n",
      "Wrote model!\n",
      "Epoch: 0, Batch: 0, Loss: 60.247406005859375\n",
      "Epoch: 0, Batch: 500, Loss: 53.51862335205078\n",
      "Epoch: 0, Batch: 1000, Loss: 45.77241516113281\n",
      "Epoch: 0, Batch: 1500, Loss: 64.95542907714844\n",
      "Epoch: 0, Batch: 2000, Loss: 40.78840255737305\n",
      "Epoch: 0, Batch: 2500, Loss: 50.1089973449707\n",
      "Epoch: 0, Batch: 3000, Loss: 39.145904541015625\n",
      "Epoch: 0, Batch: 3500, Loss: 61.59918975830078\n",
      "Epoch: 2, Validation PPL: 6.9877166748046875\n",
      "Wrote model!\n",
      "Epoch: 0, Batch: 0, Loss: 51.65488052368164\n",
      "Epoch: 0, Batch: 500, Loss: 48.934932708740234\n",
      "Epoch: 0, Batch: 1000, Loss: 42.90108108520508\n",
      "Epoch: 0, Batch: 1500, Loss: 36.47041320800781\n",
      "Epoch: 0, Batch: 2000, Loss: 38.53069305419922\n",
      "Epoch: 0, Batch: 2500, Loss: 50.944705963134766\n",
      "Epoch: 0, Batch: 3000, Loss: 48.99170684814453\n",
      "Epoch: 0, Batch: 3500, Loss: 53.969017028808594\n",
      "Epoch: 3, Validation PPL: 6.055956840515137\n",
      "Wrote model!\n",
      "Epoch: 0, Batch: 0, Loss: 60.88578796386719\n",
      "Epoch: 0, Batch: 500, Loss: 49.42599105834961\n",
      "Epoch: 0, Batch: 1000, Loss: 35.68867492675781\n",
      "Epoch: 0, Batch: 1500, Loss: 55.52373504638672\n",
      "Epoch: 0, Batch: 2000, Loss: 37.513397216796875\n",
      "Epoch: 0, Batch: 2500, Loss: 50.307437896728516\n",
      "Epoch: 0, Batch: 3000, Loss: 40.905792236328125\n",
      "Epoch: 0, Batch: 3500, Loss: 33.423126220703125\n",
      "Epoch: 4, Validation PPL: 5.746737003326416\n",
      "Wrote model!\n",
      "Epoch: 0, Batch: 0, Loss: 37.308189392089844\n",
      "Epoch: 0, Batch: 500, Loss: 36.91538619995117\n",
      "Epoch: 0, Batch: 1000, Loss: 36.18728256225586\n",
      "Epoch: 0, Batch: 1500, Loss: 39.944400787353516\n",
      "Epoch: 0, Batch: 2000, Loss: 48.39226531982422\n",
      "Epoch: 0, Batch: 2500, Loss: 58.128387451171875\n",
      "Epoch: 0, Batch: 3000, Loss: 55.74615478515625\n",
      "Epoch: 0, Batch: 3500, Loss: 33.8151969909668\n",
      "Epoch: 5, Validation PPL: 5.649074554443359\n",
      "Wrote model!\n",
      "Epoch: 0, Batch: 0, Loss: 44.23356628417969\n",
      "Epoch: 0, Batch: 500, Loss: 47.40760803222656\n",
      "Epoch: 0, Batch: 1000, Loss: 56.99021530151367\n",
      "Epoch: 0, Batch: 1500, Loss: 51.6064567565918\n",
      "Epoch: 0, Batch: 2000, Loss: 40.15235900878906\n",
      "Epoch: 0, Batch: 2500, Loss: 35.016170501708984\n",
      "Epoch: 0, Batch: 3000, Loss: 40.1732177734375\n",
      "Epoch: 0, Batch: 3500, Loss: 36.142295837402344\n",
      "Epoch: 6, Validation PPL: 5.480185508728027\n",
      "Wrote model!\n",
      "Epoch: 0, Batch: 0, Loss: 48.110809326171875\n",
      "Epoch: 0, Batch: 500, Loss: 49.41824722290039\n",
      "Epoch: 0, Batch: 1000, Loss: 33.04994201660156\n",
      "Epoch: 0, Batch: 1500, Loss: 52.832122802734375\n",
      "Epoch: 0, Batch: 2000, Loss: 33.821231842041016\n",
      "Epoch: 0, Batch: 2500, Loss: 52.74068832397461\n",
      "Epoch: 0, Batch: 3000, Loss: 33.66392517089844\n",
      "Epoch: 0, Batch: 3500, Loss: 51.5684814453125\n",
      "Epoch: 7, Validation PPL: 5.547820568084717\n",
      "Epoch: 0, Batch: 0, Loss: 47.06859588623047\n",
      "Epoch: 0, Batch: 500, Loss: 45.73291015625\n",
      "Epoch: 0, Batch: 1000, Loss: 30.587989807128906\n",
      "Epoch: 0, Batch: 1500, Loss: 44.00434875488281\n",
      "Epoch: 0, Batch: 2000, Loss: 50.32050323486328\n",
      "Epoch: 0, Batch: 2500, Loss: 28.59806251525879\n",
      "Epoch: 0, Batch: 3000, Loss: 32.46773147583008\n",
      "Epoch: 0, Batch: 3500, Loss: 49.131492614746094\n",
      "Epoch: 8, Validation PPL: 5.195768356323242\n",
      "Wrote model!\n",
      "Epoch: 0, Batch: 0, Loss: 47.75983428955078\n",
      "Epoch: 0, Batch: 500, Loss: 48.044498443603516\n",
      "Epoch: 0, Batch: 1000, Loss: 29.987545013427734\n",
      "Epoch: 0, Batch: 1500, Loss: 32.31887435913086\n",
      "Epoch: 0, Batch: 2000, Loss: 31.54817008972168\n",
      "Epoch: 0, Batch: 2500, Loss: 50.424434661865234\n",
      "Epoch: 0, Batch: 3000, Loss: 49.94459915161133\n",
      "Epoch: 0, Batch: 3500, Loss: 47.967567443847656\n",
      "Epoch: 9, Validation PPL: 5.560110092163086\n",
      "Epoch: 0, Batch: 0, Loss: 46.85457992553711\n",
      "Epoch: 0, Batch: 500, Loss: 44.2094612121582\n",
      "Epoch: 0, Batch: 1000, Loss: 57.97005081176758\n",
      "Epoch: 0, Batch: 1500, Loss: 29.221012115478516\n",
      "Epoch: 0, Batch: 2000, Loss: 31.505434036254883\n",
      "Epoch: 0, Batch: 2500, Loss: 31.321441650390625\n",
      "Epoch: 0, Batch: 3000, Loss: 27.455257415771484\n",
      "Epoch: 0, Batch: 3500, Loss: 37.29399871826172\n",
      "Epoch: 10, Validation PPL: 5.5279951095581055\n",
      "Epoch: 0, Batch: 0, Loss: 31.39138412475586\n",
      "Epoch: 0, Batch: 500, Loss: 29.183332443237305\n",
      "Epoch: 0, Batch: 1000, Loss: 50.35512924194336\n",
      "Epoch: 0, Batch: 1500, Loss: 38.345401763916016\n",
      "Epoch: 0, Batch: 2000, Loss: 29.770387649536133\n",
      "Epoch: 0, Batch: 2500, Loss: 56.83700180053711\n",
      "Epoch: 0, Batch: 3000, Loss: 31.647212982177734\n",
      "Epoch: 0, Batch: 3500, Loss: 48.04624938964844\n",
      "Epoch: 11, Validation PPL: 5.482419967651367\n",
      "Epoch: 0, Batch: 0, Loss: 43.44047927856445\n",
      "Epoch: 0, Batch: 500, Loss: 57.20026779174805\n",
      "Epoch: 0, Batch: 1000, Loss: 25.34554672241211\n",
      "Epoch: 0, Batch: 1500, Loss: 48.62223815917969\n",
      "Epoch: 0, Batch: 2000, Loss: 30.20469093322754\n",
      "Epoch: 0, Batch: 2500, Loss: 51.79816818237305\n",
      "Epoch: 0, Batch: 3000, Loss: 38.87702941894531\n",
      "Epoch: 0, Batch: 3500, Loss: 36.270423889160156\n",
      "Epoch: 12, Validation PPL: 5.491426944732666\n",
      "Epoch: 0, Batch: 0, Loss: 36.19016647338867\n",
      "Epoch: 0, Batch: 500, Loss: 27.718692779541016\n",
      "Epoch: 0, Batch: 1000, Loss: 30.856491088867188\n",
      "Epoch: 0, Batch: 1500, Loss: 45.888797760009766\n",
      "Epoch: 0, Batch: 2000, Loss: 51.63340377807617\n",
      "Epoch: 0, Batch: 2500, Loss: 35.646514892578125\n",
      "Epoch: 0, Batch: 3000, Loss: 53.701751708984375\n",
      "Epoch: 0, Batch: 3500, Loss: 34.802825927734375\n",
      "Epoch: 13, Validation PPL: 5.340119361877441\n",
      "Epoch: 0, Batch: 0, Loss: 28.98910903930664\n",
      "Epoch: 0, Batch: 500, Loss: 42.26517105102539\n",
      "Epoch: 0, Batch: 1000, Loss: 28.362255096435547\n",
      "Epoch: 0, Batch: 1500, Loss: 33.71427536010742\n",
      "Epoch: 0, Batch: 2000, Loss: 44.514835357666016\n",
      "Epoch: 0, Batch: 2500, Loss: 31.481651306152344\n",
      "Epoch: 0, Batch: 3000, Loss: 30.74703025817871\n",
      "Epoch: 0, Batch: 3500, Loss: 41.49483871459961\n",
      "Epoch: 14, Validation PPL: 5.293216228485107\n",
      "Epoch: 0, Batch: 0, Loss: 26.202939987182617\n",
      "Epoch: 0, Batch: 500, Loss: 24.78863525390625\n",
      "Epoch: 0, Batch: 1000, Loss: 29.25370979309082\n",
      "Epoch: 0, Batch: 1500, Loss: 28.606306076049805\n",
      "Epoch: 0, Batch: 2000, Loss: 32.986324310302734\n",
      "Epoch: 0, Batch: 2500, Loss: 27.92534065246582\n",
      "Epoch: 0, Batch: 3000, Loss: 24.633718490600586\n",
      "Epoch: 0, Batch: 3500, Loss: 27.799823760986328\n",
      "Epoch: 15, Validation PPL: 5.335819244384766\n",
      "Epoch: 0, Batch: 0, Loss: 25.604097366333008\n",
      "Epoch: 0, Batch: 500, Loss: 25.50118064880371\n",
      "Epoch: 0, Batch: 1000, Loss: 43.069942474365234\n",
      "Epoch: 0, Batch: 1500, Loss: 41.66365432739258\n",
      "Epoch: 0, Batch: 2000, Loss: 21.123477935791016\n",
      "Epoch: 0, Batch: 2500, Loss: 26.144948959350586\n",
      "Epoch: 0, Batch: 3000, Loss: 26.066701889038086\n",
      "Epoch: 0, Batch: 3500, Loss: 36.873531341552734\n",
      "Epoch: 16, Validation PPL: 5.361150741577148\n",
      "Epoch: 0, Batch: 0, Loss: 23.10564422607422\n",
      "Epoch: 0, Batch: 500, Loss: 24.478179931640625\n",
      "Epoch: 0, Batch: 1000, Loss: 38.47454833984375\n",
      "Epoch: 0, Batch: 1500, Loss: 28.794347763061523\n",
      "Epoch: 0, Batch: 2000, Loss: 27.284381866455078\n",
      "Epoch: 0, Batch: 2500, Loss: 38.42444610595703\n",
      "Epoch: 0, Batch: 3000, Loss: 22.919416427612305\n",
      "Epoch: 0, Batch: 3500, Loss: 26.499025344848633\n",
      "Epoch: 17, Validation PPL: 5.384551048278809\n",
      "Epoch: 0, Batch: 0, Loss: 46.23332595825195\n",
      "Epoch: 0, Batch: 500, Loss: 25.507753372192383\n",
      "Epoch: 0, Batch: 1000, Loss: 26.181936264038086\n",
      "Epoch: 0, Batch: 1500, Loss: 28.952180862426758\n",
      "Epoch: 0, Batch: 2000, Loss: 30.042749404907227\n",
      "Epoch: 0, Batch: 2500, Loss: 29.59521484375\n",
      "Epoch: 0, Batch: 3000, Loss: 29.591318130493164\n",
      "Epoch: 0, Batch: 3500, Loss: 41.921714782714844\n",
      "Epoch: 18, Validation PPL: 5.395196914672852\n",
      "Epoch: 0, Batch: 0, Loss: 44.671871185302734\n",
      "Epoch: 0, Batch: 500, Loss: 25.63466453552246\n",
      "Epoch: 0, Batch: 1000, Loss: 25.645198822021484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 1500, Loss: 22.260120391845703\n",
      "Epoch: 0, Batch: 2000, Loss: 24.752552032470703\n",
      "Epoch: 0, Batch: 2500, Loss: 37.87808609008789\n",
      "Epoch: 0, Batch: 3000, Loss: 43.870948791503906\n",
      "Epoch: 0, Batch: 3500, Loss: 29.658802032470703\n",
      "Epoch: 19, Validation PPL: 5.444913387298584\n",
      "Epoch: 0, Batch: 0, Loss: 38.40983581542969\n",
      "Epoch: 0, Batch: 500, Loss: 28.45709991455078\n",
      "Epoch: 0, Batch: 1000, Loss: 41.88131332397461\n",
      "Epoch: 0, Batch: 1500, Loss: 36.70486068725586\n",
      "Epoch: 0, Batch: 2000, Loss: 40.18235397338867\n",
      "Epoch: 0, Batch: 2500, Loss: 41.98257064819336\n",
      "Epoch: 0, Batch: 3000, Loss: 18.77935028076172\n",
      "Epoch: 0, Batch: 3500, Loss: 49.40693664550781\n",
      "Epoch: 20, Validation PPL: 5.447747230529785\n",
      "Epoch: 0, Batch: 0, Loss: 37.98321533203125\n",
      "Epoch: 0, Batch: 500, Loss: 39.4747200012207\n",
      "Epoch: 0, Batch: 1000, Loss: 29.31151008605957\n",
      "Epoch: 0, Batch: 1500, Loss: 39.950992584228516\n",
      "Epoch: 0, Batch: 2000, Loss: 23.07223129272461\n",
      "Epoch: 0, Batch: 2500, Loss: 26.125091552734375\n",
      "Epoch: 0, Batch: 3000, Loss: 39.31962585449219\n",
      "Epoch: 0, Batch: 3500, Loss: 28.705650329589844\n",
      "Epoch: 21, Validation PPL: 5.43501091003418\n",
      "Epoch: 0, Batch: 0, Loss: 25.49047088623047\n",
      "Epoch: 0, Batch: 500, Loss: 29.106508255004883\n",
      "Epoch: 0, Batch: 1000, Loss: 22.901016235351562\n",
      "Epoch: 0, Batch: 1500, Loss: 26.53655433654785\n",
      "Epoch: 0, Batch: 2000, Loss: 23.10887908935547\n",
      "Epoch: 0, Batch: 2500, Loss: 40.67189407348633\n",
      "Epoch: 0, Batch: 3000, Loss: 41.92310333251953\n",
      "Epoch: 0, Batch: 3500, Loss: 39.98645782470703\n",
      "Epoch: 22, Validation PPL: 5.437745094299316\n",
      "Epoch: 0, Batch: 0, Loss: 40.17764663696289\n",
      "Epoch: 0, Batch: 500, Loss: 24.94008445739746\n",
      "Epoch: 0, Batch: 1000, Loss: 43.73881149291992\n",
      "Epoch: 0, Batch: 1500, Loss: 41.80390167236328\n",
      "Epoch: 0, Batch: 2000, Loss: 27.780900955200195\n",
      "Epoch: 0, Batch: 2500, Loss: 25.228469848632812\n",
      "Epoch: 0, Batch: 3000, Loss: 43.84678268432617\n",
      "Epoch: 0, Batch: 3500, Loss: 40.366615295410156\n",
      "Epoch: 23, Validation PPL: 5.455326080322266\n",
      "Epoch: 0, Batch: 0, Loss: 25.989824295043945\n",
      "Epoch: 0, Batch: 500, Loss: 28.307113647460938\n",
      "Epoch: 0, Batch: 1000, Loss: 30.451156616210938\n",
      "Epoch: 0, Batch: 1500, Loss: 34.48673629760742\n",
      "Epoch: 0, Batch: 2000, Loss: 39.65629959106445\n",
      "Epoch: 0, Batch: 2500, Loss: 38.09529495239258\n",
      "Epoch: 0, Batch: 3000, Loss: 30.323665618896484\n",
      "Epoch: 0, Batch: 3500, Loss: 24.510648727416992\n",
      "Epoch: 24, Validation PPL: 5.455541133880615\n",
      "Epoch: 0, Batch: 0, Loss: 38.02555847167969\n",
      "Epoch: 0, Batch: 500, Loss: 36.82048797607422\n",
      "Epoch: 0, Batch: 1000, Loss: 25.461549758911133\n",
      "Epoch: 0, Batch: 1500, Loss: 30.065332412719727\n",
      "Epoch: 0, Batch: 2000, Loss: 26.71562385559082\n",
      "Epoch: 0, Batch: 2500, Loss: 19.433679580688477\n",
      "Epoch: 0, Batch: 3000, Loss: 21.18026351928711\n",
      "Epoch: 0, Batch: 3500, Loss: 31.77029037475586\n",
      "Epoch: 25, Validation PPL: 5.458606243133545\n",
      "Epoch: 0, Batch: 0, Loss: 34.24116516113281\n",
      "Epoch: 0, Batch: 500, Loss: 34.13676071166992\n",
      "Epoch: 0, Batch: 1000, Loss: 27.420730590820312\n",
      "Epoch: 0, Batch: 1500, Loss: 40.934417724609375\n",
      "Epoch: 0, Batch: 2000, Loss: 23.80303955078125\n",
      "Epoch: 0, Batch: 2500, Loss: 41.91179275512695\n",
      "Epoch: 0, Batch: 3000, Loss: 25.40736198425293\n",
      "Epoch: 0, Batch: 3500, Loss: 41.61838150024414\n",
      "Epoch: 26, Validation PPL: 5.463318347930908\n",
      "Epoch: 0, Batch: 0, Loss: 44.23674011230469\n",
      "Epoch: 0, Batch: 500, Loss: 26.125402450561523\n",
      "Epoch: 0, Batch: 1000, Loss: 24.40719985961914\n",
      "Epoch: 0, Batch: 1500, Loss: 23.684106826782227\n",
      "Epoch: 0, Batch: 2000, Loss: 28.20976448059082\n",
      "Epoch: 0, Batch: 2500, Loss: 44.615379333496094\n",
      "Epoch: 0, Batch: 3000, Loss: 40.388816833496094\n",
      "Epoch: 0, Batch: 3500, Loss: 26.27161407470703\n",
      "Epoch: 27, Validation PPL: 5.46722936630249\n",
      "Epoch: 0, Batch: 0, Loss: 28.906124114990234\n",
      "Epoch: 0, Batch: 500, Loss: 43.05009841918945\n",
      "Epoch: 0, Batch: 1000, Loss: 26.266616821289062\n",
      "Epoch: 0, Batch: 1500, Loss: 23.8148193359375\n",
      "Epoch: 0, Batch: 2000, Loss: 42.19816970825195\n",
      "Epoch: 0, Batch: 2500, Loss: 34.352386474609375\n",
      "Epoch: 0, Batch: 3000, Loss: 47.51388931274414\n",
      "Epoch: 0, Batch: 3500, Loss: 39.938297271728516\n",
      "Epoch: 28, Validation PPL: 5.4617838859558105\n",
      "Epoch: 0, Batch: 0, Loss: 39.509910583496094\n",
      "Epoch: 0, Batch: 500, Loss: 26.059585571289062\n",
      "Epoch: 0, Batch: 1000, Loss: 45.37926483154297\n",
      "Epoch: 0, Batch: 1500, Loss: 34.15260314941406\n",
      "Epoch: 0, Batch: 2000, Loss: 32.68597412109375\n",
      "Epoch: 0, Batch: 2500, Loss: 44.96611404418945\n",
      "Epoch: 0, Batch: 3000, Loss: 27.18592643737793\n",
      "Epoch: 0, Batch: 3500, Loss: 40.4086799621582\n",
      "Epoch: 29, Validation PPL: 5.461540699005127\n",
      "Epoch: 0, Batch: 0, Loss: 22.436983108520508\n",
      "Epoch: 0, Batch: 500, Loss: 36.87043380737305\n",
      "Epoch: 0, Batch: 1000, Loss: 38.1435432434082\n",
      "Epoch: 0, Batch: 1500, Loss: 29.771202087402344\n",
      "Epoch: 0, Batch: 2000, Loss: 26.360971450805664\n",
      "Epoch: 0, Batch: 2500, Loss: 28.670705795288086\n",
      "Epoch: 0, Batch: 3000, Loss: 26.002172470092773\n",
      "Epoch: 0, Batch: 3500, Loss: 37.68183517456055\n",
      "Epoch: 30, Validation PPL: 5.462336540222168\n",
      "Epoch: 0, Batch: 0, Loss: 37.145015716552734\n",
      "Epoch: 0, Batch: 500, Loss: 21.72811508178711\n",
      "Epoch: 0, Batch: 1000, Loss: 41.89754867553711\n",
      "Epoch: 0, Batch: 1500, Loss: 39.95174026489258\n",
      "Epoch: 0, Batch: 2000, Loss: 44.71851348876953\n",
      "Epoch: 0, Batch: 2500, Loss: 34.227577209472656\n",
      "Epoch: 0, Batch: 3000, Loss: 41.00843048095703\n",
      "Epoch: 0, Batch: 3500, Loss: 21.773237228393555\n",
      "Epoch: 31, Validation PPL: 5.462704658508301\n",
      "Epoch: 0, Batch: 0, Loss: 44.03977584838867\n",
      "Epoch: 0, Batch: 500, Loss: 46.61874771118164\n",
      "Epoch: 0, Batch: 1000, Loss: 28.08095932006836\n",
      "Epoch: 0, Batch: 1500, Loss: 39.789207458496094\n",
      "Epoch: 0, Batch: 2000, Loss: 27.602426528930664\n",
      "Epoch: 0, Batch: 2500, Loss: 38.20984649658203\n",
      "Epoch: 0, Batch: 3000, Loss: 24.73709487915039\n",
      "Epoch: 0, Batch: 3500, Loss: 44.380943298339844\n",
      "Epoch: 32, Validation PPL: 5.46309757232666\n",
      "Epoch: 0, Batch: 0, Loss: 38.66727066040039\n",
      "Epoch: 0, Batch: 500, Loss: 45.6864128112793\n",
      "Epoch: 0, Batch: 1000, Loss: 50.76747131347656\n",
      "Epoch: 0, Batch: 1500, Loss: 36.357994079589844\n",
      "Epoch: 0, Batch: 2000, Loss: 30.920034408569336\n",
      "Epoch: 0, Batch: 2500, Loss: 45.374637603759766\n",
      "Epoch: 0, Batch: 3000, Loss: 23.81057357788086\n",
      "Epoch: 0, Batch: 3500, Loss: 37.97137451171875\n",
      "Epoch: 33, Validation PPL: 5.463202953338623\n",
      "Epoch: 0, Batch: 0, Loss: 24.35947036743164\n",
      "Epoch: 0, Batch: 500, Loss: 24.86388397216797\n",
      "Epoch: 0, Batch: 1000, Loss: 32.587440490722656\n",
      "Epoch: 0, Batch: 1500, Loss: 27.972454071044922\n",
      "Epoch: 0, Batch: 2000, Loss: 49.796993255615234\n",
      "Epoch: 0, Batch: 2500, Loss: 21.894758224487305\n",
      "Epoch: 0, Batch: 3000, Loss: 29.058704376220703\n",
      "Epoch: 0, Batch: 3500, Loss: 24.66490364074707\n",
      "Epoch: 34, Validation PPL: 5.463212966918945\n",
      "Epoch: 0, Batch: 0, Loss: 43.493839263916016\n",
      "Epoch: 0, Batch: 500, Loss: 25.009963989257812\n",
      "Epoch: 0, Batch: 1000, Loss: 36.38782501220703\n",
      "Epoch: 0, Batch: 1500, Loss: 26.164743423461914\n",
      "Epoch: 0, Batch: 2000, Loss: 24.637489318847656\n",
      "Epoch: 0, Batch: 2500, Loss: 39.29090118408203\n",
      "Epoch: 0, Batch: 3000, Loss: 43.37724685668945\n",
      "Epoch: 0, Batch: 3500, Loss: 22.825443267822266\n",
      "Epoch: 35, Validation PPL: 5.463243007659912\n",
      "Epoch: 0, Batch: 0, Loss: 42.576847076416016\n",
      "Epoch: 0, Batch: 500, Loss: 27.00568389892578\n",
      "Epoch: 0, Batch: 1000, Loss: 23.133323669433594\n",
      "Epoch: 0, Batch: 1500, Loss: 33.638248443603516\n",
      "Epoch: 0, Batch: 2000, Loss: 31.335309982299805\n",
      "Epoch: 0, Batch: 2500, Loss: 36.15333557128906\n",
      "Epoch: 0, Batch: 3000, Loss: 40.9510498046875\n",
      "Epoch: 0, Batch: 3500, Loss: 29.936725616455078\n",
      "Epoch: 36, Validation PPL: 5.463272571563721\n",
      "Epoch: 0, Batch: 0, Loss: 22.840110778808594\n",
      "Epoch: 0, Batch: 500, Loss: 31.456676483154297\n",
      "Epoch: 0, Batch: 1000, Loss: 22.262075424194336\n",
      "Epoch: 0, Batch: 1500, Loss: 26.560993194580078\n",
      "Epoch: 0, Batch: 2000, Loss: 36.145668029785156\n",
      "Epoch: 0, Batch: 2500, Loss: 38.730770111083984\n",
      "Epoch: 0, Batch: 3000, Loss: 46.931663513183594\n",
      "Epoch: 0, Batch: 3500, Loss: 24.10410499572754\n",
      "Epoch: 37, Validation PPL: 5.463267803192139\n",
      "Epoch: 0, Batch: 0, Loss: 23.485002517700195\n",
      "Epoch: 0, Batch: 500, Loss: 29.078554153442383\n",
      "Epoch: 0, Batch: 1000, Loss: 24.374483108520508\n",
      "Epoch: 0, Batch: 1500, Loss: 37.89214324951172\n",
      "Epoch: 0, Batch: 2000, Loss: 29.77190399169922\n",
      "Epoch: 0, Batch: 2500, Loss: 24.68239402770996\n",
      "Epoch: 0, Batch: 3000, Loss: 25.69554328918457\n",
      "Epoch: 0, Batch: 3500, Loss: 23.118324279785156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38, Validation PPL: 5.463284492492676\n",
      "Epoch: 0, Batch: 0, Loss: 38.857749938964844\n",
      "Epoch: 0, Batch: 500, Loss: 41.44780731201172\n",
      "Epoch: 0, Batch: 1000, Loss: 23.618331909179688\n",
      "Epoch: 0, Batch: 1500, Loss: 28.234294891357422\n",
      "Epoch: 0, Batch: 2000, Loss: 42.85017013549805\n",
      "Epoch: 0, Batch: 2500, Loss: 26.964160919189453\n",
      "Epoch: 0, Batch: 3000, Loss: 43.33356857299805\n",
      "Epoch: 0, Batch: 3500, Loss: 39.65460968017578\n",
      "Epoch: 39, Validation PPL: 5.463293552398682\n",
      "Epoch: 0, Batch: 0, Loss: 36.394622802734375\n",
      "Epoch: 0, Batch: 500, Loss: 39.83744812011719\n",
      "Epoch: 0, Batch: 1000, Loss: 20.397340774536133\n",
      "Epoch: 0, Batch: 1500, Loss: 28.511592864990234\n",
      "Epoch: 0, Batch: 2000, Loss: 24.351829528808594\n",
      "Epoch: 0, Batch: 2500, Loss: 27.15172004699707\n",
      "Epoch: 0, Batch: 3000, Loss: 26.600584030151367\n",
      "Epoch: 0, Batch: 3500, Loss: 22.412168502807617\n",
      "Epoch: 40, Validation PPL: 5.463318347930908\n",
      "Epoch: 0, Batch: 0, Loss: 31.601530075073242\n",
      "Epoch: 0, Batch: 500, Loss: 23.55372428894043\n",
      "Epoch: 0, Batch: 1000, Loss: 41.339717864990234\n",
      "Epoch: 0, Batch: 1500, Loss: 39.5629997253418\n",
      "Epoch: 0, Batch: 2000, Loss: 26.456214904785156\n",
      "Epoch: 0, Batch: 2500, Loss: 43.838478088378906\n",
      "Epoch: 0, Batch: 3000, Loss: 43.010231018066406\n",
      "Epoch: 0, Batch: 3500, Loss: 25.76289939880371\n",
      "Epoch: 41, Validation PPL: 5.463335037231445\n",
      "Epoch: 0, Batch: 0, Loss: 40.89044952392578\n",
      "Epoch: 0, Batch: 500, Loss: 39.497100830078125\n",
      "Epoch: 0, Batch: 1000, Loss: 41.46205520629883\n",
      "Epoch: 0, Batch: 1500, Loss: 41.50319290161133\n",
      "Epoch: 0, Batch: 2000, Loss: 21.257713317871094\n",
      "Epoch: 0, Batch: 2500, Loss: 23.48798942565918\n",
      "Epoch: 0, Batch: 3000, Loss: 19.308822631835938\n",
      "Epoch: 0, Batch: 3500, Loss: 37.068359375\n",
      "Epoch: 42, Validation PPL: 5.463346004486084\n",
      "Epoch: 0, Batch: 0, Loss: 29.438859939575195\n",
      "Epoch: 0, Batch: 500, Loss: 35.42985916137695\n",
      "Epoch: 0, Batch: 1000, Loss: 31.06113624572754\n",
      "Epoch: 0, Batch: 1500, Loss: 34.38545608520508\n",
      "Epoch: 0, Batch: 2000, Loss: 39.1568717956543\n",
      "Epoch: 0, Batch: 2500, Loss: 38.543731689453125\n",
      "Epoch: 0, Batch: 3000, Loss: 46.832523345947266\n",
      "Epoch: 0, Batch: 3500, Loss: 21.09733009338379\n",
      "Epoch: 43, Validation PPL: 5.463352680206299\n",
      "Epoch: 0, Batch: 0, Loss: 20.09976577758789\n",
      "Epoch: 0, Batch: 500, Loss: 39.53473663330078\n",
      "Epoch: 0, Batch: 1000, Loss: 25.189546585083008\n",
      "Epoch: 0, Batch: 1500, Loss: 38.652305603027344\n",
      "Epoch: 0, Batch: 2000, Loss: 24.052433013916016\n",
      "Epoch: 0, Batch: 2500, Loss: 36.51961898803711\n",
      "Epoch: 0, Batch: 3000, Loss: 42.22458267211914\n",
      "Epoch: 0, Batch: 3500, Loss: 39.43278503417969\n",
      "Epoch: 44, Validation PPL: 5.463322639465332\n",
      "Epoch: 0, Batch: 0, Loss: 28.084779739379883\n",
      "Epoch: 0, Batch: 500, Loss: 25.771259307861328\n",
      "Epoch: 0, Batch: 1000, Loss: 24.65994644165039\n",
      "Epoch: 0, Batch: 1500, Loss: 34.01266860961914\n",
      "Epoch: 0, Batch: 2000, Loss: 35.83202362060547\n",
      "Epoch: 0, Batch: 2500, Loss: 42.77189254760742\n",
      "Epoch: 0, Batch: 3000, Loss: 36.59375762939453\n",
      "Epoch: 0, Batch: 3500, Loss: 36.760128021240234\n",
      "Epoch: 45, Validation PPL: 5.463354587554932\n",
      "Epoch: 0, Batch: 0, Loss: 25.459823608398438\n",
      "Epoch: 0, Batch: 500, Loss: 27.190793991088867\n",
      "Epoch: 0, Batch: 1000, Loss: 38.28506851196289\n",
      "Epoch: 0, Batch: 1500, Loss: 40.10899353027344\n",
      "Epoch: 0, Batch: 2000, Loss: 27.79833221435547\n",
      "Epoch: 0, Batch: 2500, Loss: 29.63818359375\n",
      "Epoch: 0, Batch: 3000, Loss: 30.289730072021484\n",
      "Epoch: 0, Batch: 3500, Loss: 25.815853118896484\n",
      "Epoch: 46, Validation PPL: 5.4633612632751465\n",
      "Epoch: 0, Batch: 0, Loss: 47.143577575683594\n",
      "Epoch: 0, Batch: 500, Loss: 40.27661895751953\n",
      "Epoch: 0, Batch: 1000, Loss: 41.0725212097168\n",
      "Epoch: 0, Batch: 1500, Loss: 29.16234588623047\n",
      "Epoch: 0, Batch: 2000, Loss: 28.796689987182617\n",
      "Epoch: 0, Batch: 2500, Loss: 42.515228271484375\n",
      "Epoch: 0, Batch: 3000, Loss: 47.335548400878906\n",
      "Epoch: 0, Batch: 3500, Loss: 32.08511734008789\n",
      "Epoch: 47, Validation PPL: 5.46334981918335\n",
      "Epoch: 0, Batch: 0, Loss: 25.30601692199707\n",
      "Epoch: 0, Batch: 500, Loss: 41.374122619628906\n",
      "Epoch: 0, Batch: 1000, Loss: 35.669334411621094\n",
      "Epoch: 0, Batch: 1500, Loss: 27.52730941772461\n",
      "Epoch: 0, Batch: 2000, Loss: 25.850950241088867\n",
      "Epoch: 0, Batch: 2500, Loss: 36.28866958618164\n",
      "Epoch: 0, Batch: 3000, Loss: 28.003604888916016\n",
      "Epoch: 0, Batch: 3500, Loss: 30.66299819946289\n",
      "Epoch: 48, Validation PPL: 5.463333606719971\n",
      "Epoch: 0, Batch: 0, Loss: 26.931468963623047\n",
      "Epoch: 0, Batch: 500, Loss: 24.68482780456543\n",
      "Epoch: 0, Batch: 1000, Loss: 29.885047912597656\n",
      "Epoch: 0, Batch: 1500, Loss: 36.01984786987305\n",
      "Epoch: 0, Batch: 2000, Loss: 38.59145736694336\n",
      "Epoch: 0, Batch: 2500, Loss: 24.104021072387695\n",
      "Epoch: 0, Batch: 3000, Loss: 39.845462799072266\n",
      "Epoch: 0, Batch: 3500, Loss: 20.778907775878906\n",
      "Epoch: 49, Validation PPL: 5.463363170623779\n",
      "Epoch: 0, Batch: 0, Loss: 27.839561462402344\n",
      "Epoch: 0, Batch: 500, Loss: 48.53353500366211\n",
      "Epoch: 0, Batch: 1000, Loss: 42.98484420776367\n",
      "Epoch: 0, Batch: 1500, Loss: 29.622039794921875\n",
      "Epoch: 0, Batch: 2000, Loss: 36.68681716918945\n",
      "Epoch: 0, Batch: 2500, Loss: 42.1932487487793\n",
      "Epoch: 0, Batch: 3000, Loss: 29.382570266723633\n",
      "Epoch: 0, Batch: 3500, Loss: 30.13600730895996\n",
      "Epoch: 50, Validation PPL: 5.463399887084961\n",
      "Epoch: 0, Batch: 0, Loss: 38.50436782836914\n",
      "Epoch: 0, Batch: 500, Loss: 43.770809173583984\n",
      "Epoch: 0, Batch: 1000, Loss: 49.296844482421875\n",
      "Epoch: 0, Batch: 1500, Loss: 23.600522994995117\n",
      "Epoch: 0, Batch: 2000, Loss: 24.203943252563477\n",
      "Epoch: 0, Batch: 2500, Loss: 29.227800369262695\n",
      "Epoch: 0, Batch: 3000, Loss: 38.462196350097656\n",
      "Epoch: 0, Batch: 3500, Loss: 35.77137756347656\n",
      "Epoch: 51, Validation PPL: 5.46340274810791\n",
      "Epoch: 0, Batch: 0, Loss: 41.08622360229492\n",
      "Epoch: 0, Batch: 500, Loss: 37.2867431640625\n",
      "Epoch: 0, Batch: 1000, Loss: 41.80751037597656\n",
      "Epoch: 0, Batch: 1500, Loss: 35.396724700927734\n",
      "Epoch: 0, Batch: 2000, Loss: 27.5843448638916\n",
      "Epoch: 0, Batch: 2500, Loss: 40.24790573120117\n",
      "Epoch: 0, Batch: 3000, Loss: 37.126583099365234\n",
      "Epoch: 0, Batch: 3500, Loss: 27.543190002441406\n",
      "Epoch: 52, Validation PPL: 5.463406085968018\n",
      "Epoch: 0, Batch: 0, Loss: 39.23899459838867\n",
      "Epoch: 0, Batch: 500, Loss: 46.61020278930664\n",
      "Epoch: 0, Batch: 1000, Loss: 32.66045379638672\n",
      "Epoch: 0, Batch: 1500, Loss: 35.81412124633789\n",
      "Epoch: 0, Batch: 2000, Loss: 39.6004524230957\n",
      "Epoch: 0, Batch: 2500, Loss: 23.998435974121094\n",
      "Epoch: 0, Batch: 3000, Loss: 30.21870231628418\n",
      "Epoch: 0, Batch: 3500, Loss: 27.81843376159668\n",
      "Epoch: 53, Validation PPL: 5.463438034057617\n",
      "Epoch: 0, Batch: 0, Loss: 40.125030517578125\n",
      "Epoch: 0, Batch: 500, Loss: 25.847440719604492\n",
      "Epoch: 0, Batch: 1000, Loss: 35.152645111083984\n",
      "Epoch: 0, Batch: 1500, Loss: 25.563980102539062\n",
      "Epoch: 0, Batch: 2000, Loss: 23.977624893188477\n",
      "Epoch: 0, Batch: 2500, Loss: 41.990299224853516\n",
      "Epoch: 0, Batch: 3000, Loss: 42.053070068359375\n",
      "Epoch: 0, Batch: 3500, Loss: 39.808712005615234\n",
      "Epoch: 54, Validation PPL: 5.46344518661499\n",
      "Epoch: 0, Batch: 0, Loss: 22.532604217529297\n",
      "Epoch: 0, Batch: 500, Loss: 37.82854461669922\n",
      "Epoch: 0, Batch: 1000, Loss: 25.997276306152344\n",
      "Epoch: 0, Batch: 1500, Loss: 23.066062927246094\n",
      "Epoch: 0, Batch: 2000, Loss: 22.29521942138672\n",
      "Epoch: 0, Batch: 2500, Loss: 41.29061508178711\n",
      "Epoch: 0, Batch: 3000, Loss: 43.474876403808594\n",
      "Epoch: 0, Batch: 3500, Loss: 21.633769989013672\n",
      "Epoch: 55, Validation PPL: 5.463466644287109\n",
      "Epoch: 0, Batch: 0, Loss: 41.512474060058594\n",
      "Epoch: 0, Batch: 500, Loss: 24.262720108032227\n",
      "Epoch: 0, Batch: 1000, Loss: 26.922199249267578\n",
      "Epoch: 0, Batch: 1500, Loss: 31.83880615234375\n",
      "Epoch: 0, Batch: 2000, Loss: 42.46782302856445\n",
      "Epoch: 0, Batch: 2500, Loss: 24.36735725402832\n",
      "Epoch: 0, Batch: 3000, Loss: 20.758451461791992\n",
      "Epoch: 0, Batch: 3500, Loss: 38.99039840698242\n",
      "Epoch: 56, Validation PPL: 5.463476657867432\n",
      "Epoch: 0, Batch: 0, Loss: 41.85801696777344\n",
      "Epoch: 0, Batch: 500, Loss: 26.345722198486328\n",
      "Epoch: 0, Batch: 1000, Loss: 21.4550724029541\n",
      "Epoch: 0, Batch: 1500, Loss: 45.7081298828125\n",
      "Epoch: 0, Batch: 2000, Loss: 28.902406692504883\n",
      "Epoch: 0, Batch: 2500, Loss: 33.88090133666992\n",
      "Epoch: 0, Batch: 3000, Loss: 40.301979064941406\n",
      "Epoch: 0, Batch: 3500, Loss: 40.855384826660156\n",
      "Epoch: 57, Validation PPL: 5.463489532470703\n",
      "Epoch: 0, Batch: 0, Loss: 25.785728454589844\n",
      "Epoch: 0, Batch: 500, Loss: 31.483009338378906\n",
      "Epoch: 0, Batch: 1000, Loss: 40.40235137939453\n",
      "Epoch: 0, Batch: 1500, Loss: 23.26797866821289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 2000, Loss: 42.66079330444336\n",
      "Epoch: 0, Batch: 2500, Loss: 36.82075881958008\n",
      "Epoch: 0, Batch: 3000, Loss: 26.00669288635254\n",
      "Epoch: 0, Batch: 3500, Loss: 45.179630279541016\n",
      "Epoch: 58, Validation PPL: 5.463516712188721\n",
      "Epoch: 0, Batch: 0, Loss: 41.242591857910156\n",
      "Epoch: 0, Batch: 500, Loss: 24.52005386352539\n",
      "Epoch: 0, Batch: 1000, Loss: 38.66619873046875\n",
      "Epoch: 0, Batch: 1500, Loss: 39.9073600769043\n",
      "Epoch: 0, Batch: 2000, Loss: 38.79574203491211\n",
      "Epoch: 0, Batch: 2500, Loss: 26.69204330444336\n",
      "Epoch: 0, Batch: 3000, Loss: 40.750770568847656\n",
      "Epoch: 0, Batch: 3500, Loss: 22.576255798339844\n",
      "Epoch: 59, Validation PPL: 5.463543891906738\n",
      "Epoch: 0, Batch: 0, Loss: 34.08090591430664\n",
      "Epoch: 0, Batch: 500, Loss: 41.23442459106445\n",
      "Epoch: 0, Batch: 1000, Loss: 45.269752502441406\n",
      "Epoch: 0, Batch: 1500, Loss: 35.664302825927734\n",
      "Epoch: 0, Batch: 2000, Loss: 42.96604919433594\n",
      "Epoch: 0, Batch: 2500, Loss: 34.56273651123047\n",
      "Epoch: 0, Batch: 3000, Loss: 37.08980178833008\n",
      "Epoch: 0, Batch: 3500, Loss: 37.883399963378906\n",
      "Epoch: 60, Validation PPL: 5.463570594787598\n",
      "Epoch: 0, Batch: 0, Loss: 22.10856056213379\n",
      "Epoch: 0, Batch: 500, Loss: 44.599525451660156\n",
      "Epoch: 0, Batch: 1000, Loss: 31.91617202758789\n",
      "Epoch: 0, Batch: 1500, Loss: 42.21768569946289\n",
      "Epoch: 0, Batch: 2000, Loss: 32.64331817626953\n",
      "Epoch: 0, Batch: 2500, Loss: 27.045093536376953\n",
      "Epoch: 0, Batch: 3000, Loss: 42.654354095458984\n",
      "Epoch: 0, Batch: 3500, Loss: 22.66849708557129\n",
      "Epoch: 61, Validation PPL: 5.463604927062988\n",
      "Epoch: 0, Batch: 0, Loss: 24.859046936035156\n",
      "Epoch: 0, Batch: 500, Loss: 28.05219841003418\n",
      "Epoch: 0, Batch: 1000, Loss: 41.83393478393555\n",
      "Epoch: 0, Batch: 1500, Loss: 25.601911544799805\n",
      "Epoch: 0, Batch: 2000, Loss: 18.667850494384766\n",
      "Epoch: 0, Batch: 2500, Loss: 30.817590713500977\n",
      "Epoch: 0, Batch: 3000, Loss: 37.93891525268555\n",
      "Epoch: 0, Batch: 3500, Loss: 37.356773376464844\n",
      "Epoch: 62, Validation PPL: 5.463626384735107\n",
      "Epoch: 0, Batch: 0, Loss: 27.17316436767578\n",
      "Epoch: 0, Batch: 500, Loss: 27.009061813354492\n",
      "Epoch: 0, Batch: 1000, Loss: 28.120403289794922\n",
      "Epoch: 0, Batch: 1500, Loss: 35.98760986328125\n",
      "Epoch: 0, Batch: 2000, Loss: 36.36164093017578\n",
      "Epoch: 0, Batch: 2500, Loss: 38.06586837768555\n",
      "Epoch: 0, Batch: 3000, Loss: 35.81535720825195\n",
      "Epoch: 0, Batch: 3500, Loss: 27.625015258789062\n",
      "Epoch: 63, Validation PPL: 5.463659286499023\n",
      "Epoch: 0, Batch: 0, Loss: 30.136234283447266\n",
      "Epoch: 0, Batch: 500, Loss: 47.163719177246094\n",
      "Epoch: 0, Batch: 1000, Loss: 28.053146362304688\n",
      "Epoch: 0, Batch: 1500, Loss: 27.443214416503906\n",
      "Epoch: 0, Batch: 2000, Loss: 41.38021469116211\n",
      "Epoch: 0, Batch: 2500, Loss: 26.417240142822266\n",
      "Epoch: 0, Batch: 3000, Loss: 27.062700271606445\n",
      "Epoch: 0, Batch: 3500, Loss: 29.10280418395996\n",
      "Epoch: 64, Validation PPL: 5.463691234588623\n",
      "Epoch: 0, Batch: 0, Loss: 40.678131103515625\n",
      "Epoch: 0, Batch: 500, Loss: 27.58761215209961\n",
      "Epoch: 0, Batch: 1000, Loss: 37.96023941040039\n",
      "Epoch: 0, Batch: 1500, Loss: 21.071672439575195\n",
      "Epoch: 0, Batch: 2000, Loss: 43.422935485839844\n",
      "Epoch: 0, Batch: 2500, Loss: 35.905128479003906\n",
      "Epoch: 0, Batch: 3000, Loss: 42.0064811706543\n",
      "Epoch: 0, Batch: 3500, Loss: 42.66653060913086\n",
      "Epoch: 65, Validation PPL: 5.463703155517578\n",
      "Epoch: 0, Batch: 0, Loss: 38.415706634521484\n",
      "Epoch: 0, Batch: 500, Loss: 26.515445709228516\n",
      "Epoch: 0, Batch: 1000, Loss: 42.75297546386719\n",
      "Epoch: 0, Batch: 1500, Loss: 39.26210021972656\n",
      "Epoch: 0, Batch: 2000, Loss: 37.312591552734375\n",
      "Epoch: 0, Batch: 2500, Loss: 23.66061019897461\n",
      "Epoch: 0, Batch: 3000, Loss: 34.96062469482422\n",
      "Epoch: 0, Batch: 3500, Loss: 35.489837646484375\n",
      "Epoch: 66, Validation PPL: 5.463713645935059\n",
      "Epoch: 0, Batch: 0, Loss: 29.587711334228516\n",
      "Epoch: 0, Batch: 500, Loss: 41.20331954956055\n",
      "Epoch: 0, Batch: 1000, Loss: 27.42961311340332\n",
      "Epoch: 0, Batch: 1500, Loss: 29.159351348876953\n",
      "Epoch: 0, Batch: 2000, Loss: 26.089345932006836\n",
      "Epoch: 0, Batch: 2500, Loss: 28.450883865356445\n",
      "Epoch: 0, Batch: 3000, Loss: 42.37646484375\n",
      "Epoch: 0, Batch: 3500, Loss: 40.55624008178711\n",
      "Epoch: 67, Validation PPL: 5.463728427886963\n",
      "Epoch: 0, Batch: 0, Loss: 33.49095916748047\n",
      "Epoch: 0, Batch: 500, Loss: 30.34620475769043\n",
      "Epoch: 0, Batch: 1000, Loss: 22.34071159362793\n",
      "Epoch: 0, Batch: 1500, Loss: 27.41547393798828\n",
      "Epoch: 0, Batch: 2000, Loss: 46.2616081237793\n",
      "Epoch: 0, Batch: 2500, Loss: 23.736120223999023\n",
      "Epoch: 0, Batch: 3000, Loss: 34.149253845214844\n",
      "Epoch: 0, Batch: 3500, Loss: 24.617631912231445\n",
      "Epoch: 68, Validation PPL: 5.463756561279297\n",
      "Epoch: 0, Batch: 0, Loss: 27.092496871948242\n",
      "Epoch: 0, Batch: 500, Loss: 30.10155487060547\n",
      "Epoch: 0, Batch: 1000, Loss: 39.15087890625\n",
      "Epoch: 0, Batch: 1500, Loss: 39.94501495361328\n",
      "Epoch: 0, Batch: 2000, Loss: 28.25604820251465\n",
      "Epoch: 0, Batch: 2500, Loss: 30.80508804321289\n",
      "Epoch: 0, Batch: 3000, Loss: 38.253578186035156\n",
      "Epoch: 0, Batch: 3500, Loss: 40.03915023803711\n",
      "Epoch: 69, Validation PPL: 5.463741779327393\n",
      "Epoch: 0, Batch: 0, Loss: 26.55036735534668\n",
      "Epoch: 0, Batch: 500, Loss: 44.791412353515625\n",
      "Epoch: 0, Batch: 1000, Loss: 40.224552154541016\n",
      "Epoch: 0, Batch: 1500, Loss: 22.707441329956055\n",
      "Epoch: 0, Batch: 2000, Loss: 31.17091178894043\n",
      "Epoch: 0, Batch: 2500, Loss: 27.26939582824707\n",
      "Epoch: 0, Batch: 3000, Loss: 30.07524299621582\n",
      "Epoch: 0, Batch: 3500, Loss: 38.964691162109375\n",
      "Epoch: 70, Validation PPL: 5.463788032531738\n",
      "Epoch: 0, Batch: 0, Loss: 29.44137954711914\n",
      "Epoch: 0, Batch: 500, Loss: 27.969392776489258\n",
      "Epoch: 0, Batch: 1000, Loss: 34.3516845703125\n",
      "Epoch: 0, Batch: 1500, Loss: 41.657936096191406\n",
      "Epoch: 0, Batch: 2000, Loss: 40.94245147705078\n",
      "Epoch: 0, Batch: 2500, Loss: 28.023296356201172\n",
      "Epoch: 0, Batch: 3000, Loss: 39.506134033203125\n",
      "Epoch: 0, Batch: 3500, Loss: 24.783052444458008\n",
      "Epoch: 71, Validation PPL: 5.463778018951416\n",
      "Epoch: 0, Batch: 0, Loss: 25.201215744018555\n",
      "Epoch: 0, Batch: 500, Loss: 40.594974517822266\n",
      "Epoch: 0, Batch: 1000, Loss: 27.05188751220703\n",
      "Epoch: 0, Batch: 1500, Loss: 27.20442008972168\n",
      "Epoch: 0, Batch: 2000, Loss: 44.909584045410156\n",
      "Epoch: 0, Batch: 2500, Loss: 31.313413619995117\n",
      "Epoch: 0, Batch: 3000, Loss: 24.867338180541992\n",
      "Epoch: 0, Batch: 3500, Loss: 39.85042953491211\n",
      "Epoch: 72, Validation PPL: 5.463786602020264\n",
      "Epoch: 0, Batch: 0, Loss: 28.825410842895508\n",
      "Epoch: 0, Batch: 500, Loss: 42.77014923095703\n",
      "Epoch: 0, Batch: 1000, Loss: 44.84235763549805\n",
      "Epoch: 0, Batch: 1500, Loss: 37.24525451660156\n",
      "Epoch: 0, Batch: 2000, Loss: 28.390464782714844\n",
      "Epoch: 0, Batch: 2500, Loss: 43.218238830566406\n",
      "Epoch: 0, Batch: 3000, Loss: 24.860950469970703\n",
      "Epoch: 0, Batch: 3500, Loss: 23.601715087890625\n",
      "Epoch: 73, Validation PPL: 5.4637885093688965\n",
      "Epoch: 0, Batch: 0, Loss: 28.170612335205078\n",
      "Epoch: 0, Batch: 500, Loss: 43.53426742553711\n",
      "Epoch: 0, Batch: 1000, Loss: 39.05916213989258\n",
      "Epoch: 0, Batch: 1500, Loss: 26.566925048828125\n",
      "Epoch: 0, Batch: 2000, Loss: 25.330595016479492\n",
      "Epoch: 0, Batch: 2500, Loss: 48.34255599975586\n",
      "Epoch: 0, Batch: 3000, Loss: 45.386234283447266\n",
      "Epoch: 0, Batch: 3500, Loss: 34.531185150146484\n",
      "Epoch: 74, Validation PPL: 5.463760852813721\n",
      "Epoch: 0, Batch: 0, Loss: 44.077049255371094\n",
      "Epoch: 0, Batch: 500, Loss: 39.805908203125\n",
      "Epoch: 0, Batch: 1000, Loss: 42.58369827270508\n",
      "Epoch: 0, Batch: 1500, Loss: 25.176254272460938\n",
      "Epoch: 0, Batch: 2000, Loss: 38.41010284423828\n",
      "Epoch: 0, Batch: 2500, Loss: 39.45850372314453\n",
      "Epoch: 0, Batch: 3000, Loss: 26.69733238220215\n",
      "Epoch: 0, Batch: 3500, Loss: 23.070051193237305\n",
      "Epoch: 75, Validation PPL: 5.463781356811523\n",
      "Epoch: 0, Batch: 0, Loss: 28.65076446533203\n",
      "Epoch: 0, Batch: 500, Loss: 37.782005310058594\n",
      "Epoch: 0, Batch: 1000, Loss: 25.945354461669922\n",
      "Epoch: 0, Batch: 1500, Loss: 24.683509826660156\n",
      "Epoch: 0, Batch: 2000, Loss: 43.435035705566406\n",
      "Epoch: 0, Batch: 2500, Loss: 27.704557418823242\n",
      "Epoch: 0, Batch: 3000, Loss: 37.97724533081055\n",
      "Epoch: 0, Batch: 3500, Loss: 48.7112922668457\n",
      "Epoch: 76, Validation PPL: 5.4637885093688965\n",
      "Epoch: 0, Batch: 0, Loss: 32.10626220703125\n",
      "Epoch: 0, Batch: 500, Loss: 39.54302215576172\n",
      "Epoch: 0, Batch: 1000, Loss: 28.680461883544922\n",
      "Epoch: 0, Batch: 1500, Loss: 27.94325828552246\n",
      "Epoch: 0, Batch: 2000, Loss: 22.05001449584961\n",
      "Epoch: 0, Batch: 2500, Loss: 23.986621856689453\n",
      "Epoch: 0, Batch: 3000, Loss: 26.869653701782227\n",
      "Epoch: 0, Batch: 3500, Loss: 24.173175811767578\n",
      "Epoch: 77, Validation PPL: 5.46379280090332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss: 37.66118621826172\n",
      "Epoch: 0, Batch: 500, Loss: 45.74074935913086\n",
      "Epoch: 0, Batch: 1000, Loss: 35.84641647338867\n",
      "Epoch: 0, Batch: 1500, Loss: 22.201576232910156\n",
      "Epoch: 0, Batch: 2000, Loss: 36.429080963134766\n",
      "Epoch: 0, Batch: 2500, Loss: 23.765439987182617\n",
      "Epoch: 0, Batch: 3000, Loss: 33.25324630737305\n",
      "Epoch: 0, Batch: 3500, Loss: 24.549882888793945\n",
      "Epoch: 78, Validation PPL: 5.4638166427612305\n",
      "Epoch: 0, Batch: 0, Loss: 25.900331497192383\n",
      "Epoch: 0, Batch: 500, Loss: 26.18463897705078\n",
      "Epoch: 0, Batch: 1000, Loss: 44.317527770996094\n",
      "Epoch: 0, Batch: 1500, Loss: 25.33626937866211\n",
      "Epoch: 0, Batch: 2000, Loss: 46.25902557373047\n",
      "Epoch: 0, Batch: 2500, Loss: 25.8304386138916\n",
      "Epoch: 0, Batch: 3000, Loss: 28.175928115844727\n",
      "Epoch: 0, Batch: 3500, Loss: 34.304325103759766\n",
      "Epoch: 79, Validation PPL: 5.463858127593994\n",
      "Epoch: 0, Batch: 0, Loss: 23.23537254333496\n",
      "Epoch: 0, Batch: 500, Loss: 25.295164108276367\n",
      "Epoch: 0, Batch: 1000, Loss: 27.01877212524414\n",
      "Epoch: 0, Batch: 1500, Loss: 24.3138427734375\n",
      "Epoch: 0, Batch: 2000, Loss: 42.100440979003906\n",
      "Epoch: 0, Batch: 2500, Loss: 38.533451080322266\n",
      "Epoch: 0, Batch: 3000, Loss: 25.95730209350586\n",
      "Epoch: 0, Batch: 3500, Loss: 41.637516021728516\n",
      "Epoch: 80, Validation PPL: 5.463876724243164\n",
      "Epoch: 0, Batch: 0, Loss: 44.374698638916016\n",
      "Epoch: 0, Batch: 500, Loss: 38.161102294921875\n",
      "Epoch: 0, Batch: 1000, Loss: 26.19396209716797\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4e05609b4066>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbest_ppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mattn_training_split_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq2context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattn_context2trg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq2context_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattn_context2trg_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_validation_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq2context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattn_context2trg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler_c2t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler_s2c\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontext_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mppl\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_ppl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cs287/hw3/common.py\u001b[0m in \u001b[0;36mattn_training_split_loop\u001b[0;34m(e, train_iter, seq2context, attn_context2trg, seq2context_optimizer, attn_context2trg_optimizer, BATCH_SIZE, context_size, EN)\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                 \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeamsearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq2context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_context2trg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m                 \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cs287/hw3/common.py\u001b[0m in \u001b[0;36mbeamsearch\u001b[0;34m(seq2context, context2trg, context_size, src, beam_width, max_len, output_width, alpha, BATCH_SIZE, padding, EN)\u001b[0m\n\u001b[1;32m    361\u001b[0m                             \u001b[0mnew_b_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlogprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoppreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                             \u001b[0mcurr_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_b_prob\u001b[0m \u001b[0;31m# set top prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m                             \u001b[0mcurr_string\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb_string\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# set sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m                             \u001b[0mcurr_string\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoppreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# set top word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# if preds are from subsequent beams, compare to existing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_ppl = 1e8\n",
    "for e in range(0,300):\n",
    "    attn_training_split_loop(0,train_iter,seq2context,attn_context2trg,seq2context_optimizer,attn_context2trg_optimizer,EN=EN)\n",
    "    ppl = attn_validation_loop(e,val_iter,seq2context,attn_context2trg,scheduler_c2t,scheduler_s2c,BATCH_SIZE=32,context_size=500)\n",
    "    if ppl < best_ppl:\n",
    "        torch.save(seq2context.state_dict(),'best_seq2seq_withattn_seq2context_splittrain.pt')\n",
    "        torch.save(attn_context2trg.state_dict(),'best_seq2seq_withattn_context2trg_splittrain.pt')\n",
    "        best_ppl = ppl\n",
    "        print('Wrote model!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Validation PPL: 1.0001404285430908\n"
     ]
    }
   ],
   "source": [
    "ppl = attn_validation_loop(0,val_iter,seq2context,attn_context2trg,scheduler_c2t,scheduler_s2c,BATCH_SIZE=32,context_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test set\n",
    "sentences = []\n",
    "for i, l in enumerate(open(\"source_test.txt\"), 1):\n",
    "    sentences.append(re.split(' ', l))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
