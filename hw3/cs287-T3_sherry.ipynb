{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 287, Homework 3: Neural Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import *\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data, datasets\n",
    "from namedtensor import ntorch, NamedTensor\n",
    "from namedtensor.text import NamedField\n",
    "import numpy as np\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n",
      "1 0\n",
      "1 0\n"
     ]
    }
   ],
   "source": [
    "# split raw data into tokens\n",
    "import spacy\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# add beginning-of-sentence and end-of-sentence tokens to target\n",
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "DE = NamedField(names=('srcSeqlen',), tokenize=tokenize_de)\n",
    "EN = NamedField(names=('trgSeqlen',), tokenize=tokenize_en,\n",
    "                init_token = BOS_WORD, eos_token = EOS_WORD) # only target needs BOS/EOS\n",
    "\n",
    "# download dataset of 200K pairs of sentences\n",
    "# start with MAXLEN = 20\n",
    "MAX_LEN = 20\n",
    "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(DE, EN), \n",
    "                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
    "#print(train.fields)\n",
    "#print(len(train))\n",
    "#print(vars(train[0]))\n",
    "\n",
    "# build vocab, convert words to indices\n",
    "MIN_FREQ = 5\n",
    "DE.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "EN.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
    "#print(DE.vocab.freqs.most_common(10))\n",
    "#print(\"Size of German vocab\", len(DE.vocab))\n",
    "#print(EN.vocab.freqs.most_common(10))\n",
    "#print(\"Size of English vocab\", len(EN.vocab))\n",
    "print(EN.vocab.stoi[\"<s>\"], EN.vocab.stoi[\"</s>\"])\n",
    "print(EN.vocab.stoi[\"<pad>\"], EN.vocab.stoi[\"<unk>\"])\n",
    "print(DE.vocab.stoi[\"<pad>\"], DE.vocab.stoi[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into batches\n",
    "BATCH_SIZE = 32\n",
    "device = torch.device('cuda:0')\n",
    "train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=BATCH_SIZE, device=device,\n",
    "                                                  repeat=False, sort_key=lambda x: len(x.src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to Sequence Learning with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- English to French translation, $p \\left( y_1, \\dots, y_{T'} \\ | \\ x_1, \\dots, x_T \\right) = \\prod_{t = 1}^{T'} p \\left( y_t \\ | \\ v, y_1, \\dots, y_{t-1} \\right)$\n",
    "- Each sentence ends in '<EOS\\>', out-of-vocab words denoted '<UNK\\>'\n",
    "- Model specs: \n",
    "    * Input vocabulary of 160,000 and output vocabulary of 80,000\n",
    "    * Deep LSTM to map (encode) input sequence to fixed-len vector\n",
    "    * Another deep LSTM to translate (decode) fixed-len vector to output sequence\n",
    "    * 4 layers per LSTM, 1000 cells per layer, 1000-dimensional word embeddings, softmax over 80,000 words\n",
    "    * Reversing order of words in source (but not target) improved performance\n",
    "        * Each word in the source is far from its corresponding word in the target (large minimal time lag); reversing the source reduces the minimal time lag, thereby allowing backprop to establish communication between source and target more easily\n",
    "- Training specs:\n",
    "    * Initialize all LSTM params $\\sim Unif[-0.08,0.08]$\n",
    "    * SGD w/o momentum, lr = 0.7\n",
    "        * After 5 epochs, halve the lr every half-epoch\n",
    "        * Train for 7.5 epochs\n",
    "    * Batch size = 128; divide gradient by batch size (denoted $g$)\n",
    "    * Hard constraint gradient norm; if $s = ||g||_2 > 5$, set $s = 5$\n",
    "    * Make sure all sentences within a minibatch are roughly the same length\n",
    "- Objective: $max \\frac{1}{|S|} \\sum_{(T,S) \\in \\mathcal{S}} log \\ p(T \\ | \\ S)$, where $\\mathcal{S}$ is the training set\n",
    "- Prediction: $\\hat{T} = argmax \\ p(T \\ | \\ S)$ via beam search, where beam size $B \\in {1,2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class SequenceModel(nn.Module):\\n    def __init__(self, src_vocab_size, context_size, num_layers, weight_init = 0.08):\\n        super(SequenceModel, self).__init__()\\n        # embedding\\n        self.embedding = nn.Embedding(src_vocab_size, context_size)\\n        # language summarization\\n        self.lstm = nn.LSTM(input_size=context_size, hidden_size=context_size, num_layers=num_layers, batch_first=True)\\n        for p in self.lstm.parameters():\\n            torch.nn.init.uniform_(p, a=weight_init, b=weight_init)\\n\\n    def forward(self, inputs, h0=None):\\n        # embed the words \\n        embedded = self.embedding(inputs)\\n        # summarize context\\n        context, hidden = self.lstm(embedded,h0)\\n        return context, hidden\\n    \\nclass LanguageModel(nn.Module):\\n    def __init__(self, target_vocab_size, hidden_size, context_size, num_layers, weight_init = 0.08):\\n        super(LanguageModel, self).__init__()\\n        # context is batch_size x seq_len x context_size\\n        # context to hidden\\n        self.embedding = nn.Embedding(target_vocab_size, hidden_size)\\n        # hidden to hidden \\n        self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\\n        # decode hidden state for y_t\\n        for p in self.lstm.parameters():\\n            torch.nn.init.uniform_(p, a=weight_init, b=weight_init)\\n            \\n        self.translate = nn.Linear(hidden_size, target_vocab_size)\\n\\n    def forward(self, inputs, h0=None):\\n        # embed the trg words\\n        embedded = self.embedding(inputs)\\n        # setting hidden state to context at t=0\\n        # otherwise context = prev hidden state\\n        output, hidden = self.lstm(embedded, h0)\\n        output = self.translate(output)\\n        return output,hidden'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class SequenceModel(nn.Module):\n",
    "    def __init__(self, src_vocab_size, context_size, num_layers, weight_init = 0.08):\n",
    "        super(SequenceModel, self).__init__()\n",
    "        # embedding\n",
    "        self.embedding = nn.Embedding(src_vocab_size, context_size)\n",
    "        # language summarization\n",
    "        self.lstm = nn.LSTM(input_size=context_size, hidden_size=context_size, num_layers=num_layers, batch_first=True)\n",
    "        for p in self.lstm.parameters():\n",
    "            torch.nn.init.uniform_(p, a=weight_init, b=weight_init)\n",
    "\n",
    "    def forward(self, inputs, h0=None):\n",
    "        # embed the words \n",
    "        embedded = self.embedding(inputs)\n",
    "        # summarize context\n",
    "        context, hidden = self.lstm(embedded,h0)\n",
    "        return context, hidden\n",
    "    \n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, target_vocab_size, hidden_size, context_size, num_layers, weight_init = 0.08):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        # context is batch_size x seq_len x context_size\n",
    "        # context to hidden\n",
    "        self.embedding = nn.Embedding(target_vocab_size, hidden_size)\n",
    "        # hidden to hidden \n",
    "        self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        # decode hidden state for y_t\n",
    "        for p in self.lstm.parameters():\n",
    "            torch.nn.init.uniform_(p, a=weight_init, b=weight_init)\n",
    "            \n",
    "        self.translate = nn.Linear(hidden_size, target_vocab_size)\n",
    "\n",
    "    def forward(self, inputs, h0=None):\n",
    "        # embed the trg words\n",
    "        embedded = self.embedding(inputs)\n",
    "        # setting hidden state to context at t=0\n",
    "        # otherwise context = prev hidden state\n",
    "        output, hidden = self.lstm(embedded, h0)\n",
    "        output = self.translate(output)\n",
    "        return output,hidden'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def repackage_hidden(h):\\n    return tuple(v.detach() for v in h)\\ndef repackage_layer(hidden_s2c,hidden=100):\\n    return tuple([hidden_s2c[0][-1].detach().view(1,BATCH_SIZE,hidden),hidden_s2c[1][-1].detach().view(1,BATCH_SIZE,hidden)])\\ndef reverse_sequence(src):\\n    length = list(src.shape)[1]\\n    idx = torch.linspace(length-1, 0, steps=length).long()\\n    rev_src = src[:,idx]\\n    return rev_src'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def repackage_hidden(h):\n",
    "    return tuple(v.detach() for v in h)\n",
    "def repackage_layer(hidden_s2c,hidden=100):\n",
    "    return tuple([hidden_s2c[0][-1].detach().view(1,BATCH_SIZE,hidden),hidden_s2c[1][-1].detach().view(1,BATCH_SIZE,hidden)])\n",
    "def reverse_sequence(src):\n",
    "    length = list(src.shape)[1]\n",
    "    idx = torch.linspace(length-1, 0, steps=length).long()\n",
    "    rev_src = src[:,idx]\n",
    "    return rev_src'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"context_size = 500\\nnum_layers = 1\\nseq2context = SequenceModel(len(DE.vocab),context_size,num_layers)\\ncontext2trg = LanguageModel(len(EN.vocab),hidden_size=context_size,context_size=context_size,num_layers=num_layers)\\nseq2context,context2trg = seq2context.cuda(),context2trg.cuda()\\nseq2context_optimizer = torch.optim.Adam(seq2context.parameters(), lr=1e-5)\\ncontext2trg_optimizer = torch.optim.Adam(context2trg.parameters(), lr=1e-5)\\ncriterion = nn.CrossEntropyLoss(reduction='none')\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''context_size = 500\n",
    "num_layers = 1\n",
    "seq2context = SequenceModel(len(DE.vocab),context_size,num_layers)\n",
    "context2trg = LanguageModel(len(EN.vocab),hidden_size=context_size,context_size=context_size,num_layers=num_layers)\n",
    "seq2context,context2trg = seq2context.cuda(),context2trg.cuda()\n",
    "seq2context_optimizer = torch.optim.Adam(seq2context.parameters(), lr=1e-5)\n",
    "context2trg_optimizer = torch.optim.Adam(context2trg.parameters(), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def training_loop(e=0):\\n    seq2context.train()\\n    context2trg.train()\\n    h0 = None\\n    for ix,batch in enumerate(train_iter):\\n        seq2context_optimizer.zero_grad()\\n        context2trg_optimizer.zero_grad()\\n        \\n        src = batch.src.values.transpose(0,1)\\n        src = reverse_sequence(src)\\n        trg = batch.trg.values.transpose(0,1)\\n        if src.shape[0]!=BATCH_SIZE:\\n            break\\n        else:\\n            # generate hidden state for decoder\\n            context, hidden_s2c = seq2context(src,h0)\\n            hidden = repackage_layer(hidden_s2c,context_size)\\n            output, hidden_lm = context2trg(trg[:,:-1],hidden)\\n            loss = criterion(output.transpose(2,1),trg[:,1:])\\n            mask = trg[:,1:]!=1\\n            loss = loss[mask].sum()\\n            #clip_grad_norm_(seq2context.parameters(), max_norm=5)\\n            #clip_grad_norm_(context2trg.parameters(), max_norm=5)\\n            loss.backward()\\n            seq2context_optimizer.step()\\n            context2trg_optimizer.step()\\n        if np.mod(ix,100) == 0:\\n            var = torch.var(torch.argmax(lsm(output).cpu().detach(),2).float())\\n            print('Epoch: {}, Batch: {}, loss: {}, var: {},'.format(e, ix, loss.cpu().detach()/BATCH_SIZE, var))\\n    loss = 0\\n    for b in iter(val_iter):\\n        src = b.src.values.transpose(0,1)\\n        src = reverse_sequence(src)\\n        trg = b.trg.values.transpose(0,1)\\n        if src.shape[0]!=BATCH_SIZE:\\n            break\\n        else:\\n            # generate hidden state for decoder\\n            context, hidden_s2c = seq2context(src,h0)\\n            hidden = repackage_layer(hidden_s2c,context_size)\\n            output, hidden_lm = context2trg(trg[:,:-1],hidden)\\n            bloss = criterion(output.transpose(2,1),trg[:,1:])\\n            mask = trg[:,1:]!=1\\n            loss += bloss[mask].sum()\\n    print('Epoch: {}, loss: {}, var: {},'.format(e, loss.cpu().detach()/(BATCH_SIZE*len(val_iter))))\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def training_loop(e=0):\n",
    "    seq2context.train()\n",
    "    context2trg.train()\n",
    "    h0 = None\n",
    "    for ix,batch in enumerate(train_iter):\n",
    "        seq2context_optimizer.zero_grad()\n",
    "        context2trg_optimizer.zero_grad()\n",
    "        \n",
    "        src = batch.src.values.transpose(0,1)\n",
    "        src = reverse_sequence(src)\n",
    "        trg = batch.trg.values.transpose(0,1)\n",
    "        if src.shape[0]!=BATCH_SIZE:\n",
    "            break\n",
    "        else:\n",
    "            # generate hidden state for decoder\n",
    "            context, hidden_s2c = seq2context(src,h0)\n",
    "            hidden = repackage_layer(hidden_s2c,context_size)\n",
    "            output, hidden_lm = context2trg(trg[:,:-1],hidden)\n",
    "            loss = criterion(output.transpose(2,1),trg[:,1:])\n",
    "            mask = trg[:,1:]!=1\n",
    "            loss = loss[mask].sum()\n",
    "            #clip_grad_norm_(seq2context.parameters(), max_norm=5)\n",
    "            #clip_grad_norm_(context2trg.parameters(), max_norm=5)\n",
    "            loss.backward()\n",
    "            seq2context_optimizer.step()\n",
    "            context2trg_optimizer.step()\n",
    "        if np.mod(ix,100) == 0:\n",
    "            var = torch.var(torch.argmax(lsm(output).cpu().detach(),2).float())\n",
    "            print('Epoch: {}, Batch: {}, loss: {}, var: {},'.format(e, ix, loss.cpu().detach()/BATCH_SIZE, var))\n",
    "    loss = 0\n",
    "    for b in iter(val_iter):\n",
    "        src = b.src.values.transpose(0,1)\n",
    "        src = reverse_sequence(src)\n",
    "        trg = b.trg.values.transpose(0,1)\n",
    "        if src.shape[0]!=BATCH_SIZE:\n",
    "            break\n",
    "        else:\n",
    "            # generate hidden state for decoder\n",
    "            context, hidden_s2c = seq2context(src,h0)\n",
    "            hidden = repackage_layer(hidden_s2c,context_size)\n",
    "            output, hidden_lm = context2trg(trg[:,:-1],hidden)\n",
    "            bloss = criterion(output.transpose(2,1),trg[:,1:])\n",
    "            mask = trg[:,1:]!=1\n",
    "            loss += bloss[mask].sum()\n",
    "    print('Epoch: {}, loss: {}, var: {},'.format(e, loss.cpu().detach()/(BATCH_SIZE*len(val_iter))))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for e in range(2):\\n    training_loop(e)\\n    #training_loop(e,train_iter,seq2context,context2trg,seq2context_optimizer,context2trg_optimizer,BATCH_SIZE)\\n    #validation_loop(e,val_iter,seq2context,context2trg,BATCH_SIZE)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for e in range(2):\n",
    "    training_loop(e)\n",
    "    #training_loop(e,train_iter,seq2context,context2trg,seq2context_optimizer,context2trg_optimizer,BATCH_SIZE)\n",
    "    #validation_loop(e,val_iter,seq2context,context2trg,BATCH_SIZE)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for ix,batch in enumerate(train_iter):\\n    src = batch.src.values.transpose(0,1)\\n    trg = batch.trg.values.transpose(0,1)\\n    break\\n\\nh0 = None\\ncontext, hidden_s2c = seq2context(reverse_sequence(src),h0)\\nhidden = repackage_layer(hidden_s2c,context_size)\\noutput, hidden_lm = context2trg(trg[:,:-1],hidden)\\n\\n[EN.vocab.itos[i] for i in torch.argmax(lsm(output),2)[30,:]]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for ix,batch in enumerate(train_iter):\n",
    "    src = batch.src.values.transpose(0,1)\n",
    "    trg = batch.trg.values.transpose(0,1)\n",
    "    break\n",
    "\n",
    "h0 = None\n",
    "context, hidden_s2c = seq2context(reverse_sequence(src),h0)\n",
    "hidden = repackage_layer(hidden_s2c,context_size)\n",
    "output, hidden_lm = context2trg(trg[:,:-1],hidden)\n",
    "\n",
    "[EN.vocab.itos[i] for i in torch.argmax(lsm(output),2)[30,:]]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# define attention-based encoder-decoder model\\nclass attn_RNNet_batched(torch.nn.Module):\\n\\n    def __init__(self, input_size, hidden_size, num_layers, dropout=0.5, weight_init=0.05):\\n        super(attn_RNNet_batched, self).__init__()\\n        self.emb = torch.nn.Sequential(torch.nn.Embedding(input_size, hidden_size), torch.nn.Dropout(dropout))\\n        self.rnn = torch.nn.LSTM(input_size=2*hidden_size, hidden_size=hidden_size, num_layers=num_layers, bias=True, batch_first=True, dropout=dropout)\\n        self.lnr = torch.nn.Sequential(torch.nn.Dropout(dropout), torch.nn.Linear(2*hidden_size, input_size))\\n    \\n        for f in self.parameters():\\n            torch.nn.init.uniform_(f, a=-weight_init, b=weight_init)\\n\\n    def forward(self, word_input, last_context, last_hidden, encoder_outputs):\\n        word_embedded = self.emb(word_input)\\n        rnn_input = torch.cat([word_embedded, last_context], 1).unsqueeze(1) # batch x 1 x hiddenx2\\n        rnn_output, hidden = self.rnn(rnn_input, last_hidden)\\n        attn_weights = rnn_output.bmm(encoder_outputs.transpose(1,2))# batch x src_seqlen x 1\\n        context = attn_weights.bmm(encoder_outputs)\\n        rnn_output = rnn_output.squeeze(1)\\n        context = context.squeeze(1)\\n        output = self.lnr(torch.cat((rnn_output, context), 1))\\n        # prediction, last_context, last_hidden, weights for vis\\n        return output, context, hidden, attn_weights '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# define attention-based encoder-decoder model\n",
    "class attn_RNNet_batched(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout=0.5, weight_init=0.05):\n",
    "        super(attn_RNNet_batched, self).__init__()\n",
    "        self.emb = torch.nn.Sequential(torch.nn.Embedding(input_size, hidden_size), torch.nn.Dropout(dropout))\n",
    "        self.rnn = torch.nn.LSTM(input_size=2*hidden_size, hidden_size=hidden_size, num_layers=num_layers, bias=True, batch_first=True, dropout=dropout)\n",
    "        self.lnr = torch.nn.Sequential(torch.nn.Dropout(dropout), torch.nn.Linear(2*hidden_size, input_size))\n",
    "    \n",
    "        for f in self.parameters():\n",
    "            torch.nn.init.uniform_(f, a=-weight_init, b=weight_init)\n",
    "\n",
    "    def forward(self, word_input, last_context, last_hidden, encoder_outputs):\n",
    "        word_embedded = self.emb(word_input)\n",
    "        rnn_input = torch.cat([word_embedded, last_context], 1).unsqueeze(1) # batch x 1 x hiddenx2\n",
    "        rnn_output, hidden = self.rnn(rnn_input, last_hidden)\n",
    "        attn_weights = rnn_output.bmm(encoder_outputs.transpose(1,2))# batch x src_seqlen x 1\n",
    "        context = attn_weights.bmm(encoder_outputs)\n",
    "        rnn_output = rnn_output.squeeze(1)\n",
    "        context = context.squeeze(1)\n",
    "        output = self.lnr(torch.cat((rnn_output, context), 1))\n",
    "        # prediction, last_context, last_hidden, weights for vis\n",
    "        return output, context, hidden, attn_weights '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# initialize model\\ncontext_size = 500\\nnum_layers = 2\\nattn_context2trg = attn_RNNet_batched(input_size=len(EN.vocab),hidden_size=context_size,num_layers=num_layers)\\nattn_context2trg = attn_context2trg.cuda()\\nseq2context = SequenceModel(len(DE.vocab),context_size,num_layers=num_layers)\\nseq2context = seq2context.cuda()'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# initialize model\n",
    "context_size = 500\n",
    "num_layers = 2\n",
    "attn_context2trg = attn_RNNet_batched(input_size=len(EN.vocab),hidden_size=context_size,num_layers=num_layers)\n",
    "attn_context2trg = attn_context2trg.cuda()\n",
    "seq2context = SequenceModel(len(DE.vocab),context_size,num_layers=num_layers)\n",
    "seq2context = seq2context.cuda()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# prep for training\\nattn_context2trg_optimizer = torch.optim.Adam(attn_context2trg.parameters(), lr=1e-3)\\nseq2context_optimizer = torch.optim.Adam(seq2context.parameters(), lr=1e-3)\\ncriterion_train = nn.CrossEntropyLoss(reduction='sum')\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# prep for training\n",
    "attn_context2trg_optimizer = torch.optim.Adam(attn_context2trg.parameters(), lr=1e-3)\n",
    "seq2context_optimizer = torch.optim.Adam(seq2context.parameters(), lr=1e-3)\n",
    "criterion_train = nn.CrossEntropyLoss(reduction='sum')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def attn_training_loop(e=0):\\n    for ix,batch in enumerate(train_iter):\\n        src = batch.src.values.transpose(0,1)\\n        src = reverse_sequence(src)\\n        trg = batch.trg.values.transpose(0,1)\\n        if trg.shape[0] == BATCH_SIZE:\\n        \\n            seq2context_optimizer.zero_grad()\\n            attn_context2trg_optimizer.zero_grad()\\n        \\n            encoder_outputs, encoder_hidden = seq2context(src)\\n            loss = 0\\n            decoder_context = torch.zeros(BATCH_SIZE, context_size, device='cuda') # 32 x 500\\n            decoder_hidden = encoder_hidden\\n            sentence = []\\n            for j in range(trg.shape[1] - 1):\\n                word_input = trg[:,j]\\n                decoder_output, decoder_context, decoder_hidden, decoder_attention = attn_context2trg(word_input, decoder_context, decoder_hidden, encoder_outputs)\\n                #print(decoder_output.shape, trg[i,j+1].view(-1).shape)\\n                loss += criterion_train(decoder_output, trg[:,j+1])\\n                \\n                if np.mod(ix,100) == 0:\\n                    sentence.extend([torch.argmax(decoder_output[0,:],dim=0)])\\n                \\n            loss.backward()\\n            seq2context_optimizer.step()\\n            attn_context2trg_optimizer.step()\\n        \\n            if np.mod(ix,100) == 0:\\n                print('Epoch: {}, Batch: {}, Loss: {}'.format(e, ix, loss.cpu().detach()/BATCH_SIZE))\\n                print([EN.vocab.itos[i] for i in sentence])\\n                print([EN.vocab.itos[i] for i in trg[0,:]])\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def attn_training_loop(e=0):\n",
    "    for ix,batch in enumerate(train_iter):\n",
    "        src = batch.src.values.transpose(0,1)\n",
    "        src = reverse_sequence(src)\n",
    "        trg = batch.trg.values.transpose(0,1)\n",
    "        if trg.shape[0] == BATCH_SIZE:\n",
    "        \n",
    "            seq2context_optimizer.zero_grad()\n",
    "            attn_context2trg_optimizer.zero_grad()\n",
    "        \n",
    "            encoder_outputs, encoder_hidden = seq2context(src)\n",
    "            loss = 0\n",
    "            decoder_context = torch.zeros(BATCH_SIZE, context_size, device='cuda') # 32 x 500\n",
    "            decoder_hidden = encoder_hidden\n",
    "            sentence = []\n",
    "            for j in range(trg.shape[1] - 1):\n",
    "                word_input = trg[:,j]\n",
    "                decoder_output, decoder_context, decoder_hidden, decoder_attention = attn_context2trg(word_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "                #print(decoder_output.shape, trg[i,j+1].view(-1).shape)\n",
    "                loss += criterion_train(decoder_output, trg[:,j+1])\n",
    "                \n",
    "                if np.mod(ix,100) == 0:\n",
    "                    sentence.extend([torch.argmax(decoder_output[0,:],dim=0)])\n",
    "                \n",
    "            loss.backward()\n",
    "            seq2context_optimizer.step()\n",
    "            attn_context2trg_optimizer.step()\n",
    "        \n",
    "            if np.mod(ix,100) == 0:\n",
    "                print('Epoch: {}, Batch: {}, Loss: {}'.format(e, ix, loss.cpu().detach()/BATCH_SIZE))\n",
    "                print([EN.vocab.itos[i] for i in sentence])\n",
    "                print([EN.vocab.itos[i] for i in trg[0,:]])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for e in range(10):\\n    attn_training_loop(e)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for e in range(10):\n",
    "    attn_training_loop(e)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search for common.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beamsearch(seq2context, context2trg, context_size, src, beam_width, max_len, output_width=1, alpha=1, padding=False):\n",
    "    '''\n",
    "    run beam search and return top predictions\n",
    "        - seq2context: encoder model\n",
    "        - context2trg: decoder model\n",
    "        - context_size: hidden size\n",
    "        - src: tensor of source sentences\n",
    "        - beam_width: beam search width\n",
    "        - max_len: maximum length for predictions\n",
    "        - output_width: number of predictions to return per sentence\n",
    "        - alpha: string length discount rate; e.g., normalizing factor = 1/(T^alpha)\n",
    "        - padding: pad predictions to max_len\n",
    "    '''\n",
    "    # set up\n",
    "    START_TKN = EN.vocab.stoi[\"<s>\"]\n",
    "    END_TKN = EN.vocab.stoi[\"</s>\"]\n",
    "    BEAM_WIDTH = beam_width\n",
    "    lsm = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    # run forward pass of encoder once\n",
    "    encoder_outputs, encoder_hidden = seq2context(src)\n",
    "    decoder_context = torch.zeros(BATCH_SIZE, context_size, device='cuda') # 32 x 500\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    # prepare for beam search\n",
    "    b_string = torch.zeros((BATCH_SIZE, max_len, BEAM_WIDTH), device='cuda') # stores the top BEAM_WIDTH strings\n",
    "    b_string[:,0,:] = START_TKN\n",
    "    b_probs = {} # stores the top BEAM_WIDTH probs\n",
    "    '''\n",
    "    b_probs key = tuple(batch idx, beam idx)\n",
    "    b_probs val = [cum log prob, length]\n",
    "    '''\n",
    "    done = {} # stores the finished strings\n",
    "    '''\n",
    "    done key = batch idx\n",
    "    done val = [str, cum log prob, length]\n",
    "    '''\n",
    "    predictions = {} # stores the top output_width predictions\n",
    "    for b in range(BATCH_SIZE):\n",
    "        done[b] = []\n",
    "        predictions[b] = []\n",
    "        for c in range(BEAM_WIDTH):\n",
    "            b_probs[(b, c)] = [0, 1]\n",
    "\n",
    "    # loop through target sequence max len\n",
    "    for i in range(1,max_len):\n",
    "        if i == 1: # if predicting the word following <s>, take top BEAM_WIDTH preds\n",
    "            word_input = b_string[:,i-1,0].long()\n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = context2trg(word_input, \n",
    "                                                                                             decoder_context, \n",
    "                                                                                             decoder_hidden, \n",
    "                                                                                             encoder_outputs)\n",
    "            logprobs = lsm(decoder_output.detach()) # BATCH_SIZE x VOCAB_SIZE\n",
    "            toppreds = torch.argsort(logprobs, dim=1, descending=True)[:,0:BEAM_WIDTH] # BATCH_SIZE x BEAM_WIDTH\n",
    "            b_string[:,i,:] = toppreds\n",
    "            for b in range(BATCH_SIZE):\n",
    "                for c in range(BEAM_WIDTH):\n",
    "                    b_probs[tuple((b,c))][0] += logprobs[b, toppreds[b,c]]\n",
    "                    b_probs[tuple((b,c))][1] += 1\n",
    "        else: # if predicting the word for positions 2+, compare top BEAM_WIDTH preds for each of BEAM_WIDTH strings\n",
    "            curr_probs = {} # temporary storage\n",
    "            curr_string = torch.zeros(BATCH_SIZE, i+1, BEAM_WIDTH) # temporary storage\n",
    "\n",
    "            for j in range(BEAM_WIDTH):\n",
    "                word_input = b_string[:,i-1,j].long()\n",
    "                decoder_output, decoder_context, decoder_hidden, decoder_attention = context2trg(word_input, \n",
    "                                                                                                 decoder_context, \n",
    "                                                                                                 decoder_hidden, \n",
    "                                                                                                 encoder_outputs)\n",
    "                logprobs = lsm(decoder_output.detach()) # unsorted log probs\n",
    "                sortedpreds = torch.argsort(logprobs, dim=1, descending=True) # sorted words\n",
    "                toppreds = sortedpreds[:,0:BEAM_WIDTH] # top words\n",
    "\n",
    "                # check if any top preds are </s>\n",
    "                for b in range(BATCH_SIZE):\n",
    "                    if END_TKN in toppreds[b,:]: # if </s> in top preds\n",
    "                        # track finished strings\n",
    "                        done_string = torch.cat((b_string[b,0:i,j],torch.tensor([END_TKN], device='cuda').float()))\n",
    "                        done_prob = b_probs[tuple((b,j))][0] + logprobs[b,END_TKN]\n",
    "                        done[b].append([done_string, done_prob, done_string.shape[0]])\n",
    "                        # replace </s> with 4th best pred\n",
    "                        done_idx = (toppreds[b,:] == END_TKN).nonzero()\n",
    "                        toppreds[b,done_idx] = sortedpreds[b,BEAM_WIDTH]\n",
    "\n",
    "                if j == 0: # if preds are from first beam, take top BEAM_WIDTH preds (temporarily)\n",
    "                    for b in range(BATCH_SIZE):\n",
    "                        for c in range(BEAM_WIDTH):\n",
    "                            new_b_prob = b_probs[tuple((b,j))][0] + logprobs[b,toppreds[b,c]]\n",
    "                            curr_probs[tuple((b,c))] = new_b_prob # set top prob\n",
    "                            curr_string[b,0:i,c] = b_string[b,0:i,j] # set sentence\n",
    "                            curr_string[b,i,c] = toppreds[b,c] # set top word\n",
    "                else: # if preds are from subsequent beams, compare to existing\n",
    "                    for b in range(BATCH_SIZE):\n",
    "                        for c in range(BEAM_WIDTH): # proposed strings\n",
    "                            replaced = False\n",
    "                            for d in range(BEAM_WIDTH): # existing strings\n",
    "                                new_b_prob = b_probs[tuple((b,j))][0] + logprobs[b,toppreds[b,c]]\n",
    "                                if new_b_prob > curr_probs[tuple((b,d))] and not replaced:\n",
    "                                    curr_probs[tuple((b,d))] = new_b_prob # update top prob\n",
    "                                    curr_string[b,0:i,d] = b_string[b,0:i,j] # update sentence\n",
    "                                    curr_string[b,i,d] = toppreds[b,c] # update top word\n",
    "                                    replaced = True                        \n",
    "            b_string[:,0:i+1,:] = curr_string\n",
    "            # update top strings, probs\n",
    "            for b in range(BATCH_SIZE):\n",
    "                for c in range(BEAM_WIDTH):\n",
    "                    b_probs[tuple((b,c))][0] = curr_probs[tuple((b,c))]\n",
    "                    b_probs[tuple((b,c))][1] += 1\n",
    "\n",
    "    K = output_width\n",
    "    for b in range(BATCH_SIZE):\n",
    "        normalized_probs = torch.tensor([], device='cuda')\n",
    "        for sentence in range(len(done[b])):\n",
    "            normalized = torch.tensor([done[b][sentence][1]/done[b][sentence][2]**alpha], device='cuda')\n",
    "            normalized_probs = torch.cat((normalized_probs,normalized),0)\n",
    "        top = torch.argsort(normalized_probs, descending=True)[0:K]\n",
    "        for k in range(K):\n",
    "            best = done[b][top[k]]\n",
    "            if padding:\n",
    "                m = nn.ConstantPad1d((0, max_len - best[2]), EN.vocab.stoi['<pad>'])\n",
    "                predictions[b].append(m(best[0].long()))\n",
    "            else:\n",
    "                predictions[b].append(best[0].long())\n",
    "            #print([EN.vocab.itos[i] for i in best[0].long()])\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 500\n",
    "num_layers = 2\n",
    "\n",
    "attn_seq2context = SequenceModel(len(DE.vocab),context_size,num_layers=num_layers)\n",
    "state_dict = torch.load('best_seq2seq_withattn_seq2context.pt')\n",
    "attn_seq2context.load_state_dict(state_dict)\n",
    "attn_seq2context = attn_seq2context.cuda()\n",
    "\n",
    "attn_context2trg = attn_RNNet_batched(input_size=len(EN.vocab),hidden_size=context_size,num_layers=num_layers)\n",
    "state_dict = torch.load('best_seq2seq_withattn_context2trg.pt')\n",
    "attn_context2trg.load_state_dict(state_dict)\n",
    "attn_context2trg = attn_context2trg.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [tensor([   2,  537,   23,   17,    8, 2608,   21,    3,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1], device='cuda:0')],\n",
       " 1: [tensor([  2,  89,  12, 657,   4,   3,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "            1,   1,   1,   1,   1], device='cuda:0')],\n",
       " 2: [tensor([  2,   0,   0,   5,  18, 339,   0,  18,   0,   4,   3,   1,   1,   1,\n",
       "            1,   1,   1,   1,   1], device='cuda:0')],\n",
       " 3: [tensor([  2,  42, 315,   9, 100, 194,  30,   0,   4,   3,   1,   1,   1,   1,\n",
       "            1,   1,   1,   1,   1], device='cuda:0')],\n",
       " 4: [tensor([  2,  27,  12, 394,  51, 319,   3,   1,   1,   1,   1,   1,   1,   1,\n",
       "            1,   1,   1,   1,   1], device='cuda:0')],\n",
       " 5: [tensor([   2,   27,   12,   35,   66, 3794,  371,    4,    3,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1], device='cuda:0')],\n",
       " 6: [tensor([   2,    0, 3010,    4,    3,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1], device='cuda:0')],\n",
       " 7: [tensor([   2,   24,    5,    6, 3691,    5,    6, 1169,    4,    3,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1], device='cuda:0')],\n",
       " 8: [tensor([   2,   34,   72,   13,    0, 2062,   29,  468,    4,    3,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1], device='cuda:0')],\n",
       " 9: [tensor([   2, 5235,    0,   39,  376,    4,    3,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1], device='cuda:0')],\n",
       " 10: [tensor([   2,   27,   12,    8, 1019, 1022, 1270,    4,    3,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1], device='cuda:0')],\n",
       " 11: [tensor([   2,   14,    5,   16,   12,  135,   50, 1581,    4,    3,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1], device='cuda:0')],\n",
       " 12: [tensor([   2, 1072,   12, 1977,    4,    3,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1], device='cuda:0')],\n",
       " 13: [tensor([   2, 3675,   16,   23,  581,   82,  549,   21,    3,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1], device='cuda:0')],\n",
       " 14: [tensor([  2,   0,  82, 645,  69,   7,   8,  45,   0,   4,   3,   1,   1,   1,\n",
       "            1,   1,   1,   1,   1], device='cuda:0')],\n",
       " 15: [tensor([  2,  42, 754,  23, 271, 373,   4,   3,   1,   1,   1,   1,   1,   1,\n",
       "            1,   1,   1,   1,   1], device='cuda:0')],\n",
       " 16: [tensor([  2,  14,  19,  28,   8, 921,   4,   3,   1,   1,   1,   1,   1,   1,\n",
       "            1,   1,   1,   1,   1], device='cuda:0')],\n",
       " 17: [tensor([  2,  27,  12,  53, 169,   4,   3,   1,   1,   1,   1,   1,   1,   1,\n",
       "            1,   1,   1,   1,   1], device='cuda:0')],\n",
       " 18: [tensor([   2,   10,  744,   20,   17,    6,  278, 2567,    4,    3,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1], device='cuda:0')],\n",
       " 19: [tensor([ 2, 24,  5, 78,  4,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "           1], device='cuda:0')],\n",
       " 20: [tensor([  2,  70,  11,   8, 123,   4,   3,   1,   1,   1,   1,   1,   1,   1,\n",
       "            1,   1,   1,   1,   1], device='cuda:0')],\n",
       " 21: [tensor([   2,   14, 1117,   50, 4736,    3,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1], device='cuda:0')],\n",
       " 22: [tensor([  2,  70,  12, 971,   7, 102,   4,   3,   1,   1,   1,   1,   1,   1,\n",
       "            1,   1,   1,   1,   1], device='cuda:0')],\n",
       " 23: [tensor([  2,  48,  11,   6, 132, 195,  17, 562, 117,   4,   3,   1,   1,   1,\n",
       "            1,   1,   1,   1,   1], device='cuda:0')],\n",
       " 24: [tensor([  2,  89,  23, 383,  21,   3,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "            1,   1,   1,   1,   1], device='cuda:0')],\n",
       " 25: [tensor([  2, 457,  21,   3,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "            1,   1,   1,   1,   1], device='cuda:0')],\n",
       " 26: [tensor([   2,   70,   12,    6, 8829,    4,    3,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1], device='cuda:0')],\n",
       " 27: [tensor([  2, 397,   4,  14,  10,  74,  35,  16,   4,   3,   1,   1,   1,   1,\n",
       "            1,   1,   1,   1,   1], device='cuda:0')],\n",
       " 28: [tensor([  2, 880,   5,  10,  84,  32,  10,  26,   4,   3,   1,   1,   1,   1,\n",
       "            1,   1,   1,   1,   1], device='cuda:0')],\n",
       " 29: [tensor([  2,   0, 100,  25, 135,   4,   3,   1,   1,   1,   1,   1,   1,   1,\n",
       "            1,   1,   1,   1,   1], device='cuda:0')],\n",
       " 30: [tensor([   2,   27,   23,    8,  168, 1460,    4,    3,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1], device='cuda:0')],\n",
       " 31: [tensor([  2,  76,  85,   7, 300,  17,  20, 900,  21,   3,   1,   1,   1,   1,\n",
       "            1,   1,   1,   1,   1], device='cuda:0')]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run beam search on one batch\n",
    "it = iter(train_iter)\n",
    "batch = next(it)\n",
    "src = batch.src.values.transpose(0,1)\n",
    "src = reverse_sequence(src)\n",
    "beam_width = 3\n",
    "max_len = src.shape[1] # restrict target sentence length to source sentence length\n",
    "beamsearch(attn_seq2context, attn_context2trg, context_size, src, beam_width, max_len, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run forward pass of encoder once\n",
    "encoder_outputs, encoder_hidden = attn_seq2context(src)\n",
    "decoder_context = torch.zeros(BATCH_SIZE, context_size, device='cuda') # 32 x 500\n",
    "decoder_hidden = encoder_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for beam search\n",
    "START_TKN = EN.vocab.stoi[\"<s>\"]\n",
    "END_TKN = EN.vocab.stoi[\"</s>\"]\n",
    "BEAM_WIDTH = 3\n",
    "lsm = nn.LogSoftmax(dim=1)\n",
    "\n",
    "b_string = torch.zeros((BATCH_SIZE, max_len, BEAM_WIDTH), device='cuda')\n",
    "b_string[:,0,:] = START_TKN\n",
    "\n",
    "b_probs = {}\n",
    "# b_probs key = tuple(batch idx, beam idx)\n",
    "# b_probs val = [cum log prob, length]\n",
    "done = {} # stores the finished strings\n",
    "# done key = batch idx\n",
    "# done val = [str, cum log prob, length]\n",
    "predictions = {} # stores the top BEAM_WIDTH predictions\n",
    "for b in range(BATCH_SIZE):\n",
    "    done[b] = []\n",
    "    predictions[b] = []\n",
    "    for c in range(BEAM_WIDTH):\n",
    "        b_probs[(b, c)] = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through target sequence max len\n",
    "for i in range(1,max_len):\n",
    "    if i == 1: # if predicting the word following <s>, take top BEAM_WIDTH preds\n",
    "        word_input = b_string[:,i-1,0].long()\n",
    "        #print(word_input)\n",
    "        decoder_output, decoder_context, decoder_hidden, decoder_attention = attn_context2trg(word_input, \n",
    "                                                                                                  decoder_context, \n",
    "                                                                                                  decoder_hidden, \n",
    "                                                                                                  encoder_outputs)\n",
    "        logprobs = lsm(decoder_output.detach()) # BATCH_SIZE x VOCAB_SIZE\n",
    "        #print(logprobs[0,:])\n",
    "        toppreds = torch.argsort(logprobs, dim=1, descending=True)[:,0:BEAM_WIDTH] # BATCH_SIZE x BEAM_WIDTH\n",
    "        #print(toppreds[0,:])\n",
    "        #print(logprobs[0,:][toppreds[0,:]])\n",
    "        b_string[:,i,:] = toppreds\n",
    "        for b in range(BATCH_SIZE):\n",
    "            for c in range(BEAM_WIDTH):\n",
    "                b_probs[tuple((b,c))][0] += logprobs[b, toppreds[b,c]]\n",
    "                b_probs[tuple((b,c))][1] += 1\n",
    "    else: # if predicting the word for positions 2+, compare top BEAM_WIDTH preds for each of BEAM_WIDTH strings\n",
    "        # temporary storage\n",
    "        curr_probs = {}\n",
    "        curr_string = torch.zeros(BATCH_SIZE, i+1, BEAM_WIDTH)\n",
    "\n",
    "        for j in range(BEAM_WIDTH):\n",
    "            word_input = b_string[:,i-1,j].long()\n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = attn_context2trg(word_input, \n",
    "                                                                                                      decoder_context, \n",
    "                                                                                                      decoder_hidden, \n",
    "                                                                                                      encoder_outputs)\n",
    "            logprobs = lsm(decoder_output.detach()) # unsorted log probs\n",
    "            sortedpreds = torch.argsort(logprobs, dim=1, descending=True) # sorted words\n",
    "            toppreds = sortedpreds[:,0:BEAM_WIDTH] # top words\n",
    "            \n",
    "            # check if any top preds are </s>\n",
    "            for b in range(BATCH_SIZE):\n",
    "                if END_TKN in toppreds[b,:]: # if </s> in top preds\n",
    "                    # track finished strings\n",
    "                    done_string = torch.cat((b_string[b,0:i,j],torch.tensor([END_TKN], device='cuda').float()))\n",
    "                    done_prob = b_probs[tuple((b,j))][0] + logprobs[b,END_TKN]\n",
    "                    done[b].append([done_string, done_prob, done_string.shape[0]])\n",
    "                    # replace </s> with 4th best pred\n",
    "                    done_idx = (toppreds[b,:] == END_TKN).nonzero()\n",
    "                    toppreds[b,done_idx] = sortedpreds[b,BEAM_WIDTH]\n",
    "               \n",
    "            if j == 0: # if preds are from first beam, take top BEAM_WIDTH preds (temporarily)\n",
    "                for b in range(BATCH_SIZE):\n",
    "                    for c in range(BEAM_WIDTH):\n",
    "                        new_b_prob = b_probs[tuple((b,j))][0] + logprobs[b,toppreds[b,c]]\n",
    "                        curr_probs[tuple((b,c))] = new_b_prob # set top prob\n",
    "                        curr_string[b,0:i,c] = b_string[b,0:i,j] # set sentence\n",
    "                        curr_string[b,i,c] = toppreds[b,c] # set top word\n",
    "            else:\n",
    "                for b in range(BATCH_SIZE):\n",
    "                    for c in range(BEAM_WIDTH): # proposed strings\n",
    "                        replaced = False\n",
    "                        for d in range(BEAM_WIDTH): # existing strings\n",
    "                            new_b_prob = b_probs[tuple((b,j))][0] + logprobs[b,toppreds[b,c]]\n",
    "                            if new_b_prob > curr_probs[tuple((b,d))] and not replaced:\n",
    "                                curr_probs[tuple((b,d))] = new_b_prob # update top prob\n",
    "                                curr_string[b,0:i,d] = b_string[b,0:i,j] # update sentence\n",
    "                                curr_string[b,i,d] = toppreds[b,c] # update top word\n",
    "                                replaced = True\n",
    "        #print(b_string[:,0:i+2,:].shape, curr_string.shape)                        \n",
    "        b_string[:,0:i+1,:] = curr_string\n",
    "        for b in range(BATCH_SIZE):\n",
    "            for c in range(BEAM_WIDTH):\n",
    "                b_probs[tuple((b,c))][0] = curr_probs[tuple((b,c))]\n",
    "                b_probs[tuple((b,c))][1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for b in range(BATCH_SIZE):\n",
    "    for c in range(BEAM_WIDTH):\n",
    "        if b_string[b,-1,c] == END_TKN:\n",
    "            done_string = b_string[b,:,c]\n",
    "            done_prob = b_probs[tuple((b,c))][0]\n",
    "            done_len = b_probs[tuple((b,c))][1]\n",
    "            done[b].append([done_string, done_prob, done_len])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'The', 'problem', 'was', 'very', 'expensive', 'and', 'it', '.', '</s>']\n",
      "['<s>', 'The', 'problem', 'is', 'it', ',', 'and', 'use', 'it', 'goes', 'very', 'difficult', '.', '</s>']\n",
      "['<s>', 'The', 'problem', 'is', 'it', ',', 'and', 'use', 'it', '.', '</s>']\n",
      "['<s>', 'And', 'the', 'result', 'is', 'the', '<unk>', '.', '</s>']\n",
      "['<s>', 'And', 'the', 'outcome', 'is', '<unk>', '.', '</s>']\n",
      "['<s>', 'And', 'the', 'result', 'is', 'the', '<unk>', '</s>']\n",
      "['<s>', 'So', 'ca', \"n't\", 'do', 'that', \"'s\", '<unk>', '.', '</s>']\n",
      "['<s>', 'So', 'ca', \"n't\", 'do', 'that', \"'s\", 'way', '.', '</s>']\n",
      "['<s>', 'So', 'ca', \"n't\", 'do', 'that', \"'s\", 'difference', '.', '</s>']\n",
      "['<s>', 'Of', 'course', 'we', '<unk>', '.', '</s>']\n",
      "['<s>', 'Of', 'course', 'we', 'do', 'dawn', '.', '</s>']\n",
      "['<s>', 'Of', 'course', 'we', 'do', 'dawn', 'are', \"n't\", '.', '</s>']\n",
      "['<s>', 'And', 'it', 'agreed', ',', 'and', 'everyone', '.', '</s>']\n",
      "['<s>', 'And', 'it', 'agreed', ',', 'and', 'agreed', '.', '</s>']\n",
      "['<s>', 'And', 'it', 'agreed', ',', 'and', 'forth', '.', '</s>']\n",
      "['<s>', 'These', 'are', '<unk>', '.', '</s>']\n",
      "['<s>', 'That', 'are', '<unk>', '.', '</s>']\n",
      "['<s>', 'This', 'is', '<unk>', '.', '</s>']\n",
      "['<s>', '96', 'percent', 'of', '<unk>', 'them', '.', '</s>']\n",
      "['<s>', '96', 'percent', 'of', '<unk>', 'themselves', '.', '</s>']\n",
      "['<s>', '96', 'percent', 'of', '<unk>', '<unk>', 'themselves', '.', '</s>']\n",
      "['<s>', 'How', 'small', ',', 'neurons', '.', '</s>']\n",
      "['<s>', 'How', 'small', ',', 'can', 'see', 'a', 'insect', '.', '</s>']\n",
      "['<s>', 'How', 'small', ',', 'can', 'look', 'at', 'insect', '.', '</s>']\n",
      "['<s>', 'One', 'is', 'the', '<unk>', ',', 'and', 'the', 'other', 'is', 'the', '<unk>', '.', '</s>']\n",
      "['<s>', 'One', 'is', 'the', '<unk>', ',', 'and', 'the', 'other', 'is', 'a', '<unk>', '.', '</s>']\n",
      "['<s>', 'One', 'is', 'the', '<unk>', ',', 'and', 'again', ',', 'the', '<unk>', 'in', 'the', 'other', 'are', 'the', '<unk>', '.', '</s>']\n",
      "['<s>', 'This', 'is', 'the', '<unk>', 'hospital', '.', '</s>']\n",
      "['<s>', 'This', 'is', 'the', '<unk>', 'hospital', 'in', 'the', 'day', '.', '</s>']\n",
      "['<s>', 'This', 'is', 'the', '<unk>', 'hospital', 'in', 'the', '<unk>', '.', '</s>']\n",
      "['<s>', 'At', 'the', 'Americans', 'did', 'not', 'eating', '<unk>', '.', '</s>']\n",
      "['<s>', 'At', 'the', 'Americans', 'do', \"n't\", '<unk>', '.', '</s>']\n",
      "['<s>', 'At', 'the', 'Americans', 'did', 'not', 'eating', 'the', 'politics', '.', '</s>']\n",
      "['<s>', 'We', \"'ve\", 'got', 'of', 'the', 'danger', 'is', \"n't\", 'there', '.', '</s>']\n",
      "['<s>', 'We', \"'ve\", 'got', 'of', 'the', 'danger', '.', '</s>']\n",
      "['<s>', 'We', \"'ve\", 'got', 'of', 'the', 'danger', 'is', \"n't\", 'there', '?', '</s>']\n",
      "['<s>', 'And', 'we', ',', 'of', 'course', ',', 'about', '<unk>', '.', '</s>']\n",
      "['<s>', 'And', 'of', 'course', '--', 'of', 'course', 'about', 'kin', '.', '</s>']\n",
      "['<s>', 'And', 'we', ',', 'of', 'course', ',', 'about', '<unk>', '<unk>', '.', '</s>']\n",
      "['<s>', 'Our', 'fault', 'is', 'the', 'company', 'that', \"'s\", 'food', '.', '</s>']\n",
      "['<s>', 'Our', 'fault', 'is', 'the', 'company', ',', '</s>']\n",
      "['<s>', 'Our', 'fault', 'is', 'the', 'company', 'that', \"'s\", 'way', '.', '</s>']\n",
      "['<s>', 'We', 'need', 'light', 'to', 'do', 'we', 'do', '.', '</s>']\n",
      "['<s>', 'We', 'need', 'light', 'to', 'do', 'we', \"'re\", '.', '</s>']\n",
      "['<s>', 'We', 'need', 'to', 'see', '.', '</s>']\n",
      "['<s>', 'But', 'they', \"'re\", 'not', 'in', 'London', '.', '</s>']\n",
      "['<s>', 'They', \"'re\", 'not', 'in', 'London', '.', '</s>']\n",
      "['<s>', 'They', 'are', 'not', 'in', 'London', '.', '</s>']\n",
      "['<s>', 'They', 'have', 'written', '100', 'ways', '.', '</s>']\n",
      "['<s>', 'They', 'have', 'written', '100', 'years', '.', '</s>']\n",
      "['<s>', 'They', 'have', 'written', '100', 'years', '</s>']\n",
      "['<s>', 'He', \"'s\", 'going', 'out', '.', '</s>']\n",
      "['<s>', 'He', \"'s\", 'coming', 'out', 'of', 'course', '.', '</s>']\n",
      "['<s>', 'He', \"'s\", 'coming', 'out', 'of', 'course', ',', '</s>']\n",
      "['<s>', 'Why', 'do', 'we', 'feel', 'that', '?', '</s>']\n",
      "['<s>', 'Why', 'do', 'we', 'feel', 'this', '?', '</s>']\n",
      "['<s>', 'Why', 'are', 'gun', 'than', 'this', '?', '</s>']\n",
      "['<s>', 'I', 'hope', 'that', 'this', 'is', 'an', 'incredible', 'story', '.', '</s>']\n",
      "['<s>', 'I', 'hope', 'that', 'this', 'is', 'a', 'amazing', 'story', '.', '</s>']\n",
      "['<s>', 'I', 'hope', 'you', 'have', 'been', 'an', 'amazing', 'story', '.', '</s>']\n",
      "['<s>', 'There', \"'s\", 'going', 'to', 'another', '.', '</s>']\n",
      "['<s>', 'There', 'is', 'another', 'source', 'of', 'them', '<pad>', '<pad>', '<pad>', '.', '</s>']\n",
      "['<s>', 'There', 'is', 'another', 'source', 'of', 'them', '<pad>', '<pad>', '<pad>', \"'s\", '<unk>', '.', '</s>']\n",
      "['<s>', 'And', 'it', 'was', 'probably', 'the', '<unk>', 'country', '.', '</s>']\n",
      "['<s>', 'And', 'it', 'was', 'probably', 'the', 'most', '<unk>', '.', '</s>']\n",
      "['<s>', 'And', 'it', 'was', 'probably', 'the', 'most', 'country', '.', '</s>']\n",
      "['<s>', 'And', 'for', 'two', 'years', ',', 'you', 'could', 'see', 'anything', 'else', '.', '</s>']\n",
      "['<s>', 'And', 'for', 'two', 'years', '.', '</s>']\n",
      "['<s>', 'And', 'for', 'two', 'years', ',', '</s>']\n",
      "['<s>', 'And', 'at', 'a', 'sphere', ',', 'the', 'circle', 'you', '.', '</s>']\n",
      "['<s>', 'And', 'on', 'a', 'sphere', ',', '<unk>', 'circle', 'that', 'can', '.', '</s>']\n",
      "['<s>', 'And', 'on', 'a', 'sphere', ',', '<unk>', 'circle', 'that', 'can', ',', '</s>']\n",
      "['<s>', 'We', 'have', 'different', 'scenarios', ',', 'and', 'the', 'picture', 'looks', 'like', 'this', 'image', '.', '</s>']\n",
      "['<s>', 'We', 'have', 'different', 'scenarios', ',', 'and', 'the', 'picture', 'looked', '.', '</s>']\n",
      "['<s>', 'We', 'have', 'different', 'scenarios', ',', 'and', 'the', 'picture', 'looks', 'like', 'this', 'picture', 'of', 'the', 'picture', '.', '</s>']\n",
      "['<s>', 'Time', 'is', 'in', 'the', 'time', '.', '</s>']\n",
      "['<s>', 'Time', 'is', 'at', 'the', 'time', '.', '</s>']\n",
      "['<s>', 'Time', 'is', 'at', 'that', 'time', '.', '</s>']\n",
      "['<s>', 'And', 'what', 'I', 'found', 'was', 'the', 'theater', '.', '</s>']\n",
      "['<s>', 'And', 'what', 'I', 'found', 'the', 'theater', '.', '</s>']\n",
      "['<s>', 'And', 'I', 'found', 'was', 'the', 'theater', '.', '</s>']\n",
      "['<s>', 'Put', 'this', '.', '</s>']\n",
      "['<s>', 'Put', 'aside', '.', '</s>']\n",
      "['<s>', 'Put', 'it', 'was', '<unk>', '.', '</s>']\n",
      "['<s>', 'And', 'the', 'answer', 'is', 'no', 'matter', '.', '</s>']\n",
      "['<s>', 'And', 'the', 'answer', 'is', 'no', 'longer', '.', '</s>']\n",
      "['<s>', 'And', 'the', 'answer', 'is', 'nothing', 'has', 'nothing', '.', '</s>']\n",
      "['<s>', 'It', \"'s\", 'kind', 'of', 'structure', '.', '</s>']\n",
      "['<s>', 'It', \"'s\", 'sort', 'of', 'structure', '.', '</s>']\n",
      "['<s>', 'It', \"'s\", 'sort', 'of', 'shape', '.', '</s>']\n",
      "['<s>', 'And', 'you', 'see', 'a', 'little', 'line', 'through', '.', '</s>']\n",
      "['<s>', 'And', 'you', 'see', 'a', 'little', 'line', '.', '</s>']\n",
      "['<s>', 'And', 'you', 'see', 'a', 'little', 'line', 'by', 'the', 'picture', '.', '</s>']\n",
      "['<s>', 'So', 'we', 'get', 'that', 'information', '.', '</s>']\n",
      "['<s>', 'So', 'we', 'get', 'this', 'information', 'in', 'the', 'way', '.', '</s>']\n",
      "['<s>', 'So', 'we', 'get', 'this', 'information', 'of', 'the', 'body', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.7\n",
    "K = 3\n",
    "for b in range(BATCH_SIZE):\n",
    "    normalized_probs = torch.tensor([], device='cuda')\n",
    "    for sentence in range(len(done[b])):\n",
    "        normalized = torch.tensor([done[b][sentence][1]/done[b][sentence][2]**alpha], device='cuda')\n",
    "        normalized_probs = torch.cat((normalized_probs,normalized),0)\n",
    "    top = torch.argsort(normalized_probs, descending=True)[0:K]\n",
    "    for k in range(K):\n",
    "        best = done[b][top[k]]\n",
    "        if padding:\n",
    "            m = nn.ConstantPad1d((0, max_len - best[2]), EN.vocab.stoi['<pad>'])\n",
    "            predictions[b].append(m(best[0].long()))\n",
    "        else:\n",
    "            predictions[b].append(best[0].long())\n",
    "        print([EN.vocab.itos[i] for i in best[0].long()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['So', 'if', 'none', 'of', 'that', 'things', 'is', '?', 'problem', '?', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'What', 'if', 'none', 'of', 'these', 'things', 'is', 'the', 'problem', '?', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "encoder_outputs, encoder_hidden = attn_seq2context(src)\n",
    "decoder_context = torch.zeros(BATCH_SIZE, context_size, device='cuda') # 32 x 500\n",
    "decoder_hidden = encoder_hidden\n",
    "\n",
    "sentence = []\n",
    "trg = batch.trg.values.transpose(0,1)\n",
    "\n",
    "b = 0\n",
    "for j in range(trg.shape[1] - 1):\n",
    "    word_input = trg[:,j]\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = attn_context2trg(word_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "    sentence.extend([torch.argmax(decoder_output.detach()[b,:],dim=0)])\n",
    "print([EN.vocab.itos[i] for i in sentence])\n",
    "print([EN.vocab.itos[i] for i in trg[b,:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'What', 'if', 'neither', 'is', '?', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'is', 'the', 'problem', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'problem', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'is', 'the', 'problem', '?', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'problem', '?', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', 'that', '?', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', \"n't\", 'the', 'problem', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', 'that', 'the', 'problem', '?', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', \"n't\", 'the', 'problem', 'is', 'that', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', 'that', 'the', 'problem', 'is', 'that', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', \"n't\", 'the', 'problem', 'is', 'that', '?', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', \"n't\", 'the', 'problem', 'is', 'the', 'problem', '?', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', \"n't\", 'the', 'problem', 'is', 'the', 'problem', ':', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', \"n't\", 'the', 'problem', 'is', 'the', 'problem', '?', '\"', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', \"n't\", 'the', 'problem', 'is', 'the', 'problem', '<pad>', '<pad>', '<pad>', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', \"n't\", 'the', 'problem', 'is', 'the', 'problem', '?', 'Right', '?', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', \"n't\", 'the', 'problem', 'is', 'the', 'problem', '?', '\"', '<pad>', '<pad>', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', \"n't\", 'the', 'problem', 'is', 'the', 'problem', '?', 'Right', '?', '<pad>', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', \"n't\", 'the', 'problem', 'is', 'the', 'problem', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', \"n't\", 'the', 'problem', 'is', 'the', 'problem', '?', '\"', '<pad>', '<pad>', '<pad>', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', \"n't\", 'the', 'problem', 'is', 'the', 'problem', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', \"n't\", 'the', 'problem', 'is', 'the', 'problem', '?', '\"', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', \"n't\", 'the', 'problem', 'is', 'the', 'problem', '?', 'Right', '?', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'What', 'if', 'none', 'of', 'these', 'things', 'is', 'the', 'problem', '?', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(done[b])):\n",
    "    print([EN.vocab.itos[i] for i in done[b][j][0].long()])\n",
    "for j in range(BEAM_WIDTH):\n",
    "    print([EN.vocab.itos[i] for i in b_string[b,:,j].long()])\n",
    "print([EN.vocab.itos[i] for i in trg[b,:].long()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:\n",
      "['<s>', 'Once', 'the', 'parasites', ',', 'nothing', 'to', 'raise', 'the', 'parasites', '.', 'Okay', ',', 'right', ',', 'the', '<unk>', 'did']\n",
      "['<s>', 'Once', 'the', 'parasites', ',', 'nothing', 'to', 'raise', 'the', 'parasites', '.', 'Okay', '?', 'the', 'parasites', ',', 'the', '<unk>']\n",
      "['<s>', 'Once', 'the', 'parasites', ',', 'nothing', 'to', 'raise', 'the', 'parasites', '.', 'Okay', '?', '<unk>', '<unk>', ',', 'the', 'report']\n",
      "['<s>', 'Once', 'the', 'parasites', ',', 'nothing', 'to', 'raise', 'the', 'parasites', '.', 'Okay', '?', '<unk>', '<unk>', ',', 'the', '<unk>']\n",
      "['<s>', 'Once', 'the', 'parasites', ',', 'nothing', 'to', 'raise', 'the', 'parasites', '.', 'Okay', '?', 'the', 'parasites', ',', 'you', 'have']\n",
      "actual:\n",
      "['<s>', 'Once', 'the', 'parasites', 'get', 'in', ',', 'the', 'hosts', 'do', \"n't\", 'get', 'a', 'say', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'And', 'nobody', 'can', 'can', 'do', 'aging', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'And', 'nobody', 'can', 'can', 'do', 'aging', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'reasonably']\n",
      "['<s>', 'And', 'nobody', 'can', 'can', 'do', 'aging', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'utter']\n",
      "['<s>', 'And', 'nobody', 'can', 'can', 'do', 'aging', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'noticing']\n",
      "['<s>', 'And', 'nobody', 'can', 'can', 'do', 'aging', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', ':']\n",
      "actual:\n",
      "['<s>', 'And', 'nobody', 'knows', 'if', 'they', 'really', 'can', 'live', 'forever', ',', 'or', 'what', 'keeps', 'them', 'from', 'aging', '.', '</s>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'We', 'all', 'that', 'long', 'pathway', '.', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'We', 'depend', 'on', 'the', '<unk>', ':', ')', 'I', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'We', 'depend', 'on', 'the', '<unk>', ':', ')', 'We', \"'re\", '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'We', 'depend', 'on', 'the', '<unk>', ':', 'Right', '?', 'Let', \"'s\", 'out', ':', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'We', 'depend', 'on', 'the', '<unk>', ':', 'Right', '?', 'Let', \"'s\", '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "actual:\n",
      "['<s>', 'We', \"'re\", 'all', 'trapped', 'in', 'these', 'lines', '!', 'These', 'long', 'lines', 'trying', 'to', 'get', 'on', 'an', 'airplane', '!', '</s>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'This', 'is', 'the', 'stored', 'right', 'in', 'the', 'winds', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'This', 'is', 'the', 'stored', 'right', 'in', 'the', 'stream', 'is', 'there', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'This', 'is', 'the', 'stored', 'right', 'in', 'the', 'stream', 'is', 'it', \"'s\", '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'This', 'is', 'the', 'stored', 'right', 'in', 'the', 'stream', 'is', 'it', \"'s\", ':', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'This', 'is', 'the', 'stored', 'right', 'in', 'the', 'stream', 'is', 'it', \"'s\", '.', '\"', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "actual:\n",
      "['<s>', 'And', 'this', 'is', 'working', 'on', 'the', 'stored', 'winds', 'in', 'the', 'bottles', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'And', 'the', 'cost', 'of', 'the', 'combinations', ':', 'the', '<unk>', ':', 'smiley', 'than', 'the', '-', 'hand', 'as', 'a']\n",
      "['<s>', 'And', 'the', 'cost', 'of', 'the', 'combinations', ':', 'the', '<unk>', ':', 'smiley', 'than', 'the', '-', 'hand', '<unk>', '<unk>']\n",
      "['<s>', 'And', 'the', 'cost', 'of', 'the', 'combinations', ':', 'the', '<unk>', ':', 'smiley', 'than', 'the', '-', 'Earth', 'limited', '.']\n",
      "['<s>', 'And', 'the', 'cost', 'of', 'the', 'combinations', ':', 'the', '<unk>', ':', 'smiley', 'than', 'the', '-', 'Earth', 'limited', ',']\n",
      "['<s>', 'And', 'the', 'cost', 'of', 'the', 'combinations', ':', 'the', '<unk>', ':', 'smiley', 'than', 'the', '-', 'Earth', 'limited', 'in']\n",
      "actual:\n",
      "['<s>', 'And', 'with', 'the', 'cost', 'of', 'these', 'tools', 'being', 'substantially', 'lower', 'than', 'the', 'bribe', 'demanded', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'And', 'we', 'were', '--', 'the', 'face', '.', '\"', 'And', 'we', \"'ve\", 'heard', 'of', 'course', ',', 'right', 'now']\n",
      "['<s>', 'And', 'we', 'were', '--', 'the', 'face', '.', '\"', 'And', 'we', \"'ve\", 'heard', 'of', 'this', 'is', 'here', 'is']\n",
      "['<s>', 'And', 'we', 'were', '--', 'the', 'face', '.', '\"', 'And', 'we', \"'ve\", 'heard', 'of', 'this', 'is', 'about', 'this']\n",
      "['<s>', 'And', 'we', 'were', '--', 'the', 'face', '.', '\"', 'And', 'we', \"'ve\", 'heard', 'of', 'this', 'is', 'now', '!']\n",
      "['<s>', 'And', 'we', 'were', '--', 'the', 'face', '.', '\"', 'And', 'we', \"'ve\", 'heard', 'of', 'this', 'is', 'there', \"'s\"]\n",
      "actual:\n",
      "['<s>', 'And', 'we', 'were', 'in', 'the', 'desert', '--', 'Richard', 'Wurman', ':', 'That', \"'s\", 'the', 'end', 'of', 'this', 'talk', '!', '</s>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'I', \"'m\", 'one', 'an', 'example', ':', '\"', 'is', 'a', 'one', 'is', ',', 'an', 'example', '.', 'China', '.']\n",
      "['<s>', 'I', \"'m\", 'one', 'an', 'example', ':', 'I', \"'m\", 'a', 'example', '.', 'I', \"'m\", 'point', ':', 'China', '.']\n",
      "['<s>', 'I', \"'m\", 'one', 'an', 'example', ':', 'I', \"'m\", 'a', 'example', '.', 'I', \"'m\", 'point', ':', 'China', 'is']\n",
      "['<s>', 'I', \"'m\", 'one', 'an', 'example', ':', 'I', \"'m\", 'a', 'example', '.', 'I', \"'m\", ',', 'the', '<unk>', ':']\n",
      "['<s>', 'I', \"'m\", 'one', 'an', 'example', ':', 'I', \"'m\", 'a', 'example', '.', 'I', \"'m\", ',', 'the', '<unk>', '.']\n",
      "actual:\n",
      "['<s>', 'I', \"'ll\", 'give', 'an', 'example', '.', 'China', 'is', 'a', '<unk>', 'country', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'What', 'has', 'the', 'War', 'called', 'the', 'world', 'to', 'do', 'that', \"'s\", 'Earth', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'What', 'has', 'the', 'War', 'called', 'the', 'world', 'to', 'the', 'world', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'What', 'has', 'the', 'War', 'called', 'the', 'world', 'to', 'the', 'world', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '?', '<pad>']\n",
      "['<s>', 'What', 'has', 'the', 'War', 'called', 'the', 'world', 'to', 'the', 'world', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', \"'s\", 'a']\n",
      "['<s>', 'What', 'has', 'the', 'War', 'called', 'the', 'world', 'to', 'the', 'world', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', \"'s\", '<unk>']\n",
      "actual:\n",
      "['<s>', 'What', 'has', 'the', 'War', 'on', 'Drugs', 'done', 'to', 'the', 'world', '?', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'Come', 'on', 'you', \"'re\", 'on', 'it', \"'s\", 'the', 'way', 'it', \"'s\", 'working', 'on', 'all', 'way', 'to', 'turn']\n",
      "['<s>', 'Come', 'on', 'you', \"'re\", 'on', 'it', \"'s\", 'the', 'way', 'it', \"'s\", 'working', 'on', 'all', 'way', 'to', 'get']\n",
      "['<s>', 'Come', 'on', 'you', \"'re\", 'on', 'it', \"'s\", 'the', 'way', 'it', \"'s\", 'working', 'on', 'all', 'way', 'to', 'keep']\n",
      "['<s>', 'Come', 'on', 'you', \"'re\", 'on', 'it', \"'s\", 'the', 'way', 'it', \"'s\", 'working', 'on', 'all', 'way', 'to', 'make']\n",
      "['<s>', 'Come', 'on', 'you', \"'re\", 'on', 'it', \"'s\", 'the', 'way', 'it', \"'s\", 'working', 'on', 'all', 'way', 'to', 'take']\n",
      "actual:\n",
      "['<s>', 'Come', 'on', ',', 'keep', 'up', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'I', 'really', ',', 'really', ',', 'really', '<unk>', '-', 'on', 'the', 'design', ':', 'I', 'really', 'interesting', 'thing', '.']\n",
      "['<s>', 'I', 'really', ',', 'really', ',', 'really', '<unk>', '-', 'on', 'the', 'design', ':', 'I', 'really', 'interesting', 'thing', ':']\n",
      "['<s>', 'I', 'really', ',', 'really', ',', 'really', '<unk>', '-', 'on', 'the', 'design', ':', 'I', 'really', 'interesting', 'way', '.']\n",
      "['<s>', 'I', 'really', ',', 'really', ',', 'really', '<unk>', '-', 'on', 'the', 'design', ':', 'I', 'really', 'interesting', 'way', 'is']\n",
      "['<s>', 'I', 'really', ',', 'really', ',', 'really', '<unk>', '-', 'on', 'the', 'design', ':', 'I', 'really', 'interesting', 'way', 'to']\n",
      "actual:\n",
      "['<s>', 'I', 'really', 'got', 'close', 'to', 'design', 'again', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'You', ',', 'there', 'is', 'no', 'time', 'and', 'growth', 'is', 'it', \"'s\", 'is', 'a', 'lot', 'about', 'growth', ':']\n",
      "['<s>', 'You', ',', 'there', 'is', 'no', 'time', 'and', 'growth', 'is', 'it', \"'s\", 'is', 'a', 'lot', 'of', 'growth', 'rape']\n",
      "['<s>', 'You', ',', 'there', 'is', 'no', 'time', 'and', 'growth', 'is', 'it', \"'s\", 'is', 'a', 'lot', 'of', 'growth', 'malnutrition']\n",
      "['<s>', 'You', ',', 'there', 'is', 'no', 'time', 'and', 'growth', 'is', 'it', \"'s\", 'is', 'a', 'lot', 'of', 'growth', 'desalination']\n",
      "['<s>', 'You', ',', 'there', 'is', 'no', 'time', 'and', 'growth', 'is', 'it', \"'s\", 'is', 'a', 'lot', 'about', 'fruition', ':']\n",
      "actual:\n",
      "['<s>', 'Obviously', ',', 'there', \"'s\", 'no', 'guarantee', 'that', 'it', 'can', 'be', 'a', 'time', 'of', 'fruition', 'and', 'growth', '.', '</s>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'For', 'instance', 'in', 'a', 'large', ',', 'in', 'a', 'material', 'we', 'know', 'me', 'know', ',', 'in', 'a', 'bit']\n",
      "['<s>', 'For', 'instance', 'in', 'a', 'large', ',', 'in', 'a', 'material', 'we', 'know', 'me', 'know', ',', 'in', 'the', '<unk>']\n",
      "['<s>', 'For', 'instance', 'in', 'a', 'large', ',', 'in', 'a', 'material', 'we', 'know', 'me', 'know', ',', 'in', '-', 'time']\n",
      "['<s>', 'For', 'instance', 'in', 'a', 'large', ',', 'in', 'a', 'material', 'we', 'know', 'me', 'know', ',', 'for', 'a', 'piece']\n",
      "['<s>', 'For', 'instance', 'in', 'a', 'large', ',', 'in', 'a', 'material', 'we', 'know', 'me', 'know', ',', 'in', 'the', 'thing']\n",
      "actual:\n",
      "['<s>', '<unk>', '<unk>', 'in', 'a', 'material', 'for', 'instance', ',', 'always', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'These', 'birds', 'they', 'flying', 'into', 'the', 'story', 'they', \"'re\", 'that', 'put', 'them', 'into', 'this', 'they', 'were', 'she']\n",
      "['<s>', 'These', 'birds', 'they', 'flying', 'into', 'the', 'story', 'they', \"'re\", 'that', 'put', 'them', 'into', 'this', 'they', 'were', 'they']\n",
      "['<s>', 'These', 'birds', 'they', 'flying', 'into', 'the', 'story', 'they', \"'re\", 'that', 'put', 'them', 'into', 'the', 'way', 'in', 'it']\n",
      "['<s>', 'These', 'birds', 'they', 'flying', 'into', 'the', 'story', 'they', \"'re\", 'that', 'put', 'them', 'into', 'the', 'way', 'in', 'the']\n",
      "['<s>', 'These', 'birds', 'they', 'flying', 'into', 'the', 'story', 'they', \"'re\", 'that', 'put', 'them', 'into', 'this', 'they', 'go', '.']\n",
      "actual:\n",
      "['<s>', 'These', 'birds', 'make', 'a', 'living', 'by', 'diving', 'into', 'the', 'water', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'We', \"'ve\", 'asked', 'for', 'us', \"'re\", 'doing', 'it', 'just', 'we', 'do', 'it', \"'s\", 'it', ',', 'we', 'do']\n",
      "['<s>', 'We', \"'ve\", 'asked', 'for', 'us', \"'re\", 'doing', 'it', 'just', 'we', 'do', 'it', \"'s\", 'it', ',', 'we', 'just']\n",
      "['<s>', 'We', \"'ve\", 'asked', 'for', 'us', \"'re\", 'doing', 'it', 'just', 'we', 'do', 'it', \"'s\", 'it', ',', 'we', \"'re\"]\n",
      "['<s>', 'We', \"'ve\", 'asked', 'for', 'us', \"'re\", 'doing', 'it', \"'s\", 'just', 'do', 'that', 'we', '.', 'And', 'we', 'do']\n",
      "['<s>', 'We', \"'ve\", 'asked', 'for', 'us', \"'re\", 'doing', 'it', 'just', 'we', 'do', 'it', \"'s\", 'it', ',', 'here', '.']\n",
      "actual:\n",
      "['<s>', 'We', \"'ve\", 'not', 'asked', 'anybody', \"'s\", 'permission', 'to', 'do', 'this', ',', 'we', \"'re\", 'just', 'doing', 'it', '.', '</s>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', '<unk>', ',', 'turn', 'it', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', '<unk>', ',', 'turn', 'it', '<pad>', '<pad>', '<pad>', ',', 'I', 'guess', '.', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', '<unk>', ',', 'turn', 'it', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '.', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', '<unk>', ',', 'turn', 'it', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '.', '<pad>', '<pad>']\n",
      "['<s>', '<unk>', ',', 'turn', 'it', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'brilliant', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "actual:\n",
      "['<s>', '<unk>', 'split', 'in', 'half', 'and', '<unk>', ',', 'and', 'mythology', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'But', 'that', 'changed', ',', 'right', ':', '<unk>', ',', 'right', 'there', 'is', 'a', 'lot', 'of', ',', 'right', '?']\n",
      "['<s>', 'But', 'that', 'changed', ',', 'right', ':', '<unk>', ',', 'right', 'there', 'is', 'a', 'lot', 'of', 'it', \"'s\", 'the']\n",
      "['<s>', 'But', 'that', 'changed', ',', 'right', ':', '<unk>', ',', 'right', 'there', 'is', 'a', 'lot', 'of', ',', 'it', '.']\n",
      "['<s>', 'But', 'that', 'changed', ',', 'right', ':', '<unk>', ',', 'right', 'there', 'is', 'a', 'lot', 'of', ',', 'it', '?']\n",
      "['<s>', 'But', 'that', 'changed', ',', 'right', ':', '<unk>', ',', 'right', 'there', 'is', 'a', 'lot', 'of', ',', 'you', '.']\n",
      "actual:\n",
      "['<s>', 'But', 'this', 'has', 'changed', ',', 'okay', '?', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'That', \"'s\", '--', 'Thank', 'you', 'know', 'it', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'That', \"'s\", '--', 'Thank', 'you', 'know', 'the', 'way', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'That', \"'s\", '--', 'thank', 'you', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '?', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'That', \"'s\", '--', 'thank', 'you', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', ':', 'Thank', \"n't\", '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'That', \"'s\", '--', 'thank', 'you', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', ':', 'Thank', \"n't\", \"'s\", '<pad>', '<pad>']\n",
      "actual:\n",
      "['<s>', 'So', 'this', '--', 'Thank', 'you', 'very', 'much', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'Yes', ',', 'it', \"'s\", 'a', 'little', 'project', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'Yes', ',', 'it', \"'s\", 'a', 'little', 'generator', 'is', 'a', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'Yes', ',', 'it', \"'s\", 'a', 'little', 'generator', '<pad>', '<pad>', 'out', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'Yes', ',', 'it', \"'s\", 'a', 'little', 'generator', '<pad>', '<pad>', 'on', 'a', 'little', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'Yes', ',', 'it', \"'s\", 'a', 'little', 'generator', '<pad>', '<pad>', 'on', '.', 'It', 'is', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "actual:\n",
      "['<s>', 'Yeah', ',', 'it', \"'s\", 'just', 'a', 'fluke', '.', 'It', \"'s\", 'a', '<unk>', 'fluke', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'Well', ',', 'I', 'was', \"n't\", 'been', '<unk>', ',', 'I', 'did', 'this', 'stuff', ':', 'I', 'was', \"n't\", '.']\n",
      "['<s>', 'Well', ',', 'I', 'was', \"n't\", 'been', '<unk>', ',', 'I', 'did', 'this', 'stuff', ':', 'I', 'was', 'not', '.']\n",
      "['<s>', 'Well', ',', 'I', 'was', \"n't\", 'been', '<unk>', ',', 'I', 'did', 'this', 'stuff', ':', 'I', 'was', 'a', '<pad>']\n",
      "['<s>', 'Well', ',', 'I', 'was', \"n't\", 'been', '<unk>', ',', 'I', 'did', 'this', 'stuff', ':', 'I', 'was', \"n't\", ',']\n",
      "['<s>', 'Well', ',', 'I', 'was', \"n't\", 'been', '<unk>', ',', 'I', 'did', 'this', 'stuff', ':', 'I', 'was', \"n't\", 'a']\n",
      "actual:\n",
      "['<s>', 'Now', 'you', 'and', 'I', 'could', 'argue', 'that', 'they', 'probably', 'did', 'not', 'need', 'to', 'touch', 'her', 'breasts', '.', '</s>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', '\"', 'How', 'you', 'guys', '?', '\"', 'Okay', '?', '\"', 'What', 'do', 'you', 'know', '?', '\"', '?', '\"']\n",
      "['<s>', '\"', 'How', 'you', 'guys', '?', '\"', 'Okay', '?', '\"', 'What', 'do', 'you', 'know', '?', '\"', '.', '\"']\n",
      "['<s>', '\"', 'How', 'you', 'guys', '?', '\"', 'Okay', '?', '\"', 'What', 'do', 'you', 'know', '?', '\"', 'about', 'you']\n",
      "['<s>', '\"', 'How', 'you', 'guys', '?', '\"', 'Okay', '?', '\"', 'What', 'do', 'you', 'know', '?', '\"', 'about', 'your']\n",
      "['<s>', '\"', 'How', 'you', 'guys', '?', '\"', 'Okay', '?', '\"', 'What', 'do', 'you', 'know', '?', '\"', 'about', '<unk>']\n",
      "actual:\n",
      "['<s>', '\"', 'How', 'do', 'you', '<unk>', 'up', 'pop', '-', 'up', 'purple', 'paper', 'people', '?', '\"', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', '<unk>', 'up', '.', 'Do', 'a', 'good', 'person', '.', \"'s\", 'a', 'high', 'is', 'a', 'good', ',', 'you', 'know']\n",
      "['<s>', '<unk>', 'up', '.', 'Do', 'a', 'good', 'person', '.', \"'s\", 'a', 'high', 'is', 'a', 'good', ',', 'you', 'can']\n",
      "['<s>', '<unk>', 'up', '.', 'Do', 'a', 'good', 'person', '.', \"'s\", 'a', 'high', 'is', 'a', 'good', 'guy', 'guess', '.']\n",
      "['<s>', '<unk>', 'up', '.', 'Do', 'a', 'good', 'person', '.', \"'s\", 'a', 'high', 'is', 'a', 'good', ',', 'this', 'way']\n",
      "['<s>', '<unk>', 'up', '.', 'Do', 'a', 'good', 'person', '.', \"'s\", 'a', 'high', 'is', 'a', 'good', 'guy', \"'m\", 'a']\n",
      "actual:\n",
      "['<s>', '<unk>', 'up', '.', '<unk>', 'out', '.', 'Be', 'a', 'good', 'person', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'The', 'faucet', 'like', 'that', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'The', 'hell', 'looks', 'on', '2020', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'The', 'hell', 'looks', 'on', '2020', '.', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'The', 'faucet', 'like', 'that', 'looks', '.', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'The', 'faucet', 'like', 'that', 'way', '.', '\"', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "actual:\n",
      "['<s>', 'and', 'the', 'overall', 'picture', 'looks', 'like', 'this', 'by', '2020', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', '<unk>', 'are', \"n't\", 'yet', ',', 'you', \"'re\", 'not', 'that', 'we', 'do', \"n't\", 'make', 'this', 'is', 'not', 'static']\n",
      "['<s>', '<unk>', 'are', \"n't\", 'yet', ',', 'you', \"'re\", 'not', 'that', 'we', 'do', \"n't\", 'be', 'static', '<unk>', ':', '<unk>']\n",
      "['<s>', '<unk>', 'are', \"n't\", 'yet', ',', 'you', \"'re\", 'not', 'that', 'we', 'do', \"n't\", 'make', 'that', 'you', \"'re\", 'not']\n",
      "['<s>', '<unk>', 'are', \"n't\", 'yet', ',', 'you', \"'re\", 'not', 'that', 'we', 'do', \"n't\", 'make', 'this', 'is', \"n't\", 'static']\n",
      "['<s>', '<unk>', 'are', \"n't\", 'yet', ',', 'you', \"'re\", 'not', 'that', 'we', 'do', \"n't\", 'make', 'this', 'is', \"n't\", 'kidding']\n",
      "actual:\n",
      "['<s>', 'Models', 'are', 'not', 'static', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'Over', 'a', 'very', 'old', 'moment', '.', 'It', \"'s\", 'ancient', 'life', 'is', 'a', 'very', 'ancient', 'captures', 'this', 'is']\n",
      "['<s>', 'Over', 'a', 'very', 'old', 'moment', '.', 'It', \"'s\", 'ancient', 'life', 'is', 'a', 'very', 'ancient', 'captures', 'a', 'ancient']\n",
      "['<s>', 'Over', 'a', 'very', 'old', 'moment', '.', 'It', \"'s\", 'ancient', 'life', 'is', 'a', 'very', 'ancient', 'captures', 'that', 'tradition']\n",
      "['<s>', 'Over', 'a', 'very', 'old', 'moment', '.', 'It', \"'s\", 'ancient', 'life', 'is', 'a', 'very', 'ancient', 'captures', 'a', 'very']\n",
      "['<s>', 'Over', 'a', 'very', 'old', 'moment', '.', 'It', \"'s\", 'ancient', 'life', 'is', 'a', 'very', 'ancient', 'argued', 'this', 'old']\n",
      "actual:\n",
      "['<s>', 'Moreover', ',', 'this', 'is', 'a', 'very', 'old', 'state', 'tradition', ',', 'a', 'very', 'old', 'tradition', 'of', '<unk>', '.', '</s>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'And', 'I', \"'m\", 'with', 'this', 'approach', ',', 'again', ',', 'OK', ',', 'what', 'you', 'know', 'with', 'this', 'piece']\n",
      "['<s>', 'And', 'I', \"'m\", 'with', 'this', 'approach', ',', 'again', ',', 'OK', ',', 'what', 'you', 'know', 'with', 'that', '?']\n",
      "['<s>', 'And', 'I', \"'m\", 'with', 'this', 'approach', ',', 'again', ',', 'OK', ',', 'what', 'you', 'know', 'with', 'that', '.']\n",
      "['<s>', 'And', 'I', \"'m\", 'with', 'this', 'approach', ',', 'again', ',', 'OK', ',', 'what', 'you', 'know', 'with', 'example', '?']\n",
      "['<s>', 'And', 'I', \"'m\", 'with', 'this', 'approach', ',', 'again', ',', 'OK', ',', 'I', 'want', 'to', 'you', 'know', '?']\n",
      "actual:\n",
      "['<s>', 'And', 'I', \"'ll\", 'show', 'you', ',', 'if', 'you', 'take', 'this', 'approach', ',', 'what', 'you', 'get', ',', 'OK', '?', '</s>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'And', 'so', ',', 'that', 'we', 'can', '<unk>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'And', 'so', ',', 'that', 'we', 'can', '<unk>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'out', 'of', 'other', 'surfaces', '<pad>']\n",
      "['<s>', 'And', 'so', ',', 'that', 'we', 'can', '<unk>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'out', 'of', 'other', 'of', 'the']\n",
      "['<s>', 'And', 'so', ',', 'that', 'we', 'can', '<unk>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'out', 'of', 'other', 'biologically', 'can']\n",
      "['<s>', 'And', 'so', ',', 'that', 'we', 'can', '<unk>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'out', 'of', 'other', 'biologically', \"'s\"]\n",
      "actual:\n",
      "['<s>', 'And', 'so', 'we', 'can', '<unk>', 'these', 'surfaces', 'biologically', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'In', 'three', 'months', ',', 'we', 'get', ',', 'and', 'we', 'said', ',', 'we', 'have', 'a', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'In', 'three', 'months', ',', 'we', 'get', ',', 'and', 'we', 'said', ',', 'we', 'have', 'a', 'little', '<unk>', '<pad>']\n",
      "['<s>', 'In', 'three', 'months', ',', 'we', 'get', ',', 'and', 'we', 'said', ',', 'we', 'have', 'a', 'tilt', '<pad>', '<pad>']\n",
      "['<s>', 'In', 'three', 'months', ',', 'we', 'get', ',', 'and', 'we', 'said', ',', 'we', 'have', 'a', 'little', '.', 'Here']\n",
      "['<s>', 'In', 'three', 'months', ',', 'we', 'get', ',', 'and', 'we', 'said', ',', 'we', 'have', 'a', 'little', '.', '\"']\n",
      "actual:\n",
      "['<s>', 'And', 'after', 'about', 'three', 'months', ',', 'the', 'nerves', 'grew', 'in', 'a', 'little', 'bit', 'and', 'we', 'could', 'get', 'a', '<unk>', '.', '</s>']\n",
      "predictions:\n",
      "['<s>', 'But', 'it', 'was', 'late', 'than', 'that', \"'s\", 'late', 'with', 'the', 'idea', 'it', 'was', 'too', 'late', 'it', 'was']\n",
      "['<s>', 'But', 'it', 'was', 'late', 'than', 'that', \"'s\", 'late', 'with', 'the', 'idea', 'it', 'was', 'too', 'late', 'it', \"'s\"]\n",
      "['<s>', 'But', 'it', 'was', 'late', 'than', 'that', \"'s\", 'late', 'with', 'the', 'idea', 'it', 'was', 'too', 'late', 'of', 'that']\n",
      "['<s>', 'But', 'it', 'was', 'late', 'than', 'that', \"'s\", 'late', 'with', 'the', 'idea', 'it', 'was', 'too', 'late', 'of', 'the']\n",
      "['<s>', 'But', 'it', 'was', 'late', 'than', 'that', \"'s\", 'late', 'with', 'the', 'idea', 'it', 'was', 'too', 'late', ',', 'too']\n",
      "actual:\n",
      "['<s>', 'But', 'it', 'was', 'too', 'late', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'It', 'led', 'to', 'the', 'next', 'this', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'It', 'led', 'to', 'the', 'next', 'this', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<unk>', '<pad>', '<pad>']\n",
      "['<s>', 'It', 'led', 'to', 'the', 'next', 'this', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'partners', '<pad>', '<pad>']\n",
      "['<s>', 'It', 'led', 'to', 'the', 'next', 'this', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'regulation', '.', '<pad>']\n",
      "['<s>', 'It', 'led', 'to', 'the', 'next', 'this', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'regulation', '.', '\"']\n",
      "actual:\n",
      "['<s>', 'And', 'that', 'moved', 'us', 'to', 'the', 'next', 'level', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'They', 'have', 'a', 'little', 'name', 'is', 'that', 'we', 'have', 'a', 'little', 'of', 'course', ',', 'we', 'called', '<unk>']\n",
      "['<s>', 'They', 'have', 'a', 'little', 'name', 'is', 'that', 'we', 'have', 'a', 'little', 'of', 'course', ',', 'we', 'called', '-']\n",
      "['<s>', 'They', 'have', 'a', 'little', 'name', 'is', 'that', 'we', 'have', 'a', 'little', 'of', 'course', ',', 'the', 'center', '.']\n",
      "['<s>', 'They', 'have', 'a', 'little', 'name', 'is', 'that', 'we', 'have', 'a', 'little', 'of', 'course', ',', 'we', 'calling', '.']\n",
      "['<s>', 'They', 'have', 'a', 'little', 'name', 'is', 'that', 'we', 'have', 'a', 'little', 'of', 'course', ',', 'are', 'we', '.']\n",
      "actual:\n",
      "['<s>', 'They', 'use', 'a', 'skeleton', 'that', 'we', 'call', 'a', '<unk>', 'skeleton', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'XL', ':', 'I', 'actually', '.', '\"', 'it', 'actually', ',', '\"', 'way', ',', 'actually', ',', '\"', '.', '\"']\n",
      "['<s>', 'XL', ':', 'I', 'actually', '.', '\"', 'it', 'actually', ',', '\"', 'way', 'it', 'worked', '.', '\"', '<unk>', '.']\n",
      "['<s>', 'XL', ':', 'I', 'actually', '.', '\"', 'it', 'actually', ',', '\"', 'way', ',', 'actually', ',', '\"', 'I', \"'ve\"]\n",
      "['<s>', 'XL', ':', 'I', 'actually', '.', '\"', 'it', 'actually', ',', '\"', 'way', ',', 'actually', ',', '\"', 'I', 'have']\n",
      "['<s>', 'XL', ':', 'I', 'actually', '.', '\"', 'it', 'actually', ',', '\"', 'way', ',', 'actually', ',', '\"', 'was', 'actually']\n",
      "actual:\n",
      "['<s>', 'XL', ':', '\"', 'Yes', ',', '\"', 'I', 'said', '.', '\"', 'Indeed', 'it', 'worked', '!', '\"', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'It', \"'s\", 'for', 'a', 'ride', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'It', \"'s\", '<unk>', 'and', 'making', 'a', 'ride', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'It', \"'s\", '<unk>', 'and', 'to', 'a', 'much', '<unk>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'It', \"'s\", '<unk>', 'and', 'to', 'a', 'much', '<unk>', '.', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'It', \"'s\", '<unk>', 'and', 'to', 'a', 'much', '<unk>', '?', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "actual:\n",
      "['<s>', 'or', 'asking', 'friends', 'and', 'family', 'for', 'a', 'ride', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "# check out prediction\n",
    "for b in range(BATCH_SIZE):\n",
    "    print('predictions:')\n",
    "    for j in range(BEAM_WIDTH):\n",
    "        print([EN.vocab.itos[i] for i in b_string[b,:,j].long()])\n",
    "    print('actual:')\n",
    "    print([EN.vocab.itos[i] for i in trg[b,:].long()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention:\n",
    "# https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb\n",
    "\n",
    "# normalization:\n",
    "# https://medium.com/machine-learning-bites/deeplearning-series-sequence-to-sequence-architectures-4c4ca89e5654"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# load test set\n",
    "sentences = []\n",
    "for i, l in enumerate(open(\"source_test.txt\"), 1):\n",
    "  sentences.append(re.split(' ', l))'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
