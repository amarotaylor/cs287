{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 287, Homework 3: Neural Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import *\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data, datasets\n",
    "from namedtensor import ntorch, NamedTensor\n",
    "from namedtensor.text import NamedField\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n",
      "1 0\n",
      "1 0\n"
     ]
    }
   ],
   "source": [
    "# split raw data into tokens\n",
    "import spacy\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# add beginning-of-sentence and end-of-sentence tokens to target\n",
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "DE = NamedField(names=('srcSeqlen',), tokenize=tokenize_de)\n",
    "EN = NamedField(names=('trgSeqlen',), tokenize=tokenize_en,\n",
    "                init_token = BOS_WORD, eos_token = EOS_WORD) # only target needs BOS/EOS\n",
    "\n",
    "# download dataset of 200K pairs of sentences\n",
    "# start with MAXLEN = 20\n",
    "MAX_LEN = 20\n",
    "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(DE, EN), \n",
    "                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
    "#print(train.fields)\n",
    "#print(len(train))\n",
    "#print(vars(train[0]))\n",
    "\n",
    "# build vocab, convert words to indices\n",
    "MIN_FREQ = 5\n",
    "DE.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "EN.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
    "#print(DE.vocab.freqs.most_common(10))\n",
    "#print(\"Size of German vocab\", len(DE.vocab))\n",
    "#print(EN.vocab.freqs.most_common(10))\n",
    "#print(\"Size of English vocab\", len(EN.vocab))\n",
    "print(EN.vocab.stoi[\"<s>\"], EN.vocab.stoi[\"</s>\"])\n",
    "print(EN.vocab.stoi[\"<pad>\"], EN.vocab.stoi[\"<unk>\"])\n",
    "print(DE.vocab.stoi[\"<pad>\"], DE.vocab.stoi[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into batches\n",
    "BATCH_SIZE = 32\n",
    "device = torch.device('cuda:0')\n",
    "train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=BATCH_SIZE, device=device,\n",
    "                                                  repeat=False, sort_key=lambda x: len(x.src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to Sequence Learning with Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 500\n",
    "num_layers = 2\n",
    "\n",
    "BEAM_WIDTH = 5\n",
    "max_len = 20\n",
    "\n",
    "attn_seq2context = SequenceModel(len(DE.vocab), context_size, num_layers=num_layers)\n",
    "state_dict = torch.load('best_seq2seq_seq2context.pt')\n",
    "attn_seq2context.load_state_dict(state_dict)\n",
    "attn_seq2context = attn_seq2context.cuda()\n",
    "\n",
    "attn_context2trg = RNNet(input_size=len(EN.vocab), hidden_size=context_size, num_layers=num_layers)\n",
    "state_dict = torch.load('best_seq2seq_context2trg.pt')\n",
    "attn_context2trg.load_state_dict(state_dict)\n",
    "attn_context2trg = attn_context2trg.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test set\n",
    "sentences = []\n",
    "for i, l in enumerate(open(\"source_test.txt\")):\n",
    "    sentences.append(re.split(' ', l))\n",
    "\n",
    "max_sent_len = 0\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i][-1] = '.'\n",
    "    if len(sentences[i]) > max_sent_len:\n",
    "        max_sent_len = len(sentences[i])\n",
    "\n",
    "batch = torch.tensor([], device='cuda')\n",
    "for b in range(len(sentences)):\n",
    "    m = nn.ConstantPad1d((0, max_sent_len - len(sentences[b])), EN.vocab.stoi['<pad>'])\n",
    "    src = m(torch.tensor([DE.vocab.stoi[i] for i in sentences[b]], device='cuda').unsqueeze(0)).float()\n",
    "    batch = torch.cat((batch,src), dim=0)\n",
    "batch_rev = reverse_sequence(batch).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "batch_rev_data = torch.utils.data.TensorDataset(batch_rev)\n",
    "batch_rev_data_loader = torch.utils.data.DataLoader(batch_rev_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_noattn(src, attn_seq2context, attn_context2trg, BEAM_WIDTH = 2, BATCH_SIZE=32, max_len=3,context_size=500,EN=None):\n",
    "    top_p = {}\n",
    "    top_s = {}\n",
    "    items = []\n",
    "    for i in range(BATCH_SIZE):\n",
    "        top_p[i] = []\n",
    "        top_s[i] = []\n",
    "        items.append(i)\n",
    "\n",
    "    encoder_outputs, encoder_hidden = attn_seq2context(src)\n",
    "    #decoder_context = torch.zeros(BATCH_SIZE, context_size, device='cuda') # 32 x 500\n",
    "    decoder_hidden = encoder_hidden\n",
    "    #word_input = (torch.zeros(BATCH_SIZE, device='cuda') + EN.vocab.stoi['<s>']).long()\n",
    "    #decoder_output, decoder_context, decoder_hidden, decoder_attention = attn_context2trg(word_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "    word_input = (torch.zeros((BATCH_SIZE,1), device='cuda') + EN.vocab.stoi['<s>']).long()\n",
    "    decoder_output, decoder_hidden = attn_context2trg(word_input, decoder_hidden)\n",
    "    decoder_output = decoder_output[:,-1,:]\n",
    "    \n",
    "    next_words = torch.argsort(lsm2(decoder_output), dim=1, descending=True)[:,0:BEAM_WIDTH].detach()\n",
    "    p_words_init = torch.stack([torch.index_select(decoder_output[i,:],-1,next_words[i,:]) for i in range(BATCH_SIZE)]).detach()\n",
    "    p_words_running = torch.stack([p_words_init[:,b].repeat(1,BEAM_WIDTH) for b in range(BEAM_WIDTH)]).view(BEAM_WIDTH**2,BATCH_SIZE).transpose(0,1)\n",
    "\n",
    "    update = []\n",
    "    for ix,p in enumerate(next_words):\n",
    "        update.append([torch.stack(([torch.tensor(EN.vocab.stoi['<s>'], device='cuda')])+([next_words[ix,b]])) for b in range(BEAM_WIDTH)])\n",
    "\n",
    "    next_words = torch.argsort(lsm2(decoder_output),dim=1, descending=True)[:,0:BEAM_WIDTH].detach()\n",
    "    p_words_init = torch.stack([torch.index_select(decoder_output[i,:],-1,next_words[i,:]) for i in range(BATCH_SIZE)]).detach()\n",
    "    p_words_running = torch.stack([p_words_init[:,b].repeat(1,BEAM_WIDTH) for b in range(BEAM_WIDTH)]).view(BEAM_WIDTH**2,BATCH_SIZE).transpose(0,1)\n",
    "    update = []\n",
    "    for ix,p in enumerate(next_words):\n",
    "        update.append([torch.stack(([torch.tensor(EN.vocab.stoi['<s>'], device='cuda')])+([next_words[ix,b]])) for b in range(BEAM_WIDTH)])\n",
    "    top_s.update(dict(zip(items, update)))\n",
    "    top_p.update(dict(zip(items, p_words_init)))\n",
    "\n",
    "    next_words = next_words.transpose(0,1).flatten().long()\n",
    "    next_words = next_words.view(-1,1)\n",
    "    #encoder_outputs = encoder_outputs.repeat(BEAM_WIDTH,1,1)\n",
    "    #decoder_context = decoder_context.repeat(BEAM_WIDTH,1)\n",
    "    decoder_hidden = tuple([h.repeat(1,BEAM_WIDTH,1) for h in decoder_hidden])\n",
    "    mask = torch.zeros(BATCH_SIZE,BEAM_WIDTH,dtype=torch.uint8).cuda()\n",
    "    \n",
    "    #print(next_words.shape)\n",
    "    for j in range(max_len-1):\n",
    "            #decoder_output, decoder_context, decoder_hidden, decoder_attention = attn_context2trg(next_words, decoder_context, decoder_hidden, encoder_outputs)\n",
    "            decoder_output, decoder_hidden = attn_context2trg(next_words, decoder_hidden)\n",
    "            decoder_output = decoder_output[:,-1,:]\n",
    "            \n",
    "            args = torch.argsort(lsm2(decoder_output),dim=1, descending=True)[:,0:BEAM_WIDTH].detach()\n",
    "            next_words = torch.cat([args[BATCH_SIZE*(b):BATCH_SIZE*(b+1),:] for b in range(BEAM_WIDTH)],dim=1).detach()\n",
    "            p_words = torch.stack([torch.index_select(decoder_output[i,:],-1,next_words[i,:]) for i in range(BATCH_SIZE)])\n",
    "            p_words_running += p_words.detach()\n",
    "            p_words_norm = p_words_running/(j+1)\n",
    "\n",
    "            word_selector = torch.argsort(p_words_norm,dim=1,descending=True)[:,:BEAM_WIDTH]        \n",
    "            beam_indicator = (word_selector/BEAM_WIDTH).float().long() #word_selector>=2\n",
    "            \n",
    "            prev_words = list(top_s.values())\n",
    "            words = [torch.stack([prev_words[s][i] for i in beam_indicator[s,:]]) for s in range(BATCH_SIZE)]\n",
    "            update = []\n",
    "            for ix,p in enumerate(words):\n",
    "                update.append([torch.cat((p[b],next_words[ix,b].unsqueeze(0))) for b in range(BEAM_WIDTH)])\n",
    "            top_s.update(dict(zip(items, update)))\n",
    "            mask += torch.stack([next_words[s,:].index_select(0,word_selector[s,:]) for s in range(BATCH_SIZE)]) == 3\n",
    "            update_p = torch.stack([torch.index_select(p_words_running[b], 0, word_selector[b]) for b in range(BATCH_SIZE)])\n",
    "            top_p.update(dict(zip(items, update_p)))\n",
    "            p_words_running = torch.stack([update_p[:,b].repeat(1,BEAM_WIDTH) for b in range(BEAM_WIDTH)]).view(BEAM_WIDTH**2,BATCH_SIZE).transpose(0,1)\n",
    "\n",
    "            indexs = torch.zeros(BATCH_SIZE,BEAM_WIDTH,device='cuda')\n",
    "            for i in range(BATCH_SIZE):\n",
    "                indexs[i,:] += i+(BATCH_SIZE*beam_indicator[i,:].float())\n",
    "            indexs = indexs.long()\n",
    "            indexs = indexs.transpose(0,1).flatten()\n",
    "            #print(indexs.shape)\n",
    "            decoder_hidden = tuple([torch.index_select(h,1,indexs) for h in decoder_hidden])\n",
    "            #decoder_context = torch.index_select(decoder_context,0,indexs)\n",
    "            next_words = torch.stack([torch.index_select(next_words[s,:],0,word_selector[s,:]) for s in range(BATCH_SIZE)]).transpose(0,1).flatten().long()\n",
    "            next_words = next_words.view(-1,1)\n",
    "    return top_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pred_bleu_update.txt\", \"w+\") as f:\n",
    "    for b in batch_rev_data_loader:  \n",
    "        src = b[0].long()\n",
    "        top_s = beam_search_noattn(src, attn_seq2context, attn_context2trg, BEAM_WIDTH=BEAM_WIDTH, BATCH_SIZE=BATCH_SIZE, max_len=max_len, context_size=context_size, EN=EN)\n",
    "        for i in range(BATCH_SIZE):\n",
    "            stop_idx = torch.tensor([max_len], device='cuda') if (top_s[i][0].long() == 3).nonzero().size()[0]==0 else min((top_s[i][0].long() == 3).nonzero())\n",
    "            f.write(\"%s\\n\"%(\" \".join([EN.vocab.itos[j] for j in top_s[i][0][1:stop_idx].long()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,  247,   10,   23, 3277,   17,   50,    0,    5,   10,   23,   50,\n",
       "         132,    0,    4,    3,    4,    3,   12,   46,  319], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_s[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- English to French translation, $p \\left( y_1, \\dots, y_{T'} \\ | \\ x_1, \\dots, x_T \\right) = \\prod_{t = 1}^{T'} p \\left( y_t \\ | \\ v, y_1, \\dots, y_{t-1} \\right)$\n",
    "- Each sentence ends in '<EOS\\>', out-of-vocab words denoted '<UNK\\>'\n",
    "- Model specs: \n",
    "    * Input vocabulary of 160,000 and output vocabulary of 80,000\n",
    "    * Deep LSTM to map (encode) input sequence to fixed-len vector\n",
    "    * Another deep LSTM to translate (decode) fixed-len vector to output sequence\n",
    "    * 4 layers per LSTM, 1000 cells per layer, 1000-dimensional word embeddings, softmax over 80,000 words\n",
    "    * Reversing order of words in source (but not target) improved performance\n",
    "        * Each word in the source is far from its corresponding word in the target (large minimal time lag); reversing the source reduces the minimal time lag, thereby allowing backprop to establish communication between source and target more easily\n",
    "- Training specs:\n",
    "    * Initialize all LSTM params $\\sim Unif[-0.08,0.08]$\n",
    "    * SGD w/o momentum, lr = 0.7\n",
    "        * After 5 epochs, halve the lr every half-epoch\n",
    "        * Train for 7.5 epochs\n",
    "    * Batch size = 128; divide gradient by batch size (denoted $g$)\n",
    "    * Hard constraint gradient norm; if $s = ||g||_2 > 5$, set $s = 5$\n",
    "    * Make sure all sentences within a minibatch are roughly the same length\n",
    "- Objective: $max \\frac{1}{|S|} \\sum_{(T,S) \\in \\mathcal{S}} log \\ p(T \\ | \\ S)$, where $\\mathcal{S}$ is the training set\n",
    "- Prediction: $\\hat{T} = argmax \\ p(T \\ | \\ S)$ via beam search, where beam size $B \\in {1,2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class SequenceModel(nn.Module):\\n    def __init__(self, src_vocab_size, context_size, num_layers, weight_init = 0.08):\\n        super(SequenceModel, self).__init__()\\n        # embedding\\n        self.embedding = nn.Embedding(src_vocab_size, context_size)\\n        # language summarization\\n        self.lstm = nn.LSTM(input_size=context_size, hidden_size=context_size, num_layers=num_layers, batch_first=True)\\n        for p in self.lstm.parameters():\\n            torch.nn.init.uniform_(p, a=weight_init, b=weight_init)\\n\\n    def forward(self, inputs, h0=None):\\n        # embed the words \\n        embedded = self.embedding(inputs)\\n        # summarize context\\n        context, hidden = self.lstm(embedded,h0)\\n        return context, hidden\\n    \\nclass LanguageModel(nn.Module):\\n    def __init__(self, target_vocab_size, hidden_size, context_size, num_layers, weight_init = 0.08):\\n        super(LanguageModel, self).__init__()\\n        # context is batch_size x seq_len x context_size\\n        # context to hidden\\n        self.embedding = nn.Embedding(target_vocab_size, hidden_size)\\n        # hidden to hidden \\n        self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\\n        # decode hidden state for y_t\\n        for p in self.lstm.parameters():\\n            torch.nn.init.uniform_(p, a=weight_init, b=weight_init)\\n            \\n        self.translate = nn.Linear(hidden_size, target_vocab_size)\\n\\n    def forward(self, inputs, h0=None):\\n        # embed the trg words\\n        embedded = self.embedding(inputs)\\n        # setting hidden state to context at t=0\\n        # otherwise context = prev hidden state\\n        output, hidden = self.lstm(embedded, h0)\\n        output = self.translate(output)\\n        return output,hidden'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class SequenceModel(nn.Module):\n",
    "    def __init__(self, src_vocab_size, context_size, num_layers, weight_init = 0.08):\n",
    "        super(SequenceModel, self).__init__()\n",
    "        # embedding\n",
    "        self.embedding = nn.Embedding(src_vocab_size, context_size)\n",
    "        # language summarization\n",
    "        self.lstm = nn.LSTM(input_size=context_size, hidden_size=context_size, num_layers=num_layers, batch_first=True)\n",
    "        for p in self.lstm.parameters():\n",
    "            torch.nn.init.uniform_(p, a=weight_init, b=weight_init)\n",
    "\n",
    "    def forward(self, inputs, h0=None):\n",
    "        # embed the words \n",
    "        embedded = self.embedding(inputs)\n",
    "        # summarize context\n",
    "        context, hidden = self.lstm(embedded,h0)\n",
    "        return context, hidden\n",
    "    \n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, target_vocab_size, hidden_size, context_size, num_layers, weight_init = 0.08):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        # context is batch_size x seq_len x context_size\n",
    "        # context to hidden\n",
    "        self.embedding = nn.Embedding(target_vocab_size, hidden_size)\n",
    "        # hidden to hidden \n",
    "        self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        # decode hidden state for y_t\n",
    "        for p in self.lstm.parameters():\n",
    "            torch.nn.init.uniform_(p, a=weight_init, b=weight_init)\n",
    "            \n",
    "        self.translate = nn.Linear(hidden_size, target_vocab_size)\n",
    "\n",
    "    def forward(self, inputs, h0=None):\n",
    "        # embed the trg words\n",
    "        embedded = self.embedding(inputs)\n",
    "        # setting hidden state to context at t=0\n",
    "        # otherwise context = prev hidden state\n",
    "        output, hidden = self.lstm(embedded, h0)\n",
    "        output = self.translate(output)\n",
    "        return output,hidden'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def repackage_hidden(h):\\n    return tuple(v.detach() for v in h)\\ndef repackage_layer(hidden_s2c,hidden=100):\\n    return tuple([hidden_s2c[0][-1].detach().view(1,BATCH_SIZE,hidden),hidden_s2c[1][-1].detach().view(1,BATCH_SIZE,hidden)])\\ndef reverse_sequence(src):\\n    length = list(src.shape)[1]\\n    idx = torch.linspace(length-1, 0, steps=length).long()\\n    rev_src = src[:,idx]\\n    return rev_src'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def repackage_hidden(h):\n",
    "    return tuple(v.detach() for v in h)\n",
    "def repackage_layer(hidden_s2c,hidden=100):\n",
    "    return tuple([hidden_s2c[0][-1].detach().view(1,BATCH_SIZE,hidden),hidden_s2c[1][-1].detach().view(1,BATCH_SIZE,hidden)])\n",
    "def reverse_sequence(src):\n",
    "    length = list(src.shape)[1]\n",
    "    idx = torch.linspace(length-1, 0, steps=length).long()\n",
    "    rev_src = src[:,idx]\n",
    "    return rev_src\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"context_size = 500\\nnum_layers = 1\\nseq2context = SequenceModel(len(DE.vocab),context_size,num_layers)\\ncontext2trg = LanguageModel(len(EN.vocab),hidden_size=context_size,context_size=context_size,num_layers=num_layers)\\nseq2context,context2trg = seq2context.cuda(),context2trg.cuda()\\nseq2context_optimizer = torch.optim.Adam(seq2context.parameters(), lr=1e-5)\\ncontext2trg_optimizer = torch.optim.Adam(context2trg.parameters(), lr=1e-5)\\ncriterion = nn.CrossEntropyLoss(reduction='none')\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''context_size = 500\n",
    "num_layers = 1\n",
    "seq2context = SequenceModel(len(DE.vocab),context_size,num_layers)\n",
    "context2trg = LanguageModel(len(EN.vocab),hidden_size=context_size,context_size=context_size,num_layers=num_layers)\n",
    "seq2context,context2trg = seq2context.cuda(),context2trg.cuda()\n",
    "seq2context_optimizer = torch.optim.Adam(seq2context.parameters(), lr=1e-5)\n",
    "context2trg_optimizer = torch.optim.Adam(context2trg.parameters(), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def training_loop(e=0):\\n    seq2context.train()\\n    context2trg.train()\\n    h0 = None\\n    for ix,batch in enumerate(train_iter):\\n        seq2context_optimizer.zero_grad()\\n        context2trg_optimizer.zero_grad()\\n        \\n        src = batch.src.values.transpose(0,1)\\n        src = reverse_sequence(src)\\n        trg = batch.trg.values.transpose(0,1)\\n        if src.shape[0]!=BATCH_SIZE:\\n            break\\n        else:\\n            # generate hidden state for decoder\\n            context, hidden_s2c = seq2context(src,h0)\\n            hidden = repackage_layer(hidden_s2c,context_size)\\n            output, hidden_lm = context2trg(trg[:,:-1],hidden)\\n            loss = criterion(output.transpose(2,1),trg[:,1:])\\n            mask = trg[:,1:]!=1\\n            loss = loss[mask].sum()\\n            #clip_grad_norm_(seq2context.parameters(), max_norm=5)\\n            #clip_grad_norm_(context2trg.parameters(), max_norm=5)\\n            loss.backward()\\n            seq2context_optimizer.step()\\n            context2trg_optimizer.step()\\n        if np.mod(ix,100) == 0:\\n            var = torch.var(torch.argmax(lsm(output).cpu().detach(),2).float())\\n            print('Epoch: {}, Batch: {}, loss: {}, var: {},'.format(e, ix, loss.cpu().detach()/BATCH_SIZE, var))\\n    loss = 0\\n    for b in iter(val_iter):\\n        src = b.src.values.transpose(0,1)\\n        src = reverse_sequence(src)\\n        trg = b.trg.values.transpose(0,1)\\n        if src.shape[0]!=BATCH_SIZE:\\n            break\\n        else:\\n            # generate hidden state for decoder\\n            context, hidden_s2c = seq2context(src,h0)\\n            hidden = repackage_layer(hidden_s2c,context_size)\\n            output, hidden_lm = context2trg(trg[:,:-1],hidden)\\n            bloss = criterion(output.transpose(2,1),trg[:,1:])\\n            mask = trg[:,1:]!=1\\n            loss += bloss[mask].sum()\\n    print('Epoch: {}, loss: {}, var: {},'.format(e, loss.cpu().detach()/(BATCH_SIZE*len(val_iter))))\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def training_loop(e=0):\n",
    "    seq2context.train()\n",
    "    context2trg.train()\n",
    "    h0 = None\n",
    "    for ix,batch in enumerate(train_iter):\n",
    "        seq2context_optimizer.zero_grad()\n",
    "        context2trg_optimizer.zero_grad()\n",
    "        \n",
    "        src = batch.src.values.transpose(0,1)\n",
    "        src = reverse_sequence(src)\n",
    "        trg = batch.trg.values.transpose(0,1)\n",
    "        if src.shape[0]!=BATCH_SIZE:\n",
    "            break\n",
    "        else:\n",
    "            # generate hidden state for decoder\n",
    "            context, hidden_s2c = seq2context(src,h0)\n",
    "            hidden = repackage_layer(hidden_s2c,context_size)\n",
    "            output, hidden_lm = context2trg(trg[:,:-1],hidden)\n",
    "            loss = criterion(output.transpose(2,1),trg[:,1:])\n",
    "            mask = trg[:,1:]!=1\n",
    "            loss = loss[mask].sum()\n",
    "            #clip_grad_norm_(seq2context.parameters(), max_norm=5)\n",
    "            #clip_grad_norm_(context2trg.parameters(), max_norm=5)\n",
    "            loss.backward()\n",
    "            seq2context_optimizer.step()\n",
    "            context2trg_optimizer.step()\n",
    "        if np.mod(ix,100) == 0:\n",
    "            var = torch.var(torch.argmax(lsm(output).cpu().detach(),2).float())\n",
    "            print('Epoch: {}, Batch: {}, loss: {}, var: {},'.format(e, ix, loss.cpu().detach()/BATCH_SIZE, var))\n",
    "    loss = 0\n",
    "    for b in iter(val_iter):\n",
    "        src = b.src.values.transpose(0,1)\n",
    "        src = reverse_sequence(src)\n",
    "        trg = b.trg.values.transpose(0,1)\n",
    "        if src.shape[0]!=BATCH_SIZE:\n",
    "            break\n",
    "        else:\n",
    "            # generate hidden state for decoder\n",
    "            context, hidden_s2c = seq2context(src,h0)\n",
    "            hidden = repackage_layer(hidden_s2c,context_size)\n",
    "            output, hidden_lm = context2trg(trg[:,:-1],hidden)\n",
    "            bloss = criterion(output.transpose(2,1),trg[:,1:])\n",
    "            mask = trg[:,1:]!=1\n",
    "            loss += bloss[mask].sum()\n",
    "    print('Epoch: {}, loss: {}, var: {},'.format(e, loss.cpu().detach()/(BATCH_SIZE*len(val_iter))))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for e in range(2):\\n    training_loop(e)\\n    #training_loop(e,train_iter,seq2context,context2trg,seq2context_optimizer,context2trg_optimizer,BATCH_SIZE)\\n    #validation_loop(e,val_iter,seq2context,context2trg,BATCH_SIZE)'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for e in range(2):\n",
    "    training_loop(e)\n",
    "    #training_loop(e,train_iter,seq2context,context2trg,seq2context_optimizer,context2trg_optimizer,BATCH_SIZE)\n",
    "    #validation_loop(e,val_iter,seq2context,context2trg,BATCH_SIZE)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for ix,batch in enumerate(train_iter):\\n    src = batch.src.values.transpose(0,1)\\n    trg = batch.trg.values.transpose(0,1)\\n    break\\n\\nh0 = None\\ncontext, hidden_s2c = seq2context(reverse_sequence(src),h0)\\nhidden = repackage_layer(hidden_s2c,context_size)\\noutput, hidden_lm = context2trg(trg[:,:-1],hidden)\\n\\n[EN.vocab.itos[i] for i in torch.argmax(lsm(output),2)[30,:]]'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for ix,batch in enumerate(train_iter):\n",
    "    src = batch.src.values.transpose(0,1)\n",
    "    trg = batch.trg.values.transpose(0,1)\n",
    "    break\n",
    "\n",
    "h0 = None\n",
    "context, hidden_s2c = seq2context(reverse_sequence(src),h0)\n",
    "hidden = repackage_layer(hidden_s2c,context_size)\n",
    "output, hidden_lm = context2trg(trg[:,:-1],hidden)\n",
    "\n",
    "[EN.vocab.itos[i] for i in torch.argmax(lsm(output),2)[30,:]]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# define attention-based encoder-decoder model\\nclass attn_RNNet_batched(torch.nn.Module):\\n\\n    def __init__(self, input_size, hidden_size, num_layers, dropout=0.5, weight_init=0.05):\\n        super(attn_RNNet_batched, self).__init__()\\n        self.emb = torch.nn.Sequential(torch.nn.Embedding(input_size, hidden_size), torch.nn.Dropout(dropout))\\n        self.rnn = torch.nn.LSTM(input_size=2*hidden_size, hidden_size=hidden_size, num_layers=num_layers, bias=True, batch_first=True, dropout=dropout)\\n        self.lnr = torch.nn.Sequential(torch.nn.Dropout(dropout), torch.nn.Linear(2*hidden_size, input_size))\\n    \\n        for f in self.parameters():\\n            torch.nn.init.uniform_(f, a=-weight_init, b=weight_init)\\n\\n    def forward(self, word_input, last_context, last_hidden, encoder_outputs):\\n        word_embedded = self.emb(word_input)\\n        rnn_input = torch.cat([word_embedded, last_context], 1).unsqueeze(1) # batch x 1 x hiddenx2\\n        rnn_output, hidden = self.rnn(rnn_input, last_hidden)\\n        attn_weights = rnn_output.bmm(encoder_outputs.transpose(1,2))# batch x src_seqlen x 1\\n        context = attn_weights.bmm(encoder_outputs)\\n        rnn_output = rnn_output.squeeze(1)\\n        context = context.squeeze(1)\\n        output = self.lnr(torch.cat((rnn_output, context), 1))\\n        # prediction, last_context, last_hidden, weights for vis\\n        return output, context, hidden, attn_weights '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# define attention-based encoder-decoder model\n",
    "class attn_RNNet_batched(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout=0.5, weight_init=0.05):\n",
    "        super(attn_RNNet_batched, self).__init__()\n",
    "        self.emb = torch.nn.Sequential(torch.nn.Embedding(input_size, hidden_size), torch.nn.Dropout(dropout))\n",
    "        self.rnn = torch.nn.LSTM(input_size=2*hidden_size, hidden_size=hidden_size, num_layers=num_layers, bias=True, batch_first=True, dropout=dropout)\n",
    "        self.lnr = torch.nn.Sequential(torch.nn.Dropout(dropout), torch.nn.Linear(2*hidden_size, input_size))\n",
    "    \n",
    "        for f in self.parameters():\n",
    "            torch.nn.init.uniform_(f, a=-weight_init, b=weight_init)\n",
    "\n",
    "    def forward(self, word_input, last_context, last_hidden, encoder_outputs):\n",
    "        word_embedded = self.emb(word_input)\n",
    "        rnn_input = torch.cat([word_embedded, last_context], 1).unsqueeze(1) # batch x 1 x hiddenx2\n",
    "        rnn_output, hidden = self.rnn(rnn_input, last_hidden)\n",
    "        attn_weights = rnn_output.bmm(encoder_outputs.transpose(1,2))# batch x src_seqlen x 1\n",
    "        context = attn_weights.bmm(encoder_outputs)\n",
    "        rnn_output = rnn_output.squeeze(1)\n",
    "        context = context.squeeze(1)\n",
    "        output = self.lnr(torch.cat((rnn_output, context), 1))\n",
    "        # prediction, last_context, last_hidden, weights for vis\n",
    "        return output, context, hidden, attn_weights '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# initialize model\\ncontext_size = 500\\nnum_layers = 2\\nattn_context2trg = attn_RNNet_batched(input_size=len(EN.vocab),hidden_size=context_size,num_layers=num_layers)\\nattn_context2trg = attn_context2trg.cuda()\\nseq2context = SequenceModel(len(DE.vocab),context_size,num_layers=num_layers)\\nseq2context = seq2context.cuda()'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# initialize model\n",
    "context_size = 500\n",
    "num_layers = 2\n",
    "attn_context2trg = attn_RNNet_batched(input_size=len(EN.vocab),hidden_size=context_size,num_layers=num_layers)\n",
    "attn_context2trg = attn_context2trg.cuda()\n",
    "seq2context = SequenceModel(len(DE.vocab),context_size,num_layers=num_layers)\n",
    "seq2context = seq2context.cuda()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# prep for training\\nattn_context2trg_optimizer = torch.optim.Adam(attn_context2trg.parameters(), lr=1e-3)\\nseq2context_optimizer = torch.optim.Adam(seq2context.parameters(), lr=1e-3)\\ncriterion_train = nn.CrossEntropyLoss(reduction='sum')\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# prep for training\n",
    "attn_context2trg_optimizer = torch.optim.Adam(attn_context2trg.parameters(), lr=1e-3)\n",
    "seq2context_optimizer = torch.optim.Adam(seq2context.parameters(), lr=1e-3)\n",
    "criterion_train = nn.CrossEntropyLoss(reduction='sum')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def attn_training_loop(e=0):\\n    for ix,batch in enumerate(train_iter):\\n        src = batch.src.values.transpose(0,1)\\n        src = reverse_sequence(src)\\n        trg = batch.trg.values.transpose(0,1)\\n        if trg.shape[0] == BATCH_SIZE:\\n        \\n            seq2context_optimizer.zero_grad()\\n            attn_context2trg_optimizer.zero_grad()\\n        \\n            encoder_outputs, encoder_hidden = seq2context(src)\\n            loss = 0\\n            decoder_context = torch.zeros(BATCH_SIZE, context_size, device='cuda') # 32 x 500\\n            decoder_hidden = encoder_hidden\\n            sentence = []\\n            for j in range(trg.shape[1] - 1):\\n                word_input = trg[:,j]\\n                decoder_output, decoder_context, decoder_hidden, decoder_attention = attn_context2trg(word_input, decoder_context, decoder_hidden, encoder_outputs)\\n                #print(decoder_output.shape, trg[i,j+1].view(-1).shape)\\n                loss += criterion_train(decoder_output, trg[:,j+1])\\n                \\n                if np.mod(ix,100) == 0:\\n                    sentence.extend([torch.argmax(decoder_output[0,:],dim=0)])\\n                \\n            loss.backward()\\n            seq2context_optimizer.step()\\n            attn_context2trg_optimizer.step()\\n        \\n            if np.mod(ix,100) == 0:\\n                print('Epoch: {}, Batch: {}, Loss: {}'.format(e, ix, loss.cpu().detach()/BATCH_SIZE))\\n                print([EN.vocab.itos[i] for i in sentence])\\n                print([EN.vocab.itos[i] for i in trg[0,:]])\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def attn_training_loop(e=0):\n",
    "    for ix,batch in enumerate(train_iter):\n",
    "        src = batch.src.values.transpose(0,1)\n",
    "        src = reverse_sequence(src)\n",
    "        trg = batch.trg.values.transpose(0,1)\n",
    "        if trg.shape[0] == BATCH_SIZE:\n",
    "        \n",
    "            seq2context_optimizer.zero_grad()\n",
    "            attn_context2trg_optimizer.zero_grad()\n",
    "        \n",
    "            encoder_outputs, encoder_hidden = seq2context(src)\n",
    "            loss = 0\n",
    "            decoder_context = torch.zeros(BATCH_SIZE, context_size, device='cuda') # 32 x 500\n",
    "            decoder_hidden = encoder_hidden\n",
    "            sentence = []\n",
    "            for j in range(trg.shape[1] - 1):\n",
    "                word_input = trg[:,j]\n",
    "                decoder_output, decoder_context, decoder_hidden, decoder_attention = attn_context2trg(word_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "                #print(decoder_output.shape, trg[i,j+1].view(-1).shape)\n",
    "                loss += criterion_train(decoder_output, trg[:,j+1])\n",
    "                \n",
    "                if np.mod(ix,100) == 0:\n",
    "                    sentence.extend([torch.argmax(decoder_output[0,:],dim=0)])\n",
    "                \n",
    "            loss.backward()\n",
    "            seq2context_optimizer.step()\n",
    "            attn_context2trg_optimizer.step()\n",
    "        \n",
    "            if np.mod(ix,100) == 0:\n",
    "                print('Epoch: {}, Batch: {}, Loss: {}'.format(e, ix, loss.cpu().detach()/BATCH_SIZE))\n",
    "                print([EN.vocab.itos[i] for i in sentence])\n",
    "                print([EN.vocab.itos[i] for i in trg[0,:]])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for e in range(10):\\n    attn_training_loop(e)'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for e in range(10):\n",
    "    attn_training_loop(e)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search for common.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def beamsearch(seq2context, context2trg, context_size, src, beam_width, max_len, output_width=1, alpha=1, padding=False):\n",
    "    '''\n",
    "    run beam search and return top predictions\n",
    "        - seq2context: encoder model\n",
    "        - context2trg: decoder model\n",
    "        - context_size: hidden size\n",
    "        - src: tensor of source sentences\n",
    "        - beam_width: beam search width\n",
    "        - max_len: maximum length for predictions\n",
    "        - output_width: number of predictions to return per sentence <= beam_width\n",
    "        - alpha: string length discount rate; e.g., normalizing factor = 1/(T^alpha)\n",
    "        - padding: pad predictions to max_len\n",
    "    '''\n",
    "    # set up\n",
    "    START_TKN = EN.vocab.stoi[\"<s>\"]\n",
    "    END_TKN = EN.vocab.stoi[\"</s>\"]\n",
    "    BEAM_WIDTH = beam_width\n",
    "    lsm = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    # run forward pass of encoder once\n",
    "    encoder_outputs, encoder_hidden = seq2context(src)\n",
    "    decoder_context = torch.zeros(BATCH_SIZE, context_size, device='cuda') # 32 x 500\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    # prepare for beam search\n",
    "    b_string = torch.zeros((BATCH_SIZE, max_len, BEAM_WIDTH), device='cuda') # stores the top BEAM_WIDTH strings\n",
    "    b_string[:,0,:] = START_TKN\n",
    "    b_probs = {} # stores the top BEAM_WIDTH probs\n",
    "    '''\n",
    "    b_probs key = tuple(batch idx, beam idx)\n",
    "    b_probs val = [cum log prob, length]\n",
    "    '''\n",
    "    done = {} # stores the finished strings\n",
    "    '''\n",
    "    done key = batch idx\n",
    "    done val = [str, cum log prob, length]\n",
    "    '''\n",
    "    predictions = {} # stores the top output_width predictions\n",
    "    for b in range(BATCH_SIZE):\n",
    "        done[b] = []\n",
    "        predictions[b] = []\n",
    "        for c in range(BEAM_WIDTH):\n",
    "            b_probs[(b, c)] = [0, 1]\n",
    "\n",
    "    # loop through target sequence max len\n",
    "    for i in range(1,max_len):\n",
    "        if i == 1: # if predicting the word following <s>, take top BEAM_WIDTH preds\n",
    "            word_input = b_string[:,i-1,0].long()\n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = context2trg(word_input, \n",
    "                                                                                             decoder_context, \n",
    "                                                                                             decoder_hidden, \n",
    "                                                                                             encoder_outputs)\n",
    "            logprobs = lsm(decoder_output.detach()) # BATCH_SIZE x VOCAB_SIZE\n",
    "            toppreds = torch.argsort(logprobs, dim=1, descending=True)[:,0:BEAM_WIDTH] # BATCH_SIZE x BEAM_WIDTH\n",
    "            b_string[:,i,:] = toppreds\n",
    "            for b in range(BATCH_SIZE):\n",
    "                for c in range(BEAM_WIDTH):\n",
    "                    b_probs[tuple((b,c))][0] += logprobs[b, toppreds[b,c]]\n",
    "                    b_probs[tuple((b,c))][1] += 1\n",
    "        else: # if predicting the word for positions 2+, compare top BEAM_WIDTH preds for each of BEAM_WIDTH strings\n",
    "            curr_probs = {} # temporary storage\n",
    "            curr_string = torch.zeros(BATCH_SIZE, i+1, BEAM_WIDTH) # temporary storage\n",
    "\n",
    "            for j in range(BEAM_WIDTH):\n",
    "                word_input = b_string[:,i-1,j].long()\n",
    "                decoder_output, decoder_context, decoder_hidden, decoder_attention = context2trg(word_input, \n",
    "                                                                                                 decoder_context, \n",
    "                                                                                                 decoder_hidden, \n",
    "                                                                                                 encoder_outputs)\n",
    "                logprobs = lsm(decoder_output.detach()) # unsorted log probs\n",
    "                sortedpreds = torch.argsort(logprobs, dim=1, descending=True) # sorted words\n",
    "                toppreds = sortedpreds[:,0:BEAM_WIDTH] # top words\n",
    "\n",
    "                # check if any top preds are </s>\n",
    "                for b in range(BATCH_SIZE):\n",
    "                    if END_TKN in toppreds[b,:]: # if </s> in top preds\n",
    "                        # track finished strings\n",
    "                        done_string = torch.cat((b_string[b,0:i,j],torch.tensor([END_TKN], device='cuda').float()))\n",
    "                        done_prob = b_probs[tuple((b,j))][0] + logprobs[b,END_TKN]\n",
    "                        done[b].append([done_string, done_prob, done_string.shape[0]])\n",
    "                        # replace </s> with 4th best pred\n",
    "                        done_idx = (toppreds[b,:] == END_TKN).nonzero()\n",
    "                        toppreds[b,done_idx] = sortedpreds[b,BEAM_WIDTH]\n",
    "\n",
    "                if j == 0: # if preds are from first beam, take top BEAM_WIDTH preds (temporarily)\n",
    "                    for b in range(BATCH_SIZE):\n",
    "                        for c in range(BEAM_WIDTH):\n",
    "                            new_b_prob = b_probs[tuple((b,j))][0] + logprobs[b,toppreds[b,c]]\n",
    "                            curr_probs[tuple((b,c))] = new_b_prob # set top prob\n",
    "                            curr_string[b,0:i,c] = b_string[b,0:i,j] # set sentence\n",
    "                            curr_string[b,i,c] = toppreds[b,c] # set top word\n",
    "                else: # if preds are from subsequent beams, compare to existing\n",
    "                    for b in range(BATCH_SIZE):\n",
    "                        for c in range(BEAM_WIDTH): # proposed strings\n",
    "                            replaced = False\n",
    "                            for d in range(BEAM_WIDTH): # existing strings\n",
    "                                new_b_prob = b_probs[tuple((b,j))][0] + logprobs[b,toppreds[b,c]]\n",
    "                                if new_b_prob > curr_probs[tuple((b,d))] and not replaced:\n",
    "                                    curr_probs[tuple((b,d))] = new_b_prob # update top prob\n",
    "                                    curr_string[b,0:i,d] = b_string[b,0:i,j] # update sentence\n",
    "                                    curr_string[b,i,d] = toppreds[b,c] # update top word\n",
    "                                    replaced = True                        \n",
    "            b_string[:,0:i+1,:] = curr_string\n",
    "            # update top strings, probs\n",
    "            for b in range(BATCH_SIZE):\n",
    "                for c in range(BEAM_WIDTH):\n",
    "                    b_probs[tuple((b,c))][0] = curr_probs[tuple((b,c))]\n",
    "                    b_probs[tuple((b,c))][1] += 1\n",
    "\n",
    "    K = output_width\n",
    "    for b in range(BATCH_SIZE):\n",
    "        if len(done[b]) < K:\n",
    "            gap = K - len(done[b])\n",
    "            probs = torch.tensor([b_probs[tuple((b,j))][0] for j in range(BEAM_WIDTH)], device='cuda')\n",
    "            for c in torch.argsort(probs, descending=True)[0:gap]:  \n",
    "                d = c.item()\n",
    "                done_string = b_string[b,:,d].long()\n",
    "                #print(b_probs[tuple((b,d))])\n",
    "                done_prob = b_probs[tuple((b,d))][0]\n",
    "                done_len = b_probs[tuple((b,d))][1]\n",
    "                done[b].append([done_string, done_prob, done_len])\n",
    "                        \n",
    "    for b in range(BATCH_SIZE):\n",
    "        normalized_probs = torch.tensor([], device='cuda')\n",
    "        for sentence in range(len(done[b])):\n",
    "            normalized = torch.tensor([done[b][sentence][1]/done[b][sentence][2]**alpha], device='cuda')\n",
    "            normalized_probs = torch.cat((normalized_probs,normalized),0)\n",
    "        top = torch.argsort(normalized_probs, descending=True)[0:K]\n",
    "        for k in range(K):\n",
    "            best = done[b][top[k]]\n",
    "            if padding:\n",
    "                m = nn.ConstantPad1d((0, max_len - best[2]), EN.vocab.stoi['<pad>'])\n",
    "                predictions[b].append(m(best[0].long()))\n",
    "            else:\n",
    "                predictions[b].append(best[0].long())\n",
    "            #print([EN.vocab.itos[i] for i in best[0].long()])\n",
    "    \n",
    "    return predictions'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''context_size = 500\n",
    "num_layers = 2\n",
    "\n",
    "attn_seq2context = SequenceModel(len(DE.vocab),context_size,num_layers=num_layers)\n",
    "#state_dict = torch.load('best_seq2seq_withattn_seq2context.pt')\n",
    "#attn_seq2context.load_state_dict(state_dict)\n",
    "attn_seq2context = attn_seq2context.cuda()\n",
    "\n",
    "attn_context2trg = attn_RNNet_batched(input_size=len(EN.vocab),hidden_size=context_size,num_layers=num_layers)\n",
    "#state_dict = torch.load('best_seq2seq_withattn_context2trg.pt')\n",
    "#attn_context2trg.load_state_dict(state_dict)\n",
    "attn_context2trg = attn_context2trg.cuda()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# run beam search on one batch\n",
    "it = iter(train_iter)\n",
    "batch = next(it)\n",
    "src = batch.src.values.transpose(0,1)\n",
    "src = reverse_sequence(src)\n",
    "beam_width = 3\n",
    "max_len = src.shape[1] # restrict target sentence length to source sentence length\n",
    "beamsearch(attn_seq2context, attn_context2trg, context_size, src, beam_width, max_len, padding=True)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# run forward pass of encoder once\n",
    "encoder_outputs, encoder_hidden = attn_seq2context(src)\n",
    "decoder_context = torch.zeros(BATCH_SIZE, context_size, device='cuda') # 32 x 500\n",
    "decoder_hidden = encoder_hidden'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# prepare for beam search\n",
    "START_TKN = EN.vocab.stoi[\"<s>\"]\n",
    "END_TKN = EN.vocab.stoi[\"</s>\"]\n",
    "BEAM_WIDTH = 3\n",
    "lsm = nn.LogSoftmax(dim=1)\n",
    "\n",
    "b_string = torch.zeros((BATCH_SIZE, max_len, BEAM_WIDTH), device='cuda')\n",
    "b_string[:,0,:] = START_TKN\n",
    "\n",
    "b_probs = {}\n",
    "# b_probs key = tuple(batch idx, beam idx)\n",
    "# b_probs val = [cum log prob, length]\n",
    "done = {} # stores the finished strings\n",
    "# done key = batch idx\n",
    "# done val = [str, cum log prob, length]\n",
    "predictions = {} # stores the top BEAM_WIDTH predictions\n",
    "for b in range(BATCH_SIZE):\n",
    "    done[b] = []\n",
    "    predictions[b] = []\n",
    "    for c in range(BEAM_WIDTH):\n",
    "        b_probs[(b, c)] = [0, 1]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# loop through target sequence max len\n",
    "for i in range(1,max_len):\n",
    "    if i == 1: # if predicting the word following <s>, take top BEAM_WIDTH preds\n",
    "        word_input = b_string[:,i-1,0].long()\n",
    "        #print(word_input)\n",
    "        decoder_output, decoder_context, decoder_hidden, decoder_attention = attn_context2trg(word_input, \n",
    "                                                                                                  decoder_context, \n",
    "                                                                                                  decoder_hidden, \n",
    "                                                                                                  encoder_outputs)\n",
    "        logprobs = lsm(decoder_output.detach()) # BATCH_SIZE x VOCAB_SIZE\n",
    "        #print(logprobs[0,:])\n",
    "        toppreds = torch.argsort(logprobs, dim=1, descending=True)[:,0:BEAM_WIDTH] # BATCH_SIZE x BEAM_WIDTH\n",
    "        #print(toppreds[0,:])\n",
    "        #print(logprobs[0,:][toppreds[0,:]])\n",
    "        b_string[:,i,:] = toppreds\n",
    "        for b in range(BATCH_SIZE):\n",
    "            for c in range(BEAM_WIDTH):\n",
    "                b_probs[tuple((b,c))][0] += logprobs[b, toppreds[b,c]]\n",
    "                b_probs[tuple((b,c))][1] += 1\n",
    "    else: # if predicting the word for positions 2+, compare top BEAM_WIDTH preds for each of BEAM_WIDTH strings\n",
    "        # temporary storage\n",
    "        curr_probs = {}\n",
    "        curr_string = torch.zeros(BATCH_SIZE, i+1, BEAM_WIDTH)\n",
    "\n",
    "        for j in range(BEAM_WIDTH):\n",
    "            word_input = b_string[:,i-1,j].long()\n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = attn_context2trg(word_input, \n",
    "                                                                                                      decoder_context, \n",
    "                                                                                                      decoder_hidden, \n",
    "                                                                                                      encoder_outputs)\n",
    "            logprobs = lsm(decoder_output.detach()) # unsorted log probs\n",
    "            sortedpreds = torch.argsort(logprobs, dim=1, descending=True) # sorted words\n",
    "            toppreds = sortedpreds[:,0:BEAM_WIDTH] # top words\n",
    "            \n",
    "            # check if any top preds are </s>\n",
    "            for b in range(BATCH_SIZE):\n",
    "                if END_TKN in toppreds[b,:]: # if </s> in top preds\n",
    "                    # track finished strings\n",
    "                    done_string = torch.cat((b_string[b,0:i,j],torch.tensor([END_TKN], device='cuda').float()))\n",
    "                    done_prob = b_probs[tuple((b,j))][0] + logprobs[b,END_TKN]\n",
    "                    done[b].append([done_string, done_prob, done_string.shape[0]])\n",
    "                    # replace </s> with 4th best pred\n",
    "                    done_idx = (toppreds[b,:] == END_TKN).nonzero()\n",
    "                    toppreds[b,done_idx] = sortedpreds[b,BEAM_WIDTH]\n",
    "               \n",
    "            if j == 0: # if preds are from first beam, take top BEAM_WIDTH preds (temporarily)\n",
    "                for b in range(BATCH_SIZE):\n",
    "                    for c in range(BEAM_WIDTH):\n",
    "                        new_b_prob = b_probs[tuple((b,j))][0] + logprobs[b,toppreds[b,c]]\n",
    "                        curr_probs[tuple((b,c))] = new_b_prob # set top prob\n",
    "                        curr_string[b,0:i,c] = b_string[b,0:i,j] # set sentence\n",
    "                        curr_string[b,i,c] = toppreds[b,c] # set top word\n",
    "            else:\n",
    "                for b in range(BATCH_SIZE):\n",
    "                    for c in range(BEAM_WIDTH): # proposed strings\n",
    "                        replaced = False\n",
    "                        for d in range(BEAM_WIDTH): # existing strings\n",
    "                            new_b_prob = b_probs[tuple((b,j))][0] + logprobs[b,toppreds[b,c]]\n",
    "                            if new_b_prob > curr_probs[tuple((b,d))] and not replaced:\n",
    "                                curr_probs[tuple((b,d))] = new_b_prob # update top prob\n",
    "                                curr_string[b,0:i,d] = b_string[b,0:i,j] # update sentence\n",
    "                                curr_string[b,i,d] = toppreds[b,c] # update top word\n",
    "                                replaced = True\n",
    "        #print(b_string[:,0:i+2,:].shape, curr_string.shape)                        \n",
    "        b_string[:,0:i+1,:] = curr_string\n",
    "        for b in range(BATCH_SIZE):\n",
    "            for c in range(BEAM_WIDTH):\n",
    "                b_probs[tuple((b,c))][0] = curr_probs[tuple((b,c))]\n",
    "                b_probs[tuple((b,c))][1] += 1'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for b in range(BATCH_SIZE):\n",
    "    for c in range(BEAM_WIDTH):\n",
    "        if b_string[b,-1,c] == END_TKN:\n",
    "            done_string = b_string[b,:,c]\n",
    "            done_prob = b_probs[tuple((b,c))][0]\n",
    "            done_len = b_probs[tuple((b,c))][1]\n",
    "            done[b].append([done_string, done_prob, done_len])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'The', 'problem', 'was', 'very', 'expensive', 'and', 'it', '.', '</s>']\n",
      "['<s>', 'The', 'problem', 'is', 'it', ',', 'and', 'use', 'it', 'goes', 'very', 'difficult', '.', '</s>']\n",
      "['<s>', 'The', 'problem', 'is', 'it', ',', 'and', 'use', 'it', '.', '</s>']\n",
      "['<s>', 'And', 'the', 'result', 'is', 'the', '<unk>', '.', '</s>']\n",
      "['<s>', 'And', 'the', 'outcome', 'is', '<unk>', '.', '</s>']\n",
      "['<s>', 'And', 'the', 'result', 'is', 'the', '<unk>', '</s>']\n",
      "['<s>', 'So', 'ca', \"n't\", 'do', 'that', \"'s\", '<unk>', '.', '</s>']\n",
      "['<s>', 'So', 'ca', \"n't\", 'do', 'that', \"'s\", 'way', '.', '</s>']\n",
      "['<s>', 'So', 'ca', \"n't\", 'do', 'that', \"'s\", 'difference', '.', '</s>']\n",
      "['<s>', 'Of', 'course', 'we', '<unk>', '.', '</s>']\n",
      "['<s>', 'Of', 'course', 'we', 'do', 'dawn', '.', '</s>']\n",
      "['<s>', 'Of', 'course', 'we', 'do', 'dawn', 'are', \"n't\", '.', '</s>']\n",
      "['<s>', 'And', 'it', 'agreed', ',', 'and', 'everyone', '.', '</s>']\n",
      "['<s>', 'And', 'it', 'agreed', ',', 'and', 'agreed', '.', '</s>']\n",
      "['<s>', 'And', 'it', 'agreed', ',', 'and', 'forth', '.', '</s>']\n",
      "['<s>', 'These', 'are', '<unk>', '.', '</s>']\n",
      "['<s>', 'That', 'are', '<unk>', '.', '</s>']\n",
      "['<s>', 'This', 'is', '<unk>', '.', '</s>']\n",
      "['<s>', '96', 'percent', 'of', '<unk>', 'them', '.', '</s>']\n",
      "['<s>', '96', 'percent', 'of', '<unk>', 'themselves', '.', '</s>']\n",
      "['<s>', '96', 'percent', 'of', '<unk>', '<unk>', 'themselves', '.', '</s>']\n",
      "['<s>', 'How', 'small', ',', 'neurons', '.', '</s>']\n",
      "['<s>', 'How', 'small', ',', 'can', 'see', 'a', 'insect', '.', '</s>']\n",
      "['<s>', 'How', 'small', ',', 'can', 'look', 'at', 'insect', '.', '</s>']\n",
      "['<s>', 'One', 'is', 'the', '<unk>', ',', 'and', 'the', 'other', 'is', 'the', '<unk>', '.', '</s>']\n",
      "['<s>', 'One', 'is', 'the', '<unk>', ',', 'and', 'the', 'other', 'is', 'a', '<unk>', '.', '</s>']\n",
      "['<s>', 'One', 'is', 'the', '<unk>', ',', 'and', 'again', ',', 'the', '<unk>', 'in', 'the', 'other', 'are', 'the', '<unk>', '.', '</s>']\n",
      "['<s>', 'This', 'is', 'the', '<unk>', 'hospital', '.', '</s>']\n",
      "['<s>', 'This', 'is', 'the', '<unk>', 'hospital', 'in', 'the', 'day', '.', '</s>']\n",
      "['<s>', 'This', 'is', 'the', '<unk>', 'hospital', 'in', 'the', '<unk>', '.', '</s>']\n",
      "['<s>', 'At', 'the', 'Americans', 'did', 'not', 'eating', '<unk>', '.', '</s>']\n",
      "['<s>', 'At', 'the', 'Americans', 'do', \"n't\", '<unk>', '.', '</s>']\n",
      "['<s>', 'At', 'the', 'Americans', 'did', 'not', 'eating', 'the', 'politics', '.', '</s>']\n",
      "['<s>', 'We', \"'ve\", 'got', 'of', 'the', 'danger', 'is', \"n't\", 'there', '.', '</s>']\n",
      "['<s>', 'We', \"'ve\", 'got', 'of', 'the', 'danger', '.', '</s>']\n",
      "['<s>', 'We', \"'ve\", 'got', 'of', 'the', 'danger', 'is', \"n't\", 'there', '?', '</s>']\n",
      "['<s>', 'And', 'we', ',', 'of', 'course', ',', 'about', '<unk>', '.', '</s>']\n",
      "['<s>', 'And', 'of', 'course', '--', 'of', 'course', 'about', 'kin', '.', '</s>']\n",
      "['<s>', 'And', 'we', ',', 'of', 'course', ',', 'about', '<unk>', '<unk>', '.', '</s>']\n",
      "['<s>', 'Our', 'fault', 'is', 'the', 'company', 'that', \"'s\", 'food', '.', '</s>']\n",
      "['<s>', 'Our', 'fault', 'is', 'the', 'company', ',', '</s>']\n",
      "['<s>', 'Our', 'fault', 'is', 'the', 'company', 'that', \"'s\", 'way', '.', '</s>']\n",
      "['<s>', 'We', 'need', 'light', 'to', 'do', 'we', 'do', '.', '</s>']\n",
      "['<s>', 'We', 'need', 'light', 'to', 'do', 'we', \"'re\", '.', '</s>']\n",
      "['<s>', 'We', 'need', 'to', 'see', '.', '</s>']\n",
      "['<s>', 'But', 'they', \"'re\", 'not', 'in', 'London', '.', '</s>']\n",
      "['<s>', 'They', \"'re\", 'not', 'in', 'London', '.', '</s>']\n",
      "['<s>', 'They', 'are', 'not', 'in', 'London', '.', '</s>']\n",
      "['<s>', 'They', 'have', 'written', '100', 'ways', '.', '</s>']\n",
      "['<s>', 'They', 'have', 'written', '100', 'years', '.', '</s>']\n",
      "['<s>', 'They', 'have', 'written', '100', 'years', '</s>']\n",
      "['<s>', 'He', \"'s\", 'going', 'out', '.', '</s>']\n",
      "['<s>', 'He', \"'s\", 'coming', 'out', 'of', 'course', '.', '</s>']\n",
      "['<s>', 'He', \"'s\", 'coming', 'out', 'of', 'course', ',', '</s>']\n",
      "['<s>', 'Why', 'do', 'we', 'feel', 'that', '?', '</s>']\n",
      "['<s>', 'Why', 'do', 'we', 'feel', 'this', '?', '</s>']\n",
      "['<s>', 'Why', 'are', 'gun', 'than', 'this', '?', '</s>']\n",
      "['<s>', 'I', 'hope', 'that', 'this', 'is', 'an', 'incredible', 'story', '.', '</s>']\n",
      "['<s>', 'I', 'hope', 'that', 'this', 'is', 'a', 'amazing', 'story', '.', '</s>']\n",
      "['<s>', 'I', 'hope', 'you', 'have', 'been', 'an', 'amazing', 'story', '.', '</s>']\n",
      "['<s>', 'There', \"'s\", 'going', 'to', 'another', '.', '</s>']\n",
      "['<s>', 'There', 'is', 'another', 'source', 'of', 'them', '<pad>', '<pad>', '<pad>', '.', '</s>']\n",
      "['<s>', 'There', 'is', 'another', 'source', 'of', 'them', '<pad>', '<pad>', '<pad>', \"'s\", '<unk>', '.', '</s>']\n",
      "['<s>', 'And', 'it', 'was', 'probably', 'the', '<unk>', 'country', '.', '</s>']\n",
      "['<s>', 'And', 'it', 'was', 'probably', 'the', 'most', '<unk>', '.', '</s>']\n",
      "['<s>', 'And', 'it', 'was', 'probably', 'the', 'most', 'country', '.', '</s>']\n",
      "['<s>', 'And', 'for', 'two', 'years', ',', 'you', 'could', 'see', 'anything', 'else', '.', '</s>']\n",
      "['<s>', 'And', 'for', 'two', 'years', '.', '</s>']\n",
      "['<s>', 'And', 'for', 'two', 'years', ',', '</s>']\n",
      "['<s>', 'And', 'at', 'a', 'sphere', ',', 'the', 'circle', 'you', '.', '</s>']\n",
      "['<s>', 'And', 'on', 'a', 'sphere', ',', '<unk>', 'circle', 'that', 'can', '.', '</s>']\n",
      "['<s>', 'And', 'on', 'a', 'sphere', ',', '<unk>', 'circle', 'that', 'can', ',', '</s>']\n",
      "['<s>', 'We', 'have', 'different', 'scenarios', ',', 'and', 'the', 'picture', 'looks', 'like', 'this', 'image', '.', '</s>']\n",
      "['<s>', 'We', 'have', 'different', 'scenarios', ',', 'and', 'the', 'picture', 'looked', '.', '</s>']\n",
      "['<s>', 'We', 'have', 'different', 'scenarios', ',', 'and', 'the', 'picture', 'looks', 'like', 'this', 'picture', 'of', 'the', 'picture', '.', '</s>']\n",
      "['<s>', 'Time', 'is', 'in', 'the', 'time', '.', '</s>']\n",
      "['<s>', 'Time', 'is', 'at', 'the', 'time', '.', '</s>']\n",
      "['<s>', 'Time', 'is', 'at', 'that', 'time', '.', '</s>']\n",
      "['<s>', 'And', 'what', 'I', 'found', 'was', 'the', 'theater', '.', '</s>']\n",
      "['<s>', 'And', 'what', 'I', 'found', 'the', 'theater', '.', '</s>']\n",
      "['<s>', 'And', 'I', 'found', 'was', 'the', 'theater', '.', '</s>']\n",
      "['<s>', 'Put', 'this', '.', '</s>']\n",
      "['<s>', 'Put', 'aside', '.', '</s>']\n",
      "['<s>', 'Put', 'it', 'was', '<unk>', '.', '</s>']\n",
      "['<s>', 'And', 'the', 'answer', 'is', 'no', 'matter', '.', '</s>']\n",
      "['<s>', 'And', 'the', 'answer', 'is', 'no', 'longer', '.', '</s>']\n",
      "['<s>', 'And', 'the', 'answer', 'is', 'nothing', 'has', 'nothing', '.', '</s>']\n",
      "['<s>', 'It', \"'s\", 'kind', 'of', 'structure', '.', '</s>']\n",
      "['<s>', 'It', \"'s\", 'sort', 'of', 'structure', '.', '</s>']\n",
      "['<s>', 'It', \"'s\", 'sort', 'of', 'shape', '.', '</s>']\n",
      "['<s>', 'And', 'you', 'see', 'a', 'little', 'line', 'through', '.', '</s>']\n",
      "['<s>', 'And', 'you', 'see', 'a', 'little', 'line', '.', '</s>']\n",
      "['<s>', 'And', 'you', 'see', 'a', 'little', 'line', 'by', 'the', 'picture', '.', '</s>']\n",
      "['<s>', 'So', 'we', 'get', 'that', 'information', '.', '</s>']\n",
      "['<s>', 'So', 'we', 'get', 'this', 'information', 'in', 'the', 'way', '.', '</s>']\n",
      "['<s>', 'So', 'we', 'get', 'this', 'information', 'of', 'the', 'body', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "'''alpha = 0.7\n",
    "K = 3\n",
    "for b in range(BATCH_SIZE):\n",
    "    normalized_probs = torch.tensor([], device='cuda')\n",
    "    for sentence in range(len(done[b])):\n",
    "        normalized = torch.tensor([done[b][sentence][1]/done[b][sentence][2]**alpha], device='cuda')\n",
    "        normalized_probs = torch.cat((normalized_probs,normalized),0)\n",
    "    top = torch.argsort(normalized_probs, descending=True)[0:K]\n",
    "    for k in range(K):\n",
    "        best = done[b][top[k]]\n",
    "        if padding:\n",
    "            m = nn.ConstantPad1d((0, max_len - best[2]), EN.vocab.stoi['<pad>'])\n",
    "            predictions[b].append(m(best[0].long()))\n",
    "        else:\n",
    "            predictions[b].append(best[0].long())\n",
    "        print([EN.vocab.itos[i] for i in best[0].long()])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['So', 'if', 'none', 'of', 'that', 'things', 'is', '?', 'problem', '?', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'What', 'if', 'none', 'of', 'these', 'things', 'is', 'the', 'problem', '?', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "encoder_outputs, encoder_hidden = attn_seq2context(src)\n",
    "decoder_context = torch.zeros(BATCH_SIZE, context_size, device='cuda') # 32 x 500\n",
    "decoder_hidden = encoder_hidden\n",
    "\n",
    "sentence = []\n",
    "trg = batch.trg.values.transpose(0,1)\n",
    "\n",
    "b = 0\n",
    "for j in range(trg.shape[1] - 1):\n",
    "    word_input = trg[:,j]\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = attn_context2trg(word_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "    sentence.extend([torch.argmax(decoder_output.detach()[b,:],dim=0)])\n",
    "print([EN.vocab.itos[i] for i in sentence])\n",
    "print([EN.vocab.itos[i] for i in trg[b,:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'What', 'if', 'neither', 'is', '?', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'is', 'the', 'problem', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'problem', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'is', 'the', 'problem', '?', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'problem', '?', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', 'that', '?', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', \"n't\", 'the', 'problem', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', 'that', 'the', 'problem', '?', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', \"n't\", 'the', 'problem', 'is', 'that', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', 'that', 'the', 'problem', 'is', 'that', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', \"n't\", 'the', 'problem', 'is', 'that', '?', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', \"n't\", 'the', 'problem', 'is', 'the', 'problem', '?', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', \"n't\", 'the', 'problem', 'is', 'the', 'problem', ':', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', \"n't\", 'the', 'problem', 'is', 'the', 'problem', '?', '\"', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', \"n't\", 'the', 'problem', 'is', 'the', 'problem', '<pad>', '<pad>', '<pad>', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', \"n't\", 'the', 'problem', 'is', 'the', 'problem', '?', 'Right', '?', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', \"n't\", 'the', 'problem', 'is', 'the', 'problem', '?', '\"', '<pad>', '<pad>', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', \"n't\", 'the', 'problem', 'is', 'the', 'problem', '?', 'Right', '?', '<pad>', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', \"n't\", 'the', 'problem', 'is', 'the', 'problem', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', \"n't\", 'the', 'problem', 'is', 'the', 'problem', '?', '\"', '<pad>', '<pad>', '<pad>', '</s>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', \"n't\", 'the', 'problem', 'is', 'the', 'problem', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', \"n't\", 'the', 'problem', 'is', 'the', 'problem', '?', '\"', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'What', 'if', 'neither', 'are', 'the', 'issue', 'is', \"n't\", 'the', 'problem', 'is', 'the', 'problem', '?', 'Right', '?', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'What', 'if', 'none', 'of', 'these', 'things', 'is', 'the', 'problem', '?', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(done[b])):\n",
    "    print([EN.vocab.itos[i] for i in done[b][j][0].long()])\n",
    "for j in range(BEAM_WIDTH):\n",
    "    print([EN.vocab.itos[i] for i in b_string[b,:,j].long()])\n",
    "print([EN.vocab.itos[i] for i in trg[b,:].long()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:\n",
      "['<s>', 'Once', 'the', 'parasites', ',', 'nothing', 'to', 'raise', 'the', 'parasites', '.', 'Okay', ',', 'right', ',', 'the', '<unk>', 'did']\n",
      "['<s>', 'Once', 'the', 'parasites', ',', 'nothing', 'to', 'raise', 'the', 'parasites', '.', 'Okay', '?', 'the', 'parasites', ',', 'the', '<unk>']\n",
      "['<s>', 'Once', 'the', 'parasites', ',', 'nothing', 'to', 'raise', 'the', 'parasites', '.', 'Okay', '?', '<unk>', '<unk>', ',', 'the', 'report']\n",
      "['<s>', 'Once', 'the', 'parasites', ',', 'nothing', 'to', 'raise', 'the', 'parasites', '.', 'Okay', '?', '<unk>', '<unk>', ',', 'the', '<unk>']\n",
      "['<s>', 'Once', 'the', 'parasites', ',', 'nothing', 'to', 'raise', 'the', 'parasites', '.', 'Okay', '?', 'the', 'parasites', ',', 'you', 'have']\n",
      "actual:\n",
      "['<s>', 'Once', 'the', 'parasites', 'get', 'in', ',', 'the', 'hosts', 'do', \"n't\", 'get', 'a', 'say', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'And', 'nobody', 'can', 'can', 'do', 'aging', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'And', 'nobody', 'can', 'can', 'do', 'aging', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'reasonably']\n",
      "['<s>', 'And', 'nobody', 'can', 'can', 'do', 'aging', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'utter']\n",
      "['<s>', 'And', 'nobody', 'can', 'can', 'do', 'aging', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'noticing']\n",
      "['<s>', 'And', 'nobody', 'can', 'can', 'do', 'aging', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', ':']\n",
      "actual:\n",
      "['<s>', 'And', 'nobody', 'knows', 'if', 'they', 'really', 'can', 'live', 'forever', ',', 'or', 'what', 'keeps', 'them', 'from', 'aging', '.', '</s>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'We', 'all', 'that', 'long', 'pathway', '.', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'We', 'depend', 'on', 'the', '<unk>', ':', ')', 'I', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'We', 'depend', 'on', 'the', '<unk>', ':', ')', 'We', \"'re\", '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'We', 'depend', 'on', 'the', '<unk>', ':', 'Right', '?', 'Let', \"'s\", 'out', ':', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'We', 'depend', 'on', 'the', '<unk>', ':', 'Right', '?', 'Let', \"'s\", '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "actual:\n",
      "['<s>', 'We', \"'re\", 'all', 'trapped', 'in', 'these', 'lines', '!', 'These', 'long', 'lines', 'trying', 'to', 'get', 'on', 'an', 'airplane', '!', '</s>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'This', 'is', 'the', 'stored', 'right', 'in', 'the', 'winds', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'This', 'is', 'the', 'stored', 'right', 'in', 'the', 'stream', 'is', 'there', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'This', 'is', 'the', 'stored', 'right', 'in', 'the', 'stream', 'is', 'it', \"'s\", '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'This', 'is', 'the', 'stored', 'right', 'in', 'the', 'stream', 'is', 'it', \"'s\", ':', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'This', 'is', 'the', 'stored', 'right', 'in', 'the', 'stream', 'is', 'it', \"'s\", '.', '\"', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "actual:\n",
      "['<s>', 'And', 'this', 'is', 'working', 'on', 'the', 'stored', 'winds', 'in', 'the', 'bottles', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'And', 'the', 'cost', 'of', 'the', 'combinations', ':', 'the', '<unk>', ':', 'smiley', 'than', 'the', '-', 'hand', 'as', 'a']\n",
      "['<s>', 'And', 'the', 'cost', 'of', 'the', 'combinations', ':', 'the', '<unk>', ':', 'smiley', 'than', 'the', '-', 'hand', '<unk>', '<unk>']\n",
      "['<s>', 'And', 'the', 'cost', 'of', 'the', 'combinations', ':', 'the', '<unk>', ':', 'smiley', 'than', 'the', '-', 'Earth', 'limited', '.']\n",
      "['<s>', 'And', 'the', 'cost', 'of', 'the', 'combinations', ':', 'the', '<unk>', ':', 'smiley', 'than', 'the', '-', 'Earth', 'limited', ',']\n",
      "['<s>', 'And', 'the', 'cost', 'of', 'the', 'combinations', ':', 'the', '<unk>', ':', 'smiley', 'than', 'the', '-', 'Earth', 'limited', 'in']\n",
      "actual:\n",
      "['<s>', 'And', 'with', 'the', 'cost', 'of', 'these', 'tools', 'being', 'substantially', 'lower', 'than', 'the', 'bribe', 'demanded', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'And', 'we', 'were', '--', 'the', 'face', '.', '\"', 'And', 'we', \"'ve\", 'heard', 'of', 'course', ',', 'right', 'now']\n",
      "['<s>', 'And', 'we', 'were', '--', 'the', 'face', '.', '\"', 'And', 'we', \"'ve\", 'heard', 'of', 'this', 'is', 'here', 'is']\n",
      "['<s>', 'And', 'we', 'were', '--', 'the', 'face', '.', '\"', 'And', 'we', \"'ve\", 'heard', 'of', 'this', 'is', 'about', 'this']\n",
      "['<s>', 'And', 'we', 'were', '--', 'the', 'face', '.', '\"', 'And', 'we', \"'ve\", 'heard', 'of', 'this', 'is', 'now', '!']\n",
      "['<s>', 'And', 'we', 'were', '--', 'the', 'face', '.', '\"', 'And', 'we', \"'ve\", 'heard', 'of', 'this', 'is', 'there', \"'s\"]\n",
      "actual:\n",
      "['<s>', 'And', 'we', 'were', 'in', 'the', 'desert', '--', 'Richard', 'Wurman', ':', 'That', \"'s\", 'the', 'end', 'of', 'this', 'talk', '!', '</s>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'I', \"'m\", 'one', 'an', 'example', ':', '\"', 'is', 'a', 'one', 'is', ',', 'an', 'example', '.', 'China', '.']\n",
      "['<s>', 'I', \"'m\", 'one', 'an', 'example', ':', 'I', \"'m\", 'a', 'example', '.', 'I', \"'m\", 'point', ':', 'China', '.']\n",
      "['<s>', 'I', \"'m\", 'one', 'an', 'example', ':', 'I', \"'m\", 'a', 'example', '.', 'I', \"'m\", 'point', ':', 'China', 'is']\n",
      "['<s>', 'I', \"'m\", 'one', 'an', 'example', ':', 'I', \"'m\", 'a', 'example', '.', 'I', \"'m\", ',', 'the', '<unk>', ':']\n",
      "['<s>', 'I', \"'m\", 'one', 'an', 'example', ':', 'I', \"'m\", 'a', 'example', '.', 'I', \"'m\", ',', 'the', '<unk>', '.']\n",
      "actual:\n",
      "['<s>', 'I', \"'ll\", 'give', 'an', 'example', '.', 'China', 'is', 'a', '<unk>', 'country', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'What', 'has', 'the', 'War', 'called', 'the', 'world', 'to', 'do', 'that', \"'s\", 'Earth', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'What', 'has', 'the', 'War', 'called', 'the', 'world', 'to', 'the', 'world', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'What', 'has', 'the', 'War', 'called', 'the', 'world', 'to', 'the', 'world', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '?', '<pad>']\n",
      "['<s>', 'What', 'has', 'the', 'War', 'called', 'the', 'world', 'to', 'the', 'world', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', \"'s\", 'a']\n",
      "['<s>', 'What', 'has', 'the', 'War', 'called', 'the', 'world', 'to', 'the', 'world', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', \"'s\", '<unk>']\n",
      "actual:\n",
      "['<s>', 'What', 'has', 'the', 'War', 'on', 'Drugs', 'done', 'to', 'the', 'world', '?', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'Come', 'on', 'you', \"'re\", 'on', 'it', \"'s\", 'the', 'way', 'it', \"'s\", 'working', 'on', 'all', 'way', 'to', 'turn']\n",
      "['<s>', 'Come', 'on', 'you', \"'re\", 'on', 'it', \"'s\", 'the', 'way', 'it', \"'s\", 'working', 'on', 'all', 'way', 'to', 'get']\n",
      "['<s>', 'Come', 'on', 'you', \"'re\", 'on', 'it', \"'s\", 'the', 'way', 'it', \"'s\", 'working', 'on', 'all', 'way', 'to', 'keep']\n",
      "['<s>', 'Come', 'on', 'you', \"'re\", 'on', 'it', \"'s\", 'the', 'way', 'it', \"'s\", 'working', 'on', 'all', 'way', 'to', 'make']\n",
      "['<s>', 'Come', 'on', 'you', \"'re\", 'on', 'it', \"'s\", 'the', 'way', 'it', \"'s\", 'working', 'on', 'all', 'way', 'to', 'take']\n",
      "actual:\n",
      "['<s>', 'Come', 'on', ',', 'keep', 'up', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'I', 'really', ',', 'really', ',', 'really', '<unk>', '-', 'on', 'the', 'design', ':', 'I', 'really', 'interesting', 'thing', '.']\n",
      "['<s>', 'I', 'really', ',', 'really', ',', 'really', '<unk>', '-', 'on', 'the', 'design', ':', 'I', 'really', 'interesting', 'thing', ':']\n",
      "['<s>', 'I', 'really', ',', 'really', ',', 'really', '<unk>', '-', 'on', 'the', 'design', ':', 'I', 'really', 'interesting', 'way', '.']\n",
      "['<s>', 'I', 'really', ',', 'really', ',', 'really', '<unk>', '-', 'on', 'the', 'design', ':', 'I', 'really', 'interesting', 'way', 'is']\n",
      "['<s>', 'I', 'really', ',', 'really', ',', 'really', '<unk>', '-', 'on', 'the', 'design', ':', 'I', 'really', 'interesting', 'way', 'to']\n",
      "actual:\n",
      "['<s>', 'I', 'really', 'got', 'close', 'to', 'design', 'again', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'You', ',', 'there', 'is', 'no', 'time', 'and', 'growth', 'is', 'it', \"'s\", 'is', 'a', 'lot', 'about', 'growth', ':']\n",
      "['<s>', 'You', ',', 'there', 'is', 'no', 'time', 'and', 'growth', 'is', 'it', \"'s\", 'is', 'a', 'lot', 'of', 'growth', 'rape']\n",
      "['<s>', 'You', ',', 'there', 'is', 'no', 'time', 'and', 'growth', 'is', 'it', \"'s\", 'is', 'a', 'lot', 'of', 'growth', 'malnutrition']\n",
      "['<s>', 'You', ',', 'there', 'is', 'no', 'time', 'and', 'growth', 'is', 'it', \"'s\", 'is', 'a', 'lot', 'of', 'growth', 'desalination']\n",
      "['<s>', 'You', ',', 'there', 'is', 'no', 'time', 'and', 'growth', 'is', 'it', \"'s\", 'is', 'a', 'lot', 'about', 'fruition', ':']\n",
      "actual:\n",
      "['<s>', 'Obviously', ',', 'there', \"'s\", 'no', 'guarantee', 'that', 'it', 'can', 'be', 'a', 'time', 'of', 'fruition', 'and', 'growth', '.', '</s>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'For', 'instance', 'in', 'a', 'large', ',', 'in', 'a', 'material', 'we', 'know', 'me', 'know', ',', 'in', 'a', 'bit']\n",
      "['<s>', 'For', 'instance', 'in', 'a', 'large', ',', 'in', 'a', 'material', 'we', 'know', 'me', 'know', ',', 'in', 'the', '<unk>']\n",
      "['<s>', 'For', 'instance', 'in', 'a', 'large', ',', 'in', 'a', 'material', 'we', 'know', 'me', 'know', ',', 'in', '-', 'time']\n",
      "['<s>', 'For', 'instance', 'in', 'a', 'large', ',', 'in', 'a', 'material', 'we', 'know', 'me', 'know', ',', 'for', 'a', 'piece']\n",
      "['<s>', 'For', 'instance', 'in', 'a', 'large', ',', 'in', 'a', 'material', 'we', 'know', 'me', 'know', ',', 'in', 'the', 'thing']\n",
      "actual:\n",
      "['<s>', '<unk>', '<unk>', 'in', 'a', 'material', 'for', 'instance', ',', 'always', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'These', 'birds', 'they', 'flying', 'into', 'the', 'story', 'they', \"'re\", 'that', 'put', 'them', 'into', 'this', 'they', 'were', 'she']\n",
      "['<s>', 'These', 'birds', 'they', 'flying', 'into', 'the', 'story', 'they', \"'re\", 'that', 'put', 'them', 'into', 'this', 'they', 'were', 'they']\n",
      "['<s>', 'These', 'birds', 'they', 'flying', 'into', 'the', 'story', 'they', \"'re\", 'that', 'put', 'them', 'into', 'the', 'way', 'in', 'it']\n",
      "['<s>', 'These', 'birds', 'they', 'flying', 'into', 'the', 'story', 'they', \"'re\", 'that', 'put', 'them', 'into', 'the', 'way', 'in', 'the']\n",
      "['<s>', 'These', 'birds', 'they', 'flying', 'into', 'the', 'story', 'they', \"'re\", 'that', 'put', 'them', 'into', 'this', 'they', 'go', '.']\n",
      "actual:\n",
      "['<s>', 'These', 'birds', 'make', 'a', 'living', 'by', 'diving', 'into', 'the', 'water', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'We', \"'ve\", 'asked', 'for', 'us', \"'re\", 'doing', 'it', 'just', 'we', 'do', 'it', \"'s\", 'it', ',', 'we', 'do']\n",
      "['<s>', 'We', \"'ve\", 'asked', 'for', 'us', \"'re\", 'doing', 'it', 'just', 'we', 'do', 'it', \"'s\", 'it', ',', 'we', 'just']\n",
      "['<s>', 'We', \"'ve\", 'asked', 'for', 'us', \"'re\", 'doing', 'it', 'just', 'we', 'do', 'it', \"'s\", 'it', ',', 'we', \"'re\"]\n",
      "['<s>', 'We', \"'ve\", 'asked', 'for', 'us', \"'re\", 'doing', 'it', \"'s\", 'just', 'do', 'that', 'we', '.', 'And', 'we', 'do']\n",
      "['<s>', 'We', \"'ve\", 'asked', 'for', 'us', \"'re\", 'doing', 'it', 'just', 'we', 'do', 'it', \"'s\", 'it', ',', 'here', '.']\n",
      "actual:\n",
      "['<s>', 'We', \"'ve\", 'not', 'asked', 'anybody', \"'s\", 'permission', 'to', 'do', 'this', ',', 'we', \"'re\", 'just', 'doing', 'it', '.', '</s>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', '<unk>', ',', 'turn', 'it', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', '<unk>', ',', 'turn', 'it', '<pad>', '<pad>', '<pad>', ',', 'I', 'guess', '.', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', '<unk>', ',', 'turn', 'it', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '.', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', '<unk>', ',', 'turn', 'it', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '.', '<pad>', '<pad>']\n",
      "['<s>', '<unk>', ',', 'turn', 'it', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'brilliant', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "actual:\n",
      "['<s>', '<unk>', 'split', 'in', 'half', 'and', '<unk>', ',', 'and', 'mythology', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'But', 'that', 'changed', ',', 'right', ':', '<unk>', ',', 'right', 'there', 'is', 'a', 'lot', 'of', ',', 'right', '?']\n",
      "['<s>', 'But', 'that', 'changed', ',', 'right', ':', '<unk>', ',', 'right', 'there', 'is', 'a', 'lot', 'of', 'it', \"'s\", 'the']\n",
      "['<s>', 'But', 'that', 'changed', ',', 'right', ':', '<unk>', ',', 'right', 'there', 'is', 'a', 'lot', 'of', ',', 'it', '.']\n",
      "['<s>', 'But', 'that', 'changed', ',', 'right', ':', '<unk>', ',', 'right', 'there', 'is', 'a', 'lot', 'of', ',', 'it', '?']\n",
      "['<s>', 'But', 'that', 'changed', ',', 'right', ':', '<unk>', ',', 'right', 'there', 'is', 'a', 'lot', 'of', ',', 'you', '.']\n",
      "actual:\n",
      "['<s>', 'But', 'this', 'has', 'changed', ',', 'okay', '?', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'That', \"'s\", '--', 'Thank', 'you', 'know', 'it', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'That', \"'s\", '--', 'Thank', 'you', 'know', 'the', 'way', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'That', \"'s\", '--', 'thank', 'you', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '?', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'That', \"'s\", '--', 'thank', 'you', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', ':', 'Thank', \"n't\", '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'That', \"'s\", '--', 'thank', 'you', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', ':', 'Thank', \"n't\", \"'s\", '<pad>', '<pad>']\n",
      "actual:\n",
      "['<s>', 'So', 'this', '--', 'Thank', 'you', 'very', 'much', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'Yes', ',', 'it', \"'s\", 'a', 'little', 'project', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'Yes', ',', 'it', \"'s\", 'a', 'little', 'generator', 'is', 'a', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'Yes', ',', 'it', \"'s\", 'a', 'little', 'generator', '<pad>', '<pad>', 'out', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'Yes', ',', 'it', \"'s\", 'a', 'little', 'generator', '<pad>', '<pad>', 'on', 'a', 'little', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'Yes', ',', 'it', \"'s\", 'a', 'little', 'generator', '<pad>', '<pad>', 'on', '.', 'It', 'is', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "actual:\n",
      "['<s>', 'Yeah', ',', 'it', \"'s\", 'just', 'a', 'fluke', '.', 'It', \"'s\", 'a', '<unk>', 'fluke', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'Well', ',', 'I', 'was', \"n't\", 'been', '<unk>', ',', 'I', 'did', 'this', 'stuff', ':', 'I', 'was', \"n't\", '.']\n",
      "['<s>', 'Well', ',', 'I', 'was', \"n't\", 'been', '<unk>', ',', 'I', 'did', 'this', 'stuff', ':', 'I', 'was', 'not', '.']\n",
      "['<s>', 'Well', ',', 'I', 'was', \"n't\", 'been', '<unk>', ',', 'I', 'did', 'this', 'stuff', ':', 'I', 'was', 'a', '<pad>']\n",
      "['<s>', 'Well', ',', 'I', 'was', \"n't\", 'been', '<unk>', ',', 'I', 'did', 'this', 'stuff', ':', 'I', 'was', \"n't\", ',']\n",
      "['<s>', 'Well', ',', 'I', 'was', \"n't\", 'been', '<unk>', ',', 'I', 'did', 'this', 'stuff', ':', 'I', 'was', \"n't\", 'a']\n",
      "actual:\n",
      "['<s>', 'Now', 'you', 'and', 'I', 'could', 'argue', 'that', 'they', 'probably', 'did', 'not', 'need', 'to', 'touch', 'her', 'breasts', '.', '</s>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', '\"', 'How', 'you', 'guys', '?', '\"', 'Okay', '?', '\"', 'What', 'do', 'you', 'know', '?', '\"', '?', '\"']\n",
      "['<s>', '\"', 'How', 'you', 'guys', '?', '\"', 'Okay', '?', '\"', 'What', 'do', 'you', 'know', '?', '\"', '.', '\"']\n",
      "['<s>', '\"', 'How', 'you', 'guys', '?', '\"', 'Okay', '?', '\"', 'What', 'do', 'you', 'know', '?', '\"', 'about', 'you']\n",
      "['<s>', '\"', 'How', 'you', 'guys', '?', '\"', 'Okay', '?', '\"', 'What', 'do', 'you', 'know', '?', '\"', 'about', 'your']\n",
      "['<s>', '\"', 'How', 'you', 'guys', '?', '\"', 'Okay', '?', '\"', 'What', 'do', 'you', 'know', '?', '\"', 'about', '<unk>']\n",
      "actual:\n",
      "['<s>', '\"', 'How', 'do', 'you', '<unk>', 'up', 'pop', '-', 'up', 'purple', 'paper', 'people', '?', '\"', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', '<unk>', 'up', '.', 'Do', 'a', 'good', 'person', '.', \"'s\", 'a', 'high', 'is', 'a', 'good', ',', 'you', 'know']\n",
      "['<s>', '<unk>', 'up', '.', 'Do', 'a', 'good', 'person', '.', \"'s\", 'a', 'high', 'is', 'a', 'good', ',', 'you', 'can']\n",
      "['<s>', '<unk>', 'up', '.', 'Do', 'a', 'good', 'person', '.', \"'s\", 'a', 'high', 'is', 'a', 'good', 'guy', 'guess', '.']\n",
      "['<s>', '<unk>', 'up', '.', 'Do', 'a', 'good', 'person', '.', \"'s\", 'a', 'high', 'is', 'a', 'good', ',', 'this', 'way']\n",
      "['<s>', '<unk>', 'up', '.', 'Do', 'a', 'good', 'person', '.', \"'s\", 'a', 'high', 'is', 'a', 'good', 'guy', \"'m\", 'a']\n",
      "actual:\n",
      "['<s>', '<unk>', 'up', '.', '<unk>', 'out', '.', 'Be', 'a', 'good', 'person', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'The', 'faucet', 'like', 'that', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'The', 'hell', 'looks', 'on', '2020', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'The', 'hell', 'looks', 'on', '2020', '.', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'The', 'faucet', 'like', 'that', 'looks', '.', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'The', 'faucet', 'like', 'that', 'way', '.', '\"', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "actual:\n",
      "['<s>', 'and', 'the', 'overall', 'picture', 'looks', 'like', 'this', 'by', '2020', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', '<unk>', 'are', \"n't\", 'yet', ',', 'you', \"'re\", 'not', 'that', 'we', 'do', \"n't\", 'make', 'this', 'is', 'not', 'static']\n",
      "['<s>', '<unk>', 'are', \"n't\", 'yet', ',', 'you', \"'re\", 'not', 'that', 'we', 'do', \"n't\", 'be', 'static', '<unk>', ':', '<unk>']\n",
      "['<s>', '<unk>', 'are', \"n't\", 'yet', ',', 'you', \"'re\", 'not', 'that', 'we', 'do', \"n't\", 'make', 'that', 'you', \"'re\", 'not']\n",
      "['<s>', '<unk>', 'are', \"n't\", 'yet', ',', 'you', \"'re\", 'not', 'that', 'we', 'do', \"n't\", 'make', 'this', 'is', \"n't\", 'static']\n",
      "['<s>', '<unk>', 'are', \"n't\", 'yet', ',', 'you', \"'re\", 'not', 'that', 'we', 'do', \"n't\", 'make', 'this', 'is', \"n't\", 'kidding']\n",
      "actual:\n",
      "['<s>', 'Models', 'are', 'not', 'static', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'Over', 'a', 'very', 'old', 'moment', '.', 'It', \"'s\", 'ancient', 'life', 'is', 'a', 'very', 'ancient', 'captures', 'this', 'is']\n",
      "['<s>', 'Over', 'a', 'very', 'old', 'moment', '.', 'It', \"'s\", 'ancient', 'life', 'is', 'a', 'very', 'ancient', 'captures', 'a', 'ancient']\n",
      "['<s>', 'Over', 'a', 'very', 'old', 'moment', '.', 'It', \"'s\", 'ancient', 'life', 'is', 'a', 'very', 'ancient', 'captures', 'that', 'tradition']\n",
      "['<s>', 'Over', 'a', 'very', 'old', 'moment', '.', 'It', \"'s\", 'ancient', 'life', 'is', 'a', 'very', 'ancient', 'captures', 'a', 'very']\n",
      "['<s>', 'Over', 'a', 'very', 'old', 'moment', '.', 'It', \"'s\", 'ancient', 'life', 'is', 'a', 'very', 'ancient', 'argued', 'this', 'old']\n",
      "actual:\n",
      "['<s>', 'Moreover', ',', 'this', 'is', 'a', 'very', 'old', 'state', 'tradition', ',', 'a', 'very', 'old', 'tradition', 'of', '<unk>', '.', '</s>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'And', 'I', \"'m\", 'with', 'this', 'approach', ',', 'again', ',', 'OK', ',', 'what', 'you', 'know', 'with', 'this', 'piece']\n",
      "['<s>', 'And', 'I', \"'m\", 'with', 'this', 'approach', ',', 'again', ',', 'OK', ',', 'what', 'you', 'know', 'with', 'that', '?']\n",
      "['<s>', 'And', 'I', \"'m\", 'with', 'this', 'approach', ',', 'again', ',', 'OK', ',', 'what', 'you', 'know', 'with', 'that', '.']\n",
      "['<s>', 'And', 'I', \"'m\", 'with', 'this', 'approach', ',', 'again', ',', 'OK', ',', 'what', 'you', 'know', 'with', 'example', '?']\n",
      "['<s>', 'And', 'I', \"'m\", 'with', 'this', 'approach', ',', 'again', ',', 'OK', ',', 'I', 'want', 'to', 'you', 'know', '?']\n",
      "actual:\n",
      "['<s>', 'And', 'I', \"'ll\", 'show', 'you', ',', 'if', 'you', 'take', 'this', 'approach', ',', 'what', 'you', 'get', ',', 'OK', '?', '</s>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'And', 'so', ',', 'that', 'we', 'can', '<unk>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'And', 'so', ',', 'that', 'we', 'can', '<unk>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'out', 'of', 'other', 'surfaces', '<pad>']\n",
      "['<s>', 'And', 'so', ',', 'that', 'we', 'can', '<unk>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'out', 'of', 'other', 'of', 'the']\n",
      "['<s>', 'And', 'so', ',', 'that', 'we', 'can', '<unk>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'out', 'of', 'other', 'biologically', 'can']\n",
      "['<s>', 'And', 'so', ',', 'that', 'we', 'can', '<unk>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'out', 'of', 'other', 'biologically', \"'s\"]\n",
      "actual:\n",
      "['<s>', 'And', 'so', 'we', 'can', '<unk>', 'these', 'surfaces', 'biologically', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'In', 'three', 'months', ',', 'we', 'get', ',', 'and', 'we', 'said', ',', 'we', 'have', 'a', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'In', 'three', 'months', ',', 'we', 'get', ',', 'and', 'we', 'said', ',', 'we', 'have', 'a', 'little', '<unk>', '<pad>']\n",
      "['<s>', 'In', 'three', 'months', ',', 'we', 'get', ',', 'and', 'we', 'said', ',', 'we', 'have', 'a', 'tilt', '<pad>', '<pad>']\n",
      "['<s>', 'In', 'three', 'months', ',', 'we', 'get', ',', 'and', 'we', 'said', ',', 'we', 'have', 'a', 'little', '.', 'Here']\n",
      "['<s>', 'In', 'three', 'months', ',', 'we', 'get', ',', 'and', 'we', 'said', ',', 'we', 'have', 'a', 'little', '.', '\"']\n",
      "actual:\n",
      "['<s>', 'And', 'after', 'about', 'three', 'months', ',', 'the', 'nerves', 'grew', 'in', 'a', 'little', 'bit', 'and', 'we', 'could', 'get', 'a', '<unk>', '.', '</s>']\n",
      "predictions:\n",
      "['<s>', 'But', 'it', 'was', 'late', 'than', 'that', \"'s\", 'late', 'with', 'the', 'idea', 'it', 'was', 'too', 'late', 'it', 'was']\n",
      "['<s>', 'But', 'it', 'was', 'late', 'than', 'that', \"'s\", 'late', 'with', 'the', 'idea', 'it', 'was', 'too', 'late', 'it', \"'s\"]\n",
      "['<s>', 'But', 'it', 'was', 'late', 'than', 'that', \"'s\", 'late', 'with', 'the', 'idea', 'it', 'was', 'too', 'late', 'of', 'that']\n",
      "['<s>', 'But', 'it', 'was', 'late', 'than', 'that', \"'s\", 'late', 'with', 'the', 'idea', 'it', 'was', 'too', 'late', 'of', 'the']\n",
      "['<s>', 'But', 'it', 'was', 'late', 'than', 'that', \"'s\", 'late', 'with', 'the', 'idea', 'it', 'was', 'too', 'late', ',', 'too']\n",
      "actual:\n",
      "['<s>', 'But', 'it', 'was', 'too', 'late', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'It', 'led', 'to', 'the', 'next', 'this', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'It', 'led', 'to', 'the', 'next', 'this', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<unk>', '<pad>', '<pad>']\n",
      "['<s>', 'It', 'led', 'to', 'the', 'next', 'this', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'partners', '<pad>', '<pad>']\n",
      "['<s>', 'It', 'led', 'to', 'the', 'next', 'this', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'regulation', '.', '<pad>']\n",
      "['<s>', 'It', 'led', 'to', 'the', 'next', 'this', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'regulation', '.', '\"']\n",
      "actual:\n",
      "['<s>', 'And', 'that', 'moved', 'us', 'to', 'the', 'next', 'level', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'They', 'have', 'a', 'little', 'name', 'is', 'that', 'we', 'have', 'a', 'little', 'of', 'course', ',', 'we', 'called', '<unk>']\n",
      "['<s>', 'They', 'have', 'a', 'little', 'name', 'is', 'that', 'we', 'have', 'a', 'little', 'of', 'course', ',', 'we', 'called', '-']\n",
      "['<s>', 'They', 'have', 'a', 'little', 'name', 'is', 'that', 'we', 'have', 'a', 'little', 'of', 'course', ',', 'the', 'center', '.']\n",
      "['<s>', 'They', 'have', 'a', 'little', 'name', 'is', 'that', 'we', 'have', 'a', 'little', 'of', 'course', ',', 'we', 'calling', '.']\n",
      "['<s>', 'They', 'have', 'a', 'little', 'name', 'is', 'that', 'we', 'have', 'a', 'little', 'of', 'course', ',', 'are', 'we', '.']\n",
      "actual:\n",
      "['<s>', 'They', 'use', 'a', 'skeleton', 'that', 'we', 'call', 'a', '<unk>', 'skeleton', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'XL', ':', 'I', 'actually', '.', '\"', 'it', 'actually', ',', '\"', 'way', ',', 'actually', ',', '\"', '.', '\"']\n",
      "['<s>', 'XL', ':', 'I', 'actually', '.', '\"', 'it', 'actually', ',', '\"', 'way', 'it', 'worked', '.', '\"', '<unk>', '.']\n",
      "['<s>', 'XL', ':', 'I', 'actually', '.', '\"', 'it', 'actually', ',', '\"', 'way', ',', 'actually', ',', '\"', 'I', \"'ve\"]\n",
      "['<s>', 'XL', ':', 'I', 'actually', '.', '\"', 'it', 'actually', ',', '\"', 'way', ',', 'actually', ',', '\"', 'I', 'have']\n",
      "['<s>', 'XL', ':', 'I', 'actually', '.', '\"', 'it', 'actually', ',', '\"', 'way', ',', 'actually', ',', '\"', 'was', 'actually']\n",
      "actual:\n",
      "['<s>', 'XL', ':', '\"', 'Yes', ',', '\"', 'I', 'said', '.', '\"', 'Indeed', 'it', 'worked', '!', '\"', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "predictions:\n",
      "['<s>', 'It', \"'s\", 'for', 'a', 'ride', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'It', \"'s\", '<unk>', 'and', 'making', 'a', 'ride', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'It', \"'s\", '<unk>', 'and', 'to', 'a', 'much', '<unk>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'It', \"'s\", '<unk>', 'and', 'to', 'a', 'much', '<unk>', '.', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'It', \"'s\", '<unk>', 'and', 'to', 'a', 'much', '<unk>', '?', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "actual:\n",
      "['<s>', 'or', 'asking', 'friends', 'and', 'family', 'for', 'a', 'ride', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "'''# check out prediction\n",
    "for b in range(BATCH_SIZE):\n",
    "    print('predictions:')\n",
    "    for j in range(BEAM_WIDTH):\n",
    "        print([EN.vocab.itos[i] for i in b_string[b,:,j].long()])\n",
    "    print('actual:')\n",
    "    print([EN.vocab.itos[i] for i in trg[b,:].long()])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention:\n",
    "# https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb\n",
    "\n",
    "# normalization:\n",
    "# https://medium.com/machine-learning-bites/deeplearning-series-sequence-to-sequence-architectures-4c4ca89e5654"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test set\n",
    "sentences = []\n",
    "for i, l in enumerate(open(\"source_test.txt\")):\n",
    "  sentences.append(re.split(' ', l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_len = 0\n",
    "for i in range(len(sentences)):\n",
    "    if len(sentences[i]) > max_sent_len:\n",
    "        max_sent_len = len(sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.tensor([], device='cuda')\n",
    "for b in range(len(sentences)):\n",
    "    m = nn.ConstantPad1d((0, max_sent_len - len(sentences[b])), EN.vocab.stoi['<pad>'])\n",
    "    src = m(torch.tensor([DE.vocab.stoi[i] for i in sentences[b]], device='cuda').unsqueeze(0)).float()\n",
    "    batch = torch.cat((batch,src), dim=0)\n",
    "batch_rev = reverse_sequence(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 500\n",
    "num_layers = 2\n",
    "\n",
    "attn_seq2context = SequenceModel(len(DE.vocab),context_size,num_layers=num_layers)\n",
    "state_dict = torch.load('best_seq2seq_withattn_seq2context.pt')\n",
    "attn_seq2context.load_state_dict(state_dict)\n",
    "attn_seq2context = attn_seq2context.cuda()\n",
    "\n",
    "attn_context2trg = attn_RNNet_batched(input_size=len(EN.vocab),hidden_size=context_size,num_layers=num_layers)\n",
    "state_dict = torch.load('best_seq2seq_withattn_context2trg.pt')\n",
    "attn_context2trg.load_state_dict(state_dict)\n",
    "attn_context2trg = attn_context2trg.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 500\n",
    "num_layers = 2\n",
    "\n",
    "seq2context = SequenceModel(len(DE.vocab),context_size,num_layers=num_layers)\n",
    "state_dict = torch.load('best_seq2seq_seq2context.pt')\n",
    "seq2context.load_state_dict(state_dict)\n",
    "seq2context = seq2context.cuda()\n",
    "\n",
    "context2trg = RNNet(input_size=len(EN.vocab),hidden_size=context_size,num_layers=num_layers,weight_tie=True)\n",
    "state_dict = torch.load('best_seq2seq_context2trg.pt')\n",
    "context2trg.load_state_dict(state_dict)\n",
    "context2trg = context2trg.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position: 1\n",
      "position: 2\n",
      "beam: 0\n",
      "beam: 1\n",
      "beam: 2\n",
      "position: 3\n",
      "beam: 0\n",
      "beam: 1\n",
      "beam: 2\n",
      "position: 4\n",
      "beam: 0\n",
      "beam: 1\n",
      "beam: 2\n",
      "position: 5\n",
      "beam: 0\n",
      "beam: 1\n",
      "beam: 2\n",
      "position: 6\n",
      "beam: 0\n",
      "beam: 1\n",
      "beam: 2\n",
      "position: 7\n",
      "beam: 0\n",
      "beam: 1\n",
      "beam: 2\n",
      "position: 8\n",
      "beam: 0\n",
      "beam: 1\n",
      "beam: 2\n",
      "position: 9\n",
      "beam: 0\n",
      "beam: 1\n",
      "beam: 2\n",
      "position: 10\n",
      "beam: 0\n",
      "beam: 1\n",
      "beam: 2\n",
      "position: 11\n",
      "beam: 0\n",
      "beam: 1\n",
      "beam: 2\n",
      "position: 12\n",
      "beam: 0\n",
      "beam: 1\n",
      "beam: 2\n",
      "position: 13\n",
      "beam: 0\n",
      "beam: 1\n",
      "beam: 2\n",
      "position: 14\n",
      "beam: 0\n",
      "beam: 1\n",
      "beam: 2\n",
      "position: 15\n",
      "beam: 0\n",
      "beam: 1\n",
      "beam: 2\n",
      "position: 16\n",
      "beam: 0\n",
      "beam: 1\n",
      "beam: 2\n",
      "position: 17\n",
      "beam: 0\n",
      "beam: 1\n",
      "beam: 2\n",
      "position: 18\n",
      "beam: 0\n",
      "beam: 1\n",
      "beam: 2\n",
      "position: 19\n",
      "batch: 0\n",
      "['<s>', 'When', 'I', 'was', 'my', '<unk>', ',', 'I', 'had', 'my', 'first', '<unk>', '.', '</s>']\n",
      "batch: 1\n",
      "['<s>', 'I', 'was', '<unk>', ',', '<unk>', '.', '</s>']\n",
      "batch: 2\n",
      "['<s>', 'She', 'was', 'a', '<unk>', 'named', 'Alex', '.', '</s>']\n",
      "batch: 3\n",
      "['<s>', 'And', 'when', 'I', 'hear', 'I', 'happy', '.', '</s>']\n",
      "batch: 4\n",
      "['<s>', 'My', '<unk>', 'as', 'first', 'patients', '.', '</s>']\n",
      "batch: 5\n",
      "['<s>', 'And', 'I', 'had', 'a', 'woman', 'to', 'talk', '.', '</s>']\n",
      "batch: 6\n",
      "['<s>', 'And', 'I', 'thought', 'about', '<unk>', 'me', '.', '</s>']\n",
      "batch: 7\n",
      "['<s>', 'But', 'I', 'did', \"n't\", 'have', 'that', 'I', 'do', '.', '</s>']\n",
      "batch: 8\n",
      "['<s>', 'Dad', 'later', 'later', ',', 'later', ',', 'death', 'later', '.', '</s>']\n",
      "batch: 9\n",
      "['<s>', 'People', 'in', 'Alex', 'and', 'nothing', '.', '</s>']\n",
      "batch: 10\n",
      "['<s>', 'But', ',', 'my', 'Alex', '<unk>', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 11\n",
      "['<s>', 'I', 'was', '<unk>', '.', '</s>']\n",
      "batch: 12\n",
      "['<s>', 'And', 'because', 'I', 'said', ',', 'but', '<unk>', 'the', 'next', '.', '</s>']\n",
      "batch: 13\n",
      "['<s>', 'The', 'first', 'the', 'time', 'is', ',', 'for', 'him', 'is', 'married', '.', '</s>']\n",
      "batch: 14\n",
      "['<s>', 'That', \"'s\", 'a', '<unk>', '.', '</s>']\n",
      "batch: 15\n",
      "['<s>', 'And', 'that', 'I', 'realized', 'that', 'the', 'new', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 16\n",
      "['<s>', 'That', 'was', '<unk>', ',', 'and', 'we', 'sat', 'there', '.', '</s>']\n",
      "batch: 17\n",
      "['<s>', 'There', ',', 'in', 'the', 'United', 'States', ',', 'in', 'the', 'United', 'States', '.', '</s>']\n",
      "batch: 18\n",
      "['<s>', 'So', 'if', 'you', \"'re\", 'a', '<unk>', '.', '\"', '<unk>', '.', '</s>']\n",
      "batch: 19\n",
      "['<s>', 'I', 'want', 'to', 'see', '.', '</s>']\n",
      "batch: 20\n",
      "['<s>', 'Oh', ',', 'yeah', ',', 'you', 'are', 'incredibly', '<unk>', '.', '</s>']\n",
      "batch: 21\n",
      "['<s>', 'This', 'is', \"n't\", 'a', 'matter', '.', '</s>']\n",
      "batch: 22\n",
      "['<s>', 'We', 'know', 'that', '<unk>', 'moments', 'to', 'close', '<unk>', '.', '</s>']\n",
      "batch: 23\n",
      "['<s>', '<unk>', 'not', 'when', 'you', \"'re\", '<unk>', '.', '</s>']\n",
      "batch: 24\n",
      "['<s>', 'This', 'group', 'will', 'be', 'going', '.', '</s>']\n",
      "batch: 25\n",
      "['<s>', 'In', 'the', '1970s', ',', 'and', 'you', '.', '</s>']\n",
      "batch: 26\n",
      "['<s>', 'But', 'this', 'is', 'not', 'going', 'to', 'the', '<unk>', '.', '</s>']\n",
      "batch: 27\n",
      "['<s>', 'The', 'press', 'is', 'the', '<unk>', '.', '</s>']\n",
      "batch: 28\n",
      "['<s>', 'Scientists', 'call', 'the', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 29\n",
      "['<s>', 'This', 'is', '<unk>', '.', '</s>']\n",
      "batch: 30\n",
      "['<s>', 'As', 'the', 'culture', ',', 'what', 'the', 'most', '<unk>', 'is', '<unk>', ':', '</s>']\n",
      "batch: 31\n",
      "['<s>', '<unk>', '<unk>', 'great', 'time', 'a', 'little', 'time', '.', '</s>']\n",
      "batch: 32\n",
      "['<s>', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 33\n",
      "['<s>', 'There', \"'s\", '<unk>', '.', '</s>']\n",
      "batch: 34\n",
      "['<s>', 'That', 'is', '<unk>', 'and', 'and', 'it', 'does', 'nothing', '.', '</s>']\n",
      "batch: 35\n",
      "['<s>', 'I', 'had', 'a', 'better', '<unk>', '.', '</s>']\n",
      "batch: 36\n",
      "['<s>', 'And', 'it', 'was', 'like', 'the', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 37\n",
      "['<s>', 'Where', 'are', 'the', '<unk>', 'here', 'in', '<unk>', '.', '</s>']\n",
      "batch: 38\n",
      "['<s>', 'Do', \"n't\", 'it', '.', '</s>']\n",
      "batch: 39\n",
      "['<s>', 'OK', ',', 'that', \"'s\", 'hard', ',', 'because', 'it', \"'s\", 'a', 'lot', '<unk>', '.', '</s>']\n",
      "batch: 40\n",
      "['<s>', '<unk>', 'does', \"n't\", 'buy', 'a', 'red', 'disaster', '.', '</s>']\n",
      "batch: 41\n",
      "['<s>', 'It', 'means', 'you', 'do', \"n't\", 'do', '.', '</s>']\n",
      "batch: 42\n",
      "['<s>', 'I', 'want', 'to', 'do', 'the', 'best', '.', '</s>']\n",
      "batch: 43\n",
      "['<s>', 'This', 'is', 'a', 'story', 'that', 'is', '<unk>', '.', '</s>']\n",
      "batch: 44\n",
      "['<s>', 'It', \"'s\", 'the', 'story', '.', '</s>']\n",
      "batch: 45\n",
      "['<s>', 'And', 'although', 'they', 'were', '<unk>', ',', 'much', 'worse', '.', '</s>']\n",
      "batch: 46\n",
      "['<s>', 'Who', \"'s\", 'going', 'to', 'have', 'I', 'have', 'I', '<unk>', '.', '</s>']\n",
      "batch: 47\n",
      "['<s>', 'I', 'had', 'to', 'say', ',', '\"', '.', '</s>']\n",
      "batch: 48\n",
      "['<s>', 'But', 'Emma', 'really', 'care', '.', '</s>']\n",
      "batch: 49\n",
      "['<s>', '<unk>', 'a', 'life', 'and', 'I', 'did', 'this', '.', '</s>']\n",
      "batch: 50\n",
      "['<s>', 'First', ',', 'she', 'did', \"n't\", '<unk>', 'a', '<unk>', '.', '</s>']\n",
      "batch: 51\n",
      "['<s>', 'And', 'I', 'was', 'a', 'little', '<unk>', '.', '</s>']\n",
      "batch: 52\n",
      "['<s>', 'This', 'is', '<unk>', '.', '</s>']\n",
      "batch: 53\n",
      "['<s>', 'I', 'Emma', ',', '<unk>', ',', 'which', 'are', '<unk>', '.', '</s>']\n",
      "batch: 54\n",
      "['<s>', 'The', 'second', ',', 'I', 'said', ',', 'is', '<unk>', '.', '</s>']\n",
      "batch: 55\n",
      "['<s>', 'This', 'new', 'new', '<unk>', 'largely', 'Goes', 'outside', '.', '</s>']\n",
      "batch: 56\n",
      "['<s>', 'New', 'Zealand', 'from', '<unk>', ',', 'friends', 'friends', '.', '</s>']\n",
      "batch: 57\n",
      "['<s>', 'It', 'is', '<unk>', 'of', 'the', '<unk>', '.', '</s>']\n",
      "batch: 58\n",
      "['<s>', 'But', 'the', 'other', 'is', ',', 'they', '<unk>', '.', '</s>']\n",
      "batch: 59\n",
      "['<s>', 'This', 'is', 'not', '<unk>', '.', '</s>']\n",
      "batch: 60\n",
      "['<s>', 'I', 'said', ',', 'a', 'new', 'family', '.', '</s>']\n",
      "batch: 61\n",
      "['<s>', 'So', 'what', 'happened', 'was', 'Emma', '.', '</s>']\n",
      "batch: 62\n",
      "['<s>', 'It', 'helped', 'us', 'to', 'be', 'a', 'work', '.', '</s>']\n",
      "batch: 63\n",
      "['<s>', 'This', '<unk>', 'gave', 'her', 'friend', ',', '<unk>', '.', '</s>']\n",
      "batch: 64\n",
      "['<s>', 'Now', 'five', 'years', 'ago', '.', '</s>']\n",
      "batch: 65\n",
      "['<s>', 'She', 'has', 'a', 'guy', 'who', 'elected', 'them', '.', '</s>']\n",
      "batch: 66\n",
      "['<s>', 'It', \"'s\", 'so', 'to', 'help', 'them', '.', '</s>']\n",
      "batch: 67\n",
      "['<s>', '<unk>', 'are', '<unk>', 'on', 'the', 'West', '<unk>', '.', '</s>']\n",
      "batch: 68\n",
      "['<s>', 'This', 'is', ',', '<unk>', 'where', 'you', 'know', 'you', 'know', '.', '</s>']\n",
      "batch: 69\n",
      "['<s>', 'It', \"'s\", 'what', 'I', 'said', 'about', 'Alex', '.', '</s>']\n",
      "batch: 70\n",
      "['<s>', 'Do', \"n't\", 'tell', 'you', 'did', \"n't\", '.', '</s>']\n",
      "batch: 71\n",
      "['<s>', 'You', 'just', 'a', 'life', '.', '</s>']\n",
      "batch: 72\n",
      "['<s>', 'Thank', 'you', '.', '</s>']\n",
      "batch: 73\n",
      "['<s>', 'But', 'I', 'wondered', 'how', 'it', 'is', 'no', 'limit', '.', '</s>']\n",
      "batch: 74\n",
      "['<s>', 'I', \"'ve\", 'looked', 'like', 'this', 'looks', 'like', 'this', '.', '</s>']\n",
      "batch: 75\n",
      "['<s>', '<unk>', 'can', '<unk>', 'and', '<unk>', 'and', 'they', 'are', '<unk>', '<pad>', '<pad>', '<pad>', '<pad>', '.', '</s>']\n",
      "batch: 76\n",
      "['<s>', 'So', 'with', 'this', 'tool', ',', '<unk>', '.', '</s>']\n",
      "batch: 77\n",
      "['<s>', 'But', 'our', 'hands', 'still', 'outside', 'of', 'the', '<unk>', '.', '</s>']\n",
      "batch: 78\n",
      "['<s>', 'As', 'we', 'can', 'grow', 'devices', 'of', 'our', '-', 'time', '.', '</s>']\n",
      "batch: 79\n",
      "['<s>', 'Architects', 'can', '<unk>', 'their', 'hands', 'and', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 80\n",
      "['<s>', 'So', 'those', 'examples', ',', 'in', 'the', 'digital', '.', '</s>']\n",
      "batch: 81\n",
      "['<s>', 'But', 'if', 'we', '<unk>', 'the', 'information', 'we', 'need', 'to', 'reverse', '.', '</s>']\n",
      "batch: 82\n",
      "['<s>', 'Many', 'of', 'many', 'experiences', 'later', '.', '</s>']\n",
      "batch: 83\n",
      "['<s>', 'Now', ',', 'you', 'have', \"n't\", 'worry', 'about', 'that', '</s>']\n",
      "batch: 84\n",
      "['<s>', 'Here', 'have', '\"', '<unk>', '.', '</s>']\n",
      "batch: 85\n",
      "['<s>', 'How', 'would', 'a', 'future', 'future', '.', '</s>']\n",
      "batch: 86\n",
      "['<s>', 'Thank', 'you', 'know', '.', '</s>']\n",
      "batch: 87\n",
      "['<s>', 'But', 'my', 'mother', 'said', ',', '\"', 'No', '.', '</s>']\n",
      "batch: 88\n",
      "['<s>', 'My', 'parents', 'wanted', 'to', '<unk>', '.', '</s>']\n",
      "batch: 89\n",
      "['<s>', 'It', \"'s\", 'no', 'matter', 'or', 'not', '.', '</s>']\n",
      "batch: 90\n",
      "['<s>', 'But', 'my', 'dream', 'was', 'to', 'be', '<unk>', '.', '</s>']\n",
      "batch: 91\n",
      "['<s>', 'This', 'is', ',', 'I', '<unk>', '.', '</s>']\n",
      "batch: 92\n",
      "['<s>', 'I', 'even', 'a', '<unk>', '.', '</s>']\n",
      "batch: 93\n",
      "['<s>', 'But', 'I', 'did', \"n't\", 'really', '.', '</s>']\n",
      "batch: 94\n",
      "['<s>', 'The', 'Chinese', 'did', 'it', 'was', 'out', '.', '</s>']\n",
      "batch: 95\n",
      "['<s>', 'Only', 'my', 'friends', 'were', 'just', 'as', 'I', 'was', '<unk>', '.', '</s>']\n",
      "batch: 96\n",
      "['<s>', 'And', 'at', 'two', ',', 'I', 'knew', 'that', 'was', '<unk>', '.', '</s>']\n",
      "batch: 97\n",
      "['<s>', 'My', 'dream', 'would', 'be', 'true', '.', '</s>']\n",
      "batch: 98\n",
      "['<s>', 'But', 'this', 'is', 'so', '<unk>', '.', '</s>']\n",
      "batch: 99\n",
      "['<s>', 'So', ',', 'I', 'had', 'a', 'different', 'head', '.', '</s>']\n",
      "batch: 100\n",
      "['<s>', 'No', 'one', 'to', 'teach', 'me', '?', 'OK', '.', '</s>']\n",
      "batch: 101\n",
      "['<s>', 'So', 'I', '<unk>', 'books', '.', '</s>']\n",
      "batch: 102\n",
      "['<s>', 'Of', 'course', '<unk>', 'in', 'China', '.', '</s>']\n",
      "batch: 103\n",
      "['<s>', '\"', 'The', 'meaning', 'is', 'life', 'state', '.', '</s>']\n",
      "batch: 104\n",
      "['<s>', 'It', 'just', 'as', 'a', 'propaganda', '.', '</s>']\n",
      "batch: 105\n",
      "['<s>', 'The', 'Bible', 'is', 'weird', '.', '</s>']\n",
      "batch: 106\n",
      "['<s>', 'But', 'this', 'is', 'a', 'different', 'subject', '.', '</s>']\n",
      "batch: 107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '\"', 'hired', ',', '\"', 'and', 'much', 'better', '.', '</s>']\n",
      "batch: 108\n",
      "['<s>', 'It', 'with', 'a', 'new', 'culture', ',', '<unk>', '.', '</s>']\n",
      "batch: 109\n",
      "['<s>', 'This', 'is', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 110\n",
      "['<s>', 'It', 'never', 'make', 'sense', 'of', 'the', 'world', '.', '</s>']\n",
      "batch: 111\n",
      "['<s>', 'So', 'a', 'map', 'says', 'about', 'personal', '<unk>', '.', '</s>']\n",
      "batch: 112\n",
      "['<s>', 'The', '<unk>', 'is', 'not', 'new', '.', '</s>']\n",
      "batch: 113\n",
      "['<s>', 'In', 'the', 'world', 'is', 'a', '<unk>', '.', '</s>']\n",
      "batch: 114\n",
      "['<s>', 'There', 'are', '<unk>', '<unk>', 'and', '<unk>', '.', '</s>']\n",
      "batch: 115\n",
      "['<s>', 'And', '<unk>', 'is', '<unk>', 'understanding', '.', '</s>']\n",
      "batch: 116\n",
      "['<s>', 'And', 'I', 'started', 'to', 'read', 'both', 'simultaneously', '.', '</s>']\n",
      "batch: 117\n",
      "['<s>', 'In', 'addition', 'was', 'political', 'and', '<unk>', '.', '</s>']\n",
      "batch: 118\n",
      "['<s>', '<unk>', 'was', ',', 'liberals', 'and', 'interesting', '<unk>', '.', '</s>']\n",
      "batch: 119\n",
      "['<s>', '\"', '<unk>', '\"', 'means', '\"', '<unk>', '.', '\"', '</s>']\n",
      "batch: 120\n",
      "['<s>', 'I', 'know', ',', 'I', \"'ve\", '<unk>', '.', '</s>']\n",
      "batch: 121\n",
      "['<s>', 'A', '<unk>', 'Copernican', 'compared', 'to', 'many', '.', '</s>']\n",
      "batch: 122\n",
      "['<s>', 'And', 'a', 'dream', 'can', 'deliver', '</s>']\n",
      "batch: 123\n",
      "['<s>', 'Fine', '<unk>', 'with', 'his', 'own', '.', '</s>']\n",
      "batch: 124\n",
      "['<s>', 'Thank', 'you', '.', '</s>']\n",
      "batch: 125\n",
      "['<s>', 'Thank', 'you', '<unk>', '.', '</s>']\n",
      "batch: 126\n",
      "['<s>', 'Thank', 'you', 'can', '.', '</s>']\n",
      "batch: 127\n",
      "['<s>', 'And', 'I', '<unk>', ',', 'I', 'plan', '.', '</s>']\n",
      "batch: 128\n",
      "['<s>', 'And', 'then', 'I', 'did', '.', '</s>']\n",
      "batch: 129\n",
      "['<s>', 'I', '<unk>', 'the', 'notes', 'of', 'course', '.', '</s>']\n",
      "batch: 130\n",
      "['<s>', 'All', 'of', '<unk>', '.', '</s>']\n",
      "batch: 131\n",
      "['<s>', 'So', 'I', 'wake', 'up', 'all', 'notes', 'without', 'any', 'time', '.', '</s>']\n",
      "batch: 132\n",
      "['<s>', 'I', 'still', '<unk>', 'about', 'my', 'behavior', '.', '</s>']\n",
      "batch: 133\n",
      "['<s>', 'I', 'do', \"n't\", 'remember', '.', '</s>']\n",
      "batch: 134\n",
      "['<s>', 'I', 'do', \"n't\", 'feel', 'why', 'I', 'know', '.', '</s>']\n",
      "batch: 135\n",
      "['<s>', 'I', 'felt', 'it', 'was', 'good', '.', '</s>']\n",
      "batch: 136\n",
      "['<s>', 'I', 'do', \"n't\", 'understand', '.', '</s>']\n",
      "batch: 137\n",
      "['<s>', 'It', 'was', 'so', '<unk>', '.', '</s>']\n",
      "batch: 138\n",
      "['<s>', 'I', 'never', 'poor', '.', '</s>']\n",
      "batch: 139\n",
      "['<s>', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 140\n",
      "['<s>', 'It', \"'s\", 'so', 'clever', '.', '</s>']\n",
      "batch: 141\n",
      "['<s>', 'We', 'know', ',', 'called', 'conflict', '.', '</s>']\n",
      "batch: 142\n",
      "['<s>', 'Also', 'as', '<unk>', 'are', 'very', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 143\n",
      "['<s>', 'We', 'know', ',', 'in', '<unk>', 'in', 'the', 'United', 'States', '.', '</s>']\n",
      "batch: 144\n",
      "['<s>', 'So', 'not', '<unk>', ',', '\"', '<unk>', '.', '</s>']\n",
      "batch: 145\n",
      "['<s>', 'No', 'king', ',', '\"', '<unk>', '.', '\"', '</s>']\n",
      "batch: 146\n",
      "['<s>', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 147\n",
      "['<s>', 'Think', 'of', 'emotions', '.', '</s>']\n",
      "batch: 148\n",
      "['<s>', 'We', 'think', 'of', 'the', 'young', '.', '</s>']\n",
      "batch: 149\n",
      "['<s>', 'We', 'think', 'of', 'a', 'piece', '.', '</s>']\n",
      "batch: 150\n",
      "['<s>', 'We', 'remember', 'how', 'I', 'was', '<unk>', '.', '</s>']\n",
      "batch: 151\n",
      "['<s>', 'We', 'forget', 'how', 'he', \"'s\", '<unk>', '.', '</s>']\n",
      "batch: 152\n",
      "['<s>', 'These', 'kinds', 'of', 'Virginia', 'so', '<unk>', '.', '</s>']\n",
      "batch: 153\n",
      "['<s>', 'I', 'do', \"n't\", 'know', 'it', \"'s\", ',', '<unk>', '.', '</s>']\n",
      "batch: 154\n",
      "['<s>', 'When', 'we', '<unk>', ',', 'we', 'tell', 'us', '.', '</s>']\n",
      "batch: 155\n",
      "['<s>', 'These', 'all', '<unk>', 'and', '<unk>', '<unk>', 'out', 'of', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 156\n",
      "['<s>', 'We', 'have', 'to', 'realized', 'and', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 157\n",
      "['<s>', 'You', 'do', \"n't\", 'to', '<unk>', 'work', 'is', '<unk>', '.', '</s>']\n",
      "batch: 158\n",
      "['<s>', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 159\n",
      "['<s>', 'It', \"'s\", 'a', '<unk>', '-', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 160\n",
      "['<s>', 'So', 'the', 'Sun', 'is', '.', '</s>']\n",
      "batch: 161\n",
      "['<s>', '<unk>', 'had', 'information', '.', '</s>']\n",
      "batch: 162\n",
      "['<s>', '<unk>', '<unk>', 'beetles', '.', '</s>']\n",
      "batch: 163\n",
      "['<s>', '<unk>', \"'s\", 'hair', ',', '<unk>', '.', '</s>']\n",
      "batch: 164\n",
      "['<s>', 'And', 'jealousy', ',', 'eight', '.', '</s>']\n",
      "batch: 165\n",
      "['<s>', 'That', \"'s\", 'also', '<unk>', '.', '</s>']\n",
      "batch: 166\n",
      "['<s>', 'Basically', ',', 'the', 'language', 'and', 'jealousy', '.', '</s>']\n",
      "batch: 167\n",
      "['<s>', 'He', 'says', ',', 'there', 'is', '<unk>', '.', '</s>']\n",
      "batch: 168\n",
      "['<s>', '<unk>', ',', '<unk>', '--', 'the', 'way', 'to', 'wisdom', ':', '</s>']\n",
      "batch: 169\n",
      "['<s>', 'Let', \"'s\", 'about', 'being', 'more', 'women', '.', '</s>']\n",
      "batch: 170\n",
      "['<s>', 'Let', \"'s\", 'a', 'sense', 'of', 'us', 'know', '.', '</s>']\n",
      "batch: 171\n",
      "['<s>', '<unk>', 'feel', 'sense', ',', 'hidden', '<unk>', '.', '</s>']\n",
      "batch: 172\n",
      "['<s>', 'Give', 'us', 'some', 'sort', 'of', 'the', 'moment', '.', '</s>']\n",
      "batch: 173\n",
      "['<s>', 'Freud', 'would', 'about', 'this', '.', '</s>']\n",
      "batch: 174\n",
      "['<s>', 'Because', 'it', \"'s\", 'innocent', '.', '</s>']\n",
      "batch: 175\n",
      "['<s>', 'The', 'rural', 'sharks', 'under', '<unk>', '.', '</s>']\n",
      "batch: 176\n",
      "['<s>', '<unk>', 'them', ',', 'or', 'a', 'man', '.', '</s>']\n",
      "batch: 177\n",
      "['<s>', 'Freud', ']', '<unk>', 'of', 'his', 'wife', \"'s\", '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 178\n",
      "['<s>', '<unk>', 'is', 'very', 'good', 'part', '.', '</s>']\n",
      "batch: 179\n",
      "['<s>', 'The', 'diagrams', 'have', '<unk>', ',', 'but', 'not', '<unk>', '.', '</s>']\n",
      "batch: 180\n",
      "['<s>', 'Because', 'we', \"'re\", 'talking', 'to', '<unk>', '.', '</s>']\n",
      "batch: 181\n",
      "['<s>', 'And', 'jealousy', 'will', 'get', 'in', '<unk>', '.', '</s>']\n",
      "batch: 182\n",
      "['<s>', 'It', \"'s\", 'long', 'year', '.', '</s>']\n",
      "batch: 183\n",
      "['<s>', 'Because', 'let', \"'s\", 'honest', '.', '</s>']\n",
      "batch: 184\n",
      "['<s>', 'It', \"'s\", '<unk>', ',', 'creativity', ',', 'right', '.', '</s>']\n",
      "batch: 185\n",
      "['<s>', 'This', 'is', 'a', 'sculpture', '.', '</s>']\n",
      "batch: 186\n",
      "['<s>', 'This', 'is', 'a', 'novel', 'out', 'of', '<unk>', '.', '</s>']\n",
      "batch: 187\n",
      "['<s>', '<unk>', 'is', 'one', 'of', '<unk>', '.', '</s>']\n",
      "batch: 188\n",
      "['<s>', 'She', \"'s\", '<unk>', 'and', '<unk>', 'American', '.', '</s>']\n",
      "batch: 189\n",
      "['<s>', 'Take', 'Tom', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 190\n",
      "['<s>', 'This', 'is', 'a', 'technique', '.', '</s>']\n",
      "batch: 191\n",
      "['<s>', 'But', 'we', 'do', 'it', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 192\n",
      "['<s>', 'It', \"'s\", '<unk>', ',', 'because', 'we', 'live', 'in', 'a', '<unk>', 'time', '.', '</s>']\n",
      "batch: 193\n",
      "['<s>', 'We', 'live', 'in', 'a', '<unk>', 'period', '.', '</s>']\n",
      "batch: 194\n",
      "['<s>', 'So', 'novel', ',', 'I', \"'m\", 'I', '<unk>', '.', '</s>']\n",
      "batch: 195\n",
      "['<s>', 'Let', \"'s\", '<unk>', '<unk>', 'Street', '<unk>', '<unk>', 'Street', '<unk>', 'me', '<unk>', 'Street', '.', '</s>']\n",
      "batch: 196\n",
      "['<s>', 'This', 'is', 'so', '<unk>', '.', '</s>']\n",
      "batch: 197\n",
      "['<s>', 'We', \"'re\", 'proud', '.', '</s>']\n",
      "batch: 198\n",
      "['<s>', 'It', 'seems', 'like', 'an', 'exploit', ',', 'but', 'problem', '.', '</s>']\n",
      "batch: 199\n",
      "['<s>', 'In', 'a', 'third', ',', 'it', 'is', '<unk>', '.', '</s>']\n",
      "batch: 200\n",
      "['<s>', 'And', 'the', 'next', 'to', '<unk>', '.', '</s>']\n",
      "batch: 201\n",
      "['<s>', 'Suddenly', '<unk>', '<unk>', 'to', '<unk>', 'this', '.', '</s>']\n",
      "batch: 202\n",
      "['<s>', 'It', \"'s\", 'really', 'easy', 'thing', '.', '</s>']\n",
      "batch: 203\n",
      "['<s>', 'Maybe', 'we', 'would', 'be', 'the', '<unk>', '.', '</s>']\n",
      "batch: 204\n",
      "['<s>', 'We', 'are', 'concerned', 'on', 'us', '.', '</s>']\n",
      "batch: 205\n",
      "['<s>', 'But', 'I', '<unk>', '.', '</s>']\n",
      "batch: 206\n",
      "['<s>', 'As', 'we', 'waited', ',', 'we', \"'ve\", 'still', '<unk>', '.', '</s>']\n",
      "batch: 207\n",
      "['<s>', '<unk>', 'is', 'the', 'jealousy', '.', '</s>']\n",
      "batch: 208\n",
      "['<s>', '<unk>', 'can', 'be', '<unk>', '.', '</s>']\n",
      "batch: 209\n",
      "['<s>', 'We', \"'ve\", 'gotten', 'society', '.', '</s>']\n",
      "batch: 210\n",
      "['<s>', 'Thank', 'you', \"'ve\", '<unk>', '.', '</s>']\n",
      "batch: 211\n",
      "['<s>', 'It', 'was', 'a', 'great', 'day', '.', '</s>']\n",
      "batch: 212\n",
      "['<s>', 'The', '<unk>', '.', '</s>']\n",
      "batch: 213\n",
      "['<s>', 'We', 'started', 'to', 'our', '<unk>', '.', '</s>']\n",
      "batch: 214\n",
      "['<s>', 'I', 'was', 'mad', 'and', '<unk>', '.', '</s>']\n",
      "batch: 215\n",
      "['<s>', 'The', 'question', 'was', 'and', 'I', '<unk>', '.', '</s>']\n",
      "batch: 216\n",
      "['<s>', '<unk>', 'us', '.', '</s>']\n",
      "batch: 217\n",
      "['<s>', 'Who', '<unk>', 'in', 'in', 'Paris', 'or', 'London', '.', '</s>']\n",
      "batch: 218\n",
      "['<s>', 'I', 'wanted', 'as', 'individuals', '.', '</s>']\n",
      "batch: 219\n",
      "['<s>', 'I', 'wanted', 'to', 'do', 'the', 'job', '.', '</s>']\n",
      "batch: 220\n",
      "['<s>', 'How', 'do', 'you', 'carry', 'to', 'wear', '.', '</s>']\n",
      "batch: 221\n",
      "['<s>', 'So', 'I', 'started', 'to', '<unk>', 'with', 'them', '.', '</s>']\n",
      "batch: 222\n",
      "['<s>', 'I', 'had', 'to', 'get', 'more', '.', '</s>']\n",
      "batch: 223\n",
      "['<s>', 'So', 'I', 'took', 'a', 'job', '.', '</s>']\n",
      "batch: 224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'I', 'was', 'not', 'alone', ',', 'I', 'went', '.', '</s>']\n",
      "batch: 225\n",
      "['<s>', 'And', 'I', '<unk>', '<unk>', 'and', 'the', 'snow', '.', '</s>']\n",
      "batch: 226\n",
      "['<s>', 'It', 'was', 'a', 'boring', 'privilege', ',', '<unk>', '.', '</s>']\n",
      "batch: 227\n",
      "['<s>', 'Everybody', 'was', '<unk>', '.', '</s>']\n",
      "batch: 228\n",
      "['<s>', 'It', \"'s\", 'hard', 'to', '<unk>', '.', '</s>']\n",
      "batch: 229\n",
      "['<s>', 'Then', 'there', 'is', '<unk>', '.', '</s>']\n",
      "batch: 230\n",
      "['<s>', 'You', \"'ve\", '<unk>', 'all', 'day', '.', '</s>']\n",
      "batch: 231\n",
      "['<s>', 'Everybody', 'wants', 'to', 'stop', 'enough', '.', '</s>']\n",
      "batch: 232\n",
      "['<s>', 'This', 'is', 'really', 'bad', '.', '</s>']\n",
      "batch: 233\n",
      "['<s>', 'I', 'learned', 'by', 'the', '<unk>', '.', '</s>']\n",
      "batch: 234\n",
      "['<s>', 'He', 'does', \"n't\", '<unk>', '.', '</s>']\n",
      "batch: 235\n",
      "['<s>', 'It', \"'s\", 'kind', 'of', '<unk>', '.', '</s>']\n",
      "batch: 236\n",
      "['<s>', 'He', 'has', 'to', 'be', 'always', '.', '</s>']\n",
      "batch: 237\n",
      "['<s>', 'And', 'then', 'it', 'is', '.', '</s>']\n",
      "batch: 238\n",
      "['<s>', 'They', \"'re\", '<unk>', 'of', 'public', 'health', '.', '</s>']\n",
      "batch: 239\n",
      "['<s>', '<unk>', 'for', 'decades', 'and', 'educating', 'us', '.', '</s>']\n",
      "batch: 240\n",
      "['<s>', 'The', 'economy', '.', '</s>']\n",
      "batch: 241\n",
      "['<s>', 'I', 'do', 'the', 'capitalism', '.', '</s>']\n",
      "batch: 242\n",
      "['<s>', 'And', 'then', ',', 'I', 'call', ',', '<unk>', '.', '</s>']\n",
      "batch: 243\n",
      "['<s>', 'And', 'I', 'mean', 'how', 'dolphins', '.', '</s>']\n",
      "batch: 244\n",
      "['<s>', 'Their', 'work', 'I', 'think', ',', 'is', '<unk>', '.', '</s>']\n",
      "batch: 245\n",
      "['<s>', 'They', 'see', 'the', 'streets', ',', '</s>']\n",
      "batch: 246\n",
      "['<s>', 'They', '<unk>', 'in', 'many', 'countries', '.', '</s>']\n",
      "batch: 247\n",
      "['<s>', 'You', 'know', 'when', 'you', 'can', 'expect', '.', '</s>']\n",
      "batch: 248\n",
      "['<s>', 'And', 'their', 'work', 'is', '<unk>', '.', '</s>']\n",
      "batch: 249\n",
      "['<s>', 'They', 'are', 'like', 'a', '<unk>', '.', '</s>']\n",
      "batch: 250\n",
      "['<s>', '\"', 'Everything', '.', '\"', '<unk>', '.', '</s>']\n",
      "batch: 251\n",
      "['<s>', 'His', 'name', 'is', 'a', 'lot', 'of', 'good', 'friends', '.', '</s>']\n",
      "batch: 252\n",
      "['<s>', 'I', 'wanted', 'that', '<unk>', 'was', '<unk>', '.', '</s>']\n",
      "batch: 253\n",
      "['<s>', 'All', 'of', '<unk>', '.', '</s>']\n",
      "batch: 254\n",
      "['<s>', 'It', \"'s\", 'a', 'remarkable', '<unk>', '.', '</s>']\n",
      "batch: 255\n",
      "['<s>', 'Take', 'a', 'second', '.', '</s>']\n",
      "batch: 256\n",
      "['<s>', 'Hello', '<unk>', '<unk>', 'is', '.', '</s>']\n",
      "batch: 257\n",
      "['<s>', 'That', \"'s\", 'not', 'work', '.', '</s>']\n",
      "batch: 258\n",
      "['<s>', 'Hello', '<unk>', '.', '</s>']\n",
      "batch: 259\n",
      "['<s>', 'My', 'name', 'is', 'drunk', ',', 'but', 'my', 'birth', '.', '</s>']\n",
      "batch: 260\n",
      "['<s>', 'He', 'six', 'six', 'inches', ',', 'and', 'I', '<unk>', '.', '</s>']\n",
      "batch: 261\n",
      "['<s>', 'So', 'I', '<unk>', '<unk>', 'that', 'I', \"'m\", '<unk>', '.', '</s>']\n",
      "batch: 262\n",
      "['<s>', 'It', \"'s\", '.', 'I', '<unk>', 'and', '<unk>', 'Ali', '.', '</s>']\n",
      "batch: 263\n",
      "['<s>', '<unk>', 'is', 'not', 'linear', '.', '</s>']\n",
      "batch: 264\n",
      "['<s>', 'It', \"'s\", 'not', '<unk>', '.', 'You', 'ca', \"n't\", 'get', 'them', '.', '</s>']\n",
      "batch: 265\n",
      "['<s>', 'You', 'only', '<unk>', 'the', 'day', 'of', 'birth', '.', '</s>']\n",
      "batch: 266\n",
      "['<s>', '<unk>', 'with', 'a', 'minute', '.', '</s>']\n",
      "batch: 267\n",
      "['<s>', '<unk>', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 268\n",
      "['<s>', 'And', 'then', 'I', 'ca', \"n't\", 'be', 'a', 'little', '?', '</s>']\n",
      "batch: 269\n",
      "['<s>', 'I', 'also', 'have', 'to', 'say', ',', '\"', '<unk>', ',', 'I', 'have', 'to', '.', '</s>']\n",
      "batch: 270\n",
      "['<s>', 'When', 'there', \"'s\", '<unk>', ',', 'I', '<unk>', '.', '</s>']\n",
      "batch: 271\n",
      "['<s>', 'Now', 'you', 'do', \"n't\", 'feel', ',', 'then', 'I', 'do', \"n't\", 'know', '.', '</s>']\n",
      "batch: 272\n",
      "['<s>', '<unk>', 'Park', 'of', '<unk>', '.', '</s>']\n",
      "batch: 273\n",
      "['<s>', 'I', 'mean', ',', 'and', 'my', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 274\n",
      "['<s>', 'The', 'mantra', 'was', ',', 'you', 'can', '.', '</s>']\n",
      "batch: 275\n",
      "['<s>', 'So', 'if', 'I', 'do', 'three', 'sisters', ',', 'he', '.', '</s>']\n",
      "batch: 276\n",
      "['<s>', 'At', 'the', 'six', '<unk>', 'in', 'my', 'family', '.', '</s>']\n",
      "batch: 277\n",
      "['<s>', 'Now', 'we', 'have', '20', 'and', 'they', \"'re\", 'still', 'all', 'my', 'family', '.', '</s>']\n",
      "batch: 278\n",
      "['<s>', 'And', 'nobody', 'did', \"n't\", 'be', '<unk>', '.', '</s>']\n",
      "batch: 279\n",
      "['<s>', 'People', 'that', 'grew', 'up', 'with', 'my', 'thinking', '.', '</s>']\n",
      "batch: 280\n",
      "['<s>', 'They', 'were', 'worried', 'that', 'I', 'could', '<unk>', '.', '</s>']\n",
      "batch: 281\n",
      "['<s>', 'I', 'have', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 282\n",
      "['<s>', 'Yes', ',', 'crazy', 'crazy', '.', '</s>']\n",
      "batch: 283\n",
      "['<s>', 'My', 'parents', 'could', \"n't\", '<unk>', ',', 'so', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 284\n",
      "['<s>', 'I', 'learned', 'of', 'a', '<unk>', '.', '</s>']\n",
      "batch: 285\n",
      "['<s>', 'And', 'my', 'friends', 'were', '<unk>', ',', 'I', 'did', 'it', '.', '</s>']\n",
      "batch: 286\n",
      "['<s>', 'But', 'a', '<unk>', ':', '<unk>', '.', '</s>']\n",
      "batch: 287\n",
      "['<s>', 'And', 'I', 'can', 'stand', 'on', 'the', 'head', '.', '</s>']\n",
      "batch: 288\n",
      "['<s>', 'I', 'was', 'this', '<unk>', '.', '</s>']\n",
      "batch: 289\n",
      "['<s>', 'Everybody', 'would', 'like', 'me', '.', '</s>']\n",
      "batch: 290\n",
      "['<s>', 'Every', 'time', 'I', '<unk>', 'a', 'scene', ',', 'my', 'teachers', '.', '</s>']\n",
      "batch: 291\n",
      "['<s>', 'But', 'I', 'never', '<unk>', '.', '</s>']\n",
      "batch: 292\n",
      "['<s>', 'A', 'piece', 'about', 'penguins', '.', '</s>']\n",
      "batch: 293\n",
      "['<s>', 'I', 'was', 'a', 'girl', '.', '</s>']\n",
      "batch: 294\n",
      "['<s>', 'So', 'I', 'go', 'to', 'design', '.', '</s>']\n",
      "batch: 295\n",
      "['<s>', 'I', 'have', '<unk>', '.', '</s>']\n",
      "batch: 296\n",
      "['<s>', '<unk>', 'free', '.', '</s>']\n",
      "batch: 297\n",
      "['<s>', 'Thank', 'God', ',', 'I', \"'m\", '<unk>', '.', '</s>']\n",
      "batch: 298\n",
      "['<s>', 'I', 'did', \"n't\", '<unk>', '.', '</s>']\n",
      "batch: 299\n",
      "['<s>', 'Whose', 'Brown', '<unk>', '.', '</s>']\n",
      "batch: 300\n",
      "['<s>', 'The', 'life', '.', '</s>']\n",
      "batch: 301\n",
      "['<s>', 'Hollywood', 'is', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 302\n",
      "['<s>', 'My', 'dream', '<unk>', '.', '</s>']\n",
      "batch: 303\n",
      "['<s>', 'They', 'only', 'made', 'people', '.', '</s>']\n",
      "batch: 304\n",
      "['<s>', 'But', 'the', 'rule', '<unk>', '.', '</s>']\n",
      "batch: 305\n",
      "['<s>', 'So', 'I', 'became', '<unk>', '.', '</s>']\n",
      "batch: 306\n",
      "['<s>', 'Some', 'people', 'say', 'the', 'first', 'think', 'of', 'the', 'Arab', '<unk>', '.', '</s>']\n",
      "batch: 307\n",
      "['<s>', 'The', '<unk>', 'much', 'much', 'more', 'than', 'the', 'crime', '.', '</s>']\n",
      "batch: 308\n",
      "['<s>', 'The', 'big', '<unk>', 'was', '2010', '.', '</s>']\n",
      "batch: 309\n",
      "['<s>', 'I', 'used', 'to', 'the', '<unk>', '\"', '<unk>', '.', '</s>']\n",
      "batch: 310\n",
      "['<s>', 'And', 'she', 'said', ',', 'said', ',', 'two', '.', '</s>']\n",
      "batch: 311\n",
      "['<s>', 'And', 'we', 'were', 'live', '.', '</s>']\n",
      "batch: 312\n",
      "['<s>', 'On', 'the', 'World', 'web', ',', 'with', 'a', 'couple', 'of', '<unk>', '.', '</s>']\n",
      "batch: 313\n",
      "['<s>', '\"', '<unk>', '?', '\"', '<unk>', '?', '</s>']\n",
      "batch: 314\n",
      "['<s>', 'And', 'my', ',', '\"', '<unk>', '.', '\"', '</s>']\n",
      "batch: 315\n",
      "['<s>', 'They', '<unk>', 'disease', '.', '</s>']\n",
      "batch: 316\n",
      "['<s>', 'We', 'should', 'pray', '.', '</s>']\n",
      "batch: 317\n",
      "['<s>', 'One', 'of', 'the', '<unk>', '<unk>', ':', '<unk>', '.', '</s>']\n",
      "batch: 318\n",
      "['<s>', 'So', ',', 'it', 'is', 'like', 'the', '<unk>', '.', '</s>']\n",
      "batch: 319\n",
      "['<s>', 'If', 'a', '<unk>', 'does', 'not', 'play', 'the', '<unk>', '.', '</s>']\n",
      "batch: 320\n",
      "['<s>', 'The', '<unk>', 'is', 'a', 'breath', '.', '</s>']\n",
      "batch: 321\n",
      "['<s>', 'People', 'are', 'the', 'largest', 'in', 'the', '<unk>', 'in', 'the', 'world', '.', '</s>']\n",
      "batch: 322\n",
      "['<s>', 'Doctors', 'say', ',', 'I', 'run', 'here', '.', '</s>']\n",
      "batch: 323\n",
      "['<s>', 'But', 'I', 'had', 'with', 'social', 'media', 'might', \"n't\", 'be', 'here', '.', '</s>']\n",
      "batch: 324\n",
      "['<s>', 'I', 'hope', 'we', 'could', 'create', 'a', 'shower', '.', '</s>']\n",
      "batch: 325\n",
      "['<s>', 'Perhaps', 'there', 'are', 'more', 'than', 'if', 'I', 'd', '.', '</s>']\n",
      "batch: 326\n",
      "['<s>', 'Or', 'maybe', 'not', '.', '</s>']\n",
      "batch: 327\n",
      "['<s>', 'There', \"'s\", 'a', 'village', 'to', 'educate', '.', '</s>']\n",
      "batch: 328\n",
      "['<s>', 'My', '<unk>', '<unk>', 'a', 'bunch', 'of', 'me', '.', '</s>']\n",
      "batch: 329\n",
      "['<s>', 'I', 'moved', 'on', 'the', 'red', 'Susan', '<unk>', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 330\n",
      "['<s>', 'I', '<unk>', '<unk>', '<unk>', 'to', 'the', 'world', '.', '</s>']\n",
      "batch: 331\n",
      "['<s>', 'I', 'was', 'the', 'New', '<unk>', 'in', '2008', '.', '</s>']\n",
      "batch: 332\n",
      "['<s>', 'There', 'was', 'some', 'time', 'to', 'the', 'problem', '.', '</s>']\n",
      "batch: 333\n",
      "['<s>', 'The', '<unk>', '<unk>', 'in', 'the', 'United', 'States', '.', '</s>']\n",
      "batch: 334\n",
      "['<s>', 'Last', 'organic', '<unk>', 'and', '<unk>', '<unk>', 'and', '<unk>', '.', '</s>']\n",
      "batch: 335\n",
      "['<s>', 'People', 'died', ',', 'and', 'Apollo', '<unk>', '.', '</s>']\n",
      "batch: 336\n",
      "['<s>', 'But', 'to', 'the', 'end', '<unk>', 'the', '<unk>', '.', '</s>']\n",
      "batch: 337\n",
      "['<s>', 'But', 'why', 'did', '.', '</s>']\n",
      "batch: 338\n",
      "['<s>', 'But', '<unk>', 'of', '<unk>', 'at', 'the', 'same', 'year', '.', '</s>']\n",
      "batch: 339\n",
      "['<s>', '<unk>', ',', 'not', 'just', '<unk>', 'over', 'the', 'War', '.', '</s>']\n",
      "batch: 340\n",
      "['<s>', 'At', 'the', 'time', ',', 'the', '<unk>', 'forces', 'of', 'technology', '.', '</s>']\n",
      "batch: 341\n",
      "['<s>', 'They', 'were', 'a', 'little', '<unk>', 'was', '<unk>', '.', '</s>']\n",
      "batch: 342\n",
      "['<s>', 'The', 'grandmother', 'was', 'a', 'long', 'time', '.', '</s>']\n",
      "batch: 343\n",
      "['<s>', 'In', 'the', 'middle', ',', 'foreign', 'smallpox', '.', '</s>']\n",
      "batch: 344\n",
      "['<s>', 'Technology', 'seemed', 'something', 'called', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 345\n",
      "['<s>', 'Since', '1970', ',', 'there', 'was', 'more', 'than', 'the', 'moon', '.', '</s>']\n",
      "batch: 346\n",
      "['<s>', 'I', 'remember', ',', 'like', 'I', 'do', '.', '</s>']\n",
      "batch: 347\n",
      "['<s>', 'You', '<unk>', 'everywhere', '.', '</s>']\n",
      "batch: 348\n",
      "['<s>', 'We', 'have', 'this', 'last', 'TED', 'days', 'in', 'the', '</s>']\n",
      "batch: 349\n",
      "['<s>', 'It', \"'s\", 'nothing', 'wrong', 'with', 'these', 'things', '.', '</s>']\n",
      "batch: 350\n",
      "['<s>', 'They', 'have', 'a', 'life', 'and', '<unk>', '.', '</s>']\n",
      "batch: 351\n",
      "['<s>', 'But', 'they', 'do', \"n't\", 'solve', 'of', 'the', 'human', 'beings', '.', '</s>']\n",
      "batch: 352\n",
      "['<s>', 'What', \"'s\", '<unk>', '.', '</s>']\n",
      "batch: 353\n",
      "['<s>', 'But', 'this', 'explanation', 'ai', \"n't\", 'enough', '.', '</s>']\n",
      "batch: 354\n",
      "['<s>', 'It', \"'s\", '<unk>', 'with', 'Silicon', 'Valley', '.', '</s>']\n",
      "batch: 355\n",
      "['<s>', 'No', ',', 'why', 'we', 'do', 'a', 'lot', '.', '</s>']\n",
      "batch: 356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'Sometimes', ',', 'not', 'know', 'the', 'issues', '.', '</s>']\n",
      "batch: 357\n",
      "['<s>', 'We', 'fly', 'to', 'the', 'Mars', ',', '</s>']\n",
      "batch: 358\n",
      "['<s>', 'NASA', 'does', 'a', 'plan', 'plan', '.', '</s>']\n",
      "batch: 359\n",
      "['<s>', 'But', ',', 'is', ',', 'and', 'therefore', 'going', 'to', 'happen', '.', '</s>']\n",
      "batch: 360\n",
      "['<s>', 'Another', 'time', 'we', 'do', \"n't\", 'solve', 'our', 'political', 'systems', '.', '</s>']\n",
      "batch: 361\n",
      "['<s>', 'gas', 'machines', 'as', 'solar', 'oil', 'as', 'a', 'barrel', '.', '</s>']\n",
      "batch: 362\n",
      "['<s>', 'We', 'need', '<unk>', 'the', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 363\n",
      "['<s>', 'Sometimes', 'there', 'are', '<unk>', ',', 'but', 'not', 'so', '<unk>', ':', '</s>']\n",
      "batch: 364\n",
      "['<s>', 'It', 'was', 'a', 'long', '-', 'failure', '.', '</s>']\n",
      "batch: 365\n",
      "['<s>', 'Ultimately', ',', 'we', 'get', '<unk>', 'because', 'we', 'do', \"n't\", 'understand', '.', '</s>']\n",
      "batch: 366\n",
      "['<s>', '<unk>', 'problems', 'are', '<unk>', '.', '</s>']\n",
      "batch: 367\n",
      "['<s>', 'It', \"'s\", 'not', 'the', 'first', 'with', 'technologies', '.', '</s>']\n",
      "batch: 368\n",
      "['<s>', 'But', 'this', 'is', 'a', 'model', 'of', 'the', 'future', 'can', 'be', '<unk>', '.', '</s>']\n",
      "batch: 369\n",
      "['<s>', 'It', \"'s\", 'year', '.', '</s>']\n",
      "batch: 370\n",
      "['<s>', 'But', 'at', 'the', 'end', 'out', 'of', 'the', 'moon', '.', '</s>']\n",
      "batch: 371\n",
      "['<s>', 'He', 'was', 'three', 'days', '.', '</s>']\n",
      "batch: 372\n",
      "['<s>', 'And', 'it', \"'s\", ',', 'if', 'this', 'problem', '.', '</s>']\n",
      "batch: 373\n",
      "['<s>', 'We', \"'re\", '<unk>', 'and', 'the', 'solutions', '.', '</s>']\n",
      "batch: 374\n",
      "['<s>', '<unk>', ',', 'it', 'did', \"n't\", 'remind', 'us', '.', '</s>']\n",
      "batch: 375\n",
      "['<s>', 'Thank', 'you', 'know', '.', '</s>']\n",
      "batch: 376\n",
      "['<s>', 'I', 'want', 'to', 'close', 'with', 'your', 'eyes', '.', '</s>']\n",
      "batch: 377\n",
      "['<s>', 'Hold', 'out', 'of', '<unk>', '.', '</s>']\n",
      "batch: 378\n",
      "['<s>', 'Now', 'you', 'have', 'a', 'group', 'on', 'its', '<unk>', '.', '</s>']\n",
      "batch: 379\n",
      "['<s>', 'It', \"'s\", 'a', '<unk>', ',', 'you', 'know', 'your', '<unk>', '.', '</s>']\n",
      "batch: 380\n",
      "['<s>', 'You', \"'ve\", 'got', 'to', 'you', '.', '</s>']\n",
      "batch: 381\n",
      "['<s>', 'They', 'were', '<unk>', ',', 'directly', 'to', 'you', '.', '</s>']\n",
      "batch: 382\n",
      "['<s>', 'And', 'then', 'put', 'directly', '<unk>', '.', '</s>']\n",
      "batch: 383\n",
      "['<s>', '<unk>', ',', '<unk>', ',', '<unk>', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 384\n",
      "['<s>', 'The', 'light', 'is', 'a', 'Monster', '.', '</s>']\n",
      "batch: 385\n",
      "['<s>', 'It', '<unk>', 'the', '<unk>', '.', '</s>']\n",
      "batch: 386\n",
      "['<s>', '<unk>', 'horse', '.', '</s>']\n",
      "batch: 387\n",
      "['<s>', 'You', 'can', 'really', 'blue', 'fur', '.', '</s>']\n",
      "batch: 388\n",
      "['<s>', 'You', '<unk>', 'the', 'mouths', '.', '</s>']\n",
      "batch: 389\n",
      "['<s>', '<unk>', 'over', 'in', 'your', 'living', 'living', '.', '</s>']\n",
      "batch: 390\n",
      "['<s>', 'And', 'you', 'have', 'of', 'your', 'imagination', '.', '</s>']\n",
      "batch: 391\n",
      "['<s>', 'Now', 'let', 'me', '.', '</s>']\n",
      "batch: 392\n",
      "['<s>', 'OK', '.', 'Now', 'the', 'eyes', 'were', 'eyes', '.', '</s>']\n",
      "batch: 393\n",
      "['<s>', 'He', \"'s\", '\"', '<unk>', '.', '\"', '</s>']\n",
      "batch: 394\n",
      "['<s>', 'It', 'was', 'a', 'bunch', 'of', 'very', 'bunch', 'of', 'older', '.', '</s>']\n",
      "batch: 395\n",
      "['<s>', 'They', 'were', '<unk>', 'from', 'random', 'numbers', '.', '</s>']\n",
      "batch: 396\n",
      "['<s>', '<unk>', '<unk>', '<unk>', 'and', '<unk>', '.', '</s>']\n",
      "batch: 397\n",
      "['<s>', 'And', 'the', 'entire', 'minutes', '.', '</s>']\n",
      "batch: 398\n",
      "['<s>', 'They', '<unk>', 'the', 'order', 'of', 'the', '<unk>', '.', '</s>']\n",
      "batch: 399\n",
      "['<s>', 'That', \"'s\", 'crazy', '.', '</s>']\n",
      "batch: 400\n",
      "['<s>', 'These', 'people', 'have', 'to', 'have', '<unk>', '.', '</s>']\n",
      "batch: 401\n",
      "['<s>', 'I', 'started', 'with', 'some', 'of', 'them', '.', '</s>']\n",
      "batch: 402\n",
      "['<s>', 'And', 'Ed', 'said', ',', '\"', '<unk>', '.', '\"', '</s>']\n",
      "batch: 403\n",
      "['<s>', 'I', 'only', 'got', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 404\n",
      "['<s>', 'Anybody', 'who', 'posted', ',', 'they', '<unk>', 'a', '<unk>', '.', '</s>']\n",
      "batch: 405\n",
      "['<s>', 'Do', 'you', '<unk>', '?', '</s>']\n",
      "batch: 406\n",
      "['<s>', 'I', 'said', ',', '\"', 'What', '?', '</s>']\n",
      "batch: 407\n",
      "['<s>', 'It', 'would', 'suggest', 'that', 'could', '.', '</s>']\n",
      "batch: 408\n",
      "['<s>', 'I', 'told', 'me', 'was', '<unk>', ',', 'but', '<unk>', 'it', '.', '</s>']\n",
      "batch: 409\n",
      "['<s>', 'I', 'mean', ',', 'you', 'do', \"n't\", 'you', '?', '</s>']\n",
      "batch: 410\n",
      "['<s>', 'And', 'this', 'was', 'a', 'very', 'nice', 'journey', '.', '</s>']\n",
      "batch: 411\n",
      "['<s>', 'I', 'met', 'a', 'interestingly', '<unk>', '.', '</s>']\n",
      "batch: 412\n",
      "['<s>', 'This', 'is', '<unk>', ',', 'for', 'example', '.', '</s>']\n",
      "batch: 413\n",
      "['<s>', 'It', \"'s\", 'suffering', 'and', 'I', 'think', 'of', 'the', 'world', '.', '</s>']\n",
      "batch: 414\n",
      "['<s>', 'The', 'other', 'side', 'of', 'the', '<unk>', '.', '</s>']\n",
      "batch: 415\n",
      "['<s>', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 416\n",
      "['<s>', 'He', 'was', 'the', '<unk>', '<unk>', '.', '\"', '</s>']\n",
      "batch: 417\n",
      "['<s>', 'And', 'I', \"'ve\", 'got', 'more', 'interesting', 'things', '.', '</s>']\n",
      "batch: 418\n",
      "['<s>', 'There', 'were', '<unk>', 'people', 'in', 'their', 'memory', '.', '</s>']\n",
      "batch: 419\n",
      "['<s>', 'And', 'our', 'modern', 'technology', ',', 'right', '?', '</s>']\n",
      "batch: 420\n",
      "['<s>', 'They', \"'ve\", 'changed', 'and', 'they', 'changed', 'also', '.', '</s>']\n",
      "batch: 421\n",
      "['<s>', 'If', 'you', 'need', 'to', 'think', 'that', \"'s\", '<unk>', '.', '</s>']\n",
      "batch: 422\n",
      "['<s>', 'In', 'fact', \"'s\", 'not', 'like', 'this', '.', '</s>']\n",
      "batch: 423\n",
      "['<s>', 'And', 'that', 'I', 'wanted', 'to', 'do', 'this', '.', '</s>']\n",
      "batch: 424\n",
      "['<s>', 'And', 'the', 'answer', ',', '</s>']\n",
      "batch: 425\n",
      "['<s>', 'Are', 'they', 'smarter', '.', '</s>']\n",
      "batch: 426\n",
      "['<s>', 'They', 'gave', 'them', 'a', 'pile', 'and', 'the', 'answer', 'was', '<unk>', '.', '</s>']\n",
      "batch: 427\n",
      "['<s>', 'Why', '?', '<unk>', 'them', '.', '</s>']\n",
      "batch: 428\n",
      "['<s>', 'This', 'is', 'my', 'friend', \"'s\", '<unk>', '<unk>', ':', '</s>']\n",
      "batch: 429\n",
      "['<s>', 'This', 'is', 'his', 'name', '.', '</s>']\n",
      "batch: 430\n",
      "['<s>', 'Do', 'you', 'know', 'it', 'was', '?', '</s>']\n",
      "batch: 431\n",
      "['<s>', 'Same', 'word', ';', 'this', 'is', 'weird', '.', '</s>']\n",
      "batch: 432\n",
      "['<s>', 'What', \"'s\", '<unk>', '.', '</s>']\n",
      "batch: 433\n",
      "['<s>', 'Well', ',', 'the', 'name', 'of', 'is', 'nothing', '.', '</s>']\n",
      "batch: 434\n",
      "['<s>', 'He', 'is', 'completely', 'independent', 'in', 'your', 'head', '<unk>', '.', '</s>']\n",
      "batch: 435\n",
      "['<s>', 'But', 'the', 'job', 'of', 'the', '<unk>', '.', '</s>']\n",
      "batch: 436\n",
      "['<s>', '<unk>', 'white', 'white', '<unk>', '.', '</s>']\n",
      "batch: 437\n",
      "['<s>', '<unk>', 'have', 'their', 'hands', '.', '</s>']\n",
      "batch: 438\n",
      "['<s>', '<unk>', 'smell', 'when', 'they', 'come', 'home', '.', '</s>']\n",
      "batch: 439\n",
      "['<s>', 'Maybe', 'we', 'have', 'a', '<unk>', '.', '</s>']\n",
      "batch: 440\n",
      "['<s>', 'One', 'of', 'effective', 'techniques', 'can', 'the', 'secret', '.', '</s>']\n",
      "batch: 441\n",
      "['<s>', 'It', 'became', 'the', 'known', '<unk>', '.', '</s>']\n",
      "batch: 442\n",
      "['<s>', 'Not', 'only', 'the', 'bodies', 'to', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 443\n",
      "['<s>', 'Nobody', 'was', 'saying', ',', 'who', 'did', '.', '</s>']\n",
      "batch: 444\n",
      "['<s>', 'The', 'body', '<unk>', 'very', '<unk>', '.', '</s>']\n",
      "batch: 445\n",
      "['<s>', 'It', \"'s\", 'a', 'hive', '.', '</s>']\n",
      "batch: 446\n",
      "['<s>', 'And', 'so', 'he', 'approaches', 'with', 'the', '<unk>', '.', '</s>']\n",
      "batch: 447\n",
      "['<s>', 'So', 'how', 'does', 'it', 'work', '.', '</s>']\n",
      "batch: 448\n",
      "['<s>', 'So', ',', 'you', 'know', ',', 'you', 'front', '.', '</s>']\n",
      "batch: 449\n",
      "['<s>', 'And', 'you', 'would', 'remember', 'your', 'friend', '<unk>', '.', '</s>']\n",
      "batch: 450\n",
      "['<s>', 'I', 'found', 'this', 'was', 'fascinating', '.', '</s>']\n",
      "batch: 451\n",
      "['<s>', 'For', 'example', ',', '<unk>', '.', '</s>']\n",
      "batch: 452\n",
      "['<s>', 'I', 'could', 'write', 'something', 'on', 'the', 'writing', '.', '</s>']\n",
      "batch: 453\n",
      "['<s>', 'But', 'there', 'was', 'a', 'problem', '.', '</s>']\n",
      "batch: 454\n",
      "['<s>', 'The', 'problem', 'that', 'the', 'most', ',', 'are', '<unk>', '.', '</s>']\n",
      "batch: 455\n",
      "['<s>', 'It', \"'s\", 'a', 'human', 'beings', ',', 'by', 'writing', '.', '</s>']\n",
      "batch: 456\n",
      "['<s>', 'It', \"'s\", 'something', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 457\n",
      "['<s>', 'I', \"'m\", 'journalist', ',', 'which', 'is', 'writing', 'is', 'not', 'write', '.', '</s>']\n",
      "batch: 458\n",
      "['<s>', 'Maybe', 'a', 'poem', '.', '</s>']\n",
      "batch: 459\n",
      "['<s>', 'The', 'names', 'of', 'that', 'I', 'saw', '<unk>', '.', '</s>']\n",
      "batch: 460\n",
      "['<s>', 'I', 'had', 'to', 'know', '.', '</s>']\n",
      "batch: 461\n",
      "['<s>', 'I', 'just', 'never', 'imagine', '.', '</s>']\n",
      "batch: 462\n",
      "['<s>', 'It', \"'s\", 'fun', 'because', 'of', 'violence', '.', '</s>']\n",
      "batch: 463\n",
      "['<s>', 'I', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 464\n",
      "['<s>', 'Here', 'I', 'am', 'with', 'my', '<unk>', '.', '</s>']\n",
      "batch: 465\n",
      "['<s>', 'I', 'went', 'on', 'the', 'same', 'year', '-', 'year', '.', '</s>']\n",
      "batch: 466\n",
      "['<s>', 'And', 'I', '<unk>', 'just', 'as', 'a', 'experiment', 'ago', '.', '</s>']\n",
      "batch: 467\n",
      "['<s>', 'It', 'may', 'be', 'a', 'nice', 'to', 'be', 'a', 'Department', '.', '</s>']\n",
      "batch: 468\n",
      "['<s>', 'The', 'problem', 'was', 'the', 'experiment', '.', '</s>']\n",
      "batch: 469\n",
      "['<s>', 'I', 'won', 'the', '<unk>', 'would', 'have', 'been', '<unk>', '.', '</s>']\n",
      "batch: 470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'That', \"'s\", '<unk>', '.', '</s>']\n",
      "batch: 471\n",
      "['<s>', '<unk>', 'because', 'they', 'simple', 'ideas', ',', 'ideas', '.', '</s>']\n",
      "batch: 472\n",
      "['<s>', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 473\n",
      "['<s>', 'We', 'are', 'better', 'things', '.', '</s>']\n",
      "batch: 474\n",
      "['<s>', 'And', 'we', 'are', '<unk>', '.', '</s>']\n",
      "batch: 475\n",
      "['<s>', '<unk>', ',', 'they', 'all', '<unk>', '.', '</s>']\n",
      "batch: 476\n",
      "['<s>', 'Actually', ',', 'it', \"'s\", 'not', 'necessarily', 'a', 'deep', '.', '</s>']\n",
      "batch: 477\n",
      "['<s>', 'They', 'work', 'because', 'they', '.', '</s>']\n",
      "batch: 478\n",
      "['<s>', 'But', 'it', \"'s\", 'not', '<unk>', '.', '</s>']\n",
      "batch: 479\n",
      "['<s>', 'This', 'is', 'what', 'we', '<unk>', 'in', 'the', 'jungle', '.', '</s>']\n",
      "batch: 480\n",
      "['<s>', 'I', 'learned', 'that', 'in', 'of', 'us', 'has', '<unk>', '.', '</s>']\n",
      "batch: 481\n",
      "['<s>', '<unk>', '<unk>', 'in', 'New', 'York', '<unk>', '.', '</s>']\n",
      "batch: 482\n",
      "['<s>', 'As', 'every', 'student', 'did', 'a', '<unk>', '.', '</s>']\n",
      "batch: 483\n",
      "['<s>', 'I', \"'d\", '<unk>', 'on', '<unk>', '.', '</s>']\n",
      "batch: 484\n",
      "['<s>', 'So', '<unk>', ',', 'I', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 485\n",
      "['<s>', 'And', 'it', 'was', 'the', 'difference', 'to', 'the', 'best', 'decades', '.', '</s>']\n",
      "batch: 486\n",
      "['<s>', 'And', 'it', 'was', 'a', 'bunch', 'of', 'the', '-', '<unk>', '.', '</s>']\n",
      "batch: 487\n",
      "['<s>', 'Some', 'of', 'my', 'kids', 'do', 'not', '<unk>', '.', '</s>']\n",
      "batch: 488\n",
      "['<s>', 'So', 'this', 'is', 'thinking', '.', '</s>']\n",
      "batch: 489\n",
      "['<s>', 'So', 'I', 'had', 'my', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 490\n",
      "['<s>', 'And', 'I', 'went', 'to', 'the', '<unk>', '.', '</s>']\n",
      "batch: 491\n",
      "['<s>', 'We', 'tried', 'to', '<unk>', ',', 'and', '<unk>', '.', '</s>']\n",
      "batch: 492\n",
      "['<s>', 'We', 'went', 'to', 'the', 'national', 'children', 'come', '.', '</s>']\n",
      "batch: 493\n",
      "['<s>', 'We', 'went', 'to', 'private', 'companies', ',', '</s>']\n",
      "batch: 494\n",
      "['<s>', 'And', 'the', 'money', 'would', 'make', 'it', '.', '</s>']\n",
      "batch: 495\n",
      "['<s>', 'All', 'of', 'these', 'kinds', 'of', 'success', '.', '</s>']\n",
      "batch: 496\n",
      "['<s>', 'It', 'was', \"n't\", '<unk>', '.', '</s>']\n",
      "batch: 497\n",
      "['<s>', 'It', 'was', 'not', 'low', ',', 'and', 'it', 'was', 'not', 'physical', '.', '</s>']\n",
      "batch: 498\n",
      "['<s>', 'It', 'was', '<unk>', '</s>']\n",
      "batch: 499\n",
      "['<s>', '<unk>', 'is', 'a', 'long', 'term', '.', '</s>']\n",
      "batch: 500\n",
      "['<s>', '<unk>', 'is', '<unk>', '.', '</s>']\n",
      "batch: 501\n",
      "['<s>', '<unk>', 'is', 'a', 'marathon', '.', '</s>']\n",
      "batch: 502\n",
      "['<s>', 'I', 'started', 'to', '<unk>', 'in', 'Chicago', 'in', 'Chicago', '.', '</s>']\n",
      "batch: 503\n",
      "['<s>', 'So', 'day', 'I', 'said', ',', 'how', 'to', '<unk>', 'children', '.', '</s>']\n",
      "batch: 504\n",
      "['<s>', 'As', 'I', 'teaching', 'a', 'function', '</s>']\n",
      "batch: 505\n",
      "['<s>', 'How', 'do', 'I', 'find', '<unk>', '?', '</s>']\n",
      "batch: 506\n",
      "['<s>', 'And', 'I', 'do', \"n't\", 'do', \"n't\", '.', '</s>']\n",
      "batch: 507\n",
      "['<s>', 'But', 'what', 'I', 'know', 'is', 'no', 'poor', '.', '</s>']\n",
      "batch: 508\n",
      "['<s>', 'Actually', ',', '<unk>', 'is', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 509\n",
      "['<s>', '<unk>', 'injury', 'is', 'to', '<unk>', '.', '</s>']\n",
      "batch: 510\n",
      "['<s>', 'But', 'we', 'need', '.', '</s>']\n",
      "batch: 511\n",
      "['<s>', 'Here', 'I', 'want', 'to', 'be', '<unk>', 'here', '.', '</s>']\n",
      "batch: 512\n",
      "['<s>', 'This', 'is', 'the', 'debate', 'that', 'we', 'have', 'a', '<unk>', '.', '</s>']\n",
      "batch: 513\n",
      "['<s>', 'We', 'have', 'our', 'best', 'ideas', 'and', 'test', '.', '</s>']\n",
      "batch: 514\n",
      "['<s>', 'In', 'other', 'words', 'have', 'to', 'give', 'your', 'children', '.', '</s>']\n",
      "batch: 515\n",
      "['<s>', 'Thank', 'you', '.', '</s>']\n",
      "batch: 516\n",
      "['<s>', 'Today', ',', 'as', '<unk>', '<unk>', 'projects', '.', '</s>']\n",
      "batch: 517\n",
      "['<s>', 'The', 'projects', 'are', '<unk>', ',', 'in', 'Tunisia', '.', '</s>']\n",
      "batch: 518\n",
      "['<s>', 'At', 'first', ',', 'I', 'wanted', 'to', '<unk>', '.', '</s>']\n",
      "batch: 519\n",
      "['<s>', 'And', 'third', ',', 'Nothing', 'about', 'Africa', '.', '</s>']\n",
      "batch: 520\n",
      "['<s>', 'This', 'is', 'a', 'place', 'with', 'a', 'local', 'tradition', '.', '</s>']\n",
      "batch: 521\n",
      "['<s>', 'It', 'has', 'also', 'technological', 'to', 'create', 'an', 'activity', '.', '</s>']\n",
      "batch: 522\n",
      "['<s>', 'So', 'I', 'like', 'I', 'want', 'to', 'create', 'a', 'mall', '.', '</s>']\n",
      "batch: 523\n",
      "['<s>', 'But', 'the', 'question', 'like', 'a', 'building', 'with', 'these', 'principles', '.', '</s>']\n",
      "batch: 524\n",
      "['<s>', 'It', \"'s\", 'two', '<unk>', 'that', 'have', 'a', 'coincidence', '.', '</s>']\n",
      "batch: 525\n",
      "['<s>', 'And', 'I', 'thought', 'of', 'course', ',', 'building', '.', '</s>']\n",
      "batch: 526\n",
      "['<s>', 'I', 'really', 'wanted', 'to', 'find', 'on', 'the', '<unk>', '.', '</s>']\n",
      "batch: 527\n",
      "['<s>', 'And', 'then', '<unk>', '.', '</s>']\n",
      "batch: 528\n",
      "['<s>', 'They', \"'ve\", 'been', 'geometric', 'geometry', ',', 'the', 'entire', '<unk>', '.', '</s>']\n",
      "batch: 529\n",
      "['<s>', 'So', 'we', 'led', 'some', 'which', 'we', 'did', 'the', 'project', '.', '</s>']\n",
      "batch: 530\n",
      "['<s>', 'We', 'created', 'a', '<unk>', 'on', 'the', 'roof', '.', '</s>']\n",
      "batch: 531\n",
      "['<s>', 'Hopefully', 'in', 'the', 'next', 'to', 'the', '<unk>', '.', '</s>']\n",
      "batch: 532\n",
      "['<s>', 'The', 'second', 'example', ',', 'in', '-', '2,000', 'in', '<unk>', '.', '</s>']\n",
      "batch: 533\n",
      "['<s>', 'What', 'would', 'could', 'be', 'a', 'device', 'for', 'a', '<unk>', '.', '</s>']\n",
      "batch: 534\n",
      "['<s>', 'And', 'it', 'is', 'a', 'combination', 'of', 'witnessed', '.', '</s>']\n",
      "batch: 535\n",
      "['<s>', 'It', 'also', 'the', 'concept', '.', '</s>']\n",
      "batch: 536\n",
      "['<s>', 'These', 'two', 'examples', '.', '</s>']\n",
      "batch: 537\n",
      "['<s>', 'They', 'are', 'in', 'which', 'we', 'can', 'grow', '.', '</s>']\n",
      "batch: 538\n",
      "['<s>', 'Thank', 'you', '.', '</s>']\n",
      "batch: 539\n",
      "['<s>', 'They', 'are', 'so', 'ubiquitous', '.', '</s>']\n",
      "batch: 540\n",
      "['<s>', 'There', 'is', 'a', 'three', 'times', '.', '</s>']\n",
      "batch: 541\n",
      "['<s>', 'First', ',', 'trust', 'is', 'a', 'lot', 'of', 'us', '.', '</s>']\n",
      "batch: 542\n",
      "['<s>', 'Then', ',', 'we', 'should', 'more', '.', '</s>']\n",
      "batch: 543\n",
      "['<s>', 'And', 'what', \"'s\", 'going', 'to', 'build', 'trust', '.', '</s>']\n",
      "batch: 544\n",
      "['<s>', 'I', 'think', 'that', 'goal', 'is', 'to', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 545\n",
      "['<s>', 'And', 'the', 'most', 'people', 'think', 'trust', 'is', '<unk>', '.', '</s>']\n",
      "batch: 546\n",
      "['<s>', 'I', 'tried', 'to', '<unk>', '.', '</s>']\n",
      "batch: 547\n",
      "['<s>', 'Look', 'at', 'a', 'time', 'that', \"'s\", 'a', 'minute', '.', '</s>']\n",
      "batch: 548\n",
      "['<s>', 'But', 'that', \"'s\", 'a', 'lot', '.', '</s>']\n",
      "batch: 549\n",
      "['<s>', '<unk>', '<unk>', '--', '<unk>', '.', '</s>']\n",
      "batch: 550\n",
      "['<s>', 'What', 'could', 'they', '<unk>', '.', '</s>']\n",
      "batch: 551\n",
      "['<s>', 'So', 'they', 'look', 'at', 'the', 'contribute', 'when', 'they', \"'re\", 'asked', '.', '</s>']\n",
      "batch: 552\n",
      "['<s>', 'trust', 'faith', '.', '</s>']\n",
      "batch: 553\n",
      "['<s>', 'If', 'you', 'asked', 'me', '.', '</s>']\n",
      "batch: 554\n",
      "['<s>', 'So', '<unk>', '.', '</s>']\n",
      "batch: 555\n",
      "['<s>', '<unk>', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 556\n",
      "['<s>', 'Now', ',', 'you', 'say', ',', '<unk>', '.', '</s>']\n",
      "batch: 557\n",
      "['<s>', 'And', 'that', 'would', 'be', 'reasonable', '<unk>', '.', '</s>']\n",
      "batch: 558\n",
      "['<s>', 'It', \"'s\", 'rational', '.', '</s>']\n",
      "batch: 559\n",
      "['<s>', 'In', 'short', ',', 'very', 'short', '<unk>', 'species', '.', '</s>']\n",
      "batch: 560\n",
      "['<s>', 'I', \"'ll\", 'not', 'aware', 'of', 'that', \"'s\", 'great', 'deal', '.', '</s>']\n",
      "batch: 561\n",
      "['<s>', 'Two', 'is', 'coming', 'to', 'target', '.', '</s>']\n",
      "batch: 562\n",
      "['<s>', 'The', 'goal', 'is', ',', 'more', '.', '</s>']\n",
      "batch: 563\n",
      "['<s>', 'In', 'fact', ',', 'the', '<unk>', '.', '</s>']\n",
      "batch: 564\n",
      "['<s>', 'I', \"'m\", 'not', '<unk>', '.', '</s>']\n",
      "batch: 565\n",
      "['<s>', 'I', 'would', 'try', 'to', 'trust', 'the', '<unk>', '.', '</s>']\n",
      "batch: 566\n",
      "['<s>', 'And', 'even', 'more', 'active', '.', '</s>']\n",
      "batch: 567\n",
      "['<s>', '\"', 'Because', '\"', 'is', 'no', 'longer', 'in', 'life', '.', '</s>']\n",
      "batch: 568\n",
      "['<s>', '<unk>', ',', 'and', 'the', 'goal', '.', '</s>']\n",
      "batch: 569\n",
      "['<s>', 'people', 'can', '<unk>', '.', '</s>']\n",
      "batch: 570\n",
      "['<s>', 'And', 'to', 'do', 'this', 'is', 'three', '.', '</s>']\n",
      "batch: 571\n",
      "['<s>', 'Are', 'they', 'compatible', '?', '</s>']\n",
      "batch: 572\n",
      "['<s>', 'So', 'the', 'person', 'is', 'not', '<unk>', '.', '</s>']\n",
      "batch: 573\n",
      "['<s>', 'Yet', ',', 'I', 'not', '<unk>', '.', '</s>']\n",
      "batch: 574\n",
      "['<s>', 'And', 'after', 'we', \"'re\", 'only', ',', 'trust', '.', '</s>']\n",
      "batch: 575\n",
      "['<s>', 'Trust', 'trust', '<unk>', '.', '</s>']\n",
      "batch: 576\n",
      "['<s>', 'The', '<unk>', '.', 'That', \"'s\", 'hard', '.', '</s>']\n",
      "batch: 577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'Many', 'of', 'systems', 'have', 'the', 'effect', '.', '</s>']\n",
      "batch: 578\n",
      "['<s>', 'They', 'do', \"n't\", 'work', '.', '</s>']\n",
      "batch: 579\n",
      "['<s>', '<unk>', 'can', 'probably', ',', 'like', '.', '</s>']\n",
      "batch: 580\n",
      "['<s>', 'So', ',', 'to', 'the', '<unk>', '.', '</s>']\n",
      "batch: 581\n",
      "['<s>', 'And', 'now', 'to', 'the', 'task', '.', '</s>']\n",
      "batch: 582\n",
      "['<s>', 'These', '<unk>', 'of', 'describing', 'the', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 583\n",
      "['<s>', 'Because', 'it', \"'s\", '<unk>', 'and', '<unk>', '.', '</s>']\n",
      "batch: 584\n",
      "['<s>', 'We', 'can', 'do', 'this', 'is', '.', '</s>']\n",
      "batch: 585\n",
      "['<s>', 'We', 'can', 'give', 'a', '<unk>', '.', '</s>']\n",
      "batch: 586\n",
      "['<s>', 'So', 'two', 'people', 'who', '<unk>', '.', '</s>']\n",
      "batch: 587\n",
      "['<s>', 'But', 'ultimately', ',', 'to', 'others', '.', '</s>']\n",
      "batch: 588\n",
      "['<s>', 'You', 'ca', \"n't\", 'build', 'what', 'you', 'you', 'do', '.', '</s>']\n",
      "batch: 589\n",
      "['<s>', 'You', \"'ve\", 'to', 'trust', 'that', 'you', 'do', '.', '</s>']\n",
      "batch: 590\n",
      "['<s>', 'So', 'you', 'have', 'to', 'be', '<unk>', '.', '</s>']\n",
      "batch: 591\n",
      "['<s>', 'But', 'you', 'have', 'to', '<unk>', 'you', 'are', '<unk>', '.', '</s>']\n",
      "batch: 592\n",
      "['<s>', 'And', 'the', '<unk>', 'is', '.', '</s>']\n",
      "batch: 593\n",
      "['<s>', 'Here', \"'s\", '<unk>', 'example', '.', '</s>']\n",
      "batch: 594\n",
      "['<s>', 'That', \"'s\", 'great', '.', '</s>']\n",
      "batch: 595\n",
      "['<s>', 'This', 'is', 'an', 'important', 'breakthrough', '.', '</s>']\n",
      "batch: 596\n",
      "['<s>', 'So', 'our', 'goal', 'is', 'very', 'hard', '.', '</s>']\n",
      "batch: 597\n",
      "['<s>', 'It', \"'s\", 'relationships', ',', 'and', 'the', 'other', 'people', 'are', '<unk>', '.', '</s>']\n",
      "batch: 598\n",
      "['<s>', 'You', 'know', 'the', 'most', 'films', 'I', 'can', '.', '</s>']\n",
      "batch: 599\n",
      "['<s>', 'The', 'film', 'does', \"n't\", '<unk>', '.', '</s>']\n",
      "batch: 600\n",
      "['<s>', 'You', '<unk>', '.', '</s>']\n",
      "batch: 601\n",
      "['<s>', 'So', 'she', 'had', 'a', 'Good', 'and', 'a', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 602\n",
      "['<s>', 'But', 'if', 'you', 'look', 'at', 'this', 'time', ',', 'you', 'can', 'be', '<unk>', '.', '</s>']\n",
      "batch: 603\n",
      "['<s>', 'But', ',', '\"', 'Oz', '<unk>', '.', '</s>']\n",
      "batch: 604\n",
      "['<s>', 'He', 'has', \"n't\", 'have', '<unk>', '.', '</s>']\n",
      "batch: 605\n",
      "['<s>', 'You', 'know', 'I', \"'m\", '<unk>', '.', '</s>']\n",
      "batch: 606\n",
      "['<s>', 'First', 'of', 'course', ',', '\"', '<unk>', ',', '\"', '</s>']\n",
      "batch: 607\n",
      "['<s>', '<unk>', 'are', 'also', 'like', '<unk>', '.', '</s>']\n",
      "batch: 608\n",
      "['<s>', 'Her', 'number', 'of', '<unk>', 'of', 'the', 'happiness', '.', '</s>']\n",
      "batch: 609\n",
      "['<s>', 'But', 'it', 'does', \"n't\", 'happen', '.', '</s>']\n",
      "batch: 610\n",
      "['<s>', 'At', 'that', 'I', 'had', 'a', 'son', '.', '</s>']\n",
      "batch: 611\n",
      "['<s>', 'He', 'was', 'at', 'once', 'three', 'years', '.', '</s>']\n",
      "batch: 612\n",
      "['<s>', 'He', 'was', \"n't\", 'going', 'to', 'that', 'he', 'was', 'young', '.', '</s>']\n",
      "batch: 613\n",
      "['<s>', 'But', 'he', 'was', 'the', 'kid', ',', 'less', 'than', 'his', 'sister', '.', '</s>']\n",
      "batch: 614\n",
      "['<s>', 'And', 'I', 'was', 'what', 'he', '<unk>', '.', '</s>']\n",
      "batch: 615\n",
      "['<s>', 'If', 'it', \"'s\", '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 616\n",
      "['<s>', 'He', 'did', 'a', 'army', 'in', 'the', 'government', '.', '</s>']\n",
      "batch: 617\n",
      "['<s>', 'Compare', 'that', \"'s\", 'with', '\"', '<unk>', '<unk>', '.', '\"', '</s>']\n",
      "batch: 618\n",
      "['<s>', 'And', 'their', 'film', '<unk>', '.', '</s>']\n",
      "batch: 619\n",
      "['<s>', 'By', '<unk>', 'and', 'a', '<unk>', 'is', '<unk>', '.', '</s>']\n",
      "batch: 620\n",
      "['<s>', 'But', 'I', 'did', \"n't\", 'much', 'of', 'the', '<unk>', '.', '</s>']\n",
      "batch: 621\n",
      "['<s>', 'I', \"'ve\", 'finished', 'my', 'mission', '.', '</s>']\n",
      "batch: 622\n",
      "['<s>', 'Why', 'are', 'there', 'still', 'get', 'there', '.', '</s>']\n",
      "batch: 623\n",
      "['<s>', 'I', 'do', \"n't\", 'know', 'what', 'I', 'have', 'to', '.', '</s>']\n",
      "batch: 624\n",
      "['<s>', 'There', 'are', 'there', ',', 'I', \"'m\", 'going', 'to', 'each', 'of', '.', '</s>']\n",
      "batch: 625\n",
      "['<s>', 'There', \"'s\", 'not', 'for', 'them', '.', '</s>']\n",
      "batch: 626\n",
      "['<s>', 'I', 'recommend', '<unk>', '.', '</s>']\n",
      "batch: 627\n",
      "['<s>', 'So', 'remember', 'what', 'was', '\"', '<unk>', '.', '\"', '</s>']\n",
      "batch: 628\n",
      "['<s>', '\"', 'Oh', ',', 'that', '<unk>', '.', '</s>']\n",
      "batch: 629\n",
      "['<s>', 'He', \"'s\", 'very', 'good', 'thing', 'you', 'did', 'it', '.', '</s>']\n",
      "batch: 630\n",
      "['<s>', 'But', ',', 'none', 'of', 'these', 'films', 'have', 'been', '<unk>', '.', '</s>']\n",
      "batch: 631\n",
      "['<s>', 'I', 'do', \"n't\", 'know', 'about', 'this', '.', '</s>']\n",
      "batch: 632\n",
      "['<s>', 'Try', 'for', 'a', '<unk>', '.', '</s>']\n",
      "batch: 633\n",
      "['<s>', 'And', 'those', 'women', \"'s\", '<unk>', '.', '</s>']\n",
      "batch: 634\n",
      "['<s>', 'And', 'it', 'looks', 'like', 'the', 'other', 'than', 'her', '.', '</s>']\n",
      "batch: 635\n",
      "['<s>', '<unk>', '?', 'Thank', 'you', '.', '</s>']\n",
      "batch: 636\n",
      "['<s>', 'Thank', 'you', 'know', '.', '</s>']\n",
      "batch: 637\n",
      "['<s>', 'Two', 'women', 'and', 'talk', 'about', 'anything', '.', '</s>']\n",
      "batch: 638\n",
      "['<s>', 'It', '<unk>', '<unk>', 'by', '<unk>', '.', '</s>']\n",
      "batch: 639\n",
      "['<s>', 'So', ',', 'eventually', ',', 'hundreds', 'of', '<unk>', '.', '</s>']\n",
      "batch: 640\n",
      "['<s>', 'So', 'let', \"'s\", 'the', 'numbers', '.', '</s>']\n",
      "batch: 641\n",
      "['<s>', '<unk>', '.', 'That', 'is', \"n't\", '<unk>', '.', '</s>']\n",
      "batch: 642\n",
      "['<s>', 'But', 'there', \"'s\", 'a', 'number', 'of', 'that', '<unk>', '.', '</s>']\n",
      "batch: 643\n",
      "['<s>', 'Last', 'year', ',', 'Last', 'study', 'the', 'year', '.', '</s>']\n",
      "batch: 644\n",
      "['<s>', 'She', \"'s\", '<unk>', '.', '</s>']\n",
      "batch: 645\n",
      "['<s>', 'One', 'of', 'the', 'United', 'States', '<unk>', '.', '</s>']\n",
      "batch: 646\n",
      "['<s>', 'I', 'do', \"n't\", 'the', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 647\n",
      "['<s>', 'I', 'do', \"n't\", 'have', 'to', 'do', 'with', 'that', '.', '</s>']\n",
      "batch: 648\n",
      "['<s>', 'Who', 'is', 'this', '?', '</s>']\n",
      "batch: 649\n",
      "['<s>', 'What', 'we', 'do', \"n't\", 'learn', '.', '</s>']\n",
      "batch: 650\n",
      "['<s>', 'So', 'this', 'story', '.', '</s>']\n",
      "batch: 651\n",
      "['<s>', 'I', 'think', 'of', 'just', 'have', 'a', 'new', 'definition', 'of', '<unk>', '.', '</s>']\n",
      "batch: 652\n",
      "['<s>', 'The', 'definition', 'has', 'been', 'changing', '.', '</s>']\n",
      "batch: 653\n",
      "['<s>', 'You', 'read', 'the', 'roles', 'roles', 'of', '<unk>', '.', '</s>']\n",
      "batch: 654\n",
      "['<s>', 'They', \"'re\", '<unk>', '.', '</s>']\n",
      "batch: 655\n",
      "['<s>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 656\n",
      "['<s>', 'So', 'in', 'the', 'together', '.', '</s>']\n",
      "batch: 657\n",
      "['<s>', 'Maybe', 'it', \"'s\", '<unk>', '.', '</s>']\n",
      "batch: 658\n",
      "['<s>', 'I', 'think', 'of', 'these', 'characters', '.', '</s>']\n",
      "batch: 659\n",
      "['<s>', 'They', 'are', 'leaders', '.', '</s>']\n",
      "batch: 660\n",
      "['<s>', 'I', 'want', 'to', 'make', 'these', '<unk>', '.', '</s>']\n",
      "batch: 661\n",
      "['<s>', 'Thank', 'you', '.', '</s>']\n",
      "batch: 662\n",
      "['<s>', 'I', 'want', 'to', 'talk', 'about', 'a', 'story', '.', '</s>']\n",
      "batch: 663\n",
      "['<s>', 'I', 'do', 'was', 'his', 'story', '.', '</s>']\n",
      "batch: 664\n",
      "['<s>', 'He', 'lives', 'in', 'small', 'village', '.', '</s>']\n",
      "batch: 665\n",
      "['<s>', 'His', 'village', 'is', 'near', '<unk>', '.', '</s>']\n",
      "batch: 666\n",
      "['<s>', 'The', 'drought', \"'s\", 'poverty', '.', '</s>']\n",
      "batch: 667\n",
      "['<s>', 'When', 'it', 'is', 'there', 'are', 'no', 'way', 'to', 'do', \"n't\", '.', '</s>']\n",
      "batch: 668\n",
      "['<s>', 'By', 'the', 'end', ',', 'he', \"'s\", '<unk>', '.', '</s>']\n",
      "batch: 669\n",
      "['<s>', 'It', \"'s\", 'a', 'year', '.', '</s>']\n",
      "batch: 670\n",
      "['<s>', 'It', \"'s\", '<unk>', 'young', 'people', ',', 'they', 'give', 'an', 'opportunity', '.', '</s>']\n",
      "batch: 671\n",
      "['<s>', 'It', '<unk>', 'this', 'lady', '.', '</s>']\n",
      "batch: 672\n",
      "['<s>', 'And', 'he', '<unk>', '.', '</s>']\n",
      "batch: 673\n",
      "['<s>', 'He', 'begins', 'a', 'new', 'life', '.', '</s>']\n",
      "batch: 674\n",
      "['<s>', 'His', 'life', 'has', 'a', 'sense', '.', '</s>']\n",
      "batch: 675\n",
      "['<s>', 'On', 'a', 'Hot', 'in', 'heaven', '<unk>', '<unk>', 'in', 'the', 'air', '.', '</s>']\n",
      "batch: 676\n",
      "['<s>', 'It', '<unk>', '.', '</s>']\n",
      "batch: 677\n",
      "['<s>', 'This', 'was', 'what', 'we', 'call', 'it', '<unk>', '.', '</s>']\n",
      "batch: 678\n",
      "['<s>', 'And', 'his', 'metaphor', 'over', 'the', 'world', '.', '</s>']\n",
      "batch: 679\n",
      "['<s>', '<unk>', 'is', 'that', \"'s\", 'budget', '.', '</s>']\n",
      "batch: 680\n",
      "['<s>', 'In', '2020', ',', 'five', 'in', '2010', '.', '</s>']\n",
      "batch: 681\n",
      "['<s>', 'On', '<unk>', ',', 'average', 'of', 'young', 'people', 'live', 'down', '.', '</s>']\n",
      "batch: 682\n",
      "['<s>', 'Two', 'have', 'no', 'work', 'to', 'school', '.', '</s>']\n",
      "batch: 683\n",
      "['<s>', 'They', \"'re\", 'not', 'anything', '.', '</s>']\n",
      "batch: 684\n",
      "['<s>', 'What', 'if', 'I', '<unk>', 'the', 'same', '.', '</s>']\n",
      "batch: 685\n",
      "['<s>', 'I', \"'ve\", 'been', 'able', 'to', '.', '</s>']\n",
      "batch: 686\n",
      "['<s>', 'I', \"'m\", 'the', 'answer', '.', '</s>']\n",
      "batch: 687\n",
      "['<s>', 'I', 'brought', 'about', '30', 'leaders', '.', '</s>']\n",
      "batch: 688\n",
      "['<s>', 'We', 'put', 'together', ',', 'for', 'the', 'challenges', '.', '</s>']\n",
      "batch: 689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'There', 'was', 'a', 'young', ',', '<unk>', '.', '</s>']\n",
      "batch: 690\n",
      "['<s>', 'And', 'he', 'asked', 'in', 'his', 'high', 'school', '.', '</s>']\n",
      "batch: 691\n",
      "['<s>', 'There', 'was', 'no', 'work', '.', '</s>']\n",
      "batch: 692\n",
      "['<s>', 'They', 'just', 'people', 'like', 'him', '.', '</s>']\n",
      "batch: 693\n",
      "['<s>', 'But', 'this', 'story', 'a', 'different', 'story', '.', '</s>']\n",
      "batch: 694\n",
      "['<s>', 'In', '<unk>', ',', '\"', 'A', 'are', 'the', 'streets', '.', '</s>']\n",
      "batch: 695\n",
      "['<s>', 'And', 'then', 'recognized', '.', '</s>']\n",
      "batch: 696\n",
      "['<s>', 'It', '<unk>', '.', '</s>']\n",
      "batch: 697\n",
      "['<s>', 'It', 'started', 'to', 'rock', ',', 'which', 'could', \"n't\", 'afford', '.', '</s>']\n",
      "batch: 698\n",
      "['<s>', 'How', 'how', 'this', 'story', 'from', '.', '</s>']\n",
      "batch: 699\n",
      "['<s>', 'What', \"'s\", 'difference', '.', '</s>']\n",
      "batch: 700\n",
      "['<s>', 'I', 'think', 'it', \"'s\", 'new', '.', '</s>']\n",
      "batch: 701\n",
      "['<s>', 'It', \"'s\", '<unk>', 'and', 'I', 'can', 'be', '<unk>', '.', '</s>']\n",
      "batch: 702\n",
      "['<s>', 'It', \"'s\", 'hard', 'to', 'creating', '<unk>', '.', '</s>']\n",
      "batch: 703\n",
      "['<s>', 'You', 'can', 'teach', 'young', '.', '</s>']\n",
      "batch: 704\n",
      "['<s>', 'So', 'this', 'was', 'a', 'opportunity', '.', '</s>']\n",
      "batch: 705\n",
      "['<s>', 'He', 'came', 'with', '<unk>', '.', '</s>']\n",
      "batch: 706\n",
      "['<s>', 'There', 'is', 'no', '<unk>', 'in', '<unk>', '.', '</s>']\n",
      "batch: 707\n",
      "['<s>', 'Just', 'on', 'the', '<unk>', ',', 'he', '<unk>', 'needs', '.', '</s>']\n",
      "batch: 708\n",
      "['<s>', 'You', 'never', 'thought', 'a', 'company', '.', '</s>']\n",
      "batch: 709\n",
      "['<s>', 'He', '<unk>', 'their', 'city', ',', 'they', '<unk>', '.', '</s>']\n",
      "batch: 710\n",
      "['<s>', 'And', 'she', 'believe', 'that', 'they', 'could', '<unk>', '.', '</s>']\n",
      "batch: 711\n",
      "['<s>', 'They', 'had', 'Victorian', 'problems', '.', '</s>']\n",
      "batch: 712\n",
      "['<s>', 'You', 'have', '<unk>', 'to', 'develop', 'their', 'business', '.', '</s>']\n",
      "batch: 713\n",
      "['<s>', 'For', 'me', ',', 'I', 'mean', 'a', '<unk>', '.', '</s>']\n",
      "batch: 714\n",
      "['<s>', 'It', \"'s\", 'about', 'social', '<unk>', '.', '</s>']\n",
      "batch: 715\n",
      "['<s>', 'And', 'not', 'sell', 'flowers', '.', '</s>']\n",
      "batch: 716\n",
      "['<s>', 'I', 'think', 'he', 'sells', '<unk>', '.', '</s>']\n",
      "batch: 717\n",
      "['<s>', '<unk>', 'has', 'to', 'help', 'the', '<unk>', 'and', '<unk>', '.', '</s>']\n",
      "batch: 718\n",
      "['<s>', 'He', 'gave', 'me', 'from', 'the', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 719\n",
      "['<s>', 'These', 'young', 'has', 'a', 'lot', 'of', 'these', 'companies', '.', '</s>']\n",
      "batch: 720\n",
      "['<s>', 'Thank', 'you', \"'ve\", 'a', 'lot', '.', '</s>']\n",
      "batch: 721\n",
      "['<s>', 'But', 'these', 'are', 'hard', 'to', 'find', '.', '</s>']\n",
      "batch: 722\n",
      "['<s>', 'They', \"'re\", '<unk>', 'by', 'sand', '.', '</s>']\n",
      "batch: 723\n",
      "['<s>', 'But', 'after', 'some', 'time', 'I', 'had', 'a', '<unk>', '.', '</s>']\n",
      "batch: 724\n",
      "['<s>', 'I', 'started', 'to', 'see', 'them', 'to', 'collect', '.', '</s>']\n",
      "batch: 725\n",
      "['<s>', 'Now', 'evolved', 'to', 'find', 'to', 'a', 'passion', '.', '</s>']\n",
      "batch: 726\n",
      "['<s>', 'So', 'led', 'to', 'me', '.', '</s>']\n",
      "batch: 727\n",
      "['<s>', 'I', 'had', 'to', 'be', 'a', 'map', '.', '</s>']\n",
      "batch: 728\n",
      "['<s>', 'I', 'want', 'to', 'do', 'we', 'use', '<unk>', '.', '</s>']\n",
      "batch: 729\n",
      "['<s>', 'This', 'is', 'a', '<unk>', ',', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 730\n",
      "['<s>', 'This', 'area', 'is', 'the', 'area', 'of', 'three', '.', '</s>']\n",
      "batch: 731\n",
      "['<s>', 'So', 'as', 'a', '<unk>', 'in', 'a', 'unusual', 'position', '.', '</s>']\n",
      "batch: 732\n",
      "['<s>', 'So', 'we', 'used', 'to', 'see', '<unk>', 'of', 'course', '.', '</s>']\n",
      "batch: 733\n",
      "['<s>', 'So', ',', 'the', '<unk>', 'the', '<unk>', '.', '</s>']\n",
      "batch: 734\n",
      "['<s>', 'And', 'we', 'used', 'to', '<unk>', 'you', 'can', 'see', 'here', '.', '</s>']\n",
      "batch: 735\n",
      "['<s>', 'So', 'we', 'found', 'a', '<unk>', 'to', 'the', 'Roman', 'Empire', '.', '</s>']\n",
      "batch: 736\n",
      "['<s>', 'Knowledge', 'is', 'the', 'whole', '<unk>', '.', '</s>']\n",
      "batch: 737\n",
      "['<s>', 'There', 'is', \"n't\", '<unk>', '.', '</s>']\n",
      "batch: 738\n",
      "['<s>', 'At', 'least', ',', 'in', 'the', '<unk>', '.', '</s>']\n",
      "batch: 739\n",
      "['<s>', 'You', 'can', 'compare', 'on', 'page', '.', '</s>']\n",
      "batch: 740\n",
      "['<s>', 'Thank', 'you', '.', '</s>']\n",
      "batch: 741\n",
      "['<s>', 'I', 'was', 'this', 'new', '<unk>', '.', '</s>']\n",
      "batch: 742\n",
      "['<s>', 'But', 'for', 'a', '<unk>', 'like', 'the', 'Great', '<unk>', '.', '</s>']\n",
      "batch: 743\n",
      "['<s>', 'I', 'thought', 'that', 'a', 'simple', ',', 'I', 'know', '.', '</s>']\n",
      "batch: 744\n",
      "['<s>', 'In', 'the', 'next', '15', 'years', 'ago', '.', '</s>']\n",
      "batch: 745\n",
      "['<s>', 'Just', 'for', 'five', 'minutes', ',', 'a', 'way', '.', '</s>']\n",
      "batch: 746\n",
      "['<s>', 'And', '<unk>', '<unk>', '<unk>', '<unk>', ':', 'It', 'is', 'about', '20,000', '<unk>', '.', '</s>']\n",
      "batch: 747\n",
      "['<s>', 'They', 'only', '1,000', '<unk>', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 748\n",
      "['<s>', 'Today', ',', 'with', 'eight', '-', 'olds', 'to', '-', 'works', 'works', '.', '</s>']\n",
      "batch: 749\n",
      "['<s>', 'Are', 'you', 'ready', '.', '</s>']\n",
      "batch: 750\n",
      "['<s>', 'Open', 'your', 'mouth', 'until', 'it', \"'s\", '<unk>', '.', '</s>']\n",
      "batch: 751\n",
      "['<s>', 'It', \"'s\", '<unk>', '.', '</s>']\n",
      "batch: 752\n",
      "['<s>', 'This', 'man', 'is', '<unk>', '.', '</s>']\n",
      "batch: 753\n",
      "['<s>', 'It', \"'s\", 'a', 'tree', '.', '</s>']\n",
      "batch: 754\n",
      "['<s>', 'This', 'is', 'a', '\"', '<unk>', '.', '</s>']\n",
      "batch: 755\n",
      "['<s>', '\"', 'The', '\"', '.', '</s>']\n",
      "batch: 756\n",
      "['<s>', 'The', 'moon', '.', '\"', '</s>']\n",
      "batch: 757\n",
      "['<s>', '<unk>', '<unk>', 'looks', 'like', 'a', 'west', '.', '</s>']\n",
      "batch: 758\n",
      "['<s>', 'Those', 'eight', '-', '<unk>', '.', '</s>']\n",
      "batch: 759\n",
      "['<s>', 'They', 'are', '<unk>', 'to', 'form', '.', '</s>']\n",
      "batch: 760\n",
      "['<s>', 'A', 'human', '.', '</s>']\n",
      "batch: 761\n",
      "['<s>', 'If', 'it', ',', 'then', 'it', \"'s\", '<unk>', '.', '\"', '.', '</s>']\n",
      "batch: 762\n",
      "['<s>', 'Or', ',', '\"', '<unk>', 'and', '\"', 'The', '\"', '</s>']\n",
      "batch: 763\n",
      "['<s>', 'The', 'human', ',', 'he', \"'s\", 'like', 'gray', '.', '</s>']\n",
      "batch: 764\n",
      "['<s>', 'He', \"'s\", 'a', '\"', ',', 'like', '<unk>', '.', '\"', \"'s\", '<unk>', '.', '</s>']\n",
      "batch: 765\n",
      "['<s>', 'One', 'tree', '.', 'Two', 'trees', ',', 'for', 'a', 'tree', '.', '</s>']\n",
      "batch: 766\n",
      "['<s>', '<unk>', 'for', ',', '\"', 'Great', '.', '\"', '</s>']\n",
      "batch: 767\n",
      "['<s>', '<unk>', 'a', 'tree', 'and', 'you', 'have', 'a', '\"', '<unk>', '.', '</s>']\n",
      "batch: 768\n",
      "['<s>', 'Put', 'me', ',', 'it', 'means', '\"', '<unk>', '.', '\"', '</s>']\n",
      "batch: 769\n",
      "['<s>', 'Simple', 'because', 'it', 'is', '<unk>', '.', '</s>']\n",
      "batch: 770\n",
      "['<s>', 'Remember', 'the', 'fire', '.', '</s>']\n",
      "batch: 771\n",
      "['<s>', 'Two', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 772\n",
      "['<s>', 'Three', 'fire', ',', '\"', 'extreme', '.', '</s>']\n",
      "batch: 773\n",
      "['<s>', 'Buy', 'two', 'trees', 'means', '.', '\"', '<unk>', '.', '\"', '</s>']\n",
      "batch: 774\n",
      "['<s>', 'It', \"'s\", 'the', 'source', 'the', 'door', '.', '</s>']\n",
      "batch: 775\n",
      "['<s>', 'Two', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 776\n",
      "['<s>', '<unk>', 'together', '\"', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 777\n",
      "['<s>', 'If', 'the', 'sun', ',', 'the', '<unk>', '\"', '<unk>', '.', '</s>']\n",
      "batch: 778\n",
      "['<s>', 'The', 'only', 'the', 'next', 'thing', 'is', '.', '</s>']\n",
      "batch: 779\n",
      "['<s>', 'The', 'sun', 'on', 'the', 'horizon', '.', '</s>']\n",
      "batch: 780\n",
      "['<s>', '<unk>', '.', 'And', '\"', '<unk>', '.', '\"', '</s>']\n",
      "batch: 781\n",
      "['<s>', '<unk>', 'a', 'door', ',', '\"', 'questions', '\"', 'questions', '.', '</s>']\n",
      "batch: 782\n",
      "['<s>', '<unk>', ',', '<unk>', 'out', 'there', '.', '</s>']\n",
      "batch: 783\n",
      "['<s>', 'This', 'person', 'out', '-', '\"', '<unk>', '\"', '</s>']\n",
      "batch: 784\n",
      "['<s>', '<unk>', 'is', 'the', '<unk>', '.', '</s>']\n",
      "batch: 785\n",
      "['<s>', 'Two', 'of', '-', '<unk>', '.', '</s>']\n",
      "batch: 786\n",
      "['<s>', '<unk>', 'for', 'three', ':', '\"', '<unk>', '.', '</s>']\n",
      "batch: 787\n",
      "['<s>', 'We', \"'ve\", 'now', '30', '<unk>', '<unk>', '.', '</s>']\n",
      "batch: 788\n",
      "['<s>', 'And', 'this', 'method', 'from', 'the', 'first', 'genetic', 'strangers', '.', '</s>']\n",
      "batch: 789\n",
      "['<s>', 'The', 'next', 'of', 'eight', '<unk>', '32', 'words', '.', '</s>']\n",
      "batch: 790\n",
      "['<s>', 'If', 'we', 'can', 'get', 'them', '.', '</s>']\n",
      "batch: 791\n",
      "['<s>', 'So', ',', 'fire', 'fire', '<unk>', ',', 'so', 'to', 'give', 'the', '<unk>', '.', '</s>']\n",
      "batch: 792\n",
      "['<s>', 'We', 'know', 'is', 'the', 'Sun', '.', '</s>']\n",
      "batch: 793\n",
      "['<s>', 'This', 'is', 'the', 'origin', 'because', 'Japan', ',', 'in', '<unk>', '.', '</s>']\n",
      "batch: 794\n",
      "['<s>', 'One', '-', 'changing', 'what', '\"', '<unk>', '.', '\"', 'Japan', '.', '\"', '</s>']\n",
      "batch: 795\n",
      "['<s>', 'A', 'human', 'Japan', '.', '</s>']\n",
      "batch: 796\n",
      "['<s>', 'A', '\"', '\"', '.', '</s>']\n",
      "batch: 797\n",
      "['<s>', 'The', '<unk>', 'is', 'the', 'left', '.', '</s>']\n",
      "batch: 798\n",
      "['<s>', 'Today', 'is', '\"', '65', '.', '</s>']\n",
      "batch: 799\n",
      "['<s>', '<unk>', ',', 'where', 'it', 'stops', 'for', '<unk>', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "model = 's2s_attn'\n",
    "src = batch_rev.long()\n",
    "beam_width = 3\n",
    "max_len = 20\n",
    "output_width = 1\n",
    "alpha = 1\n",
    "batch_size = len(sentences)\n",
    "predictions = beamsearch(model, attn_seq2context, attn_context2trg, context_size, src, beam_width, max_len, output_width=output_width, alpha=alpha, BATCH_SIZE=batch_size, padding=False, EN=EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''stored_logprobs = torch.zeros((32,3,100))\n",
    "stored_logprobs[0,1,1] = 5\n",
    "stored_logprobs[0,2,1] = 10\n",
    "probs = torch.argsort(stored_logprobs[0].view(-1), descending=True)\n",
    "temp = torch.zeros((2,300))\n",
    "sorted_logprobs = torch.tensor(divmod(probs.numpy(), 100), device='cuda')\n",
    "gap = 2\n",
    "for c in sorted_logprobs.transpose(0,1)[0:gap]:\n",
    "    print(c)\n",
    "sorted_logprobs.transpose(0,1).shape'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for kaggle\n",
    "def escape(l):\n",
    "    return l.replace(\"\\\"\", \"<quote>\").replace(\",\", \"<comma>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for kaggle\n",
    "with open(\"pred_kaggle.txt\", \"w\") as f:\n",
    "    f.write(\"Id,Predicted\\n\")\n",
    "    for i, l in enumerate(open(\"source_test.txt\")):\n",
    "        preds = ['|'.join([EN.vocab.itos[k] for k in predictions[i][j][1:]]) for j in range(output_width)]\n",
    "        f.write(\"%d,%s\"%(i, escape(\" \".join(preds))))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bleu\n",
    "with open(\"pred_bleu.txt\", \"w\") as f:\n",
    "    for i, l in enumerate(open(\"source_test.txt\")):\n",
    "        f.write(\"%s\\n\"%(\" \".join([EN.vocab.itos[k] for k in predictions[i][0][1:-1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
