{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 287 - HW 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchtext opt_einsum git+https://github.com/harvardnlp/namedtensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "from namedtensor import ntorch, NamedTensor\n",
    "from namedtensor.text import NamedField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train) 549367\n",
      "len(TEXT.vocab) 62998\n",
      "len(LABEL.vocab) 4\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "TEXT = NamedField(names=('seqlen',)) # Our input $x$\n",
    "LABEL = NamedField(sequential=False, names=()) # Our labels $y$\n",
    "train, val, test = torchtext.datasets.SNLI.splits(TEXT, LABEL)\n",
    "print('len(train)', len(train))\n",
    "TEXT.build_vocab(train)\n",
    "LABEL.build_vocab(train)\n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
    "print('len(LABEL.vocab)', len(LABEL.vocab))\n",
    "train_iter, val_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
    "    (train, val, test), batch_size=16, device=torch.device(\"cuda\"), repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word embeddings shape: OrderedDict([('word', 62998), ('embedding', 300)])\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary with word embeddings\n",
    "# out-of-vocabulary words are hashed to one of 100 random embeddings each initialized to mean 0, stdev 1 (Sec 5.1)\n",
    "unk_vectors = [torch.randn(300) for _ in range(100)]\n",
    "TEXT.vocab.load_vectors(vectors='glove.6B.300d', unk_init=lambda x:random.choice(unk_vectors))\n",
    "vectors = TEXT.vocab.vectors\n",
    "vectors = vectors / vectors.norm(dim=1, keepdim=True) # normalized to have l_2 norm of 1\n",
    "vectors = NamedTensor(vectors, ('word', 'embedding'))\n",
    "TEXT.vocab.vectors = vectors\n",
    "print(\"word embeddings shape:\", TEXT.vocab.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of premise batch: OrderedDict([('seqlen', 26), ('batch', 16)])\n",
      "Size of hypothesis batch: OrderedDict([('seqlen', 11), ('batch', 16)])\n",
      "Size of label batch: OrderedDict([('batch', 16)])\n"
     ]
    }
   ],
   "source": [
    "# here's an example of a training example\n",
    "batch = next(iter(train_iter))\n",
    "print(\"Size of premise batch:\", batch.premise.shape)\n",
    "print(\"Size of hypothesis batch:\", batch.hypothesis.shape)\n",
    "print(\"Size of label batch:\", batch.label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Decomposable Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSIONS -- input: 62998, embed: 300, hidden1: 200, hidden2: 400, output: 4\n"
     ]
    }
   ],
   "source": [
    "# dimensions\n",
    "input_size = TEXT.vocab.vectors.shape['word']\n",
    "embed_size = TEXT.vocab.vectors.shape['embedding']\n",
    "hidden_size1 = 200\n",
    "hidden_size2 = hidden_size1 * 2\n",
    "output_size = len(LABEL.vocab)\n",
    "print('DIMENSIONS -- input: %d, embed: %d, hidden1: %d, hidden2: %d, output: %d'%(input_size, embed_size, hidden_size1, hidden_size2, output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([62998, 300])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-trained embeddings\n",
    "weights = TEXT.vocab.vectors.values.cuda()\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_tkn = TEXT.vocab.stoi['<pad>']\n",
    "#weights[pad_tkn,:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedProject(torch.nn.Module):\n",
    "    def __init__(self, weights, embed_size, project_size):\n",
    "        super(EmbedProject, self).__init__()\n",
    "        self.embed = nn.Embedding.from_pretrained(weights, freeze=True) # weights: input_size x embed_size\n",
    "        self.linear = nn.Linear(embed_size, project_size)\n",
    "        torch.nn.init.normal_(self.linear.weight, mean=0, std=0.01)\n",
    "    def forward(self, inputs):\n",
    "        embedding = self.embed(inputs)\n",
    "        output = self.linear(embedding)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbedProject(\n",
       "  (embed): Embedding(62998, 300)\n",
       "  (linear): Linear(in_features=300, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EP1 = EmbedProject(weights, embed_size, hidden_size1).cuda()\n",
    "EP1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 26]),\n",
       " torch.Size([16, 11]),\n",
       " torch.Size([16, 27]),\n",
       " torch.Size([16, 12]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_sent1 = batch.premise.values.transpose(0,1)\n",
    "raw_sent2 = batch.hypothesis.values.transpose(0,1)\n",
    "null_tkn = torch.tensor(TEXT.vocab.stoi['null'], device='cuda')\n",
    "null_tkns = null_tkn.repeat(raw_sent1.shape[0],1)\n",
    "sent1 = torch.cat((null_tkns, raw_sent1), 1)\n",
    "sent2 = torch.cat((null_tkns, raw_sent2), 1)\n",
    "raw_sent1.shape, raw_sent2.shape, sent1.shape, sent2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 27, 200]), torch.Size([16, 12, 200]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj1 = EP1(sent1)\n",
    "proj2 = EP1(sent2)\n",
    "proj1.shape, proj2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''temp_embed = nn.Embedding.from_pretrained(weights, freeze=True)\n",
    "temp_linear = nn.Linear(300, 200).cuda()\n",
    "temp_embedding = temp_embed(sent1)\n",
    "#temp_embedding[0,-1,:]\n",
    "temp_output = temp_linear(temp_embedding)\n",
    "mask = sent1 != pad_tkn\n",
    "mask.shape, temp_output.shape\n",
    "temp_temp_output = mask * temp_output\n",
    "temp_temp_output.shape'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardF(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout=0.2):\n",
    "        super(FeedForwardF, self).__init__()\n",
    "        self.d = nn.Dropout(dropout)\n",
    "        self.m = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "        for param in self.parameters():\n",
    "            torch.nn.init.normal_(param, mean=0, std=0.01)\n",
    "    def forward(self, inputs):\n",
    "        hidden = self.m(self.linear1(self.d(inputs)))\n",
    "        output = self.m(self.linear2(self.d(hidden)))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForwardF(\n",
       "  (d): Dropout(p=0.2)\n",
       "  (m): ReLU()\n",
       "  (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1 = FeedForwardF(hidden_size1, hidden_size1, hidden_size1).cuda()\n",
    "F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 26, 200]), torch.Size([16, 22, 200]))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = F1(proj1)\n",
    "f2 = F1(proj2)\n",
    "f1.shape, f2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 26, 22]), torch.Size([16, 22, 26]))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score1 = torch.bmm(f1, f2.transpose(1,2))\n",
    "score2 = score1.transpose(1,2)\n",
    "score1.shape, score2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 26, 22]), torch.Size([16, 22, 26]))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob1 = F.softmax(score1, dim=2)\n",
    "prob2 = F.softmax(score2, dim=2)\n",
    "prob1.shape, prob2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 22, 200]), torch.Size([16, 26, 200]))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj1_soft = torch.bmm(prob2, proj1)\n",
    "proj2_soft = torch.bmm(prob1, proj2)\n",
    "proj1_soft.shape, proj2_soft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 26, 400]), torch.Size([16, 22, 400]))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj1_combined = torch.cat((proj1, proj2_soft), dim=2)\n",
    "proj2_combined = torch.cat((proj2, proj1_soft), dim=2)\n",
    "proj1_combined.shape, proj2_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardG(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout=0.2):\n",
    "        super(FeedForwardG, self).__init__()\n",
    "        self.d = nn.Dropout(dropout)\n",
    "        self.m = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "        for param in self.parameters():\n",
    "            torch.nn.init.normal_(param, mean=0, std=0.01)\n",
    "    def forward(self, inputs):\n",
    "        hidden = self.m(self.linear1(self.d(inputs)))\n",
    "        output = self.m(self.linear2(self.d(hidden)))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForwardG(\n",
       "  (d): Dropout(p=0.2)\n",
       "  (m): ReLU()\n",
       "  (linear1): Linear(in_features=400, out_features=200, bias=True)\n",
       "  (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G1 = FeedForwardG(hidden_size2, hidden_size1, hidden_size1).cuda()\n",
    "G1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 26, 200]), torch.Size([16, 22, 200]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1 = G1(proj1_combined)\n",
    "g2 = G1(proj2_combined)\n",
    "g1.shape, g2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 200]), torch.Size([16, 200]))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1_sum = g1.sum(dim=1)\n",
    "g2_sum = g2.sum(dim=1)\n",
    "g1_sum.shape, g2_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 400])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_all = torch.cat((g1_sum, g2_sum), dim=1)\n",
    "g_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardH(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout=0.2):\n",
    "        super(FeedForwardH, self).__init__()\n",
    "        self.d = nn.Dropout(dropout)\n",
    "        self.m = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, output_size)\n",
    "        for param in self.parameters():\n",
    "            torch.nn.init.normal_(param, mean=0, std=0.01)\n",
    "    def forward(self, inputs):\n",
    "        hidden1 = self.m(self.linear1(self.d(inputs)))\n",
    "        hidden2 = self.m(self.linear2(self.d(hidden1)))\n",
    "        output = self.linear3(hidden2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForwardH(\n",
       "  (d): Dropout(p=0.2)\n",
       "  (m): ReLU()\n",
       "  (linear1): Linear(in_features=400, out_features=200, bias=True)\n",
       "  (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (linear3): Linear(in_features=200, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H1 = FeedForwardH(hidden_size2, hidden_size1, output_size).cuda()\n",
    "H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 4])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_all = H1(g_all)\n",
    "h_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = batch.label.values\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3871, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(out_all, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "7\n",
      "11\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "parameters = [param for param in EP1.parameters()] # embed, lnr, bias\n",
    "print(len(parameters))\n",
    "parameters.extend([param for param in F1.parameters()]) # lnr1, bias1, lnr2, bias2\n",
    "print(len(parameters))\n",
    "parameters.extend([param for param in G1.parameters()]) # lnr1, bias1, lnr2, bias2\n",
    "print(len(parameters))\n",
    "parameters.extend([param for param in H1.parameters()]) # lnr1, bias1, lnr2, bias2, lnr3, bias3\n",
    "print(len(parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adagrad(parameters, lr=0.05, initial_accumulator_value=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-0.1111, -0.0486, -0.0384,  0.1044], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in EP1.parameters():\n",
    "    emb1 = param\n",
    "    break\n",
    "i = 0\n",
    "for param in H1.parameters():\n",
    "    if i == 5:\n",
    "        print(param)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "EP1.train()\n",
    "F1.train()\n",
    "G1.train()\n",
    "H1.train()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "sent1 = batch.premise.values.transpose(0,1)\n",
    "sent2 = batch.hypothesis.values.transpose(0,1)\n",
    "target = batch.label.values\n",
    "\n",
    "proj1 = EP1(sent1)\n",
    "proj2 = EP1(sent2)\n",
    "f1 = F1(proj1)\n",
    "f2 = F1(proj2)\n",
    "score1 = torch.bmm(f1, f2.transpose(1,2))\n",
    "score2 = score1.transpose(1,2)\n",
    "prob1 = F.softmax(score1, dim=2)\n",
    "prob2 = F.softmax(score2, dim=2)\n",
    "proj1_soft = torch.bmm(prob2, proj1)\n",
    "proj2_soft = torch.bmm(prob1, proj2)\n",
    "proj1_combined = torch.cat((proj1, proj2_soft), dim=2)\n",
    "proj2_combined = torch.cat((proj2, proj1_soft), dim=2)\n",
    "g1 = G1(proj1_combined)\n",
    "g2 = G1(proj2_combined)\n",
    "g1_sum = g1.sum(dim=1)\n",
    "g2_sum = g2.sum(dim=1)\n",
    "g_all = torch.cat((g1_sum, g2_sum), dim=1)\n",
    "h_all = H1(g_all)\n",
    "\n",
    "loss = criterion(h_all, target)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-0.1295, -0.0567, -0.0467,  0.1243], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in EP1.parameters():\n",
    "    emb2 = param\n",
    "    break\n",
    "i = 0\n",
    "for param in H1.parameters():\n",
    "    if i == 5:\n",
    "        print(param)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, device='cuda:0')"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(emb1 != emb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = torch.sum(torch.argmax(h_all, dim=1) == target).item() / target.shape[0]\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3147, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(sent1, sent2, EP1, F1, G1, H1):\n",
    "    proj1 = EP1(sent1)\n",
    "    proj2 = EP1(sent2)\n",
    "    f1 = F1(proj1)\n",
    "    f2 = F1(proj2)\n",
    "    score1 = torch.bmm(f1, f2.transpose(1,2))\n",
    "    score2 = score1.transpose(1,2)\n",
    "    prob1 = F.softmax(score1, dim=2)\n",
    "    prob2 = F.softmax(score2, dim=2)\n",
    "    proj1_soft = torch.bmm(prob2, proj1)\n",
    "    proj2_soft = torch.bmm(prob1, proj2)\n",
    "    proj1_combined = torch.cat((proj1, proj2_soft), dim=2)\n",
    "    proj2_combined = torch.cat((proj2, proj1_soft), dim=2)\n",
    "    g1 = G1(proj1_combined)\n",
    "    g2 = G1(proj2_combined)\n",
    "    g1_sum = g1.sum(dim=1)\n",
    "    g2_sum = g2.sum(dim=1)\n",
    "    g_all = torch.cat((g1_sum, g2_sum), dim=1)\n",
    "    h_all = H1(g_all)\n",
    "    return h_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(e, train_iter, EP1, F1, G1, H1, criterion, optimizer):\n",
    "    EP1.train()\n",
    "    F1.train()\n",
    "    G1.train()\n",
    "    H1.train()\n",
    "    \n",
    "    for ix,batch in enumerate(train_iter):\n",
    "        optimizer.zero_grad()\n",
    "        sent1 = batch.premise.values.transpose(0,1)\n",
    "        sent2 = batch.hypothesis.values.transpose(0,1)\n",
    "        target = batch.label.values\n",
    "        output = get_output(sent1, sent2, EP1, F1, G1, H1)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if ix % 1000 == 0:\n",
    "            acc = torch.sum(torch.argmax(output, dim=1) == target).item() / target.shape[0]\n",
    "            print('Epoch: {0}, Batch: {1}, Train NLL: {2:0.4f}, Train Acc:{3:0.4f}'.format(e, ix, loss.cpu().detach(), acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(e, val_iter, EP1, F1, G1, H1, criterion):\n",
    "    EP1.eval()\n",
    "    F1.eval()\n",
    "    G1.eval()\n",
    "    H1.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_sent = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    for ix,batch in enumerate(val_iter):\n",
    "        sent1 = batch.premise.values.transpose(0,1)\n",
    "        sent2 = batch.hypothesis.values.transpose(0,1)\n",
    "        target = batch.label.values\n",
    "        output = get_output(sent1, sent2, EP1, F1, G1, H1)\n",
    "        \n",
    "        loss = criterion(output, target).item()\n",
    "        sent = sent1.shape[0]\n",
    "        correct = torch.sum(torch.argmax(output, dim=1) == target).item()\n",
    "        \n",
    "        total_loss += loss*sent\n",
    "        total_sent += sent\n",
    "        total_correct += correct\n",
    "    \n",
    "    print('Epoch: {0}, Val NLL: {1:0.4f}, Val Acc: {2:0.4f}'.format(e, total_loss/total_sent, total_correct/total_sent))\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Train NLL: 1.3805, Train Acc:0.4375\n",
      "Epoch: 0, Batch: 1000, Train NLL: 1.0982, Train Acc:0.4375\n",
      "Epoch: 0, Batch: 2000, Train NLL: 1.1028, Train Acc:0.3125\n",
      "Epoch: 0, Batch: 3000, Train NLL: 1.0868, Train Acc:0.3750\n",
      "Epoch: 0, Batch: 4000, Train NLL: 1.1005, Train Acc:0.2500\n",
      "Epoch: 0, Batch: 5000, Train NLL: 1.1006, Train Acc:0.3125\n",
      "Epoch: 0, Batch: 6000, Train NLL: 1.1002, Train Acc:0.2500\n",
      "Epoch: 0, Batch: 7000, Train NLL: 1.1000, Train Acc:0.3125\n",
      "Epoch: 0, Batch: 8000, Train NLL: 1.1074, Train Acc:0.2500\n",
      "Epoch: 0, Batch: 9000, Train NLL: 1.0958, Train Acc:0.5000\n",
      "Epoch: 0, Batch: 10000, Train NLL: 1.0928, Train Acc:0.3125\n",
      "Epoch: 0, Batch: 11000, Train NLL: 1.0979, Train Acc:0.3125\n",
      "Epoch: 0, Batch: 12000, Train NLL: 1.1029, Train Acc:0.1250\n",
      "Epoch: 0, Batch: 13000, Train NLL: 1.1009, Train Acc:0.1875\n",
      "Epoch: 0, Batch: 14000, Train NLL: 1.0981, Train Acc:0.3125\n",
      "Epoch: 0, Batch: 15000, Train NLL: 1.1042, Train Acc:0.3125\n",
      "Epoch: 0, Batch: 16000, Train NLL: 1.0985, Train Acc:0.3125\n",
      "Epoch: 0, Batch: 17000, Train NLL: 1.0978, Train Acc:0.2500\n",
      "Epoch: 0, Batch: 18000, Train NLL: 1.0924, Train Acc:0.5000\n",
      "Epoch: 0, Batch: 19000, Train NLL: 1.1095, Train Acc:0.0625\n",
      "Epoch: 0, Batch: 20000, Train NLL: 1.0992, Train Acc:0.3125\n",
      "Epoch: 0, Batch: 21000, Train NLL: 1.0693, Train Acc:0.5625\n",
      "Epoch: 0, Batch: 22000, Train NLL: 1.1028, Train Acc:0.2500\n",
      "Epoch: 0, Batch: 23000, Train NLL: 1.0924, Train Acc:0.5000\n",
      "Epoch: 0, Batch: 24000, Train NLL: 1.0885, Train Acc:0.5000\n",
      "Epoch: 0, Batch: 25000, Train NLL: 1.0489, Train Acc:0.5625\n",
      "Epoch: 0, Batch: 26000, Train NLL: 1.1078, Train Acc:0.3750\n",
      "Epoch: 0, Batch: 27000, Train NLL: 0.9888, Train Acc:0.4375\n",
      "Epoch: 0, Batch: 28000, Train NLL: 1.1438, Train Acc:0.3125\n",
      "Epoch: 0, Batch: 29000, Train NLL: 1.0039, Train Acc:0.5000\n",
      "Epoch: 0, Batch: 30000, Train NLL: 1.1387, Train Acc:0.3750\n",
      "Epoch: 0, Batch: 31000, Train NLL: 1.0224, Train Acc:0.5625\n",
      "Epoch: 0, Batch: 32000, Train NLL: 0.9501, Train Acc:0.5625\n",
      "Epoch: 0, Batch: 33000, Train NLL: 0.9538, Train Acc:0.4375\n",
      "Epoch: 0, Batch: 34000, Train NLL: 1.0985, Train Acc:0.3750\n",
      "Epoch: 0, Val NLL: 0.9946, Val Acc: 0.4881\n",
      "WROTE MODEL\n",
      "Epoch: 1, Batch: 0, Train NLL: 0.9246, Train Acc:0.5625\n",
      "Epoch: 1, Batch: 1000, Train NLL: 1.0182, Train Acc:0.5000\n",
      "Epoch: 1, Batch: 2000, Train NLL: 0.8327, Train Acc:0.6875\n",
      "Epoch: 1, Batch: 3000, Train NLL: 1.0645, Train Acc:0.3125\n",
      "Epoch: 1, Batch: 4000, Train NLL: 0.8678, Train Acc:0.5625\n",
      "Epoch: 1, Batch: 5000, Train NLL: 1.0270, Train Acc:0.5000\n",
      "Epoch: 1, Batch: 6000, Train NLL: 1.0046, Train Acc:0.5000\n",
      "Epoch: 1, Batch: 7000, Train NLL: 1.0677, Train Acc:0.3125\n",
      "Epoch: 1, Batch: 8000, Train NLL: 1.0504, Train Acc:0.5625\n",
      "Epoch: 1, Batch: 9000, Train NLL: 1.0470, Train Acc:0.4375\n",
      "Epoch: 1, Batch: 10000, Train NLL: 1.1388, Train Acc:0.5000\n",
      "Epoch: 1, Batch: 11000, Train NLL: 1.0357, Train Acc:0.5000\n",
      "Epoch: 1, Batch: 12000, Train NLL: 0.8760, Train Acc:0.6875\n",
      "Epoch: 1, Batch: 13000, Train NLL: 0.9419, Train Acc:0.6250\n",
      "Epoch: 1, Batch: 14000, Train NLL: 0.9702, Train Acc:0.5000\n",
      "Epoch: 1, Batch: 15000, Train NLL: 0.8832, Train Acc:0.6250\n",
      "Epoch: 1, Batch: 16000, Train NLL: 1.1545, Train Acc:0.4375\n",
      "Epoch: 1, Batch: 17000, Train NLL: 0.8175, Train Acc:0.8125\n",
      "Epoch: 1, Batch: 18000, Train NLL: 0.7994, Train Acc:0.6875\n",
      "Epoch: 1, Batch: 19000, Train NLL: 1.0335, Train Acc:0.6250\n",
      "Epoch: 1, Batch: 20000, Train NLL: 0.8608, Train Acc:0.5625\n",
      "Epoch: 1, Batch: 21000, Train NLL: 0.9727, Train Acc:0.5000\n",
      "Epoch: 1, Batch: 22000, Train NLL: 0.9176, Train Acc:0.5625\n",
      "Epoch: 1, Batch: 23000, Train NLL: 0.9846, Train Acc:0.3750\n",
      "Epoch: 1, Batch: 24000, Train NLL: 0.8315, Train Acc:0.5625\n",
      "Epoch: 1, Batch: 25000, Train NLL: 0.9772, Train Acc:0.5000\n",
      "Epoch: 1, Batch: 26000, Train NLL: 0.6592, Train Acc:0.6875\n",
      "Epoch: 1, Batch: 27000, Train NLL: 0.6779, Train Acc:0.6875\n",
      "Epoch: 1, Batch: 28000, Train NLL: 0.9370, Train Acc:0.5625\n",
      "Epoch: 1, Batch: 29000, Train NLL: 0.6832, Train Acc:0.7500\n",
      "Epoch: 1, Batch: 30000, Train NLL: 0.8243, Train Acc:0.6250\n",
      "Epoch: 1, Batch: 31000, Train NLL: 0.9283, Train Acc:0.5000\n",
      "Epoch: 1, Batch: 32000, Train NLL: 0.5496, Train Acc:0.8125\n",
      "Epoch: 1, Batch: 33000, Train NLL: 0.8809, Train Acc:0.5625\n",
      "Epoch: 1, Batch: 34000, Train NLL: 0.7693, Train Acc:0.6250\n",
      "Epoch: 1, Val NLL: 0.8816, Val Acc: 0.5888\n",
      "WROTE MODEL\n",
      "Epoch: 2, Batch: 0, Train NLL: 0.8005, Train Acc:0.6875\n",
      "Epoch: 2, Batch: 1000, Train NLL: 0.8056, Train Acc:0.5625\n",
      "Epoch: 2, Batch: 2000, Train NLL: 0.8806, Train Acc:0.6875\n",
      "Epoch: 2, Batch: 3000, Train NLL: 0.7484, Train Acc:0.6875\n",
      "Epoch: 2, Batch: 4000, Train NLL: 0.9074, Train Acc:0.4375\n",
      "Epoch: 2, Batch: 5000, Train NLL: 0.4439, Train Acc:0.8750\n",
      "Epoch: 2, Batch: 6000, Train NLL: 1.2251, Train Acc:0.1875\n",
      "Epoch: 2, Batch: 7000, Train NLL: 0.9242, Train Acc:0.5625\n",
      "Epoch: 2, Batch: 8000, Train NLL: 0.9235, Train Acc:0.5625\n",
      "Epoch: 2, Batch: 9000, Train NLL: 0.7729, Train Acc:0.6250\n",
      "Epoch: 2, Batch: 10000, Train NLL: 0.8267, Train Acc:0.5000\n",
      "Epoch: 2, Batch: 11000, Train NLL: 0.9946, Train Acc:0.3750\n",
      "Epoch: 2, Batch: 12000, Train NLL: 1.0737, Train Acc:0.5625\n",
      "Epoch: 2, Batch: 13000, Train NLL: 0.7990, Train Acc:0.5625\n",
      "Epoch: 2, Batch: 14000, Train NLL: 0.8106, Train Acc:0.5625\n",
      "Epoch: 2, Batch: 15000, Train NLL: 0.8891, Train Acc:0.5625\n",
      "Epoch: 2, Batch: 16000, Train NLL: 0.6056, Train Acc:0.7500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3cc9b9078344>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEP1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEP1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-229284c23917>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(e, train_iter, EP1, F1, G1, H1, criterion, optimizer)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEP1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_loss = 1e8\n",
    "EP1 = EmbedProject(weights, embed_size, hidden_size1).cuda()\n",
    "F1 = FeedForwardF(hidden_size1, hidden_size1, hidden_size1).cuda()\n",
    "G1 = FeedForwardG(hidden_size2, hidden_size1, hidden_size1).cuda()\n",
    "H1 = FeedForwardH(hidden_size2, hidden_size1, output_size).cuda()\n",
    "\n",
    "parameters = [param for param in EP1.parameters()] # embed, lnr, bias\n",
    "parameters.extend([param for param in F1.parameters()]) # lnr1, bias1, lnr2, bias2\n",
    "parameters.extend([param for param in G1.parameters()]) # lnr1, bias1, lnr2, bias2\n",
    "parameters.extend([param for param in H1.parameters()]) # lnr1, bias1, lnr2, bias2, lnr3, bias3\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adagrad(parameters, lr=0.05, initial_accumulator_value=0.1)\n",
    "\n",
    "for e in range(100):\n",
    "    training_loop(e, train_iter, EP1, F1, G1, H1, criterion, optimizer)\n",
    "    loss = validation_loop(e, val_iter, EP1, F1, G1, H1, criterion)\n",
    "    if loss < best_loss:\n",
    "        #torch.save(EP1.state_dict(),'best_EP1.pt')\n",
    "        #torch.save(F1.state_dict(),'best_F1.pt')\n",
    "        #torch.save(G1.state_dict(),'best_G1.pt')\n",
    "        #torch.save(H1.state_dict(),'best_H1.pt')\n",
    "        best_loss = loss\n",
    "        print('WROTE MODEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EP1 = EmbedProject(weights, embed_size, hidden_size1).cuda()\n",
    "#state_dict = torch.load('best_EP1.pt')\n",
    "#EP1.load_state_dict(state_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
