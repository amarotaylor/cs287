{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 287 - HW 4 - Cont."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "from namedtensor import ntorch, NamedTensor\n",
    "from namedtensor.text import NamedField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train) 549367\n",
      "len(TEXT.vocab) 62998\n",
      "len(LABEL.vocab) 4\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "TEXT = NamedField(names=('seqlen',)) # Our input $x$\n",
    "LABEL = NamedField(sequential=False, names=()) # Our labels $y$\n",
    "train, val, test = torchtext.datasets.SNLI.splits(TEXT, LABEL)\n",
    "print('len(train)', len(train))\n",
    "TEXT.build_vocab(train)\n",
    "LABEL.build_vocab(train)\n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
    "print('len(LABEL.vocab)', len(LABEL.vocab))\n",
    "train_iter, val_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
    "    (train, val, test), batch_size=16, device=torch.device(\"cuda\"), repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word embeddings shape: OrderedDict([('word', 62998), ('embedding', 300)])\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary with word embeddings\n",
    "# out-of-vocabulary words are hashed to one of 100 random embeddings each initialized to mean 0, stdev 1 (Sec 5.1)\n",
    "unk_vectors = [torch.randn(300) for _ in range(100)]\n",
    "TEXT.vocab.load_vectors(vectors='glove.6B.300d', unk_init=lambda x:random.choice(unk_vectors))\n",
    "vectors = TEXT.vocab.vectors\n",
    "vectors = vectors / vectors.norm(dim=1, keepdim=True) # normalized to have l_2 norm of 1\n",
    "vectors = NamedTensor(vectors, ('word', 'embedding'))\n",
    "TEXT.vocab.vectors = vectors\n",
    "print(\"word embeddings shape:\", TEXT.vocab.vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposable Intra-Sentence Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedProject(torch.nn.Module):\n",
    "    def __init__(self, weights, embed_size, project_size):\n",
    "        super(EmbedProject, self).__init__()\n",
    "        self.embed = nn.Embedding.from_pretrained(weights, freeze=True) # weights: input_size x embed_size\n",
    "        self.linear = nn.Linear(embed_size, project_size)\n",
    "        torch.nn.init.normal_(self.linear.weight, mean=0, std=0.01)\n",
    "    def forward(self, inputs):\n",
    "        embedding = self.embed(inputs)\n",
    "        output = self.linear(embedding)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardFIntra(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout=0.2):\n",
    "        super(FeedForwardFIntra, self).__init__()\n",
    "        self.d = nn.Dropout(dropout)\n",
    "        self.m = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "        for param in self.parameters():\n",
    "            torch.nn.init.normal_(param, mean=0, std=0.01)\n",
    "    def forward(self, inputs):\n",
    "        hidden = self.m(self.linear1(self.d(inputs)))\n",
    "        output = self.m(self.linear2(self.d(hidden)))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedDist(torch.nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim): # num = 11, dim = 1\n",
    "        super(EmbedDist, self).__init__()\n",
    "        self.embed = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        torch.nn.init.normal_(self.embed.weight, mean=0, std=0.01)\n",
    "    def forward(self, inputs):\n",
    "        output = self.embed(inputs)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardF(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout=0.2):\n",
    "        super(FeedForwardF, self).__init__()\n",
    "        self.d = nn.Dropout(dropout)\n",
    "        self.m = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "        for param in self.parameters():\n",
    "            torch.nn.init.normal_(param, mean=0, std=0.01)\n",
    "    def forward(self, inputs):\n",
    "        hidden = self.m(self.linear1(self.d(inputs)))\n",
    "        output = self.m(self.linear2(self.d(hidden)))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardG(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout=0.2):\n",
    "        super(FeedForwardG, self).__init__()\n",
    "        self.d = nn.Dropout(dropout)\n",
    "        self.m = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "        for param in self.parameters():\n",
    "            torch.nn.init.normal_(param, mean=0, std=0.01)\n",
    "    def forward(self, inputs):\n",
    "        hidden = self.m(self.linear1(self.d(inputs)))\n",
    "        output = self.m(self.linear2(self.d(hidden)))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardH(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout=0.2):\n",
    "        super(FeedForwardH, self).__init__()\n",
    "        self.d = nn.Dropout(dropout)\n",
    "        self.m = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, output_size)\n",
    "        for param in self.parameters():\n",
    "            torch.nn.init.normal_(param, mean=0, std=0.01)\n",
    "    def forward(self, inputs):\n",
    "        hidden1 = self.m(self.linear1(self.d(inputs)))\n",
    "        hidden2 = self.m(self.linear2(self.d(hidden1)))\n",
    "        output = self.linear3(hidden2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSIONS -- input: 62998, embed: 300, hidden1: 200, hidden2: 400, output: 4\n"
     ]
    }
   ],
   "source": [
    "# dimensions\n",
    "input_size = TEXT.vocab.vectors.shape['word']\n",
    "embed_size = TEXT.vocab.vectors.shape['embedding']\n",
    "hidden_size1 = 200\n",
    "hidden_size2 = hidden_size1 * 2\n",
    "output_size = len(LABEL.vocab)\n",
    "print('DIMENSIONS -- input: %d, embed: %d, hidden1: %d, hidden2: %d, output: %d'%(input_size, embed_size, hidden_size1, hidden_size2, output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([62998, 300])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-trained embeddings\n",
    "weights = TEXT.vocab.vectors.values.cuda()\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>: 1 , null: tensor(56690, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "pad_tkn = TEXT.vocab.stoi['<pad>']\n",
    "null_tkn = torch.tensor(TEXT.vocab.stoi['null'], device='cuda')\n",
    "print('<pad>:', pad_tkn, ', null:', null_tkn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[LABEL.vocab.itos[i] for i in [0,1,2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[TEXT.vocab.itos[i] for i in [0,1,2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking to make sure no <unk> labels\n",
    "total = 0\n",
    "for batch in iter(train_iter):\n",
    "    total += torch.sum(batch.label.values == 0)\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no prepend with NULL!\n",
    "for batch in iter(train_iter):\n",
    "    print([TEXT.vocab.itos[i] for i in batch.premise.values[:,0]])\n",
    "    print([TEXT.vocab.itos[i] for i in batch.hypothesis.values[:,0]])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.vocab.stoi['null']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbedProject(\n",
       "  (embed): Embedding(62998, 300)\n",
       "  (linear): Linear(in_features=300, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EP1 = EmbedProject(weights, embed_size, hidden_size1).cuda()\n",
    "EP1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForwardFIntra(\n",
       "  (d): Dropout(p=0.2)\n",
       "  (m): ReLU()\n",
       "  (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FI1 = FeedForwardFIntra(hidden_size1, hidden_size1, hidden_size1).cuda()\n",
    "FI1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbedDist(\n",
       "  (embed): Embedding(11, 1)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ED1 = EmbedDist(dist + 1, 1).cuda()\n",
    "ED1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForwardF(\n",
       "  (d): Dropout(p=0.2)\n",
       "  (m): ReLU()\n",
       "  (linear1): Linear(in_features=400, out_features=200, bias=True)\n",
       "  (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1 = FeedForwardF(hidden_size2, hidden_size1, hidden_size1).cuda()\n",
    "F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForwardG(\n",
       "  (d): Dropout(p=0.2)\n",
       "  (m): ReLU()\n",
       "  (linear1): Linear(in_features=800, out_features=200, bias=True)\n",
       "  (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G1 = FeedForwardG(hidden_size2 * 2, hidden_size1, hidden_size1).cuda()\n",
    "G1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForwardH(\n",
       "  (d): Dropout(p=0.2)\n",
       "  (m): ReLU()\n",
       "  (linear1): Linear(in_features=400, out_features=200, bias=True)\n",
       "  (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (linear3): Linear(in_features=200, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H1 = FeedForwardH(hidden_size2, hidden_size1, output_size).cuda()\n",
    "H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''proj1 = EP1(sent1)\n",
    "proj2 = EP1(sent2)\n",
    "proj1.shape, proj2.shape\n",
    "\n",
    "fi1 = FI1(proj1)\n",
    "fi2 = FI1(proj2)\n",
    "fi1.shape, fi2.shape\n",
    "\n",
    "# intra-sentence attention!\n",
    "score1 = torch.bmm(fi1, fi1.transpose(1,2))\n",
    "score2 = torch.bmm(fi2, fi2.transpose(1,2))\n",
    "score1.shape, score2.shape\n",
    "\n",
    "prob1 = F.softmax(score1, dim=2)\n",
    "prob2 = F.softmax(score2, dim=2)\n",
    "prob1.shape, prob2.shape\n",
    "\n",
    "# intra-sentence attention!\n",
    "proj1_soft = torch.bmm(prob1, proj1)\n",
    "proj2_soft = torch.bmm(prob2, proj2)\n",
    "proj1_soft.shape, proj2_soft.shape\n",
    "\n",
    "# intra-sentence attention!\n",
    "proj1_intra = torch.cat((proj1, proj1_soft), dim=2)\n",
    "proj2_intra = torch.cat((proj2, proj2_soft), dim=2)\n",
    "proj1_intra.shape, proj2_intra.shape\n",
    "\n",
    "dist = 10\n",
    "seqlen = score1.shape[2]\n",
    "steps = torch.arange(0, seqlen)\n",
    "mat_steps = steps.repeat(seqlen, 1)\n",
    "flip_steps = torch.flip(steps, [0]).view(-1, 1)\n",
    "idx = torch.min(torch.abs(mat_steps - flip_steps), torch.tensor(dist))\n",
    "ED1(idx).squeeze().shape'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 19]), torch.Size([16, 14]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_sent1 = batch.premise.values.transpose(0,1)\n",
    "raw_sent2 = batch.hypothesis.values.transpose(0,1)\n",
    "raw_sent1.shape, raw_sent2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 20]), torch.Size([16, 15]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_tkns = null_tkn.repeat(raw_sent1.shape[0], 1)\n",
    "sent1 = torch.cat((null_tkns, raw_sent1), 1)\n",
    "sent2 = torch.cat((null_tkns, raw_sent2), 1)\n",
    "sent1.shape, sent2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 20, 200]), torch.Size([16, 15, 200]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj1 = EP1(sent1)\n",
    "proj2 = EP1(sent2)\n",
    "proj1.shape, proj2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 20]), torch.Size([16, 15]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask1 = (sent1 == pad_tkn)\n",
    "mask2 = (sent2 == pad_tkn)\n",
    "mask1.shape, mask2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 20, 200]), torch.Size([16, 15, 200]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi1 = FI1(proj1)\n",
    "fi2 = FI1(proj2)\n",
    "fi1.shape, fi2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 20, 20]), torch.Size([16, 15, 15]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score1 = torch.bmm(fi1, fi1.transpose(1,2))\n",
    "score2 = torch.bmm(fi2, fi2.transpose(1,2))\n",
    "score1.shape, score2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 20, 20]), torch.Size([16, 15, 15]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score1 += get_dist_bias(score1.shape[2], dist, ED1)\n",
    "score2 += get_dist_bias(score2.shape[2], dist, ED1)\n",
    "score1.shape, score2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 20, 20]), torch.Size([16, 15, 15]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask1c = mask1.unsqueeze(1).expand(-1, sent1.shape[1], -1).float()\n",
    "mask2c = mask2.unsqueeze(1).expand(-1, sent2.shape[1], -1).float()\n",
    "mask1c.shape, mask2c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 20, 20]), torch.Size([16, 15, 15]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score1 = score1 * (1 - mask1c) + (mask1c * -1e8)\n",
    "score2 = score2 * (1 - mask2c) + (mask2c * -1e8)\n",
    "score1.shape, score2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 20, 20]), torch.Size([16, 15, 15]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob1 = F.softmax(score1, dim=2)\n",
    "prob2 = F.softmax(score2, dim=2)\n",
    "prob1.shape, prob2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 20, 200]), torch.Size([16, 15, 200]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj1_soft = torch.bmm(prob1, proj1)\n",
    "proj2_soft = torch.bmm(prob2, proj2)\n",
    "proj1_soft.shape, proj2_soft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 20, 400]), torch.Size([16, 15, 400]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj1 = torch.cat((proj1, proj1_soft), dim=2)\n",
    "proj2 = torch.cat((proj2, proj2_soft), dim=2) \n",
    "proj1.shape, proj2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 20, 200]), torch.Size([16, 15, 200]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = F1(proj1)\n",
    "f2 = F1(proj2)\n",
    "f1.shape, f2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 20, 15]), torch.Size([16, 15, 20]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score1 = torch.bmm(f1, f2.transpose(1,2))\n",
    "score2 = torch.bmm(f2, f1.transpose(1,2))\n",
    "score1.shape, score2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 15, 20]), torch.Size([16, 20, 15]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask1a = mask1.unsqueeze(1).expand(-1, sent2.shape[1], -1).float()\n",
    "mask2a = mask2.unsqueeze(1).expand(-1, sent1.shape[1], -1).float()\n",
    "mask1a.shape, mask2a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 20, 15]), torch.Size([16, 15, 20]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score1 = score1 * (1 - mask2a) + (mask2a * -1e8)\n",
    "score2 = score2 * (1 - mask1a) + (mask1a * -1e8)\n",
    "score1.shape, score2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 20, 15]), torch.Size([16, 15, 20]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob1 = F.softmax(score1, dim=2)\n",
    "prob2 = F.softmax(score2, dim=2)\n",
    "prob1.shape, prob2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 15, 400]), torch.Size([16, 20, 400]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj1_soft = torch.bmm(prob2, proj1)\n",
    "proj2_soft = torch.bmm(prob1, proj2)\n",
    "proj1_soft.shape, proj2_soft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 20, 800]), torch.Size([16, 15, 800]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj1_combined = torch.cat((proj1, proj2_soft), dim=2)\n",
    "proj2_combined = torch.cat((proj2, proj1_soft), dim=2)\n",
    "proj1_combined.shape, proj2_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 20, 200]), torch.Size([16, 15, 200]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1 = G1(proj1_combined)\n",
    "g2 = G1(proj2_combined)\n",
    "g1.shape, g2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 20, 200]), torch.Size([16, 15, 200]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask1b = mask1.unsqueeze(2).expand(-1, -1, hidden_size1).float()\n",
    "mask2b = mask2.unsqueeze(2).expand(-1, -1, hidden_size1).float()\n",
    "mask1b.shape, mask2b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 20, 200]), torch.Size([16, 15, 200]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1 = g1 * (1 - mask1b)\n",
    "g2 = g2 * (1 - mask2b)\n",
    "g1.shape, g2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 200]), torch.Size([16, 200]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1_sum = g1.sum(dim=1)\n",
    "g2_sum = g2.sum(dim=1)\n",
    "g1_sum.shape, g2_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 400])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_all = torch.cat((g1_sum, g2_sum), dim=1)\n",
    "g_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 4])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_all = H1(g_all)\n",
    "h_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist_bias(seqlen, dist, ED1):\n",
    "    steps = torch.arange(0, seqlen)\n",
    "    mat_steps = steps.repeat(seqlen, 1)\n",
    "    flip_steps = torch.flip(steps, [0]).view(-1, 1)\n",
    "    idx = torch.min(torch.abs(mat_steps - flip_steps), torch.tensor(dist)).cuda()\n",
    "    return ED1(idx).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(sent1, sent2, EP1, F1, G1, H1, intra, dist, FI1, ED1):\n",
    "    proj1 = EP1(sent1)\n",
    "    proj2 = EP1(sent2)\n",
    "    \n",
    "    mask1 = (sent1 == pad_tkn)\n",
    "    mask2 = (sent2 == pad_tkn)\n",
    "        \n",
    "    if intra:\n",
    "        fi1 = FI1(proj1)\n",
    "        fi2 = FI1(proj2)\n",
    "        score1 = torch.bmm(fi1, fi1.transpose(1,2))\n",
    "        score2 = torch.bmm(fi2, fi2.transpose(1,2))\n",
    "        score1 += get_dist_bias(score1.shape[2], dist, ED1)\n",
    "        score2 += get_dist_bias(score2.shape[2], dist, ED1)\n",
    "        mask1c = mask1.unsqueeze(1).expand(-1, sent1.shape[1], -1).float()\n",
    "        mask2c = mask2.unsqueeze(1).expand(-1, sent2.shape[1], -1).float()\n",
    "        score1 = score1 * (1 - mask1c) + (mask1c * -1e8)\n",
    "        score2 = score2 * (1 - mask2c) + (mask2c * -1e8)\n",
    "        prob1 = F.softmax(score1, dim=2)\n",
    "        prob2 = F.softmax(score2, dim=2)\n",
    "        proj1_soft = torch.bmm(prob1, proj1)\n",
    "        proj2_soft = torch.bmm(prob2, proj2)\n",
    "        proj1 = torch.cat((proj1, proj1_soft), dim=2)\n",
    "        proj2 = torch.cat((proj2, proj2_soft), dim=2) \n",
    "        \n",
    "    f1 = F1(proj1)\n",
    "    f2 = F1(proj2)\n",
    "    \n",
    "    score1 = torch.bmm(f1, f2.transpose(1,2))\n",
    "    score2 = torch.bmm(f2, f1.transpose(1,2))\n",
    "    mask1a = mask1.unsqueeze(1).expand(-1, sent2.shape[1], -1).float()\n",
    "    mask2a = mask2.unsqueeze(1).expand(-1, sent1.shape[1], -1).float()\n",
    "    score1 = score1 * (1 - mask2a) + (mask2a * -1e8)\n",
    "    score2 = score2 * (1 - mask1a) + (mask1a * -1e8)\n",
    "    \n",
    "    prob1 = F.softmax(score1, dim=2)\n",
    "    prob2 = F.softmax(score2, dim=2)\n",
    "    proj1_soft = torch.bmm(prob2, proj1)\n",
    "    proj2_soft = torch.bmm(prob1, proj2)\n",
    "    proj1_combined = torch.cat((proj1, proj2_soft), dim=2)\n",
    "    proj2_combined = torch.cat((proj2, proj1_soft), dim=2)\n",
    "    \n",
    "    g1 = G1(proj1_combined)\n",
    "    g2 = G1(proj2_combined)\n",
    "    mask1b = mask1.unsqueeze(2).expand(-1, -1, hidden_size1).float()\n",
    "    mask2b = mask2.unsqueeze(2).expand(-1, -1, hidden_size1).float()\n",
    "    g1 = g1 * (1 - mask1b)\n",
    "    g2 = g2 * (1 - mask2b)\n",
    "    \n",
    "    g1_sum = g1.sum(dim=1)\n",
    "    g2_sum = g2.sum(dim=1)\n",
    "    g_all = torch.cat((g1_sum, g2_sum), dim=1)\n",
    "    h_all = H1(g_all)\n",
    "    return h_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepend_null(sent):\n",
    "    null_tkns = null_tkn.repeat(sent.shape[0], 1)\n",
    "    return torch.cat((null_tkns, sent), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(e, train_iter, EP1, F1, G1, H1, criterion, optimizer, intra=False, dist=None, FI1=None, ED1=None):\n",
    "    EP1.train()\n",
    "    F1.train()\n",
    "    G1.train()\n",
    "    H1.train()\n",
    "    if intra:\n",
    "        FI1.train()\n",
    "        ED1.train()\n",
    "    \n",
    "    for ix,batch in enumerate(train_iter):\n",
    "        optimizer.zero_grad()\n",
    "        sent1 = prepend_null(batch.premise.values.transpose(0,1))\n",
    "        sent2 = prepend_null(batch.hypothesis.values.transpose(0,1))\n",
    "        target = batch.label.values\n",
    "        output = get_output(sent1, sent2, EP1, F1, G1, H1, intra, dist, FI1, ED1)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if ix % 1000 == 0:\n",
    "            acc = torch.sum(torch.argmax(output, dim=1) == target).item() / target.shape[0]\n",
    "            print('Epoch: {0}, Batch: {1}, Train NLL: {2:0.4f}, Train Acc:{3:0.4f}'.format(e, ix, loss.cpu().detach(), acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(e, val_iter, EP1, F1, G1, H1, criterion, intra=False, dist=None, FI1=None, ED1=None):\n",
    "    EP1.eval()\n",
    "    F1.eval()\n",
    "    G1.eval()\n",
    "    H1.eval()\n",
    "    if intra:\n",
    "        FI1.eval()\n",
    "        ED1.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_sent = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    for ix,batch in enumerate(val_iter):\n",
    "        sent1 = prepend_null(batch.premise.values.transpose(0,1))\n",
    "        sent2 = prepend_null(batch.hypothesis.values.transpose(0,1))\n",
    "        target = batch.label.values\n",
    "        output = get_output(sent1, sent2, EP1, F1, G1, H1, intra, dist, FI1, ED1)\n",
    "        \n",
    "        loss = criterion(output, target).item()\n",
    "        sent = sent1.shape[0]\n",
    "        correct = torch.sum(torch.argmax(output, dim=1) == target).item()\n",
    "        \n",
    "        total_loss += loss*sent\n",
    "        total_sent += sent\n",
    "        total_correct += correct\n",
    "    \n",
    "    print('Epoch: {0}, Val NLL: {1:0.4f}, Val Acc: {2:0.4f}'.format(e, total_loss/total_sent, total_correct/total_sent))\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Train NLL: 1.3828, Train Acc:0.3125\n",
      "Epoch: 0, Batch: 1000, Train NLL: 1.0399, Train Acc:0.5625\n",
      "Epoch: 0, Batch: 2000, Train NLL: 1.0726, Train Acc:0.3750\n",
      "Epoch: 0, Batch: 3000, Train NLL: 1.0834, Train Acc:0.5625\n",
      "Epoch: 0, Batch: 4000, Train NLL: 1.0472, Train Acc:0.4375\n",
      "Epoch: 0, Batch: 5000, Train NLL: 1.1029, Train Acc:0.3750\n",
      "Epoch: 0, Batch: 6000, Train NLL: 1.0303, Train Acc:0.6250\n",
      "Epoch: 0, Batch: 7000, Train NLL: 0.9779, Train Acc:0.5625\n",
      "Epoch: 0, Batch: 8000, Train NLL: 1.1839, Train Acc:0.3125\n",
      "Epoch: 0, Batch: 9000, Train NLL: 1.0236, Train Acc:0.4375\n",
      "Epoch: 0, Batch: 10000, Train NLL: 0.9615, Train Acc:0.5000\n",
      "Epoch: 0, Batch: 11000, Train NLL: 1.0353, Train Acc:0.4375\n",
      "Epoch: 0, Batch: 12000, Train NLL: 1.1553, Train Acc:0.3125\n",
      "Epoch: 0, Batch: 13000, Train NLL: 0.9669, Train Acc:0.4375\n",
      "Epoch: 0, Batch: 14000, Train NLL: 1.0402, Train Acc:0.3125\n",
      "Epoch: 0, Batch: 15000, Train NLL: 1.1798, Train Acc:0.2500\n",
      "Epoch: 0, Batch: 16000, Train NLL: 1.0653, Train Acc:0.4375\n",
      "Epoch: 0, Batch: 17000, Train NLL: 0.8795, Train Acc:0.7500\n",
      "Epoch: 0, Batch: 18000, Train NLL: 1.0059, Train Acc:0.3750\n",
      "Epoch: 0, Batch: 19000, Train NLL: 1.0605, Train Acc:0.3125\n",
      "Epoch: 0, Batch: 20000, Train NLL: 0.8547, Train Acc:0.6875\n",
      "Epoch: 0, Batch: 21000, Train NLL: 1.1516, Train Acc:0.3750\n",
      "Epoch: 0, Batch: 22000, Train NLL: 1.0180, Train Acc:0.4375\n",
      "Epoch: 0, Batch: 23000, Train NLL: 1.0036, Train Acc:0.3750\n",
      "Epoch: 0, Batch: 24000, Train NLL: 0.8671, Train Acc:0.5625\n",
      "Epoch: 0, Batch: 25000, Train NLL: 0.9392, Train Acc:0.5625\n",
      "Epoch: 0, Batch: 26000, Train NLL: 0.9127, Train Acc:0.6250\n",
      "Epoch: 0, Batch: 27000, Train NLL: 0.8857, Train Acc:0.6250\n",
      "Epoch: 0, Batch: 28000, Train NLL: 0.9148, Train Acc:0.4375\n",
      "Epoch: 0, Batch: 29000, Train NLL: 1.2713, Train Acc:0.4375\n",
      "Epoch: 0, Batch: 30000, Train NLL: 0.9093, Train Acc:0.5000\n",
      "Epoch: 0, Batch: 31000, Train NLL: 1.1724, Train Acc:0.3750\n",
      "Epoch: 0, Batch: 32000, Train NLL: 1.0979, Train Acc:0.4375\n",
      "Epoch: 0, Batch: 33000, Train NLL: 0.8770, Train Acc:0.6250\n",
      "Epoch: 0, Batch: 34000, Train NLL: 0.8660, Train Acc:0.5000\n",
      "Epoch: 0, Val NLL: 0.9186, Val Acc: 0.5663\n",
      "WROTE MODEL\n",
      "Epoch: 1, Batch: 0, Train NLL: 1.1797, Train Acc:0.3750\n",
      "Epoch: 1, Batch: 1000, Train NLL: 0.9501, Train Acc:0.5000\n",
      "Epoch: 1, Batch: 2000, Train NLL: 1.0060, Train Acc:0.5000\n",
      "Epoch: 1, Batch: 3000, Train NLL: 0.8174, Train Acc:0.6250\n",
      "Epoch: 1, Batch: 4000, Train NLL: 0.8933, Train Acc:0.5625\n",
      "Epoch: 1, Batch: 5000, Train NLL: 1.0415, Train Acc:0.4375\n",
      "Epoch: 1, Batch: 6000, Train NLL: 0.8720, Train Acc:0.6250\n",
      "Epoch: 1, Batch: 7000, Train NLL: 1.0189, Train Acc:0.5000\n",
      "Epoch: 1, Batch: 8000, Train NLL: 0.9222, Train Acc:0.6250\n",
      "Epoch: 1, Batch: 9000, Train NLL: 1.0093, Train Acc:0.4375\n",
      "Epoch: 1, Batch: 10000, Train NLL: 0.8556, Train Acc:0.6875\n",
      "Epoch: 1, Batch: 11000, Train NLL: 1.0146, Train Acc:0.3750\n",
      "Epoch: 1, Batch: 12000, Train NLL: 1.1992, Train Acc:0.3750\n",
      "Epoch: 1, Batch: 13000, Train NLL: 0.5775, Train Acc:0.8125\n",
      "Epoch: 1, Batch: 14000, Train NLL: 0.9296, Train Acc:0.5625\n",
      "Epoch: 1, Batch: 15000, Train NLL: 1.1009, Train Acc:0.5625\n",
      "Epoch: 1, Batch: 16000, Train NLL: 0.9334, Train Acc:0.5000\n",
      "Epoch: 1, Batch: 17000, Train NLL: 0.8453, Train Acc:0.5625\n",
      "Epoch: 1, Batch: 18000, Train NLL: 1.0686, Train Acc:0.4375\n",
      "Epoch: 1, Batch: 19000, Train NLL: 0.8334, Train Acc:0.6250\n",
      "Epoch: 1, Batch: 20000, Train NLL: 0.5846, Train Acc:0.8750\n",
      "Epoch: 1, Batch: 21000, Train NLL: 0.8749, Train Acc:0.5000\n",
      "Epoch: 1, Batch: 22000, Train NLL: 0.9633, Train Acc:0.5625\n",
      "Epoch: 1, Batch: 23000, Train NLL: 0.8858, Train Acc:0.5625\n",
      "Epoch: 1, Batch: 24000, Train NLL: 0.6966, Train Acc:0.8125\n",
      "Epoch: 1, Batch: 25000, Train NLL: 0.7810, Train Acc:0.6875\n",
      "Epoch: 1, Batch: 26000, Train NLL: 0.6599, Train Acc:0.7500\n",
      "Epoch: 1, Batch: 27000, Train NLL: 0.7709, Train Acc:0.6250\n",
      "Epoch: 1, Batch: 28000, Train NLL: 0.7588, Train Acc:0.6875\n",
      "Epoch: 1, Batch: 29000, Train NLL: 0.9519, Train Acc:0.5000\n",
      "Epoch: 1, Batch: 30000, Train NLL: 0.9164, Train Acc:0.5625\n",
      "Epoch: 1, Batch: 31000, Train NLL: 1.0586, Train Acc:0.5000\n",
      "Epoch: 1, Batch: 32000, Train NLL: 0.8970, Train Acc:0.6250\n",
      "Epoch: 1, Batch: 33000, Train NLL: 1.0015, Train Acc:0.6250\n",
      "Epoch: 1, Batch: 34000, Train NLL: 0.9843, Train Acc:0.4375\n",
      "Epoch: 1, Val NLL: 0.8426, Val Acc: 0.6149\n",
      "WROTE MODEL\n",
      "Epoch: 2, Batch: 0, Train NLL: 0.5517, Train Acc:0.8125\n",
      "Epoch: 2, Batch: 1000, Train NLL: 0.9625, Train Acc:0.5000\n",
      "Epoch: 2, Batch: 2000, Train NLL: 1.2093, Train Acc:0.3125\n",
      "Epoch: 2, Batch: 3000, Train NLL: 0.9669, Train Acc:0.4375\n",
      "Epoch: 2, Batch: 4000, Train NLL: 0.9221, Train Acc:0.5625\n",
      "Epoch: 2, Batch: 5000, Train NLL: 0.7479, Train Acc:0.6250\n",
      "Epoch: 2, Batch: 6000, Train NLL: 1.0451, Train Acc:0.4375\n",
      "Epoch: 2, Batch: 7000, Train NLL: 0.9885, Train Acc:0.5000\n",
      "Epoch: 2, Batch: 8000, Train NLL: 0.6657, Train Acc:0.6875\n",
      "Epoch: 2, Batch: 9000, Train NLL: 0.9351, Train Acc:0.5625\n",
      "Epoch: 2, Batch: 10000, Train NLL: 0.7987, Train Acc:0.6875\n",
      "Epoch: 2, Batch: 11000, Train NLL: 0.6871, Train Acc:0.5625\n",
      "Epoch: 2, Batch: 12000, Train NLL: 0.5741, Train Acc:0.8750\n",
      "Epoch: 2, Batch: 13000, Train NLL: 0.5852, Train Acc:0.7500\n",
      "Epoch: 2, Batch: 14000, Train NLL: 1.0779, Train Acc:0.6250\n",
      "Epoch: 2, Batch: 15000, Train NLL: 0.4804, Train Acc:0.7500\n",
      "Epoch: 2, Batch: 16000, Train NLL: 0.9090, Train Acc:0.5625\n",
      "Epoch: 2, Batch: 17000, Train NLL: 0.7305, Train Acc:0.7500\n",
      "Epoch: 2, Batch: 18000, Train NLL: 0.8936, Train Acc:0.5000\n",
      "Epoch: 2, Batch: 19000, Train NLL: 0.8868, Train Acc:0.5625\n",
      "Epoch: 2, Batch: 20000, Train NLL: 0.5395, Train Acc:0.8125\n",
      "Epoch: 2, Batch: 21000, Train NLL: 0.9124, Train Acc:0.5625\n",
      "Epoch: 2, Batch: 22000, Train NLL: 1.0511, Train Acc:0.4375\n",
      "Epoch: 2, Batch: 23000, Train NLL: 0.9428, Train Acc:0.6250\n",
      "Epoch: 2, Batch: 24000, Train NLL: 0.7124, Train Acc:0.5625\n",
      "Epoch: 2, Batch: 25000, Train NLL: 0.7916, Train Acc:0.6875\n",
      "Epoch: 2, Batch: 26000, Train NLL: 0.7717, Train Acc:0.6250\n",
      "Epoch: 2, Batch: 27000, Train NLL: 0.7599, Train Acc:0.6875\n",
      "Epoch: 2, Batch: 28000, Train NLL: 1.0546, Train Acc:0.4375\n",
      "Epoch: 2, Batch: 29000, Train NLL: 0.9484, Train Acc:0.6875\n",
      "Epoch: 2, Batch: 30000, Train NLL: 0.9029, Train Acc:0.5000\n",
      "Epoch: 2, Batch: 31000, Train NLL: 0.8983, Train Acc:0.6875\n",
      "Epoch: 2, Batch: 32000, Train NLL: 0.9700, Train Acc:0.5625\n",
      "Epoch: 2, Batch: 33000, Train NLL: 0.9914, Train Acc:0.4375\n",
      "Epoch: 2, Batch: 34000, Train NLL: 0.6158, Train Acc:0.8125\n",
      "Epoch: 2, Val NLL: 0.7878, Val Acc: 0.6494\n",
      "WROTE MODEL\n",
      "Epoch: 3, Batch: 0, Train NLL: 0.9058, Train Acc:0.5625\n",
      "Epoch: 3, Batch: 1000, Train NLL: 0.8059, Train Acc:0.6250\n",
      "Epoch: 3, Batch: 2000, Train NLL: 0.7005, Train Acc:0.6875\n",
      "Epoch: 3, Batch: 3000, Train NLL: 0.7580, Train Acc:0.6875\n",
      "Epoch: 3, Batch: 4000, Train NLL: 0.7820, Train Acc:0.6250\n",
      "Epoch: 3, Batch: 5000, Train NLL: 0.6372, Train Acc:0.7500\n",
      "Epoch: 3, Batch: 6000, Train NLL: 0.6920, Train Acc:0.8125\n",
      "Epoch: 3, Batch: 7000, Train NLL: 1.0391, Train Acc:0.4375\n",
      "Epoch: 3, Batch: 8000, Train NLL: 0.6460, Train Acc:0.6875\n",
      "Epoch: 3, Batch: 9000, Train NLL: 0.9304, Train Acc:0.5625\n",
      "Epoch: 3, Batch: 10000, Train NLL: 0.7909, Train Acc:0.6875\n",
      "Epoch: 3, Batch: 11000, Train NLL: 0.4507, Train Acc:0.8750\n",
      "Epoch: 3, Batch: 12000, Train NLL: 0.7288, Train Acc:0.6250\n",
      "Epoch: 3, Batch: 13000, Train NLL: 0.5863, Train Acc:0.6875\n",
      "Epoch: 3, Batch: 14000, Train NLL: 0.6549, Train Acc:0.6875\n",
      "Epoch: 3, Batch: 15000, Train NLL: 0.7242, Train Acc:0.6875\n",
      "Epoch: 3, Batch: 16000, Train NLL: 0.6387, Train Acc:0.7500\n",
      "Epoch: 3, Batch: 17000, Train NLL: 0.5899, Train Acc:0.8125\n",
      "Epoch: 3, Batch: 18000, Train NLL: 0.5783, Train Acc:0.7500\n",
      "Epoch: 3, Batch: 19000, Train NLL: 0.7738, Train Acc:0.6250\n",
      "Epoch: 3, Batch: 20000, Train NLL: 0.8464, Train Acc:0.5625\n",
      "Epoch: 3, Batch: 21000, Train NLL: 0.5894, Train Acc:0.6875\n",
      "Epoch: 3, Batch: 22000, Train NLL: 0.5438, Train Acc:0.8125\n",
      "Epoch: 3, Batch: 23000, Train NLL: 0.4721, Train Acc:0.8750\n",
      "Epoch: 3, Batch: 24000, Train NLL: 0.8215, Train Acc:0.5625\n",
      "Epoch: 3, Batch: 25000, Train NLL: 0.5471, Train Acc:0.8125\n",
      "Epoch: 3, Batch: 26000, Train NLL: 0.7406, Train Acc:0.6875\n",
      "Epoch: 3, Batch: 27000, Train NLL: 0.6990, Train Acc:0.6250\n",
      "Epoch: 3, Batch: 28000, Train NLL: 0.4269, Train Acc:0.8125\n",
      "Epoch: 3, Batch: 29000, Train NLL: 0.8347, Train Acc:0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Batch: 30000, Train NLL: 0.6193, Train Acc:0.8125\n",
      "Epoch: 3, Batch: 31000, Train NLL: 0.4903, Train Acc:0.8750\n",
      "Epoch: 3, Batch: 32000, Train NLL: 0.6978, Train Acc:0.6875\n",
      "Epoch: 3, Batch: 33000, Train NLL: 0.5225, Train Acc:0.8125\n",
      "Epoch: 3, Batch: 34000, Train NLL: 0.7566, Train Acc:0.6250\n",
      "Epoch: 3, Val NLL: 0.7557, Val Acc: 0.6691\n",
      "WROTE MODEL\n",
      "Epoch: 4, Batch: 0, Train NLL: 0.9128, Train Acc:0.5000\n",
      "Epoch: 4, Batch: 1000, Train NLL: 0.7180, Train Acc:0.6250\n",
      "Epoch: 4, Batch: 2000, Train NLL: 0.5994, Train Acc:0.8125\n",
      "Epoch: 4, Batch: 3000, Train NLL: 0.8757, Train Acc:0.5000\n",
      "Epoch: 4, Batch: 4000, Train NLL: 0.6218, Train Acc:0.8125\n",
      "Epoch: 4, Batch: 5000, Train NLL: 0.8966, Train Acc:0.5000\n",
      "Epoch: 4, Batch: 6000, Train NLL: 1.0300, Train Acc:0.5625\n",
      "Epoch: 4, Batch: 7000, Train NLL: 0.7368, Train Acc:0.6875\n",
      "Epoch: 4, Batch: 8000, Train NLL: 0.7464, Train Acc:0.6250\n",
      "Epoch: 4, Batch: 9000, Train NLL: 1.0220, Train Acc:0.5625\n",
      "Epoch: 4, Batch: 10000, Train NLL: 0.9384, Train Acc:0.6250\n",
      "Epoch: 4, Batch: 11000, Train NLL: 0.6339, Train Acc:0.6250\n",
      "Epoch: 4, Batch: 12000, Train NLL: 0.9766, Train Acc:0.6250\n",
      "Epoch: 4, Batch: 13000, Train NLL: 0.8211, Train Acc:0.6250\n",
      "Epoch: 4, Batch: 14000, Train NLL: 0.8503, Train Acc:0.6250\n",
      "Epoch: 4, Batch: 15000, Train NLL: 0.7233, Train Acc:0.6875\n",
      "Epoch: 4, Batch: 16000, Train NLL: 0.8359, Train Acc:0.5625\n",
      "Epoch: 4, Batch: 17000, Train NLL: 1.0734, Train Acc:0.5000\n",
      "Epoch: 4, Batch: 18000, Train NLL: 0.9654, Train Acc:0.5000\n",
      "Epoch: 4, Batch: 19000, Train NLL: 0.7120, Train Acc:0.8125\n",
      "Epoch: 4, Batch: 20000, Train NLL: 0.8186, Train Acc:0.5625\n",
      "Epoch: 4, Batch: 21000, Train NLL: 0.8241, Train Acc:0.6250\n",
      "Epoch: 4, Batch: 22000, Train NLL: 0.7383, Train Acc:0.6875\n",
      "Epoch: 4, Batch: 23000, Train NLL: 1.3118, Train Acc:0.4375\n",
      "Epoch: 4, Batch: 24000, Train NLL: 0.9110, Train Acc:0.5625\n",
      "Epoch: 4, Batch: 25000, Train NLL: 0.7046, Train Acc:0.6875\n",
      "Epoch: 4, Batch: 26000, Train NLL: 0.7580, Train Acc:0.5000\n",
      "Epoch: 4, Batch: 27000, Train NLL: 1.0831, Train Acc:0.5625\n",
      "Epoch: 4, Batch: 28000, Train NLL: 0.7365, Train Acc:0.7500\n",
      "Epoch: 4, Batch: 29000, Train NLL: 0.6613, Train Acc:0.6875\n",
      "Epoch: 4, Batch: 30000, Train NLL: 0.6761, Train Acc:0.7500\n",
      "Epoch: 4, Batch: 31000, Train NLL: 0.7347, Train Acc:0.6250\n",
      "Epoch: 4, Batch: 32000, Train NLL: 0.6756, Train Acc:0.6250\n",
      "Epoch: 4, Batch: 33000, Train NLL: 0.8598, Train Acc:0.5625\n",
      "Epoch: 4, Batch: 34000, Train NLL: 0.4927, Train Acc:0.8125\n",
      "Epoch: 4, Val NLL: 0.7352, Val Acc: 0.6777\n",
      "WROTE MODEL\n",
      "Epoch: 5, Batch: 0, Train NLL: 0.7074, Train Acc:0.7500\n",
      "Epoch: 5, Batch: 1000, Train NLL: 0.6838, Train Acc:0.7500\n",
      "Epoch: 5, Batch: 2000, Train NLL: 0.6855, Train Acc:0.6875\n",
      "Epoch: 5, Batch: 3000, Train NLL: 0.6429, Train Acc:0.6875\n",
      "Epoch: 5, Batch: 4000, Train NLL: 0.7373, Train Acc:0.6875\n",
      "Epoch: 5, Batch: 5000, Train NLL: 0.4678, Train Acc:0.8750\n",
      "Epoch: 5, Batch: 6000, Train NLL: 0.6354, Train Acc:0.6875\n",
      "Epoch: 5, Batch: 7000, Train NLL: 0.8939, Train Acc:0.6250\n",
      "Epoch: 5, Batch: 8000, Train NLL: 0.8790, Train Acc:0.5625\n",
      "Epoch: 5, Batch: 9000, Train NLL: 0.6067, Train Acc:0.6875\n",
      "Epoch: 5, Batch: 10000, Train NLL: 0.6375, Train Acc:0.7500\n",
      "Epoch: 5, Batch: 11000, Train NLL: 0.5114, Train Acc:0.7500\n",
      "Epoch: 5, Batch: 12000, Train NLL: 0.8803, Train Acc:0.5625\n",
      "Epoch: 5, Batch: 13000, Train NLL: 0.7367, Train Acc:0.7500\n",
      "Epoch: 5, Batch: 14000, Train NLL: 0.8722, Train Acc:0.6250\n",
      "Epoch: 5, Batch: 15000, Train NLL: 0.6482, Train Acc:0.7500\n",
      "Epoch: 5, Batch: 16000, Train NLL: 0.4719, Train Acc:0.8125\n",
      "Epoch: 5, Batch: 17000, Train NLL: 0.7314, Train Acc:0.6250\n",
      "Epoch: 5, Batch: 18000, Train NLL: 0.9350, Train Acc:0.6250\n",
      "Epoch: 5, Batch: 19000, Train NLL: 0.4991, Train Acc:0.7500\n",
      "Epoch: 5, Batch: 20000, Train NLL: 0.5121, Train Acc:0.8125\n",
      "Epoch: 5, Batch: 21000, Train NLL: 0.7123, Train Acc:0.6875\n",
      "Epoch: 5, Batch: 22000, Train NLL: 0.6838, Train Acc:0.6875\n",
      "Epoch: 5, Batch: 23000, Train NLL: 0.6548, Train Acc:0.7500\n",
      "Epoch: 5, Batch: 24000, Train NLL: 0.6576, Train Acc:0.7500\n",
      "Epoch: 5, Batch: 25000, Train NLL: 0.9451, Train Acc:0.5625\n",
      "Epoch: 5, Batch: 26000, Train NLL: 1.2556, Train Acc:0.4375\n",
      "Epoch: 5, Batch: 27000, Train NLL: 0.9292, Train Acc:0.5625\n",
      "Epoch: 5, Batch: 28000, Train NLL: 0.6624, Train Acc:0.6875\n",
      "Epoch: 5, Batch: 29000, Train NLL: 0.9659, Train Acc:0.6250\n",
      "Epoch: 5, Batch: 30000, Train NLL: 0.9494, Train Acc:0.5625\n",
      "Epoch: 5, Batch: 31000, Train NLL: 0.8257, Train Acc:0.6250\n",
      "Epoch: 5, Batch: 32000, Train NLL: 0.5662, Train Acc:0.7500\n",
      "Epoch: 5, Batch: 33000, Train NLL: 0.6070, Train Acc:0.7500\n",
      "Epoch: 5, Batch: 34000, Train NLL: 0.6651, Train Acc:0.6875\n",
      "Epoch: 5, Val NLL: 0.7269, Val Acc: 0.6855\n",
      "WROTE MODEL\n",
      "Epoch: 6, Batch: 0, Train NLL: 0.6463, Train Acc:0.7500\n",
      "Epoch: 6, Batch: 1000, Train NLL: 0.7650, Train Acc:0.5000\n",
      "Epoch: 6, Batch: 2000, Train NLL: 0.3887, Train Acc:0.8750\n",
      "Epoch: 6, Batch: 3000, Train NLL: 0.5305, Train Acc:0.8750\n",
      "Epoch: 6, Batch: 4000, Train NLL: 0.6540, Train Acc:0.6250\n",
      "Epoch: 6, Batch: 5000, Train NLL: 0.5796, Train Acc:0.7500\n",
      "Epoch: 6, Batch: 6000, Train NLL: 0.7008, Train Acc:0.8125\n",
      "Epoch: 6, Batch: 7000, Train NLL: 0.5838, Train Acc:0.8125\n",
      "Epoch: 6, Batch: 8000, Train NLL: 0.6229, Train Acc:0.8125\n",
      "Epoch: 6, Batch: 9000, Train NLL: 0.5882, Train Acc:0.7500\n",
      "Epoch: 6, Batch: 10000, Train NLL: 0.6754, Train Acc:0.6875\n",
      "Epoch: 6, Batch: 11000, Train NLL: 0.6820, Train Acc:0.7500\n",
      "Epoch: 6, Batch: 12000, Train NLL: 0.4621, Train Acc:0.8750\n",
      "Epoch: 6, Batch: 13000, Train NLL: 0.7384, Train Acc:0.6875\n",
      "Epoch: 6, Batch: 14000, Train NLL: 0.8483, Train Acc:0.5625\n",
      "Epoch: 6, Batch: 15000, Train NLL: 0.8631, Train Acc:0.6250\n",
      "Epoch: 6, Batch: 16000, Train NLL: 0.5252, Train Acc:0.8750\n",
      "Epoch: 6, Batch: 17000, Train NLL: 0.5942, Train Acc:0.6250\n",
      "Epoch: 6, Batch: 18000, Train NLL: 0.9118, Train Acc:0.5000\n",
      "Epoch: 6, Batch: 19000, Train NLL: 0.6051, Train Acc:0.7500\n",
      "Epoch: 6, Batch: 20000, Train NLL: 1.0399, Train Acc:0.5000\n",
      "Epoch: 6, Batch: 21000, Train NLL: 0.9074, Train Acc:0.6875\n",
      "Epoch: 6, Batch: 22000, Train NLL: 1.0092, Train Acc:0.5625\n",
      "Epoch: 6, Batch: 23000, Train NLL: 0.6334, Train Acc:0.8125\n",
      "Epoch: 6, Batch: 24000, Train NLL: 0.8211, Train Acc:0.5000\n",
      "Epoch: 6, Batch: 25000, Train NLL: 0.8006, Train Acc:0.6875\n",
      "Epoch: 6, Batch: 26000, Train NLL: 0.9430, Train Acc:0.5625\n",
      "Epoch: 6, Batch: 27000, Train NLL: 0.7160, Train Acc:0.6875\n",
      "Epoch: 6, Batch: 28000, Train NLL: 0.7275, Train Acc:0.6875\n",
      "Epoch: 6, Batch: 29000, Train NLL: 0.8029, Train Acc:0.6250\n",
      "Epoch: 6, Batch: 30000, Train NLL: 0.6955, Train Acc:0.6875\n",
      "Epoch: 6, Batch: 31000, Train NLL: 0.6193, Train Acc:0.7500\n",
      "Epoch: 6, Batch: 32000, Train NLL: 0.7235, Train Acc:0.6250\n",
      "Epoch: 6, Batch: 33000, Train NLL: 0.7494, Train Acc:0.6250\n",
      "Epoch: 6, Batch: 34000, Train NLL: 0.7044, Train Acc:0.7500\n",
      "Epoch: 6, Val NLL: 0.7149, Val Acc: 0.6920\n",
      "WROTE MODEL\n",
      "Epoch: 7, Batch: 0, Train NLL: 0.5683, Train Acc:0.8125\n",
      "Epoch: 7, Batch: 1000, Train NLL: 0.9760, Train Acc:0.5625\n",
      "Epoch: 7, Batch: 2000, Train NLL: 0.4175, Train Acc:0.8125\n",
      "Epoch: 7, Batch: 3000, Train NLL: 0.6068, Train Acc:0.8125\n",
      "Epoch: 7, Batch: 4000, Train NLL: 0.4948, Train Acc:0.8125\n",
      "Epoch: 7, Batch: 5000, Train NLL: 0.8420, Train Acc:0.5625\n",
      "Epoch: 7, Batch: 6000, Train NLL: 0.9220, Train Acc:0.5625\n",
      "Epoch: 7, Batch: 7000, Train NLL: 0.7060, Train Acc:0.5625\n",
      "Epoch: 7, Batch: 8000, Train NLL: 0.7333, Train Acc:0.6875\n",
      "Epoch: 7, Batch: 9000, Train NLL: 0.6838, Train Acc:0.7500\n",
      "Epoch: 7, Batch: 10000, Train NLL: 0.6564, Train Acc:0.7500\n",
      "Epoch: 7, Batch: 11000, Train NLL: 0.5509, Train Acc:0.6875\n",
      "Epoch: 7, Batch: 12000, Train NLL: 0.7238, Train Acc:0.6250\n",
      "Epoch: 7, Batch: 13000, Train NLL: 0.4370, Train Acc:0.8125\n",
      "Epoch: 7, Batch: 14000, Train NLL: 0.5188, Train Acc:0.7500\n",
      "Epoch: 7, Batch: 15000, Train NLL: 0.6584, Train Acc:0.7500\n",
      "Epoch: 7, Batch: 16000, Train NLL: 0.7954, Train Acc:0.6875\n",
      "Epoch: 7, Batch: 17000, Train NLL: 0.7642, Train Acc:0.7500\n",
      "Epoch: 7, Batch: 18000, Train NLL: 0.6009, Train Acc:0.8750\n",
      "Epoch: 7, Batch: 19000, Train NLL: 0.7794, Train Acc:0.6875\n",
      "Epoch: 7, Batch: 20000, Train NLL: 0.5526, Train Acc:0.8125\n",
      "Epoch: 7, Batch: 21000, Train NLL: 0.6722, Train Acc:0.8125\n",
      "Epoch: 7, Batch: 22000, Train NLL: 0.5903, Train Acc:0.6875\n",
      "Epoch: 7, Batch: 23000, Train NLL: 1.2054, Train Acc:0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Batch: 24000, Train NLL: 0.4316, Train Acc:0.8125\n",
      "Epoch: 7, Batch: 25000, Train NLL: 0.8026, Train Acc:0.6875\n",
      "Epoch: 7, Batch: 26000, Train NLL: 0.7890, Train Acc:0.6875\n",
      "Epoch: 7, Batch: 27000, Train NLL: 0.3538, Train Acc:0.8750\n",
      "Epoch: 7, Batch: 28000, Train NLL: 0.8583, Train Acc:0.5000\n",
      "Epoch: 7, Batch: 29000, Train NLL: 0.5695, Train Acc:0.8125\n",
      "Epoch: 7, Batch: 30000, Train NLL: 0.5131, Train Acc:0.8125\n",
      "Epoch: 7, Batch: 31000, Train NLL: 0.8112, Train Acc:0.5625\n",
      "Epoch: 7, Batch: 32000, Train NLL: 1.0476, Train Acc:0.4375\n",
      "Epoch: 7, Batch: 33000, Train NLL: 0.6336, Train Acc:0.7500\n",
      "Epoch: 7, Batch: 34000, Train NLL: 0.7007, Train Acc:0.7500\n",
      "Epoch: 7, Val NLL: 0.7029, Val Acc: 0.7013\n",
      "WROTE MODEL\n",
      "Epoch: 8, Batch: 0, Train NLL: 0.7198, Train Acc:0.7500\n",
      "Epoch: 8, Batch: 1000, Train NLL: 0.6480, Train Acc:0.7500\n",
      "Epoch: 8, Batch: 2000, Train NLL: 0.6326, Train Acc:0.7500\n",
      "Epoch: 8, Batch: 3000, Train NLL: 0.4899, Train Acc:0.8750\n",
      "Epoch: 8, Batch: 4000, Train NLL: 0.8221, Train Acc:0.6875\n",
      "Epoch: 8, Batch: 5000, Train NLL: 0.5798, Train Acc:0.6875\n",
      "Epoch: 8, Batch: 6000, Train NLL: 0.6654, Train Acc:0.8125\n",
      "Epoch: 8, Batch: 7000, Train NLL: 0.6282, Train Acc:0.6250\n",
      "Epoch: 8, Batch: 8000, Train NLL: 0.8353, Train Acc:0.8125\n",
      "Epoch: 8, Batch: 9000, Train NLL: 0.5456, Train Acc:0.8750\n",
      "Epoch: 8, Batch: 10000, Train NLL: 0.5883, Train Acc:0.7500\n",
      "Epoch: 8, Batch: 11000, Train NLL: 0.7783, Train Acc:0.7500\n",
      "Epoch: 8, Batch: 12000, Train NLL: 0.6190, Train Acc:0.6875\n",
      "Epoch: 8, Batch: 13000, Train NLL: 0.8702, Train Acc:0.6875\n",
      "Epoch: 8, Batch: 14000, Train NLL: 0.7395, Train Acc:0.8125\n",
      "Epoch: 8, Batch: 15000, Train NLL: 0.7494, Train Acc:0.6250\n",
      "Epoch: 8, Batch: 16000, Train NLL: 0.4716, Train Acc:0.8125\n",
      "Epoch: 8, Batch: 17000, Train NLL: 1.0147, Train Acc:0.6875\n",
      "Epoch: 8, Batch: 18000, Train NLL: 0.4739, Train Acc:0.8125\n",
      "Epoch: 8, Batch: 19000, Train NLL: 0.5863, Train Acc:0.8125\n",
      "Epoch: 8, Batch: 20000, Train NLL: 0.4226, Train Acc:0.8125\n",
      "Epoch: 8, Batch: 21000, Train NLL: 0.8946, Train Acc:0.5625\n",
      "Epoch: 8, Batch: 22000, Train NLL: 0.7142, Train Acc:0.6875\n",
      "Epoch: 8, Batch: 23000, Train NLL: 1.1769, Train Acc:0.4375\n",
      "Epoch: 8, Batch: 24000, Train NLL: 0.5663, Train Acc:0.7500\n",
      "Epoch: 8, Batch: 25000, Train NLL: 0.8197, Train Acc:0.6250\n",
      "Epoch: 8, Batch: 26000, Train NLL: 0.5716, Train Acc:0.7500\n",
      "Epoch: 8, Batch: 27000, Train NLL: 0.4748, Train Acc:0.7500\n",
      "Epoch: 8, Batch: 28000, Train NLL: 0.7249, Train Acc:0.5000\n",
      "Epoch: 8, Batch: 29000, Train NLL: 0.5926, Train Acc:0.8125\n",
      "Epoch: 8, Batch: 30000, Train NLL: 1.2156, Train Acc:0.4375\n",
      "Epoch: 8, Batch: 31000, Train NLL: 0.3638, Train Acc:0.9375\n",
      "Epoch: 8, Batch: 32000, Train NLL: 0.7628, Train Acc:0.6250\n",
      "Epoch: 8, Batch: 33000, Train NLL: 0.7113, Train Acc:0.6875\n",
      "Epoch: 8, Batch: 34000, Train NLL: 0.6983, Train Acc:0.7500\n",
      "Epoch: 8, Val NLL: 0.6909, Val Acc: 0.7090\n",
      "WROTE MODEL\n",
      "Epoch: 9, Batch: 0, Train NLL: 0.7840, Train Acc:0.5625\n",
      "Epoch: 9, Batch: 1000, Train NLL: 0.8519, Train Acc:0.6250\n",
      "Epoch: 9, Batch: 2000, Train NLL: 0.7461, Train Acc:0.6875\n",
      "Epoch: 9, Batch: 3000, Train NLL: 0.5324, Train Acc:0.8125\n",
      "Epoch: 9, Batch: 4000, Train NLL: 0.8542, Train Acc:0.6250\n",
      "Epoch: 9, Batch: 5000, Train NLL: 0.6029, Train Acc:0.7500\n",
      "Epoch: 9, Batch: 6000, Train NLL: 0.3875, Train Acc:0.9375\n",
      "Epoch: 9, Batch: 7000, Train NLL: 0.7042, Train Acc:0.6875\n",
      "Epoch: 9, Batch: 8000, Train NLL: 0.9145, Train Acc:0.5625\n",
      "Epoch: 9, Batch: 9000, Train NLL: 1.0672, Train Acc:0.5625\n",
      "Epoch: 9, Batch: 10000, Train NLL: 1.2616, Train Acc:0.3750\n",
      "Epoch: 9, Batch: 11000, Train NLL: 0.7649, Train Acc:0.6250\n",
      "Epoch: 9, Batch: 12000, Train NLL: 0.8268, Train Acc:0.5625\n",
      "Epoch: 9, Batch: 13000, Train NLL: 1.1288, Train Acc:0.5625\n",
      "Epoch: 9, Batch: 14000, Train NLL: 0.9381, Train Acc:0.5000\n",
      "Epoch: 9, Batch: 15000, Train NLL: 0.9499, Train Acc:0.6250\n",
      "Epoch: 9, Batch: 16000, Train NLL: 0.5820, Train Acc:0.7500\n",
      "Epoch: 9, Batch: 17000, Train NLL: 0.5292, Train Acc:0.7500\n",
      "Epoch: 9, Batch: 18000, Train NLL: 0.7163, Train Acc:0.6875\n",
      "Epoch: 9, Batch: 19000, Train NLL: 0.6421, Train Acc:0.7500\n",
      "Epoch: 9, Batch: 20000, Train NLL: 0.7682, Train Acc:0.6875\n",
      "Epoch: 9, Batch: 21000, Train NLL: 0.6684, Train Acc:0.7500\n",
      "Epoch: 9, Batch: 22000, Train NLL: 0.8143, Train Acc:0.5625\n",
      "Epoch: 9, Batch: 23000, Train NLL: 0.6709, Train Acc:0.7500\n",
      "Epoch: 9, Batch: 24000, Train NLL: 0.3264, Train Acc:0.8750\n",
      "Epoch: 9, Batch: 25000, Train NLL: 0.6264, Train Acc:0.6875\n",
      "Epoch: 9, Batch: 26000, Train NLL: 0.8154, Train Acc:0.6875\n",
      "Epoch: 9, Batch: 27000, Train NLL: 0.5753, Train Acc:0.8125\n",
      "Epoch: 9, Batch: 28000, Train NLL: 0.7945, Train Acc:0.7500\n",
      "Epoch: 9, Batch: 29000, Train NLL: 0.6963, Train Acc:0.6250\n",
      "Epoch: 9, Batch: 30000, Train NLL: 0.7711, Train Acc:0.6875\n",
      "Epoch: 9, Batch: 31000, Train NLL: 0.9640, Train Acc:0.5625\n",
      "Epoch: 9, Batch: 32000, Train NLL: 0.8784, Train Acc:0.6875\n",
      "Epoch: 9, Batch: 33000, Train NLL: 0.3968, Train Acc:0.9375\n",
      "Epoch: 9, Batch: 34000, Train NLL: 0.4335, Train Acc:0.8125\n",
      "Epoch: 9, Val NLL: 0.6758, Val Acc: 0.7144\n",
      "WROTE MODEL\n",
      "Epoch: 10, Batch: 0, Train NLL: 0.8288, Train Acc:0.6875\n",
      "Epoch: 10, Batch: 1000, Train NLL: 0.7415, Train Acc:0.6875\n",
      "Epoch: 10, Batch: 2000, Train NLL: 0.6369, Train Acc:0.7500\n",
      "Epoch: 10, Batch: 3000, Train NLL: 0.4859, Train Acc:0.8750\n",
      "Epoch: 10, Batch: 4000, Train NLL: 0.4755, Train Acc:0.9375\n",
      "Epoch: 10, Batch: 5000, Train NLL: 0.6594, Train Acc:0.6875\n",
      "Epoch: 10, Batch: 6000, Train NLL: 0.7310, Train Acc:0.6875\n",
      "Epoch: 10, Batch: 7000, Train NLL: 0.8129, Train Acc:0.6250\n",
      "Epoch: 10, Batch: 8000, Train NLL: 0.7836, Train Acc:0.6875\n",
      "Epoch: 10, Batch: 9000, Train NLL: 0.5221, Train Acc:0.8125\n",
      "Epoch: 10, Batch: 10000, Train NLL: 0.4176, Train Acc:0.9375\n",
      "Epoch: 10, Batch: 11000, Train NLL: 0.9933, Train Acc:0.4375\n",
      "Epoch: 10, Batch: 12000, Train NLL: 0.8305, Train Acc:0.5625\n",
      "Epoch: 10, Batch: 13000, Train NLL: 0.4256, Train Acc:0.8750\n",
      "Epoch: 10, Batch: 14000, Train NLL: 0.7624, Train Acc:0.5000\n",
      "Epoch: 10, Batch: 15000, Train NLL: 0.4886, Train Acc:0.8125\n",
      "Epoch: 10, Batch: 16000, Train NLL: 0.6423, Train Acc:0.7500\n",
      "Epoch: 10, Batch: 17000, Train NLL: 0.7639, Train Acc:0.6250\n",
      "Epoch: 10, Batch: 18000, Train NLL: 0.6215, Train Acc:0.6250\n",
      "Epoch: 10, Batch: 19000, Train NLL: 0.5850, Train Acc:0.7500\n",
      "Epoch: 10, Batch: 20000, Train NLL: 1.0073, Train Acc:0.4375\n",
      "Epoch: 10, Batch: 21000, Train NLL: 0.4895, Train Acc:0.8125\n",
      "Epoch: 10, Batch: 22000, Train NLL: 0.6386, Train Acc:0.7500\n",
      "Epoch: 10, Batch: 23000, Train NLL: 0.6604, Train Acc:0.6250\n",
      "Epoch: 10, Batch: 24000, Train NLL: 0.5906, Train Acc:0.8125\n",
      "Epoch: 10, Batch: 25000, Train NLL: 0.6693, Train Acc:0.8125\n",
      "Epoch: 10, Batch: 26000, Train NLL: 0.6032, Train Acc:0.7500\n",
      "Epoch: 10, Batch: 27000, Train NLL: 0.6999, Train Acc:0.6875\n",
      "Epoch: 10, Batch: 28000, Train NLL: 0.6924, Train Acc:0.6250\n",
      "Epoch: 10, Batch: 29000, Train NLL: 0.4990, Train Acc:0.7500\n",
      "Epoch: 10, Batch: 30000, Train NLL: 0.8765, Train Acc:0.6875\n",
      "Epoch: 10, Batch: 31000, Train NLL: 0.4825, Train Acc:0.8125\n",
      "Epoch: 10, Batch: 32000, Train NLL: 0.7200, Train Acc:0.6250\n",
      "Epoch: 10, Batch: 33000, Train NLL: 0.9849, Train Acc:0.6250\n",
      "Epoch: 10, Batch: 34000, Train NLL: 0.7029, Train Acc:0.7500\n",
      "Epoch: 10, Val NLL: 0.6697, Val Acc: 0.7150\n",
      "WROTE MODEL\n",
      "Epoch: 11, Batch: 0, Train NLL: 0.7098, Train Acc:0.7500\n",
      "Epoch: 11, Batch: 1000, Train NLL: 0.5490, Train Acc:0.7500\n",
      "Epoch: 11, Batch: 2000, Train NLL: 0.6309, Train Acc:0.7500\n",
      "Epoch: 11, Batch: 3000, Train NLL: 0.8773, Train Acc:0.6875\n",
      "Epoch: 11, Batch: 4000, Train NLL: 0.6392, Train Acc:0.6875\n",
      "Epoch: 11, Batch: 5000, Train NLL: 0.8726, Train Acc:0.6250\n",
      "Epoch: 11, Batch: 6000, Train NLL: 0.5517, Train Acc:0.6875\n",
      "Epoch: 11, Batch: 7000, Train NLL: 0.8162, Train Acc:0.6250\n",
      "Epoch: 11, Batch: 8000, Train NLL: 0.6895, Train Acc:0.6875\n",
      "Epoch: 11, Batch: 9000, Train NLL: 0.6275, Train Acc:0.6875\n",
      "Epoch: 11, Batch: 10000, Train NLL: 0.6660, Train Acc:0.6250\n",
      "Epoch: 11, Batch: 11000, Train NLL: 0.4331, Train Acc:0.8750\n",
      "Epoch: 11, Batch: 12000, Train NLL: 0.7880, Train Acc:0.6875\n",
      "Epoch: 11, Batch: 13000, Train NLL: 0.6320, Train Acc:0.6875\n",
      "Epoch: 11, Batch: 14000, Train NLL: 0.4435, Train Acc:0.9375\n",
      "Epoch: 11, Batch: 15000, Train NLL: 0.4219, Train Acc:0.7500\n",
      "Epoch: 11, Batch: 16000, Train NLL: 0.6670, Train Acc:0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Batch: 17000, Train NLL: 0.5595, Train Acc:0.8125\n",
      "Epoch: 11, Batch: 18000, Train NLL: 0.5490, Train Acc:0.8125\n",
      "Epoch: 11, Batch: 19000, Train NLL: 0.6366, Train Acc:0.5625\n",
      "Epoch: 11, Batch: 20000, Train NLL: 0.6024, Train Acc:0.8125\n",
      "Epoch: 11, Batch: 21000, Train NLL: 0.9604, Train Acc:0.6250\n",
      "Epoch: 11, Batch: 22000, Train NLL: 0.8927, Train Acc:0.6250\n",
      "Epoch: 11, Batch: 23000, Train NLL: 0.7713, Train Acc:0.6250\n",
      "Epoch: 11, Batch: 24000, Train NLL: 0.7208, Train Acc:0.6875\n",
      "Epoch: 11, Batch: 25000, Train NLL: 0.6410, Train Acc:0.6250\n",
      "Epoch: 11, Batch: 26000, Train NLL: 0.7343, Train Acc:0.6875\n",
      "Epoch: 11, Batch: 27000, Train NLL: 0.4267, Train Acc:0.8750\n",
      "Epoch: 11, Batch: 28000, Train NLL: 0.4433, Train Acc:0.8125\n",
      "Epoch: 11, Batch: 29000, Train NLL: 0.9957, Train Acc:0.5625\n",
      "Epoch: 11, Batch: 30000, Train NLL: 0.8119, Train Acc:0.6250\n",
      "Epoch: 11, Batch: 31000, Train NLL: 0.5448, Train Acc:0.8750\n",
      "Epoch: 11, Batch: 32000, Train NLL: 0.5601, Train Acc:0.8125\n",
      "Epoch: 11, Batch: 33000, Train NLL: 0.6477, Train Acc:0.8125\n",
      "Epoch: 11, Batch: 34000, Train NLL: 0.5735, Train Acc:0.7500\n",
      "Epoch: 11, Val NLL: 0.6636, Val Acc: 0.7176\n",
      "WROTE MODEL\n",
      "Epoch: 12, Batch: 0, Train NLL: 0.5826, Train Acc:0.8125\n",
      "Epoch: 12, Batch: 1000, Train NLL: 0.6645, Train Acc:0.6875\n",
      "Epoch: 12, Batch: 2000, Train NLL: 0.8536, Train Acc:0.6250\n",
      "Epoch: 12, Batch: 3000, Train NLL: 0.5529, Train Acc:0.6875\n",
      "Epoch: 12, Batch: 4000, Train NLL: 0.3909, Train Acc:0.8125\n",
      "Epoch: 12, Batch: 5000, Train NLL: 0.8699, Train Acc:0.5625\n",
      "Epoch: 12, Batch: 6000, Train NLL: 0.4295, Train Acc:0.7500\n",
      "Epoch: 12, Batch: 7000, Train NLL: 0.3386, Train Acc:0.9375\n",
      "Epoch: 12, Batch: 8000, Train NLL: 0.6783, Train Acc:0.6250\n",
      "Epoch: 12, Batch: 9000, Train NLL: 0.5259, Train Acc:0.8125\n",
      "Epoch: 12, Batch: 10000, Train NLL: 0.6788, Train Acc:0.7500\n",
      "Epoch: 12, Batch: 11000, Train NLL: 1.1271, Train Acc:0.5625\n",
      "Epoch: 12, Batch: 12000, Train NLL: 0.9771, Train Acc:0.6250\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-e24035ff4019>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEP1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mintra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFI1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFI1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mED1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mED1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEP1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mintra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFI1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFI1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mED1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mED1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-61-df0dabf1c5c4>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(e, train_iter, EP1, F1, G1, H1, criterion, optimizer, intra, dist, FI1, ED1)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0msent2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepend_null\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEP1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFI1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mED1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-9119c2beeebe>\u001b[0m in \u001b[0;36mget_output\u001b[0;34m(sent1, sent2, EP1, F1, G1, H1, intra, dist, FI1, ED1)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mproj2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproj2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj2_soft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproj1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproj2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_loss = 1e8\n",
    "intra = True\n",
    "\n",
    "if intra:\n",
    "    dist = 10\n",
    "    num_embeddings = dist + 1\n",
    "    embedding_dim = 1\n",
    "    FI1 = FeedForwardFIntra(hidden_size1, hidden_size1, hidden_size1).cuda()\n",
    "    ED1 = EmbedDist(num_embeddings, embedding_dim).cuda()\n",
    "    F1 = FeedForwardF(hidden_size1 * 2, hidden_size1, hidden_size1).cuda()\n",
    "    G1 = FeedForwardG(hidden_size2 * 2, hidden_size1, hidden_size1).cuda()\n",
    "else:\n",
    "    dist = None    \n",
    "    FI1 = None\n",
    "    ED1 = None\n",
    "    F1 = FeedForwardF(hidden_size1, hidden_size1, hidden_size1).cuda()\n",
    "    G1 = FeedForwardG(hidden_size2, hidden_size1, hidden_size1).cuda()\n",
    "\n",
    "EP1 = EmbedProject(weights, embed_size, hidden_size1).cuda()\n",
    "H1 = FeedForwardH(hidden_size2, hidden_size1, output_size).cuda()\n",
    "\n",
    "parameters = [param for param in EP1.parameters()] # embed, lnr, bias\n",
    "parameters.extend([param for param in F1.parameters()]) # lnr1, bias1, lnr2, bias2\n",
    "parameters.extend([param for param in G1.parameters()]) # lnr1, bias1, lnr2, bias2\n",
    "parameters.extend([param for param in H1.parameters()]) # lnr1, bias1, lnr2, bias2, lnr3, bias3\n",
    "if intra:\n",
    "    parameters.extend([param for param in FI1.parameters()]) # lnr1, bias1, lnr2, bias2\n",
    "    parameters.extend([param for param in ED1.parameters()]) # embed\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adagrad(parameters, lr=0.025, initial_accumulator_value=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=4)\n",
    "\n",
    "for e in range(100):\n",
    "    training_loop(e, train_iter, EP1, F1, G1, H1, criterion, optimizer, intra=intra, dist=dist, FI1=FI1, ED1=ED1)\n",
    "    loss = validation_loop(e, val_iter, EP1, F1, G1, H1, criterion, intra=intra, dist=dist, FI1=FI1, ED1=ED1)\n",
    "    scheduler.step(loss)\n",
    "    print('LR = {}'.format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "    if loss < best_loss:\n",
    "        torch.save(EP1.state_dict(),'best_EP1_intra_v02.pt')\n",
    "        torch.save(F1.state_dict(),'best_F1_intra_v02.pt')\n",
    "        torch.save(G1.state_dict(),'best_G1_intra_v02.pt')\n",
    "        torch.save(H1.state_dict(),'best_H1_intra_v02.pt')\n",
    "        best_loss = loss\n",
    "        print('WROTE MODEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
