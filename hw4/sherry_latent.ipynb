{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 287 - HW 4 - Latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "from namedtensor import ntorch, NamedTensor\n",
    "from namedtensor.text import NamedField\n",
    "\n",
    "from common import *\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train) 549367\n",
      "len(TEXT.vocab) 62998\n",
      "len(LABEL.vocab) 4\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "TEXT = NamedField(names=('seqlen',)) # Our input $x$\n",
    "LABEL = NamedField(sequential=False, names=()) # Our labels $y$\n",
    "train, val, test = torchtext.datasets.SNLI.splits(TEXT, LABEL)\n",
    "print('len(train)', len(train))\n",
    "TEXT.build_vocab(train)\n",
    "LABEL.build_vocab(train)\n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
    "print('len(LABEL.vocab)', len(LABEL.vocab))\n",
    "train_iter, val_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
    "    (train, val, test), batch_size=32, device=torch.device(\"cuda\"), repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word embeddings shape: OrderedDict([('word', 62998), ('embedding', 300)])\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary with word embeddings\n",
    "# out-of-vocabulary words are hashed to one of 100 random embeddings each initialized to mean 0, stdev 1 (Sec 5.1)\n",
    "unk_vectors = [torch.randn(300) for _ in range(100)]\n",
    "TEXT.vocab.load_vectors(vectors='glove.6B.300d', unk_init=lambda x:random.choice(unk_vectors))\n",
    "vectors = TEXT.vocab.vectors\n",
    "vectors = vectors / vectors.norm(dim=1, keepdim=True) # normalized to have l_2 norm of 1\n",
    "vectors = NamedTensor(vectors, ('word', 'embedding'))\n",
    "TEXT.vocab.vectors = vectors\n",
    "print(\"word embeddings shape:\", TEXT.vocab.vectors.shape)\n",
    "weights = TEXT.vocab.vectors.values.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of premise batch: OrderedDict([('seqlen', 38), ('batch', 32)])\n",
      "Size of hypothesis batch: OrderedDict([('seqlen', 19), ('batch', 32)])\n",
      "Size of label batch: OrderedDict([('batch', 32)])\n"
     ]
    }
   ],
   "source": [
    "# here's an example of a training example\n",
    "batch = next(iter(train_iter))\n",
    "print(\"Size of premise batch:\", batch.premise.shape)\n",
    "print(\"Size of hypothesis batch:\", batch.hypothesis.shape)\n",
    "print(\"Size of label batch:\", batch.label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMS - input: 62998, embed: 300, hidden1: 200, output: 4\n"
     ]
    }
   ],
   "source": [
    "# dimensions\n",
    "input_size = TEXT.vocab.vectors.shape['word']\n",
    "embed_size = TEXT.vocab.vectors.shape['embedding']\n",
    "hidden_size1 = 200\n",
    "output_size = len(LABEL.vocab)\n",
    "print('DIMS - input: %d, embed: %d, hidden1: %d, output: %d'%(input_size, embed_size, hidden_size1, output_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Variable Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(e, train_iter, networks, criterion, optimizer):\n",
    "    K = len(networks)\n",
    "    for network in networks:\n",
    "        network.train()\n",
    "    \n",
    "    for ix,batch in enumerate(train_iter):\n",
    "        optimizer.zero_grad()\n",
    "        sent1 = prepend_null(batch.premise.values.transpose(0,1))\n",
    "        sent2 = prepend_null(batch.hypothesis.values.transpose(0,1))\n",
    "        target = batch.label.values\n",
    "        \n",
    "        output = torch.zeros((K, sent1.shape[0], output_size), device='cuda')\n",
    "        for c in range(K):\n",
    "            network = networks[c]\n",
    "            output[c,:,:] = F.log_softmax(network(sent1, sent2), dim=1) # K x BATCH x OUTPUT_SIZE\n",
    "        output = torch.logsumexp(output, dim=0) + torch.log(torch.tensor(1/K))\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if ix % 1000 == 0:\n",
    "            acc = torch.sum(torch.argmax(output, dim=1) == target).item() / target.shape[0]\n",
    "            print('Epoch: {0}, Batch: {1}, Train NLL: {2:0.4f}, Train Acc:{3:0.4f}'.format(e, ix, loss.cpu().detach(), acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(e, val_iter, networks, criterion):\n",
    "    K = len(networks)\n",
    "    for network in networks:\n",
    "        network.eval()\n",
    "    total_loss = 0\n",
    "    total_sent = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    for ix,batch in enumerate(val_iter):\n",
    "        sent1 = prepend_null(batch.premise.values.transpose(0,1))\n",
    "        sent2 = prepend_null(batch.hypothesis.values.transpose(0,1))\n",
    "        target = batch.label.values\n",
    "        \n",
    "        output = torch.zeros((K, sent1.shape[0], output_size), device='cuda')\n",
    "        for c in range(K):\n",
    "            network = networks[c]\n",
    "            output[c,:,:] = F.log_softmax(network(sent1, sent2), dim=1) # K x BATCH x OUTPUT_SIZE\n",
    "        output = torch.logsumexp(output, dim=0) + torch.log(torch.tensor(1/K))\n",
    "        \n",
    "        loss = criterion(output, target).item()\n",
    "        sent = sent1.shape[0]\n",
    "        correct = torch.sum(torch.argmax(output, dim=1) == target).item()\n",
    "        \n",
    "        total_loss += loss*sent\n",
    "        total_sent += sent\n",
    "        total_correct += correct\n",
    "    \n",
    "    print('Epoch: {0}, Val NLL: {1:0.4f}, Val Acc: {2:0.4f}'.format(e, total_loss/total_sent, total_correct/total_sent))\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFA_net1 = Decomposable_Attn_Network(input_size, embed_size, hidden_size1, output_size, weights).cuda()\n",
    "FFA_net2 = Decomposable_Attn_Network(input_size, embed_size, hidden_size1, output_size, weights).cuda()\n",
    "FFA_net3 = Decomposable_Attn_Network(input_size, embed_size, hidden_size1, output_size, weights).cuda()\n",
    "networks = [FFA_net1, FFA_net2, FFA_net3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [param for param in FFA_net1.parameters()] \n",
    "parameters.extend([param for param in FFA_net2.parameters()]) \n",
    "parameters.extend([param for param in FFA_net3.parameters()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adagrad(parameters, lr=0.05, initial_accumulator_value=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Train NLL: 1.3822, Train Acc:0.2812\n",
      "Epoch: 0, Batch: 1000, Train NLL: 1.1061, Train Acc:0.2812\n",
      "Epoch: 0, Batch: 2000, Train NLL: 1.1057, Train Acc:0.3438\n",
      "Epoch: 0, Batch: 3000, Train NLL: 1.0938, Train Acc:0.4688\n",
      "Epoch: 0, Batch: 4000, Train NLL: 1.0358, Train Acc:0.3750\n",
      "Epoch: 0, Batch: 5000, Train NLL: 1.0422, Train Acc:0.5000\n",
      "Epoch: 0, Batch: 6000, Train NLL: 1.1214, Train Acc:0.3125\n",
      "Epoch: 0, Batch: 7000, Train NLL: 1.0393, Train Acc:0.5000\n",
      "Epoch: 0, Batch: 8000, Train NLL: 1.1032, Train Acc:0.3750\n",
      "Epoch: 0, Batch: 9000, Train NLL: 1.0455, Train Acc:0.5625\n",
      "Epoch: 0, Batch: 10000, Train NLL: 1.0915, Train Acc:0.5000\n",
      "Epoch: 0, Batch: 11000, Train NLL: 1.0621, Train Acc:0.5000\n",
      "Epoch: 0, Batch: 12000, Train NLL: 0.8859, Train Acc:0.6875\n",
      "Epoch: 0, Batch: 13000, Train NLL: 0.9469, Train Acc:0.5625\n",
      "Epoch: 0, Batch: 14000, Train NLL: 0.9406, Train Acc:0.5938\n",
      "Epoch: 0, Batch: 15000, Train NLL: 1.0251, Train Acc:0.5000\n",
      "Epoch: 0, Batch: 16000, Train NLL: 1.0496, Train Acc:0.5938\n",
      "Epoch: 0, Batch: 17000, Train NLL: 1.0236, Train Acc:0.6250\n",
      "Epoch: 0, Val NLL: 1.0070, Val Acc: 0.4933\n",
      "LR = 0.05\n",
      "WROTE MODEL\n",
      "Epoch: 1, Batch: 0, Train NLL: 0.9991, Train Acc:0.5938\n",
      "Epoch: 1, Batch: 1000, Train NLL: 0.9502, Train Acc:0.4688\n",
      "Epoch: 1, Batch: 2000, Train NLL: 1.0046, Train Acc:0.4062\n",
      "Epoch: 1, Batch: 3000, Train NLL: 0.9247, Train Acc:0.5938\n",
      "Epoch: 1, Batch: 4000, Train NLL: 0.9926, Train Acc:0.5625\n",
      "Epoch: 1, Batch: 5000, Train NLL: 1.0302, Train Acc:0.4688\n",
      "Epoch: 1, Batch: 6000, Train NLL: 1.0771, Train Acc:0.4375\n",
      "Epoch: 1, Batch: 7000, Train NLL: 0.9976, Train Acc:0.4688\n",
      "Epoch: 1, Batch: 8000, Train NLL: 1.1144, Train Acc:0.4062\n",
      "Epoch: 1, Batch: 9000, Train NLL: 0.9178, Train Acc:0.5000\n",
      "Epoch: 1, Batch: 10000, Train NLL: 0.9760, Train Acc:0.5000\n",
      "Epoch: 1, Batch: 11000, Train NLL: 0.9370, Train Acc:0.5938\n",
      "Epoch: 1, Batch: 12000, Train NLL: 0.9333, Train Acc:0.5312\n",
      "Epoch: 1, Batch: 13000, Train NLL: 0.8210, Train Acc:0.6875\n",
      "Epoch: 1, Batch: 14000, Train NLL: 0.9311, Train Acc:0.5312\n",
      "Epoch: 1, Batch: 15000, Train NLL: 1.0051, Train Acc:0.5625\n",
      "Epoch: 1, Batch: 16000, Train NLL: 0.9035, Train Acc:0.5625\n",
      "Epoch: 1, Batch: 17000, Train NLL: 0.9294, Train Acc:0.5000\n",
      "Epoch: 1, Val NLL: 0.9165, Val Acc: 0.5753\n",
      "LR = 0.05\n",
      "WROTE MODEL\n",
      "Epoch: 2, Batch: 0, Train NLL: 1.0759, Train Acc:0.4062\n",
      "Epoch: 2, Batch: 1000, Train NLL: 1.0232, Train Acc:0.4062\n",
      "Epoch: 2, Batch: 2000, Train NLL: 0.9682, Train Acc:0.4688\n",
      "Epoch: 2, Batch: 3000, Train NLL: 0.8408, Train Acc:0.6250\n",
      "Epoch: 2, Batch: 4000, Train NLL: 1.0050, Train Acc:0.4062\n",
      "Epoch: 2, Batch: 5000, Train NLL: 0.8849, Train Acc:0.5938\n",
      "Epoch: 2, Batch: 6000, Train NLL: 0.9080, Train Acc:0.5625\n",
      "Epoch: 2, Batch: 7000, Train NLL: 1.0692, Train Acc:0.3125\n",
      "Epoch: 2, Batch: 8000, Train NLL: 1.0081, Train Acc:0.5938\n",
      "Epoch: 2, Batch: 9000, Train NLL: 0.9357, Train Acc:0.5938\n",
      "Epoch: 2, Batch: 10000, Train NLL: 0.9401, Train Acc:0.5625\n",
      "Epoch: 2, Batch: 11000, Train NLL: 0.9256, Train Acc:0.4375\n",
      "Epoch: 2, Batch: 12000, Train NLL: 0.7458, Train Acc:0.7812\n",
      "Epoch: 2, Batch: 13000, Train NLL: 0.9209, Train Acc:0.6250\n",
      "Epoch: 2, Batch: 14000, Train NLL: 0.9201, Train Acc:0.5938\n",
      "Epoch: 2, Batch: 15000, Train NLL: 0.7756, Train Acc:0.7188\n",
      "Epoch: 2, Batch: 16000, Train NLL: 0.9705, Train Acc:0.4062\n",
      "Epoch: 2, Batch: 17000, Train NLL: 0.8999, Train Acc:0.6250\n",
      "Epoch: 2, Val NLL: 0.8791, Val Acc: 0.6030\n",
      "LR = 0.05\n",
      "WROTE MODEL\n",
      "Epoch: 3, Batch: 0, Train NLL: 0.9460, Train Acc:0.6250\n",
      "Epoch: 3, Batch: 1000, Train NLL: 0.9357, Train Acc:0.5000\n",
      "Epoch: 3, Batch: 2000, Train NLL: 0.8311, Train Acc:0.5938\n",
      "Epoch: 3, Batch: 3000, Train NLL: 0.7010, Train Acc:0.7500\n",
      "Epoch: 3, Batch: 4000, Train NLL: 0.8246, Train Acc:0.6250\n",
      "Epoch: 3, Batch: 5000, Train NLL: 0.9863, Train Acc:0.5938\n",
      "Epoch: 3, Batch: 6000, Train NLL: 0.8505, Train Acc:0.7188\n",
      "Epoch: 3, Batch: 7000, Train NLL: 0.9791, Train Acc:0.4375\n",
      "Epoch: 3, Batch: 8000, Train NLL: 0.8208, Train Acc:0.5938\n",
      "Epoch: 3, Batch: 9000, Train NLL: 0.9043, Train Acc:0.5625\n",
      "Epoch: 3, Batch: 10000, Train NLL: 0.8521, Train Acc:0.6250\n",
      "Epoch: 3, Batch: 11000, Train NLL: 0.6986, Train Acc:0.6875\n",
      "Epoch: 3, Batch: 12000, Train NLL: 0.9024, Train Acc:0.4688\n",
      "Epoch: 3, Batch: 13000, Train NLL: 0.9352, Train Acc:0.6875\n",
      "Epoch: 3, Batch: 14000, Train NLL: 0.8195, Train Acc:0.6562\n",
      "Epoch: 3, Batch: 15000, Train NLL: 0.8393, Train Acc:0.6250\n",
      "Epoch: 3, Batch: 16000, Train NLL: 0.8381, Train Acc:0.5938\n",
      "Epoch: 3, Batch: 17000, Train NLL: 0.8174, Train Acc:0.6250\n",
      "Epoch: 3, Val NLL: 0.8650, Val Acc: 0.6128\n",
      "LR = 0.05\n",
      "WROTE MODEL\n",
      "Epoch: 4, Batch: 0, Train NLL: 0.8172, Train Acc:0.6250\n",
      "Epoch: 4, Batch: 1000, Train NLL: 0.9396, Train Acc:0.5000\n",
      "Epoch: 4, Batch: 2000, Train NLL: 0.8705, Train Acc:0.6250\n",
      "Epoch: 4, Batch: 3000, Train NLL: 0.8692, Train Acc:0.5938\n",
      "Epoch: 4, Batch: 4000, Train NLL: 1.1814, Train Acc:0.3750\n",
      "Epoch: 4, Batch: 5000, Train NLL: 0.8422, Train Acc:0.6875\n",
      "Epoch: 4, Batch: 6000, Train NLL: 0.7316, Train Acc:0.6250\n",
      "Epoch: 4, Batch: 7000, Train NLL: 0.8117, Train Acc:0.6250\n",
      "Epoch: 4, Batch: 8000, Train NLL: 0.6323, Train Acc:0.7812\n",
      "Epoch: 4, Batch: 9000, Train NLL: 0.8889, Train Acc:0.6875\n",
      "Epoch: 4, Batch: 10000, Train NLL: 0.7264, Train Acc:0.6562\n",
      "Epoch: 4, Batch: 11000, Train NLL: 0.8954, Train Acc:0.5938\n",
      "Epoch: 4, Batch: 12000, Train NLL: 0.6404, Train Acc:0.8125\n",
      "Epoch: 4, Batch: 13000, Train NLL: 0.8846, Train Acc:0.5938\n",
      "Epoch: 4, Batch: 14000, Train NLL: 0.8827, Train Acc:0.6250\n",
      "Epoch: 4, Batch: 15000, Train NLL: 0.6197, Train Acc:0.8438\n",
      "Epoch: 4, Batch: 16000, Train NLL: 0.9880, Train Acc:0.6875\n",
      "Epoch: 4, Batch: 17000, Train NLL: 0.9441, Train Acc:0.5312\n",
      "Epoch: 4, Val NLL: 0.8370, Val Acc: 0.6265\n",
      "LR = 0.05\n",
      "WROTE MODEL\n",
      "Epoch: 5, Batch: 0, Train NLL: 1.0140, Train Acc:0.5000\n",
      "Epoch: 5, Batch: 1000, Train NLL: 0.6605, Train Acc:0.7500\n",
      "Epoch: 5, Batch: 2000, Train NLL: 0.8421, Train Acc:0.5625\n",
      "Epoch: 5, Batch: 3000, Train NLL: 0.7680, Train Acc:0.6875\n",
      "Epoch: 5, Batch: 4000, Train NLL: 0.9910, Train Acc:0.4688\n",
      "Epoch: 5, Batch: 5000, Train NLL: 0.7634, Train Acc:0.6562\n",
      "Epoch: 5, Batch: 6000, Train NLL: 0.9281, Train Acc:0.5312\n",
      "Epoch: 5, Batch: 7000, Train NLL: 0.8614, Train Acc:0.7500\n",
      "Epoch: 5, Batch: 8000, Train NLL: 0.7757, Train Acc:0.6875\n",
      "Epoch: 5, Batch: 9000, Train NLL: 0.9029, Train Acc:0.6562\n",
      "Epoch: 5, Batch: 10000, Train NLL: 0.7814, Train Acc:0.7188\n",
      "Epoch: 5, Batch: 11000, Train NLL: 0.8891, Train Acc:0.6250\n",
      "Epoch: 5, Batch: 12000, Train NLL: 0.8590, Train Acc:0.5625\n",
      "Epoch: 5, Batch: 13000, Train NLL: 0.7851, Train Acc:0.5938\n",
      "Epoch: 5, Batch: 14000, Train NLL: 0.8460, Train Acc:0.5938\n",
      "Epoch: 5, Batch: 15000, Train NLL: 0.9604, Train Acc:0.4688\n",
      "Epoch: 5, Batch: 16000, Train NLL: 0.7288, Train Acc:0.6250\n",
      "Epoch: 5, Batch: 17000, Train NLL: 0.9168, Train Acc:0.6562\n",
      "Epoch: 5, Val NLL: 0.8133, Val Acc: 0.6417\n",
      "LR = 0.05\n",
      "WROTE MODEL\n",
      "Epoch: 6, Batch: 0, Train NLL: 0.9528, Train Acc:0.6250\n",
      "Epoch: 6, Batch: 1000, Train NLL: 0.8062, Train Acc:0.6562\n",
      "Epoch: 6, Batch: 2000, Train NLL: 0.7718, Train Acc:0.6250\n",
      "Epoch: 6, Batch: 3000, Train NLL: 0.7017, Train Acc:0.7500\n",
      "Epoch: 6, Batch: 4000, Train NLL: 0.9330, Train Acc:0.6250\n",
      "Epoch: 6, Batch: 5000, Train NLL: 0.7719, Train Acc:0.7188\n",
      "Epoch: 6, Batch: 6000, Train NLL: 0.8027, Train Acc:0.6562\n",
      "Epoch: 6, Batch: 7000, Train NLL: 0.8565, Train Acc:0.6250\n",
      "Epoch: 6, Batch: 8000, Train NLL: 0.7322, Train Acc:0.6562\n",
      "Epoch: 6, Batch: 9000, Train NLL: 0.7646, Train Acc:0.7500\n",
      "Epoch: 6, Batch: 10000, Train NLL: 0.9664, Train Acc:0.5625\n",
      "Epoch: 6, Batch: 11000, Train NLL: 0.6087, Train Acc:0.7812\n",
      "Epoch: 6, Batch: 12000, Train NLL: 0.7261, Train Acc:0.7188\n",
      "Epoch: 6, Batch: 13000, Train NLL: 0.8835, Train Acc:0.5938\n",
      "Epoch: 6, Batch: 14000, Train NLL: 1.0150, Train Acc:0.4375\n",
      "Epoch: 6, Batch: 15000, Train NLL: 0.8161, Train Acc:0.7188\n",
      "Epoch: 6, Batch: 16000, Train NLL: 1.0080, Train Acc:0.5312\n",
      "Epoch: 6, Batch: 17000, Train NLL: 0.7302, Train Acc:0.6875\n",
      "Epoch: 6, Val NLL: 0.7946, Val Acc: 0.6515\n",
      "LR = 0.05\n",
      "WROTE MODEL\n",
      "Epoch: 7, Batch: 0, Train NLL: 0.7889, Train Acc:0.6562\n",
      "Epoch: 7, Batch: 1000, Train NLL: 0.6045, Train Acc:0.7812\n",
      "Epoch: 7, Batch: 2000, Train NLL: 0.7923, Train Acc:0.6562\n",
      "Epoch: 7, Batch: 3000, Train NLL: 0.6918, Train Acc:0.6875\n",
      "Epoch: 7, Batch: 4000, Train NLL: 0.9196, Train Acc:0.5938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Batch: 5000, Train NLL: 0.6991, Train Acc:0.7500\n",
      "Epoch: 7, Batch: 6000, Train NLL: 0.7824, Train Acc:0.6562\n",
      "Epoch: 7, Batch: 7000, Train NLL: 0.6791, Train Acc:0.7500\n",
      "Epoch: 7, Batch: 8000, Train NLL: 0.7781, Train Acc:0.5938\n",
      "Epoch: 7, Batch: 9000, Train NLL: 0.8550, Train Acc:0.6250\n",
      "Epoch: 7, Batch: 10000, Train NLL: 0.8553, Train Acc:0.6250\n",
      "Epoch: 7, Batch: 11000, Train NLL: 0.8018, Train Acc:0.6562\n",
      "Epoch: 7, Batch: 12000, Train NLL: 0.6868, Train Acc:0.7188\n",
      "Epoch: 7, Batch: 13000, Train NLL: 0.6601, Train Acc:0.7500\n",
      "Epoch: 7, Batch: 14000, Train NLL: 0.7282, Train Acc:0.7188\n",
      "Epoch: 7, Batch: 15000, Train NLL: 0.6236, Train Acc:0.8125\n",
      "Epoch: 7, Batch: 16000, Train NLL: 0.7829, Train Acc:0.6875\n",
      "Epoch: 7, Batch: 17000, Train NLL: 0.8425, Train Acc:0.5938\n",
      "Epoch: 7, Val NLL: 0.7781, Val Acc: 0.6607\n",
      "LR = 0.05\n",
      "WROTE MODEL\n",
      "Epoch: 8, Batch: 0, Train NLL: 0.6710, Train Acc:0.7812\n",
      "Epoch: 8, Batch: 1000, Train NLL: 0.7644, Train Acc:0.6562\n",
      "Epoch: 8, Batch: 2000, Train NLL: 0.7023, Train Acc:0.6875\n",
      "Epoch: 8, Batch: 3000, Train NLL: 0.7456, Train Acc:0.6562\n",
      "Epoch: 8, Batch: 4000, Train NLL: 0.6432, Train Acc:0.7500\n",
      "Epoch: 8, Batch: 5000, Train NLL: 0.9001, Train Acc:0.5625\n",
      "Epoch: 8, Batch: 6000, Train NLL: 0.7829, Train Acc:0.6250\n",
      "Epoch: 8, Batch: 7000, Train NLL: 0.7315, Train Acc:0.6875\n",
      "Epoch: 8, Batch: 8000, Train NLL: 1.0452, Train Acc:0.5938\n",
      "Epoch: 8, Batch: 9000, Train NLL: 0.8026, Train Acc:0.6875\n",
      "Epoch: 8, Batch: 10000, Train NLL: 0.8451, Train Acc:0.6250\n",
      "Epoch: 8, Batch: 11000, Train NLL: 0.6559, Train Acc:0.7188\n",
      "Epoch: 8, Batch: 12000, Train NLL: 0.8434, Train Acc:0.6562\n",
      "Epoch: 8, Batch: 13000, Train NLL: 0.7325, Train Acc:0.6562\n",
      "Epoch: 8, Batch: 14000, Train NLL: 0.7013, Train Acc:0.7188\n",
      "Epoch: 8, Batch: 15000, Train NLL: 0.6672, Train Acc:0.7812\n",
      "Epoch: 8, Batch: 16000, Train NLL: 0.8777, Train Acc:0.5625\n",
      "Epoch: 8, Batch: 17000, Train NLL: 0.8751, Train Acc:0.5312\n",
      "Epoch: 8, Val NLL: 0.7617, Val Acc: 0.6721\n",
      "LR = 0.05\n",
      "WROTE MODEL\n",
      "Epoch: 9, Batch: 0, Train NLL: 0.6889, Train Acc:0.7500\n",
      "Epoch: 9, Batch: 1000, Train NLL: 0.9054, Train Acc:0.5625\n",
      "Epoch: 9, Batch: 2000, Train NLL: 0.8326, Train Acc:0.6875\n",
      "Epoch: 9, Batch: 3000, Train NLL: 0.8816, Train Acc:0.7188\n",
      "Epoch: 9, Batch: 4000, Train NLL: 0.7555, Train Acc:0.7188\n",
      "Epoch: 9, Batch: 5000, Train NLL: 0.7302, Train Acc:0.7500\n",
      "Epoch: 9, Batch: 6000, Train NLL: 0.8132, Train Acc:0.6875\n",
      "Epoch: 9, Batch: 7000, Train NLL: 0.8449, Train Acc:0.6250\n",
      "Epoch: 9, Batch: 8000, Train NLL: 0.7199, Train Acc:0.7500\n",
      "Epoch: 9, Batch: 9000, Train NLL: 0.7530, Train Acc:0.8125\n",
      "Epoch: 9, Batch: 10000, Train NLL: 0.8375, Train Acc:0.6562\n",
      "Epoch: 9, Batch: 11000, Train NLL: 0.6092, Train Acc:0.8125\n",
      "Epoch: 9, Batch: 12000, Train NLL: 0.8681, Train Acc:0.6250\n",
      "Epoch: 9, Batch: 13000, Train NLL: 0.7080, Train Acc:0.7812\n",
      "Epoch: 9, Batch: 14000, Train NLL: 0.7841, Train Acc:0.6875\n",
      "Epoch: 9, Batch: 15000, Train NLL: 0.8886, Train Acc:0.5625\n",
      "Epoch: 9, Batch: 16000, Train NLL: 0.9768, Train Acc:0.6250\n",
      "Epoch: 9, Batch: 17000, Train NLL: 0.6818, Train Acc:0.6875\n",
      "Epoch: 9, Val NLL: 0.7470, Val Acc: 0.6831\n",
      "LR = 0.05\n",
      "WROTE MODEL\n",
      "Epoch: 10, Batch: 0, Train NLL: 0.7305, Train Acc:0.6250\n",
      "Epoch: 10, Batch: 1000, Train NLL: 0.6655, Train Acc:0.6562\n",
      "Epoch: 10, Batch: 2000, Train NLL: 0.6713, Train Acc:0.8125\n",
      "Epoch: 10, Batch: 3000, Train NLL: 0.7329, Train Acc:0.5938\n",
      "Epoch: 10, Batch: 4000, Train NLL: 0.5961, Train Acc:0.8438\n",
      "Epoch: 10, Batch: 5000, Train NLL: 0.6407, Train Acc:0.6875\n",
      "Epoch: 10, Batch: 6000, Train NLL: 0.8351, Train Acc:0.6250\n",
      "Epoch: 10, Batch: 7000, Train NLL: 0.7199, Train Acc:0.7500\n",
      "Epoch: 10, Batch: 8000, Train NLL: 0.8089, Train Acc:0.6875\n",
      "Epoch: 10, Batch: 9000, Train NLL: 0.9643, Train Acc:0.5000\n",
      "Epoch: 10, Batch: 10000, Train NLL: 0.6600, Train Acc:0.7500\n",
      "Epoch: 10, Batch: 11000, Train NLL: 0.7934, Train Acc:0.6875\n",
      "Epoch: 10, Batch: 12000, Train NLL: 0.9002, Train Acc:0.6562\n",
      "Epoch: 10, Batch: 13000, Train NLL: 0.6970, Train Acc:0.6562\n",
      "Epoch: 10, Batch: 14000, Train NLL: 0.6757, Train Acc:0.7188\n",
      "Epoch: 10, Batch: 15000, Train NLL: 0.5828, Train Acc:0.8438\n",
      "Epoch: 10, Batch: 16000, Train NLL: 0.7702, Train Acc:0.5312\n",
      "Epoch: 10, Batch: 17000, Train NLL: 0.4988, Train Acc:0.9062\n",
      "Epoch: 10, Val NLL: 0.7368, Val Acc: 0.6882\n",
      "LR = 0.05\n",
      "WROTE MODEL\n",
      "Epoch: 11, Batch: 0, Train NLL: 0.8156, Train Acc:0.7188\n",
      "Epoch: 11, Batch: 1000, Train NLL: 0.8763, Train Acc:0.5938\n",
      "Epoch: 11, Batch: 2000, Train NLL: 0.8297, Train Acc:0.5625\n",
      "Epoch: 11, Batch: 3000, Train NLL: 0.6810, Train Acc:0.7188\n",
      "Epoch: 11, Batch: 4000, Train NLL: 0.8249, Train Acc:0.7500\n",
      "Epoch: 11, Batch: 5000, Train NLL: 0.8225, Train Acc:0.5312\n",
      "Epoch: 11, Batch: 6000, Train NLL: 0.5159, Train Acc:0.8438\n",
      "Epoch: 11, Batch: 7000, Train NLL: 0.6073, Train Acc:0.7500\n",
      "Epoch: 11, Batch: 8000, Train NLL: 0.8377, Train Acc:0.5625\n",
      "Epoch: 11, Batch: 9000, Train NLL: 0.9226, Train Acc:0.5625\n",
      "Epoch: 11, Batch: 10000, Train NLL: 0.7110, Train Acc:0.7188\n",
      "Epoch: 11, Batch: 11000, Train NLL: 0.6746, Train Acc:0.7812\n",
      "Epoch: 11, Batch: 12000, Train NLL: 0.6001, Train Acc:0.7188\n",
      "Epoch: 11, Batch: 13000, Train NLL: 0.9034, Train Acc:0.5312\n",
      "Epoch: 11, Batch: 14000, Train NLL: 0.9511, Train Acc:0.6562\n",
      "Epoch: 11, Batch: 15000, Train NLL: 0.6847, Train Acc:0.6875\n",
      "Epoch: 11, Batch: 16000, Train NLL: 0.7662, Train Acc:0.6875\n",
      "Epoch: 11, Batch: 17000, Train NLL: 0.6669, Train Acc:0.7188\n",
      "Epoch: 11, Val NLL: 0.7267, Val Acc: 0.6929\n",
      "LR = 0.05\n",
      "WROTE MODEL\n",
      "Epoch: 12, Batch: 0, Train NLL: 0.7776, Train Acc:0.5938\n",
      "Epoch: 12, Batch: 1000, Train NLL: 0.9104, Train Acc:0.6250\n",
      "Epoch: 12, Batch: 2000, Train NLL: 0.8293, Train Acc:0.5625\n",
      "Epoch: 12, Batch: 3000, Train NLL: 0.7277, Train Acc:0.6562\n",
      "Epoch: 12, Batch: 4000, Train NLL: 0.7733, Train Acc:0.6875\n",
      "Epoch: 12, Batch: 5000, Train NLL: 0.7124, Train Acc:0.7188\n",
      "Epoch: 12, Batch: 6000, Train NLL: 0.8050, Train Acc:0.5938\n",
      "Epoch: 12, Batch: 7000, Train NLL: 0.6130, Train Acc:0.7500\n",
      "Epoch: 12, Batch: 8000, Train NLL: 0.6016, Train Acc:0.7500\n",
      "Epoch: 12, Batch: 9000, Train NLL: 0.6759, Train Acc:0.7500\n",
      "Epoch: 12, Batch: 10000, Train NLL: 0.7105, Train Acc:0.7188\n",
      "Epoch: 12, Batch: 11000, Train NLL: 0.6811, Train Acc:0.6875\n",
      "Epoch: 12, Batch: 12000, Train NLL: 0.6905, Train Acc:0.7500\n",
      "Epoch: 12, Batch: 13000, Train NLL: 0.7518, Train Acc:0.6562\n",
      "Epoch: 12, Batch: 14000, Train NLL: 0.9538, Train Acc:0.5312\n",
      "Epoch: 12, Batch: 15000, Train NLL: 0.9137, Train Acc:0.6250\n",
      "Epoch: 12, Batch: 16000, Train NLL: 0.6608, Train Acc:0.6875\n",
      "Epoch: 12, Batch: 17000, Train NLL: 0.6992, Train Acc:0.7188\n",
      "Epoch: 12, Val NLL: 0.7155, Val Acc: 0.6982\n",
      "LR = 0.05\n",
      "WROTE MODEL\n",
      "Epoch: 13, Batch: 0, Train NLL: 0.7737, Train Acc:0.7188\n",
      "Epoch: 13, Batch: 1000, Train NLL: 0.4061, Train Acc:0.8750\n",
      "Epoch: 13, Batch: 2000, Train NLL: 0.7889, Train Acc:0.6250\n",
      "Epoch: 13, Batch: 3000, Train NLL: 0.6644, Train Acc:0.6562\n",
      "Epoch: 13, Batch: 4000, Train NLL: 0.6308, Train Acc:0.7188\n",
      "Epoch: 13, Batch: 5000, Train NLL: 0.6543, Train Acc:0.7500\n",
      "Epoch: 13, Batch: 6000, Train NLL: 0.7213, Train Acc:0.6875\n",
      "Epoch: 13, Batch: 7000, Train NLL: 0.7180, Train Acc:0.7500\n",
      "Epoch: 13, Batch: 8000, Train NLL: 0.7525, Train Acc:0.7188\n",
      "Epoch: 13, Batch: 9000, Train NLL: 0.5857, Train Acc:0.7500\n",
      "Epoch: 13, Batch: 10000, Train NLL: 0.6035, Train Acc:0.7188\n",
      "Epoch: 13, Batch: 11000, Train NLL: 0.6779, Train Acc:0.7188\n",
      "Epoch: 13, Batch: 12000, Train NLL: 0.4668, Train Acc:0.8125\n",
      "Epoch: 13, Batch: 13000, Train NLL: 0.7717, Train Acc:0.5938\n",
      "Epoch: 13, Batch: 14000, Train NLL: 0.8512, Train Acc:0.5000\n",
      "Epoch: 13, Batch: 15000, Train NLL: 0.7429, Train Acc:0.6562\n",
      "Epoch: 13, Batch: 16000, Train NLL: 0.7727, Train Acc:0.6562\n",
      "Epoch: 13, Batch: 17000, Train NLL: 0.8618, Train Acc:0.5938\n",
      "Epoch: 13, Val NLL: 0.7033, Val Acc: 0.7070\n",
      "LR = 0.05\n",
      "WROTE MODEL\n",
      "Epoch: 14, Batch: 0, Train NLL: 0.8608, Train Acc:0.5938\n",
      "Epoch: 14, Batch: 1000, Train NLL: 0.8212, Train Acc:0.6875\n",
      "Epoch: 14, Batch: 2000, Train NLL: 0.6463, Train Acc:0.7188\n",
      "Epoch: 14, Batch: 3000, Train NLL: 0.7709, Train Acc:0.6250\n",
      "Epoch: 14, Batch: 4000, Train NLL: 0.7787, Train Acc:0.6875\n",
      "Epoch: 14, Batch: 5000, Train NLL: 0.8687, Train Acc:0.5938\n",
      "Epoch: 14, Batch: 6000, Train NLL: 0.5975, Train Acc:0.7500\n",
      "Epoch: 14, Batch: 7000, Train NLL: 0.6846, Train Acc:0.6875\n",
      "Epoch: 14, Batch: 8000, Train NLL: 0.7394, Train Acc:0.7188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Batch: 9000, Train NLL: 0.7441, Train Acc:0.6562\n",
      "Epoch: 14, Batch: 10000, Train NLL: 0.7047, Train Acc:0.7188\n",
      "Epoch: 14, Batch: 11000, Train NLL: 0.7579, Train Acc:0.6250\n",
      "Epoch: 14, Batch: 12000, Train NLL: 0.9016, Train Acc:0.6562\n",
      "Epoch: 14, Batch: 13000, Train NLL: 0.6455, Train Acc:0.6562\n",
      "Epoch: 14, Batch: 14000, Train NLL: 0.5783, Train Acc:0.8125\n",
      "Epoch: 14, Batch: 15000, Train NLL: 0.7429, Train Acc:0.6250\n",
      "Epoch: 14, Batch: 16000, Train NLL: 0.7772, Train Acc:0.6250\n",
      "Epoch: 14, Batch: 17000, Train NLL: 0.4943, Train Acc:0.8750\n",
      "Epoch: 14, Val NLL: 0.6915, Val Acc: 0.7112\n",
      "LR = 0.05\n",
      "WROTE MODEL\n",
      "Epoch: 15, Batch: 0, Train NLL: 0.6751, Train Acc:0.7188\n",
      "Epoch: 15, Batch: 1000, Train NLL: 0.7078, Train Acc:0.7500\n",
      "Epoch: 15, Batch: 2000, Train NLL: 0.8742, Train Acc:0.5938\n",
      "Epoch: 15, Batch: 3000, Train NLL: 0.6082, Train Acc:0.8125\n",
      "Epoch: 15, Batch: 4000, Train NLL: 0.7054, Train Acc:0.6562\n"
     ]
    }
   ],
   "source": [
    "best_loss = 1e8\n",
    "K = len(networks)\n",
    "\n",
    "for e in range(100):\n",
    "    training_loop(e, train_iter, networks, criterion, optimizer)\n",
    "    loss = validation_loop(e, val_iter, networks, criterion)\n",
    "    scheduler.step(loss)\n",
    "    print('LR = {}'.format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "    if loss < best_loss:\n",
    "        for c in range(K):\n",
    "            torch.save(networks[c].state_dict(), ''.join(('best_FFA_net', str(c), '.pt')))\n",
    "        best_loss = loss\n",
    "        print('WROTE MODEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
