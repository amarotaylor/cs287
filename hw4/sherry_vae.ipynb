{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 287 - HW 4 - VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "from namedtensor import ntorch, NamedTensor\n",
    "from namedtensor.text import NamedField\n",
    "\n",
    "from common import *\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train) 549367\n",
      "len(TEXT.vocab) 62998\n",
      "len(LABEL.vocab) 4\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "TEXT = NamedField(names=('seqlen',)) # Our input $x$\n",
    "LABEL = NamedField(sequential=False, names=()) # Our labels $y$\n",
    "train, val, test = torchtext.datasets.SNLI.splits(TEXT, LABEL)\n",
    "print('len(train)', len(train))\n",
    "TEXT.build_vocab(train)\n",
    "LABEL.build_vocab(train)\n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
    "print('len(LABEL.vocab)', len(LABEL.vocab))\n",
    "train_iter, val_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
    "    (train, val, test), batch_size=128, device=torch.device(\"cuda\"), repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word embeddings shape: OrderedDict([('word', 62998), ('embedding', 300)])\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary with word embeddings\n",
    "# out-of-vocabulary words are hashed to one of 100 random embeddings each initialized to mean 0, stdev 1 (Sec 5.1)\n",
    "unk_vectors = [torch.randn(300) for _ in range(100)]\n",
    "TEXT.vocab.load_vectors(vectors='glove.6B.300d', unk_init=lambda x:random.choice(unk_vectors))\n",
    "vectors = TEXT.vocab.vectors\n",
    "vectors = vectors / vectors.norm(dim=1, keepdim=True) # normalized to have l_2 norm of 1\n",
    "vectors = NamedTensor(vectors, ('word', 'embedding'))\n",
    "TEXT.vocab.vectors = vectors\n",
    "print(\"word embeddings shape:\", TEXT.vocab.vectors.shape)\n",
    "weights = TEXT.vocab.vectors.values.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of premise batch: OrderedDict([('seqlen', 35), ('batch', 128)])\n",
      "Size of hypothesis batch: OrderedDict([('seqlen', 22), ('batch', 128)])\n",
      "Size of label batch: OrderedDict([('batch', 128)])\n"
     ]
    }
   ],
   "source": [
    "# here's an example of a training example\n",
    "batch = next(iter(train_iter))\n",
    "print(\"Size of premise batch:\", batch.premise.shape)\n",
    "print(\"Size of hypothesis batch:\", batch.hypothesis.shape)\n",
    "print(\"Size of label batch:\", batch.label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMS - input: 62998, embed: 300, hidden1: 200, output: 4\n"
     ]
    }
   ],
   "source": [
    "# dimensions\n",
    "input_size = TEXT.vocab.vectors.shape['word']\n",
    "embed_size = TEXT.vocab.vectors.shape['embedding']\n",
    "hidden_size1 = 200\n",
    "output_size = len(LABEL.vocab)\n",
    "print('DIMS - input: %d, embed: %d, hidden1: %d, output: %d'%(input_size, embed_size, hidden_size1, output_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Variable Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Network(torch.nn.Module):\n",
    "    def __init__(self, embed_size, output_size, weights, networks):\n",
    "        super(Q_Network, self).__init__()\n",
    "        self.embed_size = embed_size # HIDDEN = embed_size\n",
    "        self.K = len(networks)\n",
    "        self.output_size = output_size\n",
    "        self.weights = weights\n",
    "        self.networks = networks\n",
    "        \n",
    "        self.embed = nn.Embedding.from_pretrained(self.weights, freeze=True)\n",
    "        self.linear = nn.Linear(self.embed_size * 2, self.K, bias=False)\n",
    "        \n",
    "        #self.lmb = self.linear.weight\n",
    "        self.m = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "        self.probs = torch.tensor(1/self.K, device='cuda').repeat(self.K)\n",
    "        self.prior = torch.distributions.categorical.Categorical(probs=self.probs)\n",
    "        \n",
    "    def mask(self, sent1, sent2, proj1, proj2, pad_tkn=1):\n",
    "        mask1 = (sent1 == pad_tkn) # BATCH x SEQLEN\n",
    "        mask2 = (sent2 == pad_tkn)\n",
    "        mask1a = mask1.unsqueeze(2).expand(-1, -1, self.embed_size).float() # BATCH x SEQLEN x HIDDEN\n",
    "        mask2a = mask2.unsqueeze(2).expand(-1, -1, self.embed_size).float()\n",
    "        score1 = proj1 * (1 - mask1a) # BATCH x SEQLEN x HIDDEN\n",
    "        score2 = proj2 * (1 - mask2a)\n",
    "        return score1, score2\n",
    "    \n",
    "    def forward(self, sent1, sent2, pad_tkn = 1):\n",
    "        proj1 = self.embed(sent1) # BATCH x SEQLEN x HIDDEN\n",
    "        proj2 = self.embed(sent2)\n",
    "        score1, score2 = self.mask(sent1, sent2, proj1, proj2) # BATCH x SEQLEN x HIDDEN\n",
    "        score1_sum = torch.mean(score1, dim=1) # BATCH x HIDDEN\n",
    "        score2_sum = torch.mean(score2, dim=1)\n",
    "        score_all = torch.cat((score1_sum, score2_sum), dim=1) # BATCH x HIDDEN*2\n",
    "        output = self.m(self.linear(score_all)) # BATCH x K\n",
    "        self.output = output\n",
    "        self.q = torch.distributions.categorical.Categorical(logits=self.output)\n",
    "        return output\n",
    "    \n",
    "    def run_networks(self, sent1, sent2):\n",
    "        y_hats = torch.zeros((self.K, sent1.shape[0], self.output_size), device='cuda')\n",
    "        for c in range(self.K):\n",
    "            net = self.networks[c]\n",
    "            y_hat = net(sent1, sent2)\n",
    "            y_hats[c,:,:] = y_hat\n",
    "        return y_hats\n",
    "    \n",
    "    def get_grad(self, sent1, sent2, y, N=1):\n",
    "        ELBO = torch.zeros(1)\n",
    "        grads = torch.zeros((self.K, self.embed_size * 2, N), device='cuda')\n",
    "        y_hats = self.run_networks(sent1, sent2)\n",
    "        for i in range(N):      \n",
    "            # zero grads\n",
    "            try:\n",
    "                self.linear.weight.grad.data.zero_()\n",
    "            except AttributeError:\n",
    "                pass\n",
    "            # forward pass, sample\n",
    "            _ = self.forward(sent1, sent2)\n",
    "            c = self.q.sample()\n",
    "            y_hat = torch.stack([torch.index_select(y_hats[:,i,:],0,c[i]) for i in range(sent1.shape[0])]).squeeze()\n",
    "            # calc ELBO, dELBO for the sample\n",
    "            logq = self.q.log_prob(c).sum() \n",
    "            logq.backward()\n",
    "            nll = self.criterion(y_hat, y)\n",
    "            logprior = self.prior.log_prob(c).sum()\n",
    "            ELBO_ = -nll + logprior - logq\n",
    "            ELBO.add_(ELBO_)\n",
    "            grads[:,:,i] = self.linear.weight.grad.data.mul(ELBO_)\n",
    "        ELBO.div_(N)\n",
    "        return ELBO, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_training_loop(e, train_iter, test_net, networks, eta = 1e-2):\n",
    "    test_net.train()\n",
    "    for net in networks:\n",
    "        net.eval()\n",
    "    ELBOs = []\n",
    "    \n",
    "    for ix,batch in enumerate(train_iter):\n",
    "        sent1 = prepend_null(batch.premise.values.transpose(0,1))\n",
    "        sent2 = prepend_null(batch.hypothesis.values.transpose(0,1))\n",
    "        target = batch.label.values\n",
    "        ELBO, grads = test_net.get_grad(sent1, sent2, target, N=10) # get grad\n",
    "        ELBOs.extend(ELBO.detach().cpu())\n",
    "        test_net.linear.weight.data = test_net.linear.weight.data + (eta * grads.mean(dim=-1)) # step\n",
    "        \n",
    "        if ix % 1 == 0:\n",
    "            output = torch.zeros((K, sent1.shape[0], output_size), device='cuda')\n",
    "            for c in range(K):\n",
    "                network = networks[c]\n",
    "                output[c,:,:] = F.log_softmax(network(sent1, sent2), dim=1) + test_net.output[:,c].unsqueeze(1) # K x BATCH x OUTPUT_SIZE\n",
    "            output = torch.logsumexp(output, dim=0)\n",
    "            loss = test_net.criterion(output, target).item()\n",
    "            sent = sent1.shape[0]\n",
    "            correct = torch.sum(torch.argmax(output, dim=1) == target).item()\n",
    "            print('Epoch: {0}, Batch: {1}, Train NLL: {2:0.4f}, Train Acc: {3:0.4f}, ELBO: {4:0.4f}, Grad: {5}'.format(e, ix, loss/sent, correct/sent, ELBO.detach().cpu().item(), grads.mean(dim=-1).sum().detach().cpu().item()))\n",
    "            print(test_net.output)\n",
    "    return ELBOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_validation_loop(e, val_iter, test_net, networks):\n",
    "    test_net.eval()\n",
    "    for net in networks:\n",
    "        net.eval()\n",
    "    K = len(networks)\n",
    "    total_loss = 0\n",
    "    total_sent = 0\n",
    "    total_correct = 0\n",
    "    criterion = nn.NLLLoss(reduction='sum')\n",
    "    \n",
    "    for ix,batch in enumerate(val_iter):\n",
    "        sent1 = prepend_null(batch.premise.values.transpose(0,1))\n",
    "        sent2 = prepend_null(batch.hypothesis.values.transpose(0,1))\n",
    "        target = batch.label.values\n",
    "        \n",
    "        sent = sent1.shape[0]\n",
    "        if sent == 128:\n",
    "            output = torch.zeros((K, sent1.shape[0], output_size), device='cuda')\n",
    "            for c in range(K):\n",
    "                network = networks[c]\n",
    "                output[c,:,:] = F.log_softmax(network(sent1, sent2), dim=1) + test_net.output[:,c].unsqueeze(1) # K x BATCH x OUTPUT_SIZE\n",
    "            output = torch.logsumexp(output, dim=0)\n",
    "            loss = criterion(output, target).item()\n",
    "            correct = torch.sum(torch.argmax(output, dim=1) == target).item()\n",
    "\n",
    "            total_loss += loss\n",
    "            total_sent += sent\n",
    "            total_correct += correct\n",
    "    \n",
    "    print('Epoch: {0}, Val NLL: {1:0.4f}, Val Acc: {2:0.4f}'.format(e, total_loss/total_sent, total_correct/total_sent))\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFA_net1 = Decomposable_Attn_Network(input_size, embed_size, hidden_size1, output_size, weights).cuda()\n",
    "state_dict1 = torch.load('best_FFA_net0.pt')\n",
    "FFA_net1.load_state_dict(state_dict1)\n",
    "\n",
    "FFA_net2 = Decomposable_Attn_Network(input_size, embed_size, hidden_size1, output_size, weights).cuda()\n",
    "state_dict2 = torch.load('best_FFA_net1.pt')\n",
    "FFA_net2.load_state_dict(state_dict2)\n",
    "\n",
    "FFA_net3 = Decomposable_Attn_Network(input_size, embed_size, hidden_size1, output_size, weights).cuda()\n",
    "state_dict3 = torch.load('best_FFA_net2.pt')\n",
    "FFA_net3.load_state_dict(state_dict3)\n",
    "\n",
    "networks = [FFA_net1, FFA_net2, FFA_net3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_elbo = -1e32\n",
    "K = len(networks)\n",
    "test_net = Q_Network(embed_size, output_size, weights, networks).cuda()\n",
    "all_elbos = []\n",
    "for e in range(1):\n",
    "    elbos = mix_training_loop(e, train_iter, test_net, networks)\n",
    "    all_elbos.extend(elbos)\n",
    "    val_loss = mix_validation_loop(e, val_iter, test_net, networks)\n",
    "    if elbos[-1] > best_elbo:\n",
    "        torch.save(test_net.state_dict(), 'best_q.pt')\n",
    "        best_elbo = elbos[-1]\n",
    "        print('WROTE MODEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Val NLL: 0.5837, Val Acc: 0.7606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5678.614990234375"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix_validation_loop(e, val_iter, test_net, networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 0, Val NLL: 0.7333, Val Acc: 0.7114\n",
      "Model: 1, Val NLL: 0.8516, Val Acc: 0.7290\n",
      "Model: 2, Val NLL: 0.8394, Val Acc: 0.7369\n"
     ]
    }
   ],
   "source": [
    "# vanilla validation\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "c = 0\n",
    "for net in networks:\n",
    "    net.eval()\n",
    "    total_loss = 0\n",
    "    total_sent = 0\n",
    "    total_correct = 0\n",
    "    for ix,batch in enumerate(val_iter):\n",
    "        sent1 = prepend_null(batch.premise.values.transpose(0,1))\n",
    "        sent2 = prepend_null(batch.hypothesis.values.transpose(0,1))\n",
    "        target = batch.label.values\n",
    "        output = net(sent1, sent2)\n",
    "        loss = criterion(output, target).item()\n",
    "        sent = sent1.shape[0]\n",
    "        correct = torch.sum(torch.argmax(output, dim=1) == target).item()\n",
    "        total_loss += loss\n",
    "        total_sent += sent\n",
    "        total_correct += correct\n",
    "    print('Model: {0}, Val NLL: {1:0.4f}, Val Acc: {2:0.4f}'.format(c, total_loss/total_sent, total_correct/total_sent))\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Val NLL: 72.5272, Val Acc: 0.7702\n"
     ]
    }
   ],
   "source": [
    "K = len(networks)\n",
    "for network in networks:\n",
    "    network.eval()\n",
    "total_loss = 0\n",
    "total_sent = 0\n",
    "total_correct = 0\n",
    "\n",
    "for ix,batch in enumerate(val_iter):\n",
    "    sent1 = prepend_null(batch.premise.values.transpose(0,1))\n",
    "    sent2 = prepend_null(batch.hypothesis.values.transpose(0,1))\n",
    "    target = batch.label.values\n",
    "\n",
    "    output = torch.zeros((K, sent1.shape[0], output_size), device='cuda')\n",
    "    for c in range(K):\n",
    "        network = networks[c]\n",
    "        output[c,:,:] = F.log_softmax(network(sent1, sent2), dim=1) # K x BATCH x OUTPUT_SIZE\n",
    "    output = torch.logsumexp(output, dim=0) + torch.log(torch.tensor(1/K))\n",
    "\n",
    "    loss = criterion(output, target).item()\n",
    "    sent = sent1.shape[0]\n",
    "    correct = torch.sum(torch.argmax(output, dim=1) == target).item()\n",
    "\n",
    "    total_loss += loss*sent\n",
    "    total_sent += sent\n",
    "    total_correct += correct\n",
    "\n",
    "print('Epoch: {0}, Val NLL: {1:0.4f}, Val Acc: {2:0.4f}'.format(e, total_loss/total_sent, total_correct/total_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
