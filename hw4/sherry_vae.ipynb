{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 287 - HW 4 - VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "from namedtensor import ntorch, NamedTensor\n",
    "from namedtensor.text import NamedField\n",
    "\n",
    "from common import *\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train) 549367\n",
      "len(TEXT.vocab) 62998\n",
      "len(LABEL.vocab) 4\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "TEXT = NamedField(names=('seqlen',)) # Our input $x$\n",
    "LABEL = NamedField(sequential=False, names=()) # Our labels $y$\n",
    "train, val, test = torchtext.datasets.SNLI.splits(TEXT, LABEL)\n",
    "print('len(train)', len(train))\n",
    "TEXT.build_vocab(train)\n",
    "LABEL.build_vocab(train)\n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
    "print('len(LABEL.vocab)', len(LABEL.vocab))\n",
    "train_iter, val_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
    "    (train, val, test), batch_size=32, device=torch.device(\"cuda\"), repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word embeddings shape: OrderedDict([('word', 62998), ('embedding', 300)])\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary with word embeddings\n",
    "# out-of-vocabulary words are hashed to one of 100 random embeddings each initialized to mean 0, stdev 1 (Sec 5.1)\n",
    "unk_vectors = [torch.randn(300) for _ in range(100)]\n",
    "TEXT.vocab.load_vectors(vectors='glove.6B.300d', unk_init=lambda x:random.choice(unk_vectors))\n",
    "vectors = TEXT.vocab.vectors\n",
    "vectors = vectors / vectors.norm(dim=1, keepdim=True) # normalized to have l_2 norm of 1\n",
    "vectors = NamedTensor(vectors, ('word', 'embedding'))\n",
    "TEXT.vocab.vectors = vectors\n",
    "print(\"word embeddings shape:\", TEXT.vocab.vectors.shape)\n",
    "weights = TEXT.vocab.vectors.values.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of premise batch: OrderedDict([('seqlen', 40), ('batch', 128)])\n",
      "Size of hypothesis batch: OrderedDict([('seqlen', 19), ('batch', 128)])\n",
      "Size of label batch: OrderedDict([('batch', 128)])\n"
     ]
    }
   ],
   "source": [
    "# here's an example of a training example\n",
    "batch = next(iter(train_iter))\n",
    "print(\"Size of premise batch:\", batch.premise.shape)\n",
    "print(\"Size of hypothesis batch:\", batch.hypothesis.shape)\n",
    "print(\"Size of label batch:\", batch.label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMS - input: 62998, embed: 300, hidden1: 200, output: 4\n"
     ]
    }
   ],
   "source": [
    "# dimensions\n",
    "input_size = TEXT.vocab.vectors.shape['word']\n",
    "embed_size = TEXT.vocab.vectors.shape['embedding']\n",
    "hidden_size1 = 200\n",
    "output_size = len(LABEL.vocab)\n",
    "print('DIMS - input: %d, embed: %d, hidden1: %d, output: %d'%(input_size, embed_size, hidden_size1, output_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Variable Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Network(torch.nn.Module):\n",
    "    def __init__(self, embed_size, output_size, weights, networks):\n",
    "        super(Q_Network, self).__init__()\n",
    "        self.embed_size = embed_size # HIDDEN = embed_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.embed = nn.Embedding.from_pretrained(weights, freeze=True)\n",
    "        self.linear = nn.Linear(self.embed_size * 2, self.output_size)\n",
    "        \n",
    "        self.lmb = self.linear.weight\n",
    "        self.m = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self.criterion = nn.NLLLoss()\n",
    "        K = len(networks)\n",
    "        probs = torch.repeat(torch.tensor(1/K), K)\n",
    "        self.prior = torch.distributions.categorical.Categorical(probs=probs)\n",
    "        \n",
    "    def mask(self, sent1, sent2, proj1, proj2, pad_tkn=1):\n",
    "        mask1 = (sent1 == pad_tkn) # BATCH x SEQLEN\n",
    "        mask2 = (sent2 == pad_tkn)\n",
    "        mask1a = mask1.unsqueeze(2).expand(-1, -1, self.hidden_size).float() # BATCH x SEQLEN x HIDDEN\n",
    "        mask2a = mask2.unsqueeze(2).expand(-1, -1, self.hidden_size).float()\n",
    "        score1 = proj1 * (1 - mask1a) # BATCH x SEQLEN x HIDDEN\n",
    "        score2 = proj2 * (1 - mask2a)\n",
    "        return score1, score2\n",
    "    \n",
    "    def forward(self, sent1, sent2, pad_tkn = 1):\n",
    "        proj1 = self.embed(sent1) # BATCH x SEQLEN x HIDDEN\n",
    "        proj2 = self.embed(sent2)\n",
    "        score1, score2 = self.mask(sent1, sent2, proj1, proj2) # BATCH x SEQLEN x HIDDEN\n",
    "        score1_sum = torch.sum(score1, dim=1) # BATCH x HIDDEN\n",
    "        score2_sum = torch.sum(score2, dim=1)\n",
    "        score_all = torch.cat((score1_sum, score2_sum), dim=1) # BATCH x HIDDEN*2\n",
    "        output = self.m(self.linear(score_all)) # BATCH x OUTPUT\n",
    "        self.q = torch.distributions.categorical.Categorical(logits=output)\n",
    "        return output\n",
    "    \n",
    "    def basic_grad(self, sent1, sent2, y, N=1):\n",
    "        ELBO = torch.zeros(1)\n",
    "        lmb_out = torch.zeros((self.lmb.shape, N))\n",
    "        for i in range(N):\n",
    "            try:\n",
    "                self.lmb.grad.data.zero_()\n",
    "            except AttributeError:\n",
    "                pass\n",
    "            # sample, forward pass\n",
    "            q = self.q\n",
    "            c = q.sample()\n",
    "            net = networks[c]\n",
    "            y_hat = net(sent1, sent2)\n",
    "            # calc ELBO, dELBO for the sample\n",
    "            loss = q.log_prob(c).sum()\n",
    "            loss.backward()\n",
    "            xent = criterion(y_hat, y)\n",
    "            ELBO_ = self.prior.log_prob(c).sum() - xent - loss\n",
    "            ELBO.add_(ELBO_)\n",
    "            lmb_out[:,i] = self.lmb.grad.data.mul(ELBO_)\n",
    "        ELBO.div_(N)\n",
    "        self.lmb.grad.data = lmb_out.mean(dim=-1)\n",
    "        return ELBO.sum(), lmb_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(e, train_iter, test_net, eta = 1e-5):\n",
    "    for ix,batch in enumerate(train_iter):\n",
    "        sent1 = prepend_null(batch.premise.values.transpose(0,1))\n",
    "        sent2 = prepend_null(batch.hypothesis.values.transpose(0,1))\n",
    "        target = batch.label.values\n",
    "        _ = test_net(sent1, sent2) # forward pass\n",
    "        ELBO, lmb_out = test_net.get_grad(sent1, sent2, target, N=200) # get grad\n",
    "        test_net.lmb.data = test_net.lmb.data + (eta * test_net.lmb.grad.data) # step\n",
    "        \n",
    "        if ix % 1000 == 0:\n",
    "            print('Epoch: {0}, Batch: {1}, ELBO: {2:0.4f}, Grad: {3:0.4f}'.format(e, ix, ELBO.data, test_net.lmb.grad.data.abs().mean())\n",
    "\n",
    "    return ELBO.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def validation_loop(e, val_iter, networks, criterion):\n",
    "    K = len(networks)\n",
    "    for network in networks:\n",
    "        network.eval()\n",
    "    total_loss = 0\n",
    "    total_sent = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    for ix,batch in enumerate(val_iter):\n",
    "        sent1 = prepend_null(batch.premise.values.transpose(0,1))\n",
    "        sent2 = prepend_null(batch.hypothesis.values.transpose(0,1))\n",
    "        target = batch.label.values\n",
    "        \n",
    "        output = torch.zeros(output_size)\n",
    "        for c in range(K):\n",
    "            network = networks[c]\n",
    "            output += F.softmax(network(sent1, sent2), dim=1) # BATCH x OUTPUT_SIZE\n",
    "        output = torch.log(output) + torch.log(torch.tensor(1/K))\n",
    "        \n",
    "        loss = criterion(output, target).item()\n",
    "        sent = sent1.shape[0]\n",
    "        correct = torch.sum(torch.argmax(output, dim=1) == target).item()\n",
    "        \n",
    "        total_loss += loss*sent\n",
    "        total_sent += sent\n",
    "        total_correct += correct\n",
    "    \n",
    "    print('Epoch: {0}, Val NLL: {1:0.4f}, Val Acc: {2:0.4f}'.format(e, total_loss/total_sent, total_correct/total_sent))\n",
    "    return total_loss'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFA_net1 = Decomposable_Attn_Network(input_size, embed_size, hidden_size1, output_size, weights).cuda()\n",
    "state_dict = torch.load('best_FFA_net1.pt')\n",
    "FFA_net1.load_state_dict(state_dict)\n",
    "\n",
    "FFA_net2 = Decomposable_Attn_Network(input_size, embed_size, hidden_size1, output_size, weights).cuda()\n",
    "state_dict = torch.load('best_FFA_net2.pt')\n",
    "FFA_net2.load_state_dict(state_dict)\n",
    "\n",
    "FFA_net3 = Decomposable_Attn_Network(input_size, embed_size, hidden_size1, output_size, weights).cuda()\n",
    "state_dict = torch.load('best_FFA_net3.pt')\n",
    "FFA_net3.load_state_dict(state_dict)\n",
    "\n",
    "networks = [FFA_net1, FFA_net2, FFA_net3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_elbo = -1e32\n",
    "test_net = Q_Network(embed_size, output_size, weights, networks)\n",
    "\n",
    "for e in range(100):\n",
    "    elbo = training_loop(e, train_iter, test_net)\n",
    "    if elbo > best_elbo:\n",
    "        torch.save(test_net.state_dict(), 'best_q.pt')\n",
    "        best_elbo = elbo\n",
    "        print('WROTE MODEL')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
