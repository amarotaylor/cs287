{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 287 - HW 4 - VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "from namedtensor import ntorch, NamedTensor\n",
    "from namedtensor.text import NamedField\n",
    "\n",
    "from common import *\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train) 549367\n",
      "len(TEXT.vocab) 62998\n",
      "len(LABEL.vocab) 4\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "TEXT = NamedField(names=('seqlen',)) # Our input $x$\n",
    "LABEL = NamedField(sequential=False, names=()) # Our labels $y$\n",
    "train, val, test = torchtext.datasets.SNLI.splits(TEXT, LABEL)\n",
    "print('len(train)', len(train))\n",
    "TEXT.build_vocab(train)\n",
    "LABEL.build_vocab(train)\n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
    "print('len(LABEL.vocab)', len(LABEL.vocab))\n",
    "train_iter, val_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
    "    (train, val, test), batch_size=32, device=torch.device(\"cuda\"), repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word embeddings shape: OrderedDict([('word', 62998), ('embedding', 300)])\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary with word embeddings\n",
    "# out-of-vocabulary words are hashed to one of 100 random embeddings each initialized to mean 0, stdev 1 (Sec 5.1)\n",
    "unk_vectors = [torch.randn(300) for _ in range(100)]\n",
    "TEXT.vocab.load_vectors(vectors='glove.6B.300d', unk_init=lambda x:random.choice(unk_vectors))\n",
    "vectors = TEXT.vocab.vectors\n",
    "vectors = vectors / vectors.norm(dim=1, keepdim=True) # normalized to have l_2 norm of 1\n",
    "vectors = NamedTensor(vectors, ('word', 'embedding'))\n",
    "TEXT.vocab.vectors = vectors\n",
    "print(\"word embeddings shape:\", TEXT.vocab.vectors.shape)\n",
    "weights = TEXT.vocab.vectors.values.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of premise batch: OrderedDict([('seqlen', 33), ('batch', 32)])\n",
      "Size of hypothesis batch: OrderedDict([('seqlen', 12), ('batch', 32)])\n",
      "Size of label batch: OrderedDict([('batch', 32)])\n"
     ]
    }
   ],
   "source": [
    "# here's an example of a training example\n",
    "batch = next(iter(train_iter))\n",
    "print(\"Size of premise batch:\", batch.premise.shape)\n",
    "print(\"Size of hypothesis batch:\", batch.hypothesis.shape)\n",
    "print(\"Size of label batch:\", batch.label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMS - input: 62998, embed: 300, hidden1: 200, output: 4\n"
     ]
    }
   ],
   "source": [
    "# dimensions\n",
    "input_size = TEXT.vocab.vectors.shape['word']\n",
    "embed_size = TEXT.vocab.vectors.shape['embedding']\n",
    "hidden_size1 = 200\n",
    "output_size = len(LABEL.vocab)\n",
    "print('DIMS - input: %d, embed: %d, hidden1: %d, output: %d'%(input_size, embed_size, hidden_size1, output_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Variable Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Network(torch.nn.Module):\n",
    "    def __init__(self, embed_size, output_size, weights, networks):\n",
    "        super(Q_Network, self).__init__()\n",
    "        self.embed_size = embed_size # HIDDEN = embed_size\n",
    "        self.K = len(networks)\n",
    "        self.output_size = output_size\n",
    "        self.weights = weights\n",
    "        self.networks = networks\n",
    "        \n",
    "        self.embed = nn.Embedding.from_pretrained(self.weights, freeze=True)\n",
    "        self.linear = nn.Linear(self.embed_size * 2, self.K, bias=False)\n",
    "        \n",
    "        self.lmb = self.linear.weight\n",
    "        self.m = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self.criterion = nn.NLLLoss(reduction='sum')\n",
    "        self.probs = torch.tensor(1/self.K, device='cuda').repeat(self.K)\n",
    "        self.prior = torch.distributions.categorical.Categorical(probs=self.probs)\n",
    "        \n",
    "    def mask(self, sent1, sent2, proj1, proj2, pad_tkn=1):\n",
    "        mask1 = (sent1 == pad_tkn) # BATCH x SEQLEN\n",
    "        mask2 = (sent2 == pad_tkn)\n",
    "        mask1a = mask1.unsqueeze(2).expand(-1, -1, self.embed_size).float() # BATCH x SEQLEN x HIDDEN\n",
    "        mask2a = mask2.unsqueeze(2).expand(-1, -1, self.embed_size).float()\n",
    "        score1 = proj1 * (1 - mask1a) # BATCH x SEQLEN x HIDDEN\n",
    "        score2 = proj2 * (1 - mask2a)\n",
    "        return score1, score2\n",
    "    \n",
    "    def forward(self, sent1, sent2, pad_tkn = 1):\n",
    "        proj1 = self.embed(sent1) # BATCH x SEQLEN x HIDDEN\n",
    "        proj2 = self.embed(sent2)\n",
    "        score1, score2 = self.mask(sent1, sent2, proj1, proj2) # BATCH x SEQLEN x HIDDEN\n",
    "        score1_sum = torch.sum(score1, dim=1) # BATCH x HIDDEN\n",
    "        score2_sum = torch.sum(score2, dim=1)\n",
    "        score_all = torch.cat((score1_sum, score2_sum), dim=1) # BATCH x HIDDEN*2\n",
    "        output = self.m(self.linear(score_all)) # BATCH x K\n",
    "        self.output = output\n",
    "        self.q = torch.distributions.categorical.Categorical(logits=output)\n",
    "        return output\n",
    "    \n",
    "    def run_networks(self, sent1, sent2):\n",
    "        y_hats = torch.zeros((self.K, sent1.shape[0], self.output_size), device='cuda')\n",
    "        for c in range(self.K):\n",
    "            net = self.networks[c]\n",
    "            y_hat = net(sent1, sent2)\n",
    "            y_hats[c,:,:] = y_hat\n",
    "        return y_hats\n",
    "    \n",
    "    def get_grad(self, sent1, sent2, y, N=1):\n",
    "        ELBO = torch.zeros(1)\n",
    "        lmb_out = torch.zeros((self.K, self.embed_size * 2, N))\n",
    "        y_hats = self.run_networks(sent1, sent2)\n",
    "        q = self.q\n",
    "        for i in range(N):      \n",
    "            print(i)\n",
    "            # zero grads\n",
    "            try:\n",
    "                self.lmb.grad.data.zero_()\n",
    "            except AttributeError:\n",
    "                pass\n",
    "            # sample, forward pass\n",
    "            c = q.sample()\n",
    "            y_hat = torch.stack([torch.index_select(y_hats[:,i,:],0,c[i]) for i in range(sent1.shape[0])]).squeeze()\n",
    "            # calc ELBO, dELBO for the sample\n",
    "            loss = self.criterion(self.output, c)#q.log_prob(c).sum()\n",
    "            loss.backward()\n",
    "            nll = self.criterion(y_hat, y)\n",
    "            priorlp = self.prior.log_prob(c).sum()\n",
    "            ELBO_ = priorlp - nll - loss\n",
    "            ELBO.add_(ELBO_)\n",
    "            lmb_out[:,:,i] = self.lmb.grad.data.mul(ELBO_)\n",
    "        ELBO.div_(N)\n",
    "        self.lmb.grad.data = lmb_out.mean(dim=-1)\n",
    "        return ELBO.sum(), lmb_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(e, train_iter, test_net, eta = 1e-5):\n",
    "    for ix,batch in enumerate(train_iter):\n",
    "        sent1 = prepend_null(batch.premise.values.transpose(0,1))\n",
    "        sent2 = prepend_null(batch.hypothesis.values.transpose(0,1))\n",
    "        target = batch.label.values\n",
    "        _ = test_net(sent1, sent2) # forward pass\n",
    "        ELBO, lmb_out = test_net.get_grad(sent1, sent2, target, N=200) # get grad\n",
    "        test_net.lmb.data = test_net.lmb.data + (eta * test_net.lmb.grad.data) # step\n",
    "        \n",
    "        if ix % 1000 == 0:\n",
    "            print('Epoch: {0}, Batch: {1}, ELBO: {2:0.4f}, Grad: {3:0.4f}'.format(e, ix, ELBO.data, test_net.lmb.grad.data.abs().mean()))\n",
    "\n",
    "    return ELBO.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def validation_loop(e, val_iter, networks, criterion):\\n    K = len(networks)\\n    for network in networks:\\n        network.eval()\\n    total_loss = 0\\n    total_sent = 0\\n    total_correct = 0\\n    \\n    for ix,batch in enumerate(val_iter):\\n        sent1 = prepend_null(batch.premise.values.transpose(0,1))\\n        sent2 = prepend_null(batch.hypothesis.values.transpose(0,1))\\n        target = batch.label.values\\n        \\n        output = torch.zeros(output_size)\\n        for c in range(K):\\n            network = networks[c]\\n            output += F.softmax(network(sent1, sent2), dim=1) # BATCH x OUTPUT_SIZE\\n        output = torch.log(output) + torch.log(torch.tensor(1/K))\\n        \\n        loss = criterion(output, target).item()\\n        sent = sent1.shape[0]\\n        correct = torch.sum(torch.argmax(output, dim=1) == target).item()\\n        \\n        total_loss += loss*sent\\n        total_sent += sent\\n        total_correct += correct\\n    \\n    print('Epoch: {0}, Val NLL: {1:0.4f}, Val Acc: {2:0.4f}'.format(e, total_loss/total_sent, total_correct/total_sent))\\n    return total_loss\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def validation_loop(e, val_iter, networks, criterion):\n",
    "    K = len(networks)\n",
    "    for network in networks:\n",
    "        network.eval()\n",
    "    total_loss = 0\n",
    "    total_sent = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    for ix,batch in enumerate(val_iter):\n",
    "        sent1 = prepend_null(batch.premise.values.transpose(0,1))\n",
    "        sent2 = prepend_null(batch.hypothesis.values.transpose(0,1))\n",
    "        target = batch.label.values\n",
    "        \n",
    "        output = torch.zeros(output_size)\n",
    "        for c in range(K):\n",
    "            network = networks[c]\n",
    "            output += F.softmax(network(sent1, sent2), dim=1) # BATCH x OUTPUT_SIZE\n",
    "        output = torch.log(output) + torch.log(torch.tensor(1/K))\n",
    "        \n",
    "        loss = criterion(output, target).item()\n",
    "        sent = sent1.shape[0]\n",
    "        correct = torch.sum(torch.argmax(output, dim=1) == target).item()\n",
    "        \n",
    "        total_loss += loss*sent\n",
    "        total_sent += sent\n",
    "        total_correct += correct\n",
    "    \n",
    "    print('Epoch: {0}, Val NLL: {1:0.4f}, Val Acc: {2:0.4f}'.format(e, total_loss/total_sent, total_correct/total_sent))\n",
    "    return total_loss'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFA_net1 = Decomposable_Attn_Network(input_size, embed_size, hidden_size1, output_size, weights).cuda()\n",
    "state_dict1 = torch.load('best_FFA_net0.pt')\n",
    "FFA_net1.load_state_dict(state_dict1)\n",
    "\n",
    "FFA_net2 = Decomposable_Attn_Network(input_size, embed_size, hidden_size1, output_size, weights).cuda()\n",
    "state_dict2 = torch.load('best_FFA_net1.pt')\n",
    "FFA_net2.load_state_dict(state_dict2)\n",
    "\n",
    "FFA_net3 = Decomposable_Attn_Network(input_size, embed_size, hidden_size1, output_size, weights).cuda()\n",
    "state_dict3 = torch.load('best_FFA_net2.pt')\n",
    "FFA_net3.load_state_dict(state_dict3)\n",
    "\n",
    "networks = [FFA_net1, FFA_net2, FFA_net3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-396d16876ea4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0melbo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0melbo\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_elbo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'best_q.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-111-c4439d25e714>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(e, train_iter, test_net, eta)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mELBO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmb_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# get grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mtest_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlmb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlmb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0meta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtest_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlmb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-110-bb9e1fa06dbd>\u001b[0m in \u001b[0;36mget_grad\u001b[0;34m(self, sent1, sent2, y, N)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# calc ELBO, dELBO for the sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#q.log_prob(c).sum()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0mnll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mpriorlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "best_elbo = -1e32\n",
    "K = len(networks)\n",
    "test_net = Q_Network(embed_size, output_size, weights, networks).cuda()\n",
    "\n",
    "for e in range(100):\n",
    "    elbo = training_loop(e, train_iter, test_net)\n",
    "    if elbo > best_elbo:\n",
    "        torch.save(test_net.state_dict(), 'best_q.pt')\n",
    "        best_elbo = elbo\n",
    "        print('WROTE MODEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torch.distributions.categorical.Categorical(probs=torch.tensor([[0.3,0.4,0.3],[0.3,0.4,0.3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampl = torch.tensor([1,2])\n",
    "lp = temp.log_prob(sampl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 600])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_net.lmb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
