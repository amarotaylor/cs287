{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "from namedtensor import ntorch, NamedTensor\n",
    "from namedtensor.text import NamedField\n",
    "from common import *\n",
    "\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train) 549367\n",
      "len(TEXT.vocab) 62998\n",
      "len(LABEL.vocab) 4\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "TEXT = NamedField(names=('seqlen',)) # Our input $x$\n",
    "LABEL = NamedField(sequential=False, names=()) # Our labels $y$\n",
    "train, val, test = torchtext.datasets.SNLI.splits(TEXT, LABEL)\n",
    "print('len(train)', len(train))\n",
    "TEXT.build_vocab(train)\n",
    "LABEL.build_vocab(train)\n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
    "print('len(LABEL.vocab)', len(LABEL.vocab))\n",
    "train_iter, val_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
    "    (train, val, test), batch_size=128, device=torch.device(\"cuda\"), repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word embeddings shape: OrderedDict([('word', 62998), ('embedding', 300)])\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary with word embeddings\n",
    "# out-of-vocabulary words are hashed to one of 100 random embeddings each initialized to mean 0, stdev 1 (Sec 5.1)\n",
    "unk_vectors = [torch.randn(300) for _ in range(100)]\n",
    "TEXT.vocab.load_vectors(vectors='glove.6B.300d', unk_init=lambda x:random.choice(unk_vectors))\n",
    "vectors = TEXT.vocab.vectors\n",
    "vectors = vectors / vectors.norm(dim=1, keepdim=True) # normalized to have l_2 norm of 1\n",
    "vectors = NamedTensor(vectors, ('word', 'embedding'))\n",
    "TEXT.vocab.vectors = vectors\n",
    "print(\"word embeddings shape:\", TEXT.vocab.vectors.shape)\n",
    "weights = TEXT.vocab.vectors.values.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMS - input: 62998, embed: 300, hidden1: 200, hidden2: 400, output: 4\n"
     ]
    }
   ],
   "source": [
    "# dimensions\n",
    "input_size = TEXT.vocab.vectors.shape['word']\n",
    "embed_size = TEXT.vocab.vectors.shape['embedding']\n",
    "hidden_size1 = 200\n",
    "hidden_size2 = hidden_size1 * 2\n",
    "output_size = len(LABEL.vocab)\n",
    "print('DIMS - input: %d, embed: %d, hidden1: %d, hidden2: %d, output: %d'%(input_size, embed_size, hidden_size1, hidden_size2, output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(e, train_iter, network, criterion, optimizer):\n",
    "    network.train()\n",
    "    for ix,batch in enumerate(train_iter):\n",
    "        optimizer.zero_grad()\n",
    "        sent1 = prepend_null(batch.premise.values.transpose(0,1))\n",
    "        sent2 = prepend_null(batch.hypothesis.values.transpose(0,1))\n",
    "        target = batch.label.values\n",
    "        output = network(sent1, sent2)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if ix % 1000 == 0:\n",
    "            acc = torch.sum(torch.argmax(output, dim=1) == target).item() / target.shape[0]\n",
    "            print('Epoch: {0}, Batch: {1}, Train NLL: {2:0.4f}, Train Acc:{3:0.4f}'.format(e, ix, loss.cpu().detach(), acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(e, val_iter, network, criterion):\n",
    "    network.train()\n",
    "    total_loss = 0\n",
    "    total_sent = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    for ix,batch in enumerate(val_iter):\n",
    "        sent1 = prepend_null(batch.premise.values.transpose(0,1))\n",
    "        sent2 = prepend_null(batch.hypothesis.values.transpose(0,1))\n",
    "        target = batch.label.values\n",
    "        output = network(sent1, sent2)\n",
    "        \n",
    "        loss = criterion(output, target).item()\n",
    "        sent = sent1.shape[0]\n",
    "        correct = torch.sum(torch.argmax(output, dim=1) == target).item()\n",
    "        \n",
    "        total_loss += loss*sent\n",
    "        total_sent += sent\n",
    "        total_correct += correct\n",
    "    \n",
    "    print('Epoch: {0}, Val NLL: {1:0.4f}, Val Acc: {2:0.4f}'.format(e, total_loss/total_sent, total_correct/total_sent))\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decomposable_Attn_Network(\n",
       "  (Embedding_layer): EmbedProject(\n",
       "    (embed): Embedding(62998, 300)\n",
       "    (linear): Linear(in_features=300, out_features=200, bias=True)\n",
       "  )\n",
       "  (F): FeedForward_layer(\n",
       "    (d): Dropout(p=0.2)\n",
       "    (m): ReLU()\n",
       "    (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
       "    (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
       "  )\n",
       "  (G): FeedForward_layer(\n",
       "    (d): Dropout(p=0.2)\n",
       "    (m): ReLU()\n",
       "    (linear1): Linear(in_features=400, out_features=200, bias=True)\n",
       "    (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
       "  )\n",
       "  (H): FeedForward_layer(\n",
       "    (d): Dropout(p=0.2)\n",
       "    (m): ReLU()\n",
       "    (linear1): Linear(in_features=400, out_features=200, bias=True)\n",
       "    (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
       "  )\n",
       "  (linear): Linear(in_features=200, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FFA_net = Decomposable_Attn_Network(input_size,embed_size,hidden_size1,output_size,weights)\n",
    "FFA_net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adagrad(FFA_net.parameters(), lr=0.05, initial_accumulator_value=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = 1e8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Train NLL: 0.6408, Train Acc:0.7422\n",
      "Epoch: 0, Batch: 1000, Train NLL: 0.6840, Train Acc:0.7500\n",
      "Epoch: 0, Batch: 2000, Train NLL: 0.5563, Train Acc:0.8203\n",
      "Epoch: 0, Batch: 3000, Train NLL: 0.6455, Train Acc:0.7109\n",
      "Epoch: 0, Batch: 4000, Train NLL: 0.5970, Train Acc:0.7344\n",
      "Epoch: 0, Val NLL: 0.6281, Val Acc: 0.7360\n",
      "LR = 0.0005000000000000001\n",
      "Epoch: 1, Batch: 0, Train NLL: 0.5822, Train Acc:0.7734\n",
      "Epoch: 1, Batch: 1000, Train NLL: 0.5847, Train Acc:0.7344\n",
      "Epoch: 1, Batch: 2000, Train NLL: 0.5953, Train Acc:0.7734\n",
      "Epoch: 1, Batch: 3000, Train NLL: 0.6449, Train Acc:0.7422\n",
      "Epoch: 1, Batch: 4000, Train NLL: 0.6546, Train Acc:0.7031\n",
      "Epoch: 1, Val NLL: 0.6229, Val Acc: 0.7382\n",
      "LR = 0.0005000000000000001\n",
      "WROTE MODEL\n",
      "Epoch: 2, Batch: 0, Train NLL: 0.6175, Train Acc:0.7109\n",
      "Epoch: 2, Batch: 1000, Train NLL: 0.7402, Train Acc:0.6406\n",
      "Epoch: 2, Batch: 2000, Train NLL: 0.7072, Train Acc:0.7266\n",
      "Epoch: 2, Batch: 3000, Train NLL: 0.5294, Train Acc:0.8125\n",
      "Epoch: 2, Batch: 4000, Train NLL: 0.5367, Train Acc:0.7891\n",
      "Epoch: 2, Val NLL: 0.6282, Val Acc: 0.7379\n",
      "LR = 0.0005000000000000001\n",
      "Epoch: 3, Batch: 0, Train NLL: 0.5965, Train Acc:0.7734\n",
      "Epoch: 3, Batch: 1000, Train NLL: 0.6737, Train Acc:0.6953\n",
      "Epoch: 3, Batch: 2000, Train NLL: 0.6434, Train Acc:0.7266\n",
      "Epoch: 3, Batch: 3000, Train NLL: 0.6476, Train Acc:0.7266\n",
      "Epoch: 3, Batch: 4000, Train NLL: 0.5289, Train Acc:0.7969\n",
      "Epoch: 3, Val NLL: 0.6349, Val Acc: 0.7355\n",
      "LR = 0.0005000000000000001\n",
      "Epoch: 4, Batch: 0, Train NLL: 0.6622, Train Acc:0.6953\n",
      "Epoch: 4, Batch: 1000, Train NLL: 0.5008, Train Acc:0.8047\n",
      "Epoch: 4, Batch: 2000, Train NLL: 0.5288, Train Acc:0.7969\n",
      "Epoch: 4, Batch: 3000, Train NLL: 0.4891, Train Acc:0.8125\n",
      "Epoch: 4, Batch: 4000, Train NLL: 0.5507, Train Acc:0.7578\n",
      "Epoch: 4, Val NLL: 0.6230, Val Acc: 0.7408\n",
      "LR = 0.0005000000000000001\n",
      "Epoch: 5, Batch: 0, Train NLL: 0.5997, Train Acc:0.7422\n",
      "Epoch: 5, Batch: 1000, Train NLL: 0.5596, Train Acc:0.7891\n",
      "Epoch: 5, Batch: 2000, Train NLL: 0.4683, Train Acc:0.8125\n",
      "Epoch: 5, Batch: 3000, Train NLL: 0.6786, Train Acc:0.7031\n",
      "Epoch: 5, Batch: 4000, Train NLL: 0.6538, Train Acc:0.7188\n",
      "Epoch: 5, Val NLL: 0.6275, Val Acc: 0.7388\n",
      "LR = 0.0005000000000000001\n",
      "Epoch: 6, Batch: 0, Train NLL: 0.5306, Train Acc:0.7344\n",
      "Epoch: 6, Batch: 1000, Train NLL: 0.5734, Train Acc:0.7891\n",
      "Epoch: 6, Batch: 2000, Train NLL: 0.6484, Train Acc:0.7422\n",
      "Epoch: 6, Batch: 3000, Train NLL: 0.5591, Train Acc:0.7500\n",
      "Epoch: 6, Batch: 4000, Train NLL: 0.4699, Train Acc:0.7969\n",
      "Epoch: 6, Val NLL: 0.6290, Val Acc: 0.7367\n",
      "LR = 5.0000000000000016e-05\n",
      "Epoch: 7, Batch: 0, Train NLL: 0.6627, Train Acc:0.7422\n",
      "Epoch: 7, Batch: 1000, Train NLL: 0.6358, Train Acc:0.7500\n",
      "Epoch: 7, Batch: 2000, Train NLL: 0.6039, Train Acc:0.7969\n",
      "Epoch: 7, Batch: 3000, Train NLL: 0.6110, Train Acc:0.7656\n",
      "Epoch: 7, Batch: 4000, Train NLL: 0.6788, Train Acc:0.7031\n",
      "Epoch: 7, Val NLL: 0.6322, Val Acc: 0.7374\n",
      "LR = 5.0000000000000016e-05\n",
      "Epoch: 8, Batch: 0, Train NLL: 0.5885, Train Acc:0.7188\n",
      "Epoch: 8, Batch: 1000, Train NLL: 0.5683, Train Acc:0.7500\n",
      "Epoch: 8, Batch: 2000, Train NLL: 0.5330, Train Acc:0.8047\n",
      "Epoch: 8, Batch: 3000, Train NLL: 0.5796, Train Acc:0.7734\n",
      "Epoch: 8, Batch: 4000, Train NLL: 0.6254, Train Acc:0.7266\n",
      "Epoch: 8, Val NLL: 0.6249, Val Acc: 0.7389\n",
      "LR = 5.0000000000000016e-05\n",
      "Epoch: 9, Batch: 0, Train NLL: 0.6971, Train Acc:0.6875\n",
      "Epoch: 9, Batch: 1000, Train NLL: 0.6391, Train Acc:0.7344\n",
      "Epoch: 9, Batch: 2000, Train NLL: 0.5775, Train Acc:0.8047\n",
      "Epoch: 9, Batch: 3000, Train NLL: 0.5324, Train Acc:0.7891\n",
      "Epoch: 9, Batch: 4000, Train NLL: 0.5582, Train Acc:0.8125\n",
      "Epoch: 9, Val NLL: 0.6288, Val Acc: 0.7379\n",
      "LR = 5.0000000000000016e-05\n",
      "Epoch: 10, Batch: 0, Train NLL: 0.5761, Train Acc:0.7578\n",
      "Epoch: 10, Batch: 1000, Train NLL: 0.5919, Train Acc:0.7656\n",
      "Epoch: 10, Batch: 2000, Train NLL: 0.5219, Train Acc:0.7812\n",
      "Epoch: 10, Batch: 3000, Train NLL: 0.6620, Train Acc:0.7109\n",
      "Epoch: 10, Batch: 4000, Train NLL: 0.5994, Train Acc:0.7812\n",
      "Epoch: 10, Val NLL: 0.6251, Val Acc: 0.7390\n",
      "LR = 5.0000000000000016e-05\n",
      "Epoch: 11, Batch: 0, Train NLL: 0.5228, Train Acc:0.7812\n",
      "Epoch: 11, Batch: 1000, Train NLL: 0.5309, Train Acc:0.7812\n",
      "Epoch: 11, Batch: 2000, Train NLL: 0.5348, Train Acc:0.8203\n",
      "Epoch: 11, Batch: 3000, Train NLL: 0.6988, Train Acc:0.7422\n",
      "Epoch: 11, Batch: 4000, Train NLL: 0.5475, Train Acc:0.8047\n",
      "Epoch: 11, Val NLL: 0.6280, Val Acc: 0.7361\n",
      "LR = 5.000000000000002e-06\n",
      "Epoch: 12, Batch: 0, Train NLL: 0.6429, Train Acc:0.7266\n",
      "Epoch: 12, Batch: 1000, Train NLL: 0.5456, Train Acc:0.7500\n",
      "Epoch: 12, Batch: 2000, Train NLL: 0.5914, Train Acc:0.7266\n",
      "Epoch: 12, Batch: 3000, Train NLL: 0.6479, Train Acc:0.7109\n",
      "Epoch: 12, Batch: 4000, Train NLL: 0.5654, Train Acc:0.7344\n",
      "Epoch: 12, Val NLL: 0.6312, Val Acc: 0.7369\n",
      "LR = 5.000000000000002e-06\n",
      "Epoch: 13, Batch: 0, Train NLL: 0.4851, Train Acc:0.8203\n",
      "Epoch: 13, Batch: 1000, Train NLL: 0.5771, Train Acc:0.7578\n",
      "Epoch: 13, Batch: 2000, Train NLL: 0.4844, Train Acc:0.8125\n",
      "Epoch: 13, Batch: 3000, Train NLL: 0.6241, Train Acc:0.7266\n",
      "Epoch: 13, Batch: 4000, Train NLL: 0.6780, Train Acc:0.7109\n",
      "Epoch: 13, Val NLL: 0.6227, Val Acc: 0.7351\n",
      "LR = 5.000000000000002e-06\n",
      "WROTE MODEL\n",
      "Epoch: 14, Batch: 0, Train NLL: 0.6090, Train Acc:0.7344\n",
      "Epoch: 14, Batch: 1000, Train NLL: 0.6229, Train Acc:0.7422\n",
      "Epoch: 14, Batch: 2000, Train NLL: 0.6196, Train Acc:0.7812\n",
      "Epoch: 14, Batch: 3000, Train NLL: 0.5159, Train Acc:0.7812\n",
      "Epoch: 14, Batch: 4000, Train NLL: 0.5935, Train Acc:0.7656\n",
      "Epoch: 14, Val NLL: 0.6282, Val Acc: 0.7352\n",
      "LR = 5.000000000000002e-06\n",
      "Epoch: 15, Batch: 0, Train NLL: 0.5590, Train Acc:0.7812\n",
      "Epoch: 15, Batch: 1000, Train NLL: 0.6891, Train Acc:0.7344\n",
      "Epoch: 15, Batch: 2000, Train NLL: 0.7019, Train Acc:0.6875\n",
      "Epoch: 15, Batch: 3000, Train NLL: 0.5189, Train Acc:0.7734\n",
      "Epoch: 15, Batch: 4000, Train NLL: 0.6224, Train Acc:0.7031\n",
      "Epoch: 15, Val NLL: 0.6258, Val Acc: 0.7376\n",
      "LR = 5.000000000000002e-06\n",
      "Epoch: 16, Batch: 0, Train NLL: 0.7214, Train Acc:0.6719\n",
      "Epoch: 16, Batch: 1000, Train NLL: 0.6077, Train Acc:0.7188\n",
      "Epoch: 16, Batch: 2000, Train NLL: 0.6396, Train Acc:0.7344\n",
      "Epoch: 16, Batch: 3000, Train NLL: 0.6642, Train Acc:0.7031\n",
      "Epoch: 16, Batch: 4000, Train NLL: 0.5916, Train Acc:0.7422\n",
      "Epoch: 16, Val NLL: 0.6244, Val Acc: 0.7397\n",
      "LR = 5.000000000000002e-06\n",
      "Epoch: 17, Batch: 0, Train NLL: 0.6798, Train Acc:0.7109\n",
      "Epoch: 17, Batch: 1000, Train NLL: 0.6573, Train Acc:0.6953\n",
      "Epoch: 17, Batch: 2000, Train NLL: 0.5620, Train Acc:0.7812\n",
      "Epoch: 17, Batch: 3000, Train NLL: 0.7376, Train Acc:0.6953\n",
      "Epoch: 17, Batch: 4000, Train NLL: 0.5754, Train Acc:0.7500\n",
      "Epoch: 17, Val NLL: 0.6258, Val Acc: 0.7402\n",
      "LR = 5.000000000000002e-06\n",
      "Epoch: 18, Batch: 0, Train NLL: 0.6392, Train Acc:0.7188\n",
      "Epoch: 18, Batch: 1000, Train NLL: 0.5944, Train Acc:0.7422\n",
      "Epoch: 18, Batch: 2000, Train NLL: 0.7136, Train Acc:0.6875\n",
      "Epoch: 18, Batch: 3000, Train NLL: 0.5769, Train Acc:0.7891\n",
      "Epoch: 18, Batch: 4000, Train NLL: 0.6366, Train Acc:0.7266\n",
      "Epoch: 18, Val NLL: 0.6253, Val Acc: 0.7357\n",
      "LR = 5.000000000000002e-07\n",
      "Epoch: 19, Batch: 0, Train NLL: 0.6281, Train Acc:0.6953\n",
      "Epoch: 19, Batch: 1000, Train NLL: 0.6071, Train Acc:0.7578\n",
      "Epoch: 19, Batch: 2000, Train NLL: 0.5910, Train Acc:0.7891\n",
      "Epoch: 19, Batch: 3000, Train NLL: 0.6294, Train Acc:0.7344\n",
      "Epoch: 19, Batch: 4000, Train NLL: 0.6289, Train Acc:0.7188\n",
      "Epoch: 19, Val NLL: 0.6236, Val Acc: 0.7398\n",
      "LR = 5.000000000000002e-07\n",
      "Epoch: 20, Batch: 0, Train NLL: 0.5418, Train Acc:0.7734\n",
      "Epoch: 20, Batch: 1000, Train NLL: 0.5156, Train Acc:0.7734\n",
      "Epoch: 20, Batch: 2000, Train NLL: 0.5938, Train Acc:0.7500\n",
      "Epoch: 20, Batch: 3000, Train NLL: 0.6742, Train Acc:0.7344\n",
      "Epoch: 20, Batch: 4000, Train NLL: 0.5575, Train Acc:0.8047\n",
      "Epoch: 20, Val NLL: 0.6241, Val Acc: 0.7424\n",
      "LR = 5.000000000000002e-07\n",
      "Epoch: 21, Batch: 0, Train NLL: 0.5457, Train Acc:0.7812\n",
      "Epoch: 21, Batch: 1000, Train NLL: 0.6957, Train Acc:0.7266\n",
      "Epoch: 21, Batch: 2000, Train NLL: 0.6757, Train Acc:0.6719\n",
      "Epoch: 21, Batch: 3000, Train NLL: 0.5045, Train Acc:0.8047\n",
      "Epoch: 21, Batch: 4000, Train NLL: 0.5361, Train Acc:0.7656\n",
      "Epoch: 21, Val NLL: 0.6254, Val Acc: 0.7427\n",
      "LR = 5.000000000000002e-07\n",
      "Epoch: 22, Batch: 0, Train NLL: 0.6345, Train Acc:0.7500\n",
      "Epoch: 22, Batch: 1000, Train NLL: 0.6638, Train Acc:0.6953\n",
      "Epoch: 22, Batch: 2000, Train NLL: 0.6578, Train Acc:0.7422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Batch: 3000, Train NLL: 0.6336, Train Acc:0.7344\n",
      "Epoch: 22, Batch: 4000, Train NLL: 0.4242, Train Acc:0.8281\n",
      "Epoch: 22, Val NLL: 0.6302, Val Acc: 0.7371\n",
      "LR = 5.000000000000002e-07\n",
      "Epoch: 23, Batch: 0, Train NLL: 0.6479, Train Acc:0.6953\n",
      "Epoch: 23, Batch: 1000, Train NLL: 0.5799, Train Acc:0.7578\n",
      "Epoch: 23, Batch: 2000, Train NLL: 0.6442, Train Acc:0.7500\n",
      "Epoch: 23, Batch: 3000, Train NLL: 0.7112, Train Acc:0.7031\n",
      "Epoch: 23, Batch: 4000, Train NLL: 0.6425, Train Acc:0.7031\n",
      "Epoch: 23, Val NLL: 0.6275, Val Acc: 0.7396\n",
      "LR = 5.0000000000000024e-08\n",
      "Epoch: 24, Batch: 0, Train NLL: 0.6747, Train Acc:0.6484\n",
      "Epoch: 24, Batch: 1000, Train NLL: 0.6049, Train Acc:0.7578\n",
      "Epoch: 24, Batch: 2000, Train NLL: 0.6387, Train Acc:0.7266\n",
      "Epoch: 24, Batch: 3000, Train NLL: 0.6466, Train Acc:0.7578\n",
      "Epoch: 24, Batch: 4000, Train NLL: 0.6205, Train Acc:0.7031\n",
      "Epoch: 24, Val NLL: 0.6269, Val Acc: 0.7359\n",
      "LR = 5.0000000000000024e-08\n",
      "Epoch: 25, Batch: 0, Train NLL: 0.6278, Train Acc:0.7656\n",
      "Epoch: 25, Batch: 1000, Train NLL: 0.5903, Train Acc:0.7344\n",
      "Epoch: 25, Batch: 2000, Train NLL: 0.5745, Train Acc:0.7812\n",
      "Epoch: 25, Batch: 3000, Train NLL: 0.7322, Train Acc:0.7188\n",
      "Epoch: 25, Batch: 4000, Train NLL: 0.5809, Train Acc:0.8125\n",
      "Epoch: 25, Val NLL: 0.6280, Val Acc: 0.7411\n",
      "LR = 5.0000000000000024e-08\n",
      "Epoch: 26, Batch: 0, Train NLL: 0.6374, Train Acc:0.7578\n",
      "Epoch: 26, Batch: 1000, Train NLL: 0.6005, Train Acc:0.7656\n",
      "Epoch: 26, Batch: 2000, Train NLL: 0.6492, Train Acc:0.7266\n",
      "Epoch: 26, Batch: 3000, Train NLL: 0.6247, Train Acc:0.7188\n",
      "Epoch: 26, Batch: 4000, Train NLL: 0.6213, Train Acc:0.7734\n",
      "Epoch: 26, Val NLL: 0.6238, Val Acc: 0.7387\n",
      "LR = 5.0000000000000024e-08\n",
      "Epoch: 27, Batch: 0, Train NLL: 0.5558, Train Acc:0.7578\n",
      "Epoch: 27, Batch: 1000, Train NLL: 0.5902, Train Acc:0.7656\n",
      "Epoch: 27, Batch: 2000, Train NLL: 0.5237, Train Acc:0.7969\n",
      "Epoch: 27, Batch: 3000, Train NLL: 0.5844, Train Acc:0.7109\n",
      "Epoch: 27, Batch: 4000, Train NLL: 0.6101, Train Acc:0.7500\n",
      "Epoch: 27, Val NLL: 0.6286, Val Acc: 0.7413\n",
      "LR = 5.0000000000000024e-08\n",
      "Epoch: 28, Batch: 0, Train NLL: 0.6270, Train Acc:0.7188\n",
      "Epoch: 28, Batch: 1000, Train NLL: 0.6449, Train Acc:0.7109\n",
      "Epoch: 28, Batch: 2000, Train NLL: 0.7750, Train Acc:0.6172\n",
      "Epoch: 28, Batch: 3000, Train NLL: 0.5879, Train Acc:0.7578\n",
      "Epoch: 28, Batch: 4000, Train NLL: 0.5956, Train Acc:0.7500\n",
      "Epoch: 28, Val NLL: 0.6336, Val Acc: 0.7313\n",
      "LR = 5.0000000000000026e-09\n",
      "Epoch: 29, Batch: 0, Train NLL: 0.6517, Train Acc:0.7344\n",
      "Epoch: 29, Batch: 1000, Train NLL: 0.6124, Train Acc:0.7188\n",
      "Epoch: 29, Batch: 2000, Train NLL: 0.6859, Train Acc:0.7031\n",
      "Epoch: 29, Batch: 3000, Train NLL: 0.6393, Train Acc:0.7500\n",
      "Epoch: 29, Batch: 4000, Train NLL: 0.5761, Train Acc:0.7422\n",
      "Epoch: 29, Val NLL: 0.6213, Val Acc: 0.7384\n",
      "LR = 5.0000000000000026e-09\n",
      "WROTE MODEL\n",
      "Epoch: 30, Batch: 0, Train NLL: 0.6474, Train Acc:0.7500\n",
      "Epoch: 30, Batch: 1000, Train NLL: 0.6426, Train Acc:0.7344\n",
      "Epoch: 30, Batch: 2000, Train NLL: 0.6113, Train Acc:0.7266\n",
      "Epoch: 30, Batch: 3000, Train NLL: 0.6223, Train Acc:0.7188\n",
      "Epoch: 30, Batch: 4000, Train NLL: 0.5437, Train Acc:0.7891\n",
      "Epoch: 30, Val NLL: 0.6260, Val Acc: 0.7387\n",
      "LR = 5.0000000000000026e-09\n",
      "Epoch: 31, Batch: 0, Train NLL: 0.5841, Train Acc:0.7344\n",
      "Epoch: 31, Batch: 1000, Train NLL: 0.5422, Train Acc:0.7500\n",
      "Epoch: 31, Batch: 2000, Train NLL: 0.7437, Train Acc:0.6797\n",
      "Epoch: 31, Batch: 3000, Train NLL: 0.6818, Train Acc:0.7344\n",
      "Epoch: 31, Batch: 4000, Train NLL: 0.6537, Train Acc:0.7031\n",
      "Epoch: 31, Val NLL: 0.6277, Val Acc: 0.7382\n",
      "LR = 5.0000000000000026e-09\n",
      "Epoch: 32, Batch: 0, Train NLL: 0.4427, Train Acc:0.8516\n",
      "Epoch: 32, Batch: 1000, Train NLL: 0.5635, Train Acc:0.8203\n",
      "Epoch: 32, Batch: 2000, Train NLL: 0.5989, Train Acc:0.7422\n",
      "Epoch: 32, Batch: 3000, Train NLL: 0.6793, Train Acc:0.7188\n",
      "Epoch: 32, Batch: 4000, Train NLL: 0.6066, Train Acc:0.7188\n",
      "Epoch: 32, Val NLL: 0.6223, Val Acc: 0.7451\n",
      "LR = 5.0000000000000026e-09\n",
      "Epoch: 33, Batch: 0, Train NLL: 0.5125, Train Acc:0.8047\n",
      "Epoch: 33, Batch: 1000, Train NLL: 0.6336, Train Acc:0.7578\n",
      "Epoch: 33, Batch: 2000, Train NLL: 0.6213, Train Acc:0.7500\n",
      "Epoch: 33, Batch: 3000, Train NLL: 0.6072, Train Acc:0.7656\n",
      "Epoch: 33, Batch: 4000, Train NLL: 0.5624, Train Acc:0.7969\n",
      "Epoch: 33, Val NLL: 0.6269, Val Acc: 0.7392\n",
      "LR = 5.0000000000000026e-09\n",
      "Epoch: 34, Batch: 0, Train NLL: 0.6341, Train Acc:0.7891\n",
      "Epoch: 34, Batch: 1000, Train NLL: 0.3810, Train Acc:0.8516\n",
      "Epoch: 34, Batch: 2000, Train NLL: 0.5784, Train Acc:0.7656\n",
      "Epoch: 34, Batch: 3000, Train NLL: 0.6079, Train Acc:0.7891\n",
      "Epoch: 34, Batch: 4000, Train NLL: 0.6186, Train Acc:0.7656\n",
      "Epoch: 34, Val NLL: 0.6257, Val Acc: 0.7375\n",
      "LR = 5.0000000000000026e-09\n",
      "Epoch: 35, Batch: 0, Train NLL: 0.5850, Train Acc:0.7500\n",
      "Epoch: 35, Batch: 1000, Train NLL: 0.6319, Train Acc:0.7656\n",
      "Epoch: 35, Batch: 2000, Train NLL: 0.6395, Train Acc:0.7109\n",
      "Epoch: 35, Batch: 3000, Train NLL: 0.6383, Train Acc:0.7031\n",
      "Epoch: 35, Batch: 4000, Train NLL: 0.6706, Train Acc:0.6953\n",
      "Epoch: 35, Val NLL: 0.6238, Val Acc: 0.7395\n",
      "LR = 5.0000000000000026e-09\n",
      "Epoch: 36, Batch: 0, Train NLL: 0.5737, Train Acc:0.7734\n",
      "Epoch: 36, Batch: 1000, Train NLL: 0.7977, Train Acc:0.6719\n",
      "Epoch: 36, Batch: 2000, Train NLL: 0.6874, Train Acc:0.7109\n",
      "Epoch: 36, Batch: 3000, Train NLL: 0.6149, Train Acc:0.7656\n",
      "Epoch: 36, Batch: 4000, Train NLL: 0.5545, Train Acc:0.7891\n",
      "Epoch: 36, Val NLL: 0.6309, Val Acc: 0.7353\n",
      "LR = 5.0000000000000026e-09\n",
      "Epoch: 37, Batch: 0, Train NLL: 0.4461, Train Acc:0.8516\n",
      "Epoch: 37, Batch: 1000, Train NLL: 0.5785, Train Acc:0.7422\n",
      "Epoch: 37, Batch: 2000, Train NLL: 0.7008, Train Acc:0.6875\n",
      "Epoch: 37, Batch: 3000, Train NLL: 0.5924, Train Acc:0.7266\n",
      "Epoch: 37, Batch: 4000, Train NLL: 0.5801, Train Acc:0.8047\n",
      "Epoch: 37, Val NLL: 0.6272, Val Acc: 0.7377\n",
      "LR = 5.0000000000000026e-09\n",
      "Epoch: 38, Batch: 0, Train NLL: 0.7069, Train Acc:0.7188\n",
      "Epoch: 38, Batch: 1000, Train NLL: 0.5629, Train Acc:0.7734\n",
      "Epoch: 38, Batch: 2000, Train NLL: 0.5957, Train Acc:0.7500\n",
      "Epoch: 38, Batch: 3000, Train NLL: 0.6334, Train Acc:0.6953\n",
      "Epoch: 38, Batch: 4000, Train NLL: 0.7051, Train Acc:0.7188\n",
      "Epoch: 38, Val NLL: 0.6274, Val Acc: 0.7367\n",
      "LR = 5.0000000000000026e-09\n",
      "Epoch: 39, Batch: 0, Train NLL: 0.6627, Train Acc:0.6797\n",
      "Epoch: 39, Batch: 1000, Train NLL: 0.6406, Train Acc:0.7188\n",
      "Epoch: 39, Batch: 2000, Train NLL: 0.4510, Train Acc:0.8438\n",
      "Epoch: 39, Batch: 3000, Train NLL: 0.6388, Train Acc:0.7500\n",
      "Epoch: 39, Batch: 4000, Train NLL: 0.6143, Train Acc:0.7109\n",
      "Epoch: 39, Val NLL: 0.6273, Val Acc: 0.7434\n",
      "LR = 5.0000000000000026e-09\n",
      "Epoch: 40, Batch: 0, Train NLL: 0.6483, Train Acc:0.7422\n",
      "Epoch: 40, Batch: 1000, Train NLL: 0.5761, Train Acc:0.7734\n",
      "Epoch: 40, Batch: 2000, Train NLL: 0.7114, Train Acc:0.7031\n",
      "Epoch: 40, Batch: 3000, Train NLL: 0.5899, Train Acc:0.7500\n",
      "Epoch: 40, Batch: 4000, Train NLL: 0.6327, Train Acc:0.7422\n",
      "Epoch: 40, Val NLL: 0.6297, Val Acc: 0.7382\n",
      "LR = 5.0000000000000026e-09\n",
      "Epoch: 41, Batch: 0, Train NLL: 0.5190, Train Acc:0.7969\n",
      "Epoch: 41, Batch: 1000, Train NLL: 0.6625, Train Acc:0.6719\n",
      "Epoch: 41, Batch: 2000, Train NLL: 0.7180, Train Acc:0.6797\n",
      "Epoch: 41, Batch: 3000, Train NLL: 0.7535, Train Acc:0.6250\n",
      "Epoch: 41, Batch: 4000, Train NLL: 0.6882, Train Acc:0.7109\n",
      "Epoch: 41, Val NLL: 0.6229, Val Acc: 0.7390\n",
      "LR = 5.0000000000000026e-09\n",
      "Epoch: 42, Batch: 0, Train NLL: 0.4999, Train Acc:0.7969\n",
      "Epoch: 42, Batch: 1000, Train NLL: 0.6357, Train Acc:0.7422\n",
      "Epoch: 42, Batch: 2000, Train NLL: 0.6709, Train Acc:0.7344\n",
      "Epoch: 42, Batch: 3000, Train NLL: 0.5705, Train Acc:0.7109\n",
      "Epoch: 42, Batch: 4000, Train NLL: 0.6655, Train Acc:0.6719\n",
      "Epoch: 42, Val NLL: 0.6263, Val Acc: 0.7441\n",
      "LR = 5.0000000000000026e-09\n",
      "Epoch: 43, Batch: 0, Train NLL: 0.5186, Train Acc:0.8125\n",
      "Epoch: 43, Batch: 1000, Train NLL: 0.6089, Train Acc:0.7266\n",
      "Epoch: 43, Batch: 2000, Train NLL: 0.4865, Train Acc:0.7969\n",
      "Epoch: 43, Batch: 3000, Train NLL: 0.4737, Train Acc:0.8203\n",
      "Epoch: 43, Batch: 4000, Train NLL: 0.7499, Train Acc:0.6406\n",
      "Epoch: 43, Val NLL: 0.6245, Val Acc: 0.7379\n",
      "LR = 5.0000000000000026e-09\n",
      "Epoch: 44, Batch: 0, Train NLL: 0.6748, Train Acc:0.7422\n",
      "Epoch: 44, Batch: 1000, Train NLL: 0.5531, Train Acc:0.7578\n",
      "Epoch: 44, Batch: 2000, Train NLL: 0.4788, Train Acc:0.8359\n",
      "Epoch: 44, Batch: 3000, Train NLL: 0.4945, Train Acc:0.8047\n",
      "Epoch: 44, Batch: 4000, Train NLL: 0.5354, Train Acc:0.7578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44, Val NLL: 0.6296, Val Acc: 0.7415\n",
      "LR = 5.0000000000000026e-09\n",
      "Epoch: 45, Batch: 0, Train NLL: 0.4848, Train Acc:0.7891\n",
      "Epoch: 45, Batch: 1000, Train NLL: 0.5683, Train Acc:0.7578\n",
      "Epoch: 45, Batch: 2000, Train NLL: 0.6035, Train Acc:0.7344\n",
      "Epoch: 45, Batch: 3000, Train NLL: 0.5892, Train Acc:0.7891\n",
      "Epoch: 45, Batch: 4000, Train NLL: 0.7232, Train Acc:0.6953\n",
      "Epoch: 45, Val NLL: 0.6269, Val Acc: 0.7374\n",
      "LR = 5.0000000000000026e-09\n",
      "Epoch: 46, Batch: 0, Train NLL: 0.6350, Train Acc:0.7188\n",
      "Epoch: 46, Batch: 1000, Train NLL: 0.5315, Train Acc:0.7812\n",
      "Epoch: 46, Batch: 2000, Train NLL: 0.6376, Train Acc:0.7109\n",
      "Epoch: 46, Batch: 3000, Train NLL: 0.6119, Train Acc:0.7500\n",
      "Epoch: 46, Batch: 4000, Train NLL: 0.6238, Train Acc:0.7422\n",
      "Epoch: 46, Val NLL: 0.6237, Val Acc: 0.7451\n",
      "LR = 5.0000000000000026e-09\n",
      "Epoch: 47, Batch: 0, Train NLL: 0.6050, Train Acc:0.7031\n",
      "Epoch: 47, Batch: 1000, Train NLL: 0.6534, Train Acc:0.7109\n",
      "Epoch: 47, Batch: 2000, Train NLL: 0.5962, Train Acc:0.7500\n",
      "Epoch: 47, Batch: 3000, Train NLL: 0.7095, Train Acc:0.7109\n",
      "Epoch: 47, Batch: 4000, Train NLL: 0.6437, Train Acc:0.6953\n",
      "Epoch: 47, Val NLL: 0.6270, Val Acc: 0.7354\n",
      "LR = 5.0000000000000026e-09\n",
      "Epoch: 48, Batch: 0, Train NLL: 0.6824, Train Acc:0.7031\n",
      "Epoch: 48, Batch: 1000, Train NLL: 0.5152, Train Acc:0.7734\n",
      "Epoch: 48, Batch: 2000, Train NLL: 0.6668, Train Acc:0.6719\n",
      "Epoch: 48, Batch: 3000, Train NLL: 0.5664, Train Acc:0.7891\n",
      "Epoch: 48, Batch: 4000, Train NLL: 0.6320, Train Acc:0.7188\n",
      "Epoch: 48, Val NLL: 0.6282, Val Acc: 0.7366\n",
      "LR = 5.0000000000000026e-09\n",
      "Epoch: 49, Batch: 0, Train NLL: 0.6666, Train Acc:0.6953\n",
      "Epoch: 49, Batch: 1000, Train NLL: 0.5304, Train Acc:0.8125\n",
      "Epoch: 49, Batch: 2000, Train NLL: 0.6058, Train Acc:0.7656\n",
      "Epoch: 49, Batch: 3000, Train NLL: 0.6918, Train Acc:0.6953\n",
      "Epoch: 49, Batch: 4000, Train NLL: 0.5656, Train Acc:0.7578\n",
      "Epoch: 49, Val NLL: 0.6241, Val Acc: 0.7362\n",
      "LR = 5.0000000000000026e-09\n"
     ]
    }
   ],
   "source": [
    "for e in range(50):\n",
    "    training_loop(e, train_iter, FFA_net, criterion, optimizer)\n",
    "    loss = validation_loop(e, val_iter,FFA_net, criterion)\n",
    "    scheduler.step(loss)\n",
    "    print('LR = {}'.format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "    if loss < best_loss:\n",
    "        torch.save(FFA_net.state_dict(),'best_FFA_net.pt')\n",
    "        best_loss = loss\n",
    "        print('WROTE MODEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
